| ID | Title | Authors | Summary (zh) | Abstract (en) | Abstract (zh) | 
| --- | --- | --- | --- | --- | --- |
2501.05441	 | The GAN is dead; long live the GAN! A Modern GAN Baseline	 | Yiwen Huang,Aaron Gokaslan,Volodymyr Kuleshov,James Tompkin	 | 该研究反驳了GAN难以训练的说法，提出了一个原则性的现代GAN基线——R3GAN，通过改进的正则化相对GAN损失和简化架构，R3GAN在多个数据集上超过了StyleGAN2，并表现出色。	 | There is a widely-spread claim that GANs are difficult to train, and GAN architectures in the literature are littered with empirical tricks. We provide evidence against this claim and build a modern GAN baseline in a more principled manner. First, we derive a well-behaved regularized relativistic GAN loss that addresses issues of mode dropping and non-convergence that were previously tackled via a bag of ad-hoc tricks. We analyze our loss mathematically and prove that it admits local convergence guarantees, unlike most existing relativistic losses. Second, our new loss allows us to discard all ad-hoc tricks and replace outdated backbones used in common GANs with modern architectures. Using StyleGAN2 as an example, we present a roadmap of simplification and modernization that results in a new minimalist baseline -- R3GAN. Despite being simple, our approach surpasses StyleGAN2 on FFHQ, ImageNet, CIFAR, and Stacked MNIST datasets, and compares favorably against state-of-the-art GANs and diffusion models.	 | 关于生成对抗网络（GANs）难以训练的说法在广泛传播，文献中的GAN架构中充斥着各种经验性的小技巧。我们提供了证据来反驳这一说法，并以一种更加原则性的方式构建了一个现代GAN基线。首先，我们推导出一个良好的正则化相对GAN损失，该损失解决了之前通过一堆随意的小技巧来处理的模式缺失和非收敛问题。我们从数学上分析了该损失并证明，与大多数现有的相对损失相比，它允许局部收敛保证。其次，我们的新损失使我们能够摒弃所有随意的小技巧，并用现代架构替换普通GAN中过时的基础架构。以StyleGAN2为例，我们介绍了一种简化和现代化的路线图，最终结果是一个新的极简主义基线——R3GAN。尽管方法简单，但我们的方法在FFHQ、ImageNet、CIFAR和Stacked MNIST等数据集上超过了StyleGAN2，并且在与其他最先进的GANs和扩散模型的比较中表现良好。
2501.05415	 | Uncertainty-aware Knowledge Tracing	 | Weihua Cheng,Hanwen Du,Chunxiao Li,Ersheng Ni,Liangdi Tan,Tianqi Xu,Yongxin Ni	 | 本文提出了一种具有不确定性感知的知识追踪模型（UKT），该模型通过随机分布嵌入和Wasserstein自注意力机制来捕捉学生交互过程中的不确定性，并引入了一种aleatory不确定性感知的对比学习损失，以提高模型的鲁棒性。实验结果表明，UKT在多个真实世界数据集上的KT预测中显著优于现有模型，并能更好地处理学生交互的不确定性。	 | Knowledge Tracing (KT) is crucial in education assessment, which focuses on depicting students' learning states and assessing students' mastery of subjects. With the rise of modern online learning platforms, particularly massive open online courses (MOOCs), an abundance of interaction data has greatly advanced the development of the KT technology. Previous research commonly adopts deterministic representation to capture students' knowledge states, which neglects the uncertainty during student interactions and thus fails to model the true knowledge state in learning process. In light of this, we propose an Uncertainty-Aware Knowledge Tracing model (UKT) which employs stochastic distribution embeddings to represent the uncertainty in student interactions, with a Wasserstein self-attention mechanism designed to capture the transition of state distribution in student learning behaviors. Additionally, we introduce the aleatory uncertainty-aware contrastive learning loss, which strengthens the model's robustness towards different types of uncertainties. Extensive experiments on six real-world datasets demonstrate that UKT not only significantly surpasses existing deep learning-based models in KT prediction, but also shows unique advantages in handling the uncertainty of student interactions.	 | 知识追踪（KT）在教育评估中至关重要，其主要目的是描绘学生的学习状态并评估学生对学科的掌握程度。随着现代在线学习平台的兴起，尤其是大规模开放在线课程（MOOCs），大量交互数据极大地促进了KT技术的发展。先前的研究通常采用确定性表示来捕捉学生的学习状态，而这种表示忽略了学生交互过程中的不确定性，因此无法准确建模学习过程中的真实知识状态。鉴于此，我们提出了一种具有不确定性感知的知识追踪模型（UKT），该模型采用随机分布嵌入来表示学生交互中的不确定性，并设计了 Wasserstein 自注意力机制以捕捉学生学习行为状态分布的转换。此外，我们引入了一种 aleatory 不确定性感知的对比学习损失，以增强模型对不同类型的不确定性的鲁棒性。在六个真实世界数据集上的广泛实验表明，UKT 不仅在KT预测中显著优于现有的深度学习模型，还在处理学生交互的不确定性方面展现出独特的优势。
2501.05407	 | On-line Policy Improvement using Monte-Carlo Search	 | Gerald Tesauro,Gregory R. Galperin	 | 该研究提出了一种蒙特卡洛模拟算法，用于实时改进自适应控制器的策略，并已在国际象棋赌博领域取得显著成效，能够大幅降低玩家的错误率，具有广泛的应用潜力。	 | We present a Monte-Carlo simulation algorithm for real-time policy improvement of an adaptive controller. In the Monte-Carlo simulation, the long-term expected reward of each possible action is statistically measured, using the initial policy to make decisions in each step of the simulation. The action maximizing the measured expected reward is then taken, resulting in an improved policy. Our algorithm is easily parallelizable and has been implemented on the IBM SP1 and SP2 parallel-RISC supercomputers.   We have obtained promising initial results in applying this algorithm to the domain of backgammon. Results are reported for a wide variety of initial policies, ranging from a random policy to TD-Gammon, an extremely strong multi-layer neural network. In each case, the Monte-Carlo algorithm gives a substantial reduction, by as much as a factor of 5 or more, in the error rate of the base players. The algorithm is also potentially useful in many other adaptive control applications in which it is possible to simulate the environment.	 | 我们提出了一种蒙特卡洛模拟算法，用于实时改进自适应控制器的策略。在蒙特卡洛模拟中，使用初始策略在每一步模拟中做出决策，以统计测量每种可能行动的长期期望奖励。然后采取测量出的最大期望奖励的行动，从而改进了策略。我们的算法易于并行化，并已在IBM SP1和SP2并行RISC超级计算机上实现。我们已经将此算法应用于国际象棋赌博（Backgammon）领域，并获得了令人鼓舞的初步结果。在各种初始策略的情况下，从随机策略到TD-Gammon（一种极为强大的多层神经网络），蒙特卡洛算法均能显著降低基础玩家的错误率，最多可减少5倍或更多。此外，该算法在许多其他可模拟环境的自适应控制应用中也具有潜在用途。
2501.05403	 | TimeDP: Learning to Generate Multi-Domain Time Series with Domain Prompts	 | Yu-Hao Huang,Chang Xu,Yueying Wu,Wu-Jun Li,Jiang Bian	 | 本文提出了一种基于领域提示的多领域时间序列扩散模型TimeDP，通过利用时间序列语义原型和原型分配模块学习领域提示，从而在保持领域内生成质量的同时增强对未见过领域的生成能力。	 | Time series generation models are crucial for applications like data augmentation and privacy preservation. Most existing time series generation models are typically designed to generate data from one specified domain. While leveraging data from other domain for better generalization is proved to work in other application areas, this approach remains challenging for time series modeling due to the large divergence in patterns among different real world time series categories. In this paper, we propose a multi-domain time series diffusion model with domain prompts, named TimeDP. In TimeDP, we utilize a time series semantic prototype module which defines time series prototypes to represent time series basis, each prototype vector serving as "word" representing some elementary time series feature. A prototype assignment module is applied to extract the extract domain specific prototype weights, for learning domain prompts as generation condition. During sampling, we extract "domain prompt" with few-shot samples from the target domain and use the domain prompts as condition to generate time series samples. Experiments demonstrate that our method outperforms baselines to provide the state-of-the-art in-domain generation quality and strong unseen domain generation capability.	 | 时间序列生成模型在数据增强和隐私保护等应用中至关重要。大多数现有的时间序列生成模型通常设计为从一个特定领域生成数据。虽然在其他应用领域中利用来自其他领域的数据以提高泛化能力已被证明有效，但在时间序列建模中，由于不同现实世界时间序列类别的模式差异很大，这种方法仍然具有挑战性。在本文中，我们提出了一种基于领域提示的多领域时间序列扩散模型，命名为TimeDP。在TimeDP中，我们利用时间序列语义原型模块定义时间序列原型以表示时间序列的基础，每个原型向量作为“词”来表示某些基本时间序列特征。应用原型分配模块以提取特定领域的原型权重，用于学习生成条件下的领域提示。在采样过程中，我们从目标领域提取少量样本以获取“领域提示”，并将领域提示作为条件以生成时间序列样本。实验表明，我们的方法在领域内生成质量和对未见过领域的生成能力上均优于基线方法，具有领先的表现。
2501.05401	 | BRATI: Bidirectional Recurrent Attention for Time-Series Imputation	 | Armando Collado-Villaverde,Pablo Muñoz,Maria D. R-Moreno	 | BRATI是一种结合了双向循环网络和注意力机制的深度学习模型，用于解决多变量时间序列的插补问题，能够在不同缺失数据场景下提供更高的准确性和鲁棒性。	 | Missing data in time-series analysis poses significant challenges, affecting the reliability of downstream applications. Imputation, the process of estimating missing values, has emerged as a key solution. This paper introduces BRATI, a novel deep-learning model designed to address multivariate time-series imputation by combining Bidirectional Recurrent Networks and Attention mechanisms. BRATI processes temporal dependencies and feature correlations across long and short time horizons, utilizing two imputation blocks that operate in opposite temporal directions. Each block integrates recurrent layers and attention mechanisms to effectively resolve long-term dependencies.   We evaluate BRATI on three real-world datasets under diverse missing-data scenarios: randomly missing values, fixed-length missing sequences, and variable-length missing sequences. Our findings demonstrate that BRATI consistently outperforms state-of-the-art models, delivering superior accuracy and robustness in imputing multivariate time-series data.	 | 时间序列分析中的缺失数据给下游应用的可靠性带来了重大挑战。插补，即估计缺失值的过程，已成为解决这一问题的关键方法。本文介绍了一种名为BRATI的新型深度学习模型，该模型结合了双向循环网络和注意力机制，以解决多变量时间序列插补问题。BRATI能够处理长短期时间范围内的临时依赖性和特征相关性，并通过两个反向操作的插补块来实现。每个块都整合了循环层和注意力机制，以有效解决长期依赖性。  我们在三种真实世界的数据集上对BRATI进行了评估，这些数据集涵盖了不同的缺失数据场景：随机缺失值、固定长度缺失序列和变长缺失序列。我们的研究表明，BRATI在插补多变量时间序列数据方面始终优于现有最先进的模型，表现出更高的准确性和鲁棒性。
2501.05398	 | Mechanistic understanding and validation of large AI models with SemanticLens	 | Maximilian Dreyer,Jim Berend,Tobias Labarta,Johanna Vielhaben,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek	 | SemanticLens 是一种通用的解释方法，用于神经网络，通过将隐藏的知识映射到语义结构化的多模态空间中，实现了文本搜索、模型表示分析、自动标记和审计等功能，从而提高对AI模型的信任。通过组件级别的理解和验证，SemanticLens 有助于弥合AI模型与传统工程系统之间的信任差距。	 | Unlike human-engineered systems such as aeroplanes, where each component's role and dependencies are well understood, the inner workings of AI models remain largely opaque, hindering verifiability and undermining trust. This paper introduces SemanticLens, a universal explanation method for neural networks that maps hidden knowledge encoded by components (e.g., individual neurons) into the semantically structured, multimodal space of a foundation model such as CLIP. In this space, unique operations become possible, including (i) textual search to identify neurons encoding specific concepts, (ii) systematic analysis and comparison of model representations, (iii) automated labelling of neurons and explanation of their functional roles, and (iv) audits to validate decision-making against requirements. Fully scalable and operating without human input, SemanticLens is shown to be effective for debugging and validation, summarizing model knowledge, aligning reasoning with expectations (e.g., adherence to the ABCDE-rule in melanoma classification), and detecting components tied to spurious correlations and their associated training data. By enabling component-level understanding and validation, the proposed approach helps bridge the "trust gap" between AI models and traditional engineered systems. We provide code for SemanticLens on https://github.com/jim-berend/semanticlens and a demo on https://semanticlens.hhi-research-insights.eu.	 | 与飞机等人类工程系统不同，飞机的每个部件及其依赖关系都非常明确，而AI模型的内部工作机制仍然很大程度上是不透明的，这阻碍了验证并削弱了信任。本文介绍了SemanticLens，这是一种适用于神经网络的通用解释方法，将隐藏在各个组件（例如单个神经元）中的知识映射到基础模型（如CLIP）的语义结构化、多模态空间中。在这个空间中，可以实现一些独特的操作，包括：(i) 文本搜索以识别编码特定概念的神经元，(ii) 对模型表示进行系统分析和比较，(iii) 自动标记神经元及其功能角色的解释，以及(iv) 审计以验证决策是否符合要求。SemanticLens 完全可扩展且无需人工输入，已被证明对调试和验证有效，能够总结模型知识，使推理与预期（例如，在黑色素瘤分类中的ABCDE规则）保持一致，并检测与错误相关联的组件及其相关训练数据。通过实现组件级别的理解和验证，所提出的方法有助于弥合AI模型与传统工程系统之间的“信任差距”。我们已在 https://github.com/jim-berend/semanticlens 提供了SemanticLens的代码，并在 https://semanticlens.hhi-research-insights.eu 提供了演示。
2501.05370	 | Accelerated Diffusion Models via Speculative Sampling	 | Valentin De Bortoli,Alexandre Galashov,Arthur Gretton,Arnaud Doucet	 | 推测性采样通过快速生成候选标记并基于目标模型的分布进行接受或拒绝，已被扩展到连续的向量值马尔可夫链生成模型，显著加速了生成过程，同时保持了样本质量。	 | Speculative sampling is a popular technique for accelerating inference in Large Language Models by generating candidate tokens using a fast draft model and accepting or rejecting them based on the target model's distribution. While speculative sampling was previously limited to discrete sequences, we extend it to diffusion models, which generate samples via continuous, vector-valued Markov chains. In this context, the target model is a high-quality but computationally expensive diffusion model. We propose various drafting strategies, including a simple and effective approach that does not require training a draft model and is applicable out of the box to any diffusion model. Our experiments demonstrate significant generation speedup on various diffusion models, halving the number of function evaluations, while generating exact samples from the target model.	 | 推测性采样是一种用于加速大型语言模型推理的技术，它通过快速草稿模型生成候选标记，并基于目标模型的分布接受或拒绝这些标记。虽然推测性采样以前仅限于离散序列，但我们将它扩展到了生成模型（diffusion models），这些模型通过连续的向量值马尔可夫链生成样本。在这种情况下，目标模型是一个高质量但计算成本高的生成模型。我们提出了一些草稿策略，包括一个简单且有效的策略，该策略不需要训练草稿模型，并可以直接应用于任何生成模型。我们的实验结果显示，在各种生成模型上显著提高了生成速度，将函数评估次数减半，同时从目标模型生成了精确样本。
2501.05361	 | No-Regret Linear Bandits under Gap-Adjusted Misspecification	 | Chong Liu,Dan Qiao,Ming Yin,Ilija Bogunovic,Yu-Xiang Wang	 | 本文扩展了Liu等人（2023）的工作，提出了在“调整间隔差的非线性假设”下的线性bandits方法，展示了经典LinUCB算法可以自动适应这种非线性假设，并且提出了一种新的分阶段淘汰算法，该算法实现了最优的遗憾界并具有高效的部署效率。	 | This work studies linear bandits under a new notion of gap-adjusted misspecification and is an extension of Liu et al. (2023). When the underlying reward function is not linear, existing linear bandits work usually relies on a uniform misspecification parameter $ε$ that measures the sup-norm error of the best linear approximation. This results in an unavoidable linear regret whenever $ε> 0$. We propose a more natural model of misspecification which only requires the approximation error at each input $x$ to be proportional to the suboptimality gap at $x$. It captures the intuition that, for optimization problems, near-optimal regions should matter more and we can tolerate larger approximation errors in suboptimal regions.   Quite surprisingly, we show that the classical LinUCB algorithm -- designed for the realizable case -- is automatically robust against such $ρ$-gap-adjusted misspecification with parameter $ρ$ diminishing at $O(1/(d \sqrt{\log T}))$. It achieves a near-optimal $O(\sqrt{T})$ regret for problems that the best-known regret is almost linear in time horizon $T$. We further advance this frontier by presenting a novel phased elimination-based algorithm whose gap-adjusted misspecification parameter $ρ= O(1/\sqrt{d})$ does not scale with $T$. This algorithm attains optimal $O(\sqrt{T})$ regret and is deployment-efficient, requiring only $\log T$ batches of exploration. It also enjoys an adaptive $O(\log T)$ regret when a constant suboptimality gap exists. Technically, our proof relies on a novel self-bounding argument that bounds the part of the regret due to misspecification by the regret itself, and a new inductive lemma that limits the misspecification error within the suboptimality gap for all valid actions in each batch selected by G-optimal design.	 | 本文研究了线性bandits在一种新的“调整间隔差的非线性假设”的情况下，是对Liu等人（2023）工作的扩展。当潜在的奖励函数不是线性时，现有的线性bandits方法通常依赖于一个统一的非线性参数$ε$，它衡量最佳线性近似的最佳线性逼近的范数误差。这导致了当$ε > 0$时不可避免的线性遗憾。我们提出了一种更自然的非线性假设模型，只需要每个输入$x$的近似误差与$x$处的次优间隔成比例。这种模型捕捉到优化问题的一个直观想法：对于优化问题，近最优区域应更重要，我们可以容忍次优区域中的较大近似误差。令人惊讶的是，我们展示了经典的LinUCB算法——设计用于可实现情况——可以自动对抗这种“ρ-间隔差调整的非线性假设”，参数$ρ$以$O(1/(d \sqrt{\log T}))$的速度减少。它在多种问题上实现了近乎最优的$O(\sqrt{T})$遗憾，这些问题是已知的最优遗憾几乎线性依赖于时间范围$T$。我们进一步推进了这一前沿，提出了一个基于分阶段淘汰的新算法，其间隔差调整的非线性假设参数$ρ= O(1/\sqrt{d})$不随$T$变化。这个算法实现了最优的$O(\sqrt{T})$遗憾，并具有高效的部署效率，只需$\log T$批探索。它还享有适应性$O(\log T)$遗憾，存在恒定的次优间隔。技术上，我们的证明依赖于一种新颖的自我约束论证，该论证通过遗憾本身来限制非线性假设导致的部分遗憾，以及一个新的归纳引理，该引理限制了所有由G-最优设计选出的有效动作在每个批次中的非线性假设误差在次优间隔内。
2501.05333	 | Stability and List-Replicability for Agnostic Learners	 | Ari Blonda,Shan Gao,Hamed Hatami,Pooya Hatami	 | 两篇开创性论文建立了在线可学习性和全局稳定PAC可学习性的等价性，但Chase等人指出这种等价性在无偏假设下不成立，仅有限假设类是全局稳定可学习的。为克服这一限制，本文通过两种放松条件刻画了可学习的类，并证明在稳定性参数依赖于剩余误差时，无偏稳定性可由Littlestone维数刻画，同时证明即使限制在小总体损失的假设类，也只有有限假设类是全局稳定可学习的。	 | Two seminal papers--Alon, Livni, Malliaris, Moran (STOC 2019) and Bun, Livni, and Moran (FOCS 2020)--established the equivalence between online learnability and globally stable PAC learnability in binary classification. However, Chase, Chornomaz, Moran, and Yehudayoff (STOC 2024) recently showed that this equivalence does not hold in the agnostic setting. Specifically, they proved that in the agnostic setting, only finite hypothesis classes are globally stable learnable. Therefore, agnostic global stability is too restrictive to capture interesting hypothesis classes.   To address this limitation, Chase \emph{et al.} introduced two relaxations of agnostic global stability. In this paper, we characterize the classes that are learnable under their proposed relaxed conditions, resolving the two open problems raised in their work.   First, we prove that in the setting where the stability parameter can depend on the excess error (the gap between the learner's error and the best achievable error by the hypothesis class), agnostic stability is fully characterized by the Littlestone dimension. Consequently, as in the realizable case, this form of learnability is equivalent to online learnability.   As part of the proof of this theorem, we strengthen the celebrated result of Bun et al. by showing that classes with infinite Littlestone dimension are not stably PAC learnable, even if we allow the stability parameter to depend on the excess error.   For the second relaxation proposed by Chase et al., we prove that only finite hypothesis classes are globally stable learnable even if we restrict the agnostic setting to distributions with small population loss.	 | 以下是该摘要的中文翻译：  两篇开创性的论文——Alon, Livni, Malliaris, Moran（STOC 2019）和Bun, Livni, and Moran（FOCS 2020）——建立了在线可学习性和全局稳定PAC可学习性在二分类中的等价性。然而，Chase, Chornomaz, Moran, and Yehudayoff（STOC 2024）最近指出，在无偏假设下这种等价性并不成立。具体来说，他们证明了在无偏假设下，只有有限假设类是全局稳定可学习的。因此，无偏全局稳定性过于限制，无法捕捉有趣的假设类。为了克服这一限制，Chase等人引入了无偏全局稳定性的两种放松。在本文中，我们刻画了在他们提出放松条件下可学习的类，解决了他们在工作中提出的两个开放问题。  首先，我们证明，在稳定性参数可以依赖于剩余误差（即学习器的误差与假设类中可达到的最佳误差之间的差距）的情况下，无偏稳定性完全由Littlestone维数来刻画。因此，类似于可实现情况，这种形式的可学习性等同于在线可学习性。  作为证明该定理的一部分，我们加强了Bun等人的一项著名结果，证明即使允许稳定性参数依赖于剩余误差，无限Littlestone维数的类也是不可全局稳定PAC学习的。  对于Chase等人提出的第二种放松，我们证明，即使我们将无偏假设限制在分布具有小总体损失的情况下，也只有有限假设类是全局稳定可学习的。
2501.05329	 | Knowledge Transfer in Model-Based Reinforcement Learning Agents for Efficient Multi-Task Learning	 | Dmytro Kuzmenko,Nadiya Shvai	 | 该研究提出了一种高效的知识转移方法，将一个包含317M参数的多任务智能体提炼成一个1M参数的紧凑模型，在MT30基准测试中得分显著提升至28.45，并通过FP16后训练量化技术进一步减小模型大小50%的同时保持性能，从而解决了大型世界模型在资源受限环境中的部署挑战。	 | We propose an efficient knowledge transfer approach for model-based reinforcement learning, addressing the challenge of deploying large world models in resource-constrained environments. Our method distills a high-capacity multi-task agent (317M parameters) into a compact 1M parameter model, achieving state-of-the-art performance on the MT30 benchmark with a normalized score of 28.45, a substantial improvement over the original 1M parameter model's score of 18.93. This demonstrates the ability of our distillation technique to consolidate complex multi-task knowledge effectively. Additionally, we apply FP16 post-training quantization, reducing the model size by 50% while maintaining performance. Our work bridges the gap between the power of large models and practical deployment constraints, offering a scalable solution for efficient and accessible multi-task reinforcement learning in robotics and other resource-limited domains.	 | 我们提出了一种高效的知识转移方法，用于基于模型的强化学习，该方法解决了在资源受限环境中部署大型世界模型的挑战。我们的方法将一个高容量的多任务智能体（包含317M个参数）提炼成一个紧凑的1M参数模型，该模型在MT30基准测试中获得了28.45的标准化得分，远超原始1M参数模型的得分18.93。这表明我们的提炼技术能够有效地集中复杂多任务的知识。此外，我们还应用了FP16后训练量化技术，使模型大小减少了50%，同时保持了性能。我们的工作填补了大型模型强大功能与实际部署限制之间的差距，提供了一个可扩展的解决方案，以实现高效且易于访问的多任务强化学习，适用于机器人学及其他资源受限领域。
2501.05323	 | Distributed Learning and Inference Systems: A Networking Perspective	 | Hesham G. Moussa,Arashmid Akhavain,S. Maryam Hosseini,Bill McCormick	 | 该研究提出了一种名为数据和动力感知的推断和训练网络（DA-ITN）的新框架，旨在解决集中式机器学习模型的隐私、存储、故障和计算需求等问题，通过去中心化和分布式方法提高AI系统的效率和安全性。	 | Machine learning models have achieved, and in some cases surpassed, human-level performance in various tasks, mainly through centralized training of static models and the use of large models stored in centralized clouds for inference. However, this centralized approach has several drawbacks, including privacy concerns, high storage demands, a single point of failure, and significant computing requirements. These challenges have driven interest in developing alternative decentralized and distributed methods for AI training and inference. Distribution introduces additional complexity, as it requires managing multiple moving parts. To address these complexities and fill a gap in the development of distributed AI systems, this work proposes a novel framework, Data and Dynamics-Aware Inference and Training Networks (DA-ITN). The different components of DA-ITN and their functions are explored, and the associated challenges and research areas are highlighted.	 | 机器学习模型已经在各种任务中取得了与人类相当甚至超越人类水平的成就，主要依赖于集中式训练的静态模型以及在集中式云中存储的大模型用于推断。然而，这种集中式方法有几个缺点，包括隐私问题、高存储需求、单一故障点，以及显著的计算要求。这些挑战驱使人们关注开发替代的去中心化和分布式方法，用于AI的训练和推断。分布引入了额外的复杂性，因为它需要管理多个移动部分。为了应对这些复杂性并填补分布式AI系统开发中的空白，本研究提出了一种新型框架——数据和动力感知的推断和训练网络（DA-ITN）。详细探讨了DA-ITN的不同组成部分及其功能，并指出了相关挑战和研究领域。
2501.05279	 | Learning convolution operators on compact Abelian groups	 | Emilia Magnani,Ernesto De Vito,Philipp Hennig,Lorenzo Rosasco	 | 本文研究了与紧致阿贝尔群相关的卷积算子的学习问题，提出了基于平移不变希尔伯特空间的正则化方法，并通过岭回归估计器分析了其精度，理论结果得到了数值模拟的验证。	 | We consider the problem of learning convolution operators associated to compact Abelian groups. We study a regularization-based approach and provide corresponding learning guarantees, discussing natural regularity condition on the convolution kernel. More precisely, we assume the convolution kernel is a function in a translation invariant Hilbert space and analyze a natural ridge regression (RR) estimator. Building on existing results for RR, we characterize the accuracy of the estimator in terms of finite sample bounds. Interestingly, regularity assumptions which are classical in the analysis of RR, have a novel and natural interpretation in terms of space/frequency localization. Theoretical results are illustrated by numerical simulations.	 | 我们考虑与紧致阿贝尔群相关的卷积算子的学习问题。我们研究了一种正则化方法，并提供了相应的学习保证，讨论了卷积核的自然正则性条件。更具体地说，我们假设卷积核属于一个平移不变希尔伯特空间，并分析了一个自然的岭回归（RR）估计器。基于现有关于岭回归的成果，我们从有限样本误差的角度来描述估计器的精度。有趣的是，经典地在岭回归分析中出现的正则性假设，在空间/频率局部化方面具有新颖且自然的解释。理论结果通过数值模拟得到了验证。
2501.05248	 | Deriving Coding-Specific Sub-Models from LLMs using Resource-Efficient Pruning	 | Laura Puccioni,Alireza Farshin,Mariano Scazzariello,Changjie Wang,Marco Chiesa,Dejan Kostic	 | 本文探讨了通过无结构剪枝技术有效提取编程语言特定子模型的方法，并使用适当的校准数据集高效地生成了针对Python、Java、C++和JavaScript的子模型，同时保持了与完整模型相当的精度，表明这种技术有潜力降低大型语言模型在编码任务中的计算需求，提升其在消费级硬件上的可用性和实时开发反馈速度。	 | Large Language Models (LLMs) have demonstrated their exceptional performance in various complex code generation tasks. However, their broader adoption is limited by significant computational demands and high resource requirements, particularly memory and processing power. To mitigate such requirements, model pruning techniques are used to create more compact models with significantly fewer parameters. However, current approaches do not focus on the efficient extraction of programming-language-specific sub-models. In this work, we explore the idea of efficiently deriving coding-specific sub-models through unstructured pruning (i.e., Wanda). We investigate the impact of different domain-specific calibration datasets on pruning outcomes across three distinct domains and extend our analysis to extracting four language-specific sub-models: Python, Java, C++, and JavaScript. We are the first to efficiently extract programming-language-specific sub-models using appropriate calibration datasets while maintaining acceptable accuracy w.r.t. full models. We are also the first to provide analytical evidence that domain-specific tasks activate distinct regions within LLMs, supporting the creation of specialized sub-models through unstructured pruning. We believe that this work has significant potential to enhance LLM accessibility for coding by reducing computational requirements to enable local execution on consumer-grade hardware, and supporting faster inference times critical for real-time development feedback.	 | 大型语言模型（LLMs）在各种复杂的代码生成任务中展现了卓越的性能。然而，它们的广泛应用受到了显著的计算需求和高资源要求（特别是内存和处理能力）的限制。为了减轻这些需求，使用模型剪枝技术来创建更紧凑的模型，参数数量大大减少。然而，当前的方法并没有专注于高效提取编程语言特定的子模型。在本文中，我们探讨了通过无结构剪枝（即Wanda）有效地提取编程语言特定子模型的想法。我们研究了不同特定领域校准数据集对不同领域剪枝结果的影响，并将分析扩展到提取四种语言特定子模型：Python、Java、C++和JavaScript。我们首次利用适当的校准数据集高效地提取编程语言特定的子模型，并在相对于完整模型的可接受精度方面保持了竞争力。我们还首次提供了分析证据，表明特定领域的任务在LLMs中激活了不同的区域，支持通过无结构剪枝创建专门的子模型。我们认为，这项工作有显著潜力通过降低计算需求来增强LLM在编码中的可访问性，使得本地执行成为消费级硬件的可能，同时支持对实时开发反馈至关重要的快速推断时间。
2501.05197	 | An Algorithmic Approach for Causal Health Equity: A Look at Race Differentials in Intensive Care Unit (ICU) Outcomes	 | Drago Plecko,Paul Secombe,Andrea Clarke,Amelia Fiske,Samarra Toby,Donisha Duff,David Pilcher,Leo Anthony Celi,Rinaldo Bellomo,Elias Bareinboim	 | 该研究提出了一种因果推断框架来分析健康不平等，通过比较澳大利亚和美国少数群体与多数群体在重症监护病房结果上的差异，揭示了这些群体在入院时机和病情严重程度上的不平等，并开发了土著重症监护公平雷达（IICE雷达）来监测和跟踪这些不平等现象。	 | The new era of large-scale data collection and analysis presents an opportunity for diagnosing and understanding the causes of health inequities. In this study, we describe a framework for systematically analyzing health disparities using causal inference. The framework is illustrated by investigating racial and ethnic disparities in intensive care unit (ICU) outcome between majority and minority groups in Australia (Indigenous vs. Non-Indigenous) and the United States (African-American vs. White). We demonstrate that commonly used statistical measures for quantifying inequity are insufficient, and focus on attributing the observed disparity to the causal mechanisms that generate it. We find that minority patients are younger at admission, have worse chronic health, are more likely to be admitted for urgent and non-elective reasons, and have higher illness severity. At the same time, however, we find a protective direct effect of belonging to a minority group, with minority patients showing improved survival compared to their majority counterparts, with all other variables kept equal. We demonstrate that this protective effect is related to the increased probability of being admitted to ICU, with minority patients having an increased risk of ICU admission. We also find that minority patients, while showing improved survival, are more likely to be readmitted to ICU. Thus, due to worse access to primary health care, minority patients are more likely to end up in ICU for preventable conditions, causing a reduction in the mortality rates and creating an effect that appears to be protective. Since the baseline risk of ICU admission may serve as proxy for lack of access to primary care, we developed the Indigenous Intensive Care Equity (IICE) Radar, a monitoring system for tracking the over-utilization of ICU resources by the Indigenous population of Australia across geographical areas.	 | 大数据收集和分析的新时代为诊断和理解健康不平等的原因提供了机会。本研究描述了一种系统分析健康不平等的框架，采用因果推断的方法。该框架通过比较澳大利亚（土著与非土著）和美国（非裔美国人与白人）少数群体和多数群体在重症监护病房（ICU）结果上的种族和 Ethnic 不平等来说明。我们证明了常用的统计数据衡量不平等的方法是不够的，并将重点放在将观察到的不平等归因于其产生的因果机制上。我们发现，少数群体患者入院时年龄较小，慢性健康状况较差，更有可能因紧急和非择期原因入院，并且病情更为严重。然而，我们同时发现，对于相同的其他变量，少数群体患者具有保护性直接效应，生存率优于其多数群体的对照组。我们还证明这种保护性效应与被ICU收治几率增加有关，少数群体患者在ICU入院方面具有更高的风险。此外，尽管少数群体患者显示出生存率改善，但他们更有可能再次被送往ICU。因此，由于对初级保健的较差访问，少数群体患者更有可能因可预防的条件不得不进入ICU，导致死亡率降低并产生看似保护性的影响。由于ICU入院的基础风险可能作为缺乏初级保健访问的代理指标，我们开发了土著重症监护公平雷达（IICE雷达），这是一个监测系统，用于跟踪澳大利亚不同地理区域土著人口对ICU资源的过度使用。
2501.05130	 | Learning In-Distribution Representations for Anomaly Detection	 | William T. Lunardi,Abdulrahman Banabila,Dania Herzalla,Martin L. Andreoni	 | 该研究提出了一种新的对比学习目标——Focusd In-distribution Representation Modeling (FIRM)，旨在通过积极整合合成异常值来改进表示空间，从而提高异常检测性能。实验结果表明，FIRM 在多个基准测试中显著优于现有方法，并且具有鲁棒的表示质量。	 | Anomaly detection involves identifying data patterns that deviate from the anticipated norm. Traditional methods struggle in high-dimensional spaces due to the curse of dimensionality. In recent years, self-supervised learning, particularly through contrastive objectives, has driven advances in anomaly detection. However, vanilla contrastive learning struggles to align with the unique demands of anomaly detection, as it lacks a pretext task tailored to the homogeneous nature of In-Distribution (ID) data and the diversity of Out-of-Distribution (OOD) anomalies. Methods that attempt to address these challenges, such as introducing hard negatives through synthetic outliers, Outlier Exposure (OE), and supervised objectives, often rely on pretext tasks that fail to balance compact clustering of ID samples with sufficient separation from OOD data. In this work, we propose Focused In-distribution Representation Modeling (FIRM), a contrastive learning objective specifically designed for anomaly detection. Unlike existing approaches, FIRM incorporates synthetic outliers into its pretext task in a way that actively shapes the representation space, promoting compact clustering of ID samples while enforcing strong separation from outliers. This formulation addresses the challenges of class collision, enhancing both the compactness of ID representations and the discriminative power of the learned feature space. We show that FIRM surpasses other contrastive methods in standard benchmarks, significantly enhancing anomaly detection compared to both traditional and supervised contrastive learning objectives. Our ablation studies confirm that FIRM consistently improves the quality of representations and shows robustness across a range of scoring methods. The code is available at: https://github.com/willtl/firm.	 | 异常检测涉及识别与预期规范相偏离的数据模式。传统的异常检测方法在高维空间中难以应对，因为存在维度灾难题。近年来，自监督学习，尤其是通过对比目标，推动了异常检测的进步。然而，普通的对比学习方法难以与异常检测的独特需求对齐，因为它缺乏针对同质性ID数据和多样性OOD异常的定制先验任务。尝试解决这些挑战的方法，如通过引入合成异常值的困难负例、Outlier Exposure（OE）以及监督目标等，往往依赖于平衡ID样本的紧凑聚类与与OOD数据的分离度的先验任务，常常失败。在此工作中，我们提出了一种专门用于异常检测的对比学习目标——Focusd In-distribution Representation Modeling (FIRM)。不同于现有方法，FIRM 以一种积极塑造表示空间的方式将合成异常值纳入先验任务中，促进ID样本的紧凑聚类同时加强与异常值的分离。这种表述形式解决了类别碰撞问题，增强了ID表示的紧凑性和学习特征空间的判别能力。我们表明，在标准基准测试中，FIRM 超越了其他对比方法，在传统和监督对比学习目标中显著提升了异常检测性能。我们的消融研究证实了FIRM 一致地改进了表示质量，并且在多种评分方法中表现出鲁棒性。代码可在 https://github.com/willtl/firm 获取。
2501.05109	 | EquiBoost: An Equivariant Boosting Approach to Molecular Conformation Generation	 | Yixuan Yang,Xingyu Fang,Zhaowen Cheng,Pengju Yan,Xiaolin Li	 | EquiBoost是一种通过堆叠等变图变换器逐步细化分子3D构象的增强模型，无需依赖扩散技术，相比基于扩散的方法在生成质量和效率上均有显著提升。这一模型重新激发了增强学习在分子构象生成领域的活力，并提供了对扩散模型的稳健替代方案。	 | Molecular conformation generation plays key roles in computational drug design. Recently developed deep learning methods, particularly diffusion models have reached competitive performance over traditional cheminformatical approaches. However, these methods are often time-consuming or require extra support from traditional methods. We propose EquiBoost, a boosting model that stacks several equivariant graph transformers as weak learners, to iteratively refine 3D conformations of molecules. Without relying on diffusion techniques, EquiBoost balances accuracy and efficiency more effectively than diffusion-based methods. Notably, compared to the previous state-of-the-art diffusion method, EquiBoost improves generation quality and preserves diversity, achieving considerably better precision of Average Minimum RMSD (AMR) on the GEOM datasets. This work rejuvenates boosting and sheds light on its potential to be a robust alternative to diffusion models in certain scenarios.	 | 分子构象生成在计算药物设计中扮演着关键角色。最近开发的深度学习方法，尤其是扩散模型，已经在传统化学信息学方法上达到了竞争力。然而，这些方法往往耗时或需要传统方法的支持。我们提出了一种名为EquiBoost的增强模型，该模型通过堆叠多个等变图变换器作为弱学习器，逐步细化分子的3D构象。无需依赖扩散技术，EquiBoost在准确性和效率方面比基于扩散的方法更胜一筹。值得注意的是，与之前的最先进的扩散方法相比，EquiBoost在生成质量和保持多样性方面都有所提高，在GEOM数据集上实现了显著更好的平均最小RMSD（Average Minimum RMSD，AMR）精度。这项工作重新激发了增强学习的活力，并为其实现某些场景下对扩散模型的稳健替代方案提供了新的视角。
2501.05093	 | Hierarchical Decomposed Dual-domain Deep Learning for Sparse-View CT Reconstruction	 | Yoseob Han	 | 本文提出了一个利用分层分解测量的新双域深度学习框架，通过结合深度卷积框架的低秩性质和傅里叶域中的分层分解支持，以提高稀疏投影视图CT重建的性能，并且重建性能优于传统方法，为医学成像领域的研究提供了新的理论依据和方法。	 | Objective: X-ray computed tomography employing sparse projection views has emerged as a contemporary technique to mitigate radiation dose. However, due to the inadequate number of projection views, an analytic reconstruction method utilizing filtered backprojection results in severe streaking artifacts. Recently, deep learning strategies employing image-domain networks have demonstrated remarkable performance in eliminating the streaking artifact caused by analytic reconstruction methods with sparse projection views. Nevertheless, it is difficult to clarify the theoretical justification for applying deep learning to sparse view CT reconstruction, and it has been understood as restoration by removing image artifacts, not reconstruction.   Approach: By leveraging the theory of deep convolutional framelets and the hierarchical decomposition of measurement, this research reveals the constraints of conventional image- and projection-domain deep learning methodologies, subsequently, the research proposes a novel dual-domain deep learning framework utilizing hierarchical decomposed measurements. Specifically, the research elucidates how the performance of the projection-domain network can be enhanced through a low-rank property of deep convolutional framelets and a bowtie support of hierarchical decomposed measurement in the Fourier domain.   Main Results: This study demonstrated performance improvement of the proposed framework based on the low-rank property, resulting in superior reconstruction performance compared to conventional analytic and deep learning methods.   Significance: By providing a theoretically justified deep learning approach for sparse-view CT reconstruction, this study not only offers a superior alternative to existing methods but also opens new avenues for research in medical imaging.	 | 目标：通过稀疏投影视图的X射线计算机断层成像技术，已经出现了一种减少辐射剂量的现代方法。然而，由于投影视图数量不足，传统的滤波反投影重建方法会导致严重的条带状伪影。最近，采用图像域网络的深度学习策略已经显示出在稀疏投影视图下消除由传统重建方法引起的条带状伪影的出色性能。然而，将深度学习应用于稀疏视图CT重建的理论依据尚不清楚，一直被认为是一种通过去除图像伪影来进行的修复，而非真正的重建。  方法：通过利用深度卷积框架理论和测量的分层分解，本研究揭示了传统图像域和投影域深度学习方法的局限性，随后提出了一个利用分层分解测量的新双域深度学习框架。具体而言，研究阐明了通过深度卷积框架的低秩性质和傅里叶域中分层分解测量的倒Y形支持来提高投影域网络性能的方法。  主要结果：本研究展示了基于低秩性质的提出的框架的性能改进，其重建性能优于传统的分析和深度学习方法。  意义：通过提供一个理论上合理的方法来解决稀疏视图CT重建中的深度学习问题，本研究不仅提供了一种优于现有方法的替代方案，而且为医学成像研究开辟了新的途径。
2501.05081	 | DriVLM: Domain Adaptation of Vision-Language Models in Autonomous Driving	 | Xuran Zheng,Chang D. Yoo	 | 本文探讨了小规模多模态大型语言模型（MLLM）的实用性，并将其应用于自动驾驶领域，旨在促进MLLM在实际场景中的应用，应对现有模型所需高计算资源的挑战。	 | In recent years, large language models have had a very impressive performance, which largely contributed to the development and application of artificial intelligence, and the parameters and performance of the models are still growing rapidly. In particular, multimodal large language models (MLLM) can combine multiple modalities such as pictures, videos, sounds, texts, etc., and have great potential in various tasks. However, most MLLMs require very high computational resources, which is a major challenge for most researchers and developers. In this paper, we explored the utility of small-scale MLLMs and applied small-scale MLLMs to the field of autonomous driving. We hope that this will advance the application of MLLMs in real-world scenarios.	 | 近年来，大型语言模型取得了非常令人印象深刻的表现，极大地促进了人工智能的发展与应用，并且模型的参数和性能仍在快速增加。特别是，多模态大型语言模型（MLLM）能够结合多种模态，如图片、视频、声音、文本等，具有在各种任务中巨大潜力。然而，大多数MLLM需要非常高的计算资源，这给大多数研究人员和开发者带来了重大挑战。在本文中，我们探讨了小规模MLLM的实用性，并将小规模MLLM应用于自动驾驶领域。我们希望这能够促进MLLM在实际场景中的应用。
2501.05078	 | Analyzing Memorization in Large Language Models through the Lens of Model Attribution	 | Tarun Ram Menta,Susmit Agrawal,Chirag Agarwal	 | 该研究从架构角度分析了不同层级的注意力模块对大型语言模型（LLMs）记忆和泛化性能的影响，发现较深的变换块中的注意力模块主要负责记忆，而较早的块对于模型的泛化能力和推理能力至关重要。通过系统地干预LLM架构，研究证实了这些洞察，并提供了减轻LLMs记忆问题的同时保持模型性能的方法。	 | Large Language Models (LLMs) are prevalent in modern applications but often memorize training data, leading to privacy breaches and copyright issues. Existing research has mainly focused on posthoc analyses, such as extracting memorized content or developing memorization metrics, without exploring the underlying architectural factors that contribute to memorization. In this work, we investigate memorization from an architectural lens by analyzing how attention modules at different layers impact its memorization and generalization performance. Using attribution techniques, we systematically intervene in the LLM architecture by bypassing attention modules at specific blocks while keeping other components like layer normalization and MLP transformations intact. We provide theorems analyzing our intervention mechanism from a mathematical view, bounding the difference in layer outputs with and without our attributions. Our theoretical and empirical analyses reveal that attention modules in deeper transformer blocks are primarily responsible for memorization, whereas earlier blocks are crucial for the models generalization and reasoning capabilities. We validate our findings through comprehensive experiments on different LLM families (Pythia and GPTNeo) and five benchmark datasets. Our insights offer a practical approach to mitigate memorization in LLMs while preserving their performance, contributing to safer and more ethical deployment in real world applications.	 | 大型语言模型（LLMs）在现代应用中非常普遍，但往往会记忆训练数据，导致隐私泄露和版权问题。现有的研究主要集中在事后分析，如提取记忆的内容或开发记忆度量，而没有探索导致记忆的潜在架构因素。在本工作中，我们从架构角度出发，研究不同层级的注意力模块如何影响记忆和泛化性能。通过归因技术，我们系统地干预LLM架构，绕过特定块中的注意力模块，同时保持其他组件如层标准化和MLP变换不变。我们从数学角度分析我们的干预机制，推导出有和没有我们归因时各层输出的差异界限。我们的理论分析和实验证明显示，较深的变换块中的注意力模块主要负责记忆，而较早的块对于模型的泛化能力和推理能力至关重要。我们通过在不同LLM家族（Pythia和GPTNeo）和五个基准数据集上的全面实验验证了我们的发现。我们的洞察提供了一种实用的方法，在保持模型性能的同时减轻LLMs中的记忆，从而有助于在实际应用中实现更安全和更符合伦理的部署。
2501.05015	 | On Measuring Unnoticeability of Graph Adversarial Attacks: Observations, New Measure, and Applications	 | Hyeonsoo Jo,Hyunjin Hwang,Fanchen Bu,Soo Yong Lee,Chanyoung Park,Kijung Shin	 | 尽管传统的图攻击可察觉性度量声称不易察觉，但现有方法存在显著限制：一是过于简单的规则容易被攻击者绕过，二是依赖全局统计信息导致攻击直到严重扰动才被察觉。为此，本文提出HideNSeek，这是一种可学习的图攻击可察觉性度量，通过引入可学习边评分器（LEO）和不平衡感知聚合来分别解决绕过和忽略问题，并在多种攻击方法下显著优于现有方法，同时还能增强GNN的鲁棒性。	 | Adversarial attacks are allegedly unnoticeable. Prior studies have designed attack noticeability measures on graphs, primarily using statistical tests to compare the topology of original and (possibly) attacked graphs. However, we observe two critical limitations in the existing measures. First, because the measures rely on simple rules, attackers can readily enhance their attacks to bypass them, reducing their attack "noticeability" and, yet, maintaining their attack performance. Second, because the measures naively leverage global statistics, such as degree distributions, they may entirely overlook attacks until severe perturbations occur, letting the attacks be almost "totally unnoticeable." To address the limitations, we introduce HideNSeek, a learnable measure for graph attack noticeability. First, to mitigate the bypass problem, HideNSeek learns to distinguish the original and (potential) attack edges using a learnable edge scorer (LEO), which scores each edge on its likelihood of being an attack. Second, to mitigate the overlooking problem, HideNSeek conducts imbalance-aware aggregation of all the edge scores to obtain the final noticeability score. Using six real-world graphs, we empirically demonstrate that HideNSeek effectively alleviates the observed limitations, and LEO (i.e., our learnable edge scorer) outperforms eleven competitors in distinguishing attack edges under five different attack methods. For an additional application, we show that LEO boost the performance of robust GNNs by removing attack-like edges.	 |  adversarial攻击据说不易察觉。先前的研究在图上设计了攻击可察觉性的度量，主要通过统计测试来比较原始图和可能被攻击后的图的拓扑结构。然而，我们观察到现有度量的两个关键限制。首先，由于这些度量依赖于简单的规则，攻击者可以轻松增强他们的攻击以绕过这些规则，减少攻击的“可察觉性”同时仍然维持其攻击性能。其次，由于这些度量天真地利用全局统计信息，如度分布，它们可能会直到严重的扰动发生才察觉到攻击，从而使攻击变得“完全不可察觉”。为了解决这些问题，我们引入了HideNSeek，这是一种可学习的图攻击可察觉性度量。首先，为缓解绕过问题，HideNSeek学习使用可学习边评分器（LEO）来区分原始边和潜在攻击边，LEO会根据每条边成为攻击边的可能性对其评分。其次，为缓解忽略问题，HideNSeek进行不平衡感知的聚合来得到最终的可察觉性评分。使用六个真实世界的图，我们实证表明，HideNSeek有效地缓解了观察到的限制，并且LEO（即我们的可学习边评分器）在五种不同的攻击方法下优于十一个竞争对手，在区分攻击边方面表现更佳。作为额外的应用，我们展示了LEO能够通过去除攻击样式的边来提升鲁棒GNN的性能。
2501.05005	 | A High-accuracy Calibration Method of Transient TSEPs for Power Semiconductor Devices	 | Qinghao Zhang,Wenrui Li,Pinjia Zhang	 | 该研究提出了一种用于瞬态热敏电气参数(TSEP)的高精度校准方法，通过温度补偿策略、识别耦合参数以及利用神经网络建模来减小由负载电流引起的温度差异，从而提高结温监控的精度，实验结果表明该方法比传统方法平均绝对误差减少了超过30%。	 | The thermal sensitive electrical parameter (TSEP) method is crucial for enhancing the reliability of power devices through junction temperature monitoring. The TSEP method comprises three key processes: calibration, regression, and application. While significant efforts have been devoted to improving regression algorithms and increasing TSEP sensitivity to enhance junction temperature monitoring accuracy, these approaches have reached a bottleneck. In reality, the calibration method significantly influences monitoring accuracy, an aspect often overlooked in conventional TSEP methods. To address this issue, we propose a high-accuracy calibration method for transient TSEPs. First, a temperature compensation strategy based on thermal analysis is introduced to mitigate the temperature difference caused by load current during dual pulse tests. Second, the impact of stray parameters is analyzed to identify coupled parameters, which are typically neglected in existing methods. Third, it is observed that random errors follow a logarithm Gaussian distribution, covering a hidden variable. A neural network is used to obtain the junction temperature predictive model. The proposed calibration method is experimental validated in threshold voltage as an example. Compared with conventional calibration methods, the mean absolute error is reduced by over 30%. Moreover, this method does not require additional hardware cost and has good generalization.	 | 热敏电气参数（TSEP）方法对于通过结温监控提高功率器件的可靠性至关重要。TSEP方法包括三个关键步骤：校准、回归和应用。尽管在改进回归算法和提高TSEP对结温监控的敏感性方面投入了大量努力，但这些方法也已达到瓶颈。实际上，校准方法对监控精度的影响至关重要，这一点在传统的TSEP方法中常常被忽视。为了解决这个问题，我们提出了一种用于瞬态TSEP的高精度校准方法。首先，基于热分析的温度补偿策略用于在双脉冲测试中减小由负载电流引起的温度差异。其次，分析了散杂参数的影响，以识别耦合参数，这些参数在现有方法中通常被忽略。第三，观察到随机误差遵循对数正态分布，覆盖了一个隐藏变量。使用神经网络获得结温预测模型。所提出的校准方法以阈值电压为例进行了实验验证。与传统校准方法相比，平均绝对误差减少了超过30%。此外，该方法不需要额外的硬件成本，并且具有良好的泛化能力。
2501.05000	 | Load Forecasting for Households and Energy Communities: Are Deep Learning Models Worth the Effort?	 | Lukas Moosbrugger,Valentin Seiler,Philipp Wohlgenannt,Sebastian Hegenbart,Sashko Ristov,Peter Kepplinger	 | 本研究提出了一种利用深度学习模型进行能源社区短期负荷预测的方法，并与多种基准模型进行了比较，结果显示在有限训练数据情况下简单的持续性模型更优，而通过迁移学习和增加聚合程度可以显著提升深度学习模型的预测性能，进而带来显著的经济效益。	 | Accurate load forecasting is crucial for predictive control in many energy domain applications, with significant economic and ecological implications. To address these implications, this study provides an extensive benchmark of state-of-the-art deep learning models for short-term load forecasting in energy communities. Namely, LSTM, xLSTM, and Transformers are compared with benchmarks such as KNNs, synthetic load models, and persistence forecasting models. This comparison considers different scales of aggregation (e.g., number of household loads) and varying training data availability (e.g., training data time spans). Further, the impact of transfer learning from synthetic (standard) load profiles and the deep learning model size (i.e., parameter count) is investigated in terms of forecasting error. Implementations are publicly available and other researchers are encouraged to benchmark models using this framework. Additionally, a comprehensive case study, comprising an energy community of 50 households and a battery storage demonstrates the beneficial financial implications of accurate predictions. Key findings of this research include: (1) Simple persistence benchmarks outperform deep learning models for short-term load forecasting when the available training data is limited to six months or less; (2) Pretraining with publicly available synthetic load profiles improves the normalized Mean Absolute Error (nMAE) by an average of 1.28%pt during the first nine months of training data; (3) Increased aggregation significantly enhances the performance of deep learning models relative to persistence benchmarks; (4) Improved load forecasting, with an nMAE reduction of 1.1%pt, translates to an economic benefit of approximately 600EUR per year in an energy community comprising 50 households.	 | 准确的负荷预测对能源领域中的预测控制至关重要，具有重要的经济和生态意义。为应对这些意义，本研究提供了一种最新的深度学习模型基准，用于能源社区的短期负荷预测。具体而言，本文将LSTM、xLSTM和Transformer与KNNs、合成负荷模型和持续性预测模型等基准进行了比较。这种比较考虑了不同的聚合尺度（例如，家庭负荷的数量）和不同的训练数据可用性（例如，训练数据的时间跨度）。此外，本文还探讨了从合成（标准）负荷配置文件迁移学习的影响以及深度学习模型大小（即参数数量）对预测误差的影响。这些实现是公开的，鼓励其他研究人员使用该框架进行模型基准测试。此外，本文还进行了一项全面的案例研究，包括一个由50户家庭组成的能源社区和电池存储，以展示准确预测的有利的经济影响。本文的主要发现包括：（1）当可用的训练数据仅限于六个月或更短时，简单的持续性基准模型在短期负荷预测中优于深度学习模型；（2）使用公开可用的合成负荷配置文件进行预训练，可以提高前九个月训练数据期间的平均归一化绝对误差（nMAE）0.128个百分点；（3）聚合程度的增加显著提高了深度学习模型相对于持续性基准的性能；（4）改进的负荷预测，nMAE减少了1.1个百分点，相当于在一个由50户家庭组成的能源社区中每年约获得600欧元的经济效益。
2501.04997	 | GiNet: Integrating Sequential and Context-Aware Learning for Battery Capacity Prediction	 | Sara Sameer,Wei Zhang,Xin Lou,Qingyu Yan,Terence Goh,Yulin Gao	 | 本文提出了一种名为GiNet的增强门控循环单元Informer网络，通过学习历史测量数据来准确预测电池容量，并展示了其在预测精度上的显著优势，平均绝对误差降低了27%。	 | The surging demand for batteries requires advanced battery management systems, where battery capacity modelling is a key functionality. In this paper, we aim to achieve accurate battery capacity prediction by learning from historical measurements of battery dynamics. We propose GiNet, a gated recurrent units enhanced Informer network, for predicting battery's capacity. The novelty and competitiveness of GiNet lies in its capability of capturing sequential and contextual information from raw battery data and reflecting the battery's complex behaviors with both temporal dynamics and long-term dependencies. We conducted an experimental study based on a publicly available dataset to showcase GiNet's strength of gaining a holistic understanding of battery behavior and predicting battery capacity accurately. GiNet achieves 0.11 mean absolute error for predicting the battery capacity in a sequence of future time slots without knowing the historical battery capacity. It also outperforms the latest algorithms significantly with 27% error reduction on average compared to Informer. The promising results highlight the importance of customized and optimized integration of algorithm and battery knowledge and shed light on other industry applications as well.	 | 随着电池需求的激增，先进的电池管理系统变得至关重要，其中电池容量建模是关键技术之一。本文旨在通过学习电池动力学的历史测量数据，实现对电池容量的准确预测。我们提出了一种名为GiNet的模型，这是一种增强的门控循环单元Informer网络，用于预测电池容量。GiNet的创新性和竞争力在于它能够从原始电池数据中捕捉到序列和上下文信息，并通过时间和长期依赖关系反映电池的复杂行为。基于一个公开可用的数据集，我们进行了一项实验研究，以展示GiNet能够全面理解电池行为并准确预测电池容量的能力。在没有了解历史电池容量的情况下，GiNet在一系列未来时间槽中预测电池容量的平均绝对误差为0.11。与Informer相比，它在平均误差上降低了27%的性能优势。这些令人鼓舞的结果突显了算法和电池知识定制化和优化整合的重要性，并为其他行业的应用提供了启示。
2501.04970	 | Battling the Non-stationarity in Time Series Forecasting via Test-time Adaptation	 | HyunGi Kim,Siwon Kim,Jisoo Mok,Sungroh Yoon	 | 该研究提出了一种针对时间序列预测的测试时适应框架（TSF-TTA），引入了TAFAS方法，能够灵活适应持续变化的测试分布，同时保留预训练的核心语义信息，实验结果表明TAFAS在多种基准数据集上表现出色，尤其在面对显著分布偏移时。	 | Deep Neural Networks have spearheaded remarkable advancements in time series forecasting (TSF), one of the major tasks in time series modeling. Nonetheless, the non-stationarity of time series undermines the reliability of pre-trained source time series forecasters in mission-critical deployment settings. In this study, we introduce a pioneering test-time adaptation framework tailored for TSF (TSF-TTA). TAFAS, the proposed approach to TSF-TTA, flexibly adapts source forecasters to continuously shifting test distributions while preserving the core semantic information learned during pre-training. The novel utilization of partially-observed ground truth and gated calibration module enables proactive, robust, and model-agnostic adaptation of source forecasters. Experiments on diverse benchmark datasets and cutting-edge architectures demonstrate the efficacy and generality of TAFAS, especially in long-term forecasting scenarios that suffer from significant distribution shifts. The code is available at https://github.com/kimanki/TAFAS.	 | 深度神经网络在时间序列预测（TSF）方面取得了显著进展，这是时间序列建模中的主要任务之一。然而，时间序列的非平稳性削弱了预训练源时间序列预测器在关键任务部署环境中的可靠性。在这项研究中，我们提出了一种针对TSF的创新测试时适应框架（TSF-TTA）。我们提出的TSF-TTA适应方法TAFAS能够灵活地适应源预测器以应对持续变化的测试分布，同时保留预训练期间学到的核心语义信息。部分观察到的真实地面真相的新型利用和门控校准模块使源预测器能够实现前瞻性的、稳健的、模型无关的适应。在多种基准数据集和先进架构上的实验表明，TAFAS的有效性和通用性，特别是在经历显著分布偏移的长期预测场景中尤为明显。相关代码可在https://github.com/kimanki/TAFAS获取。
2501.04967	 | Targeted Adversarial Denoising Autoencoders (TADA) for Neural Time Series Filtration	 | Benjamin J. Choi(1),Griffin Milsap(2),Clara A. Scholl(2),Francesco Tenore(2),Mattson Ogg(2) ((1) Harvard John A. Paulson School of Engineering and Applied Sciences, Cambridge, MA, United States of America, (2) Johns Hopkins University Applied Physics Laboratory, Laurel, MD, United States of America)	 | 本文提出了一种由逻辑关联目标对抗去噪自编码器（TADA）驱动的机器学习算法，用于有效过滤脑电图（EEG）时间序列数据，该算法在减少计算需求的同时保持了信号质量，并在EEGdenoiseNet数据集上优于传统信号过滤算法。	 | Current machine learning (ML)-based algorithms for filtering electroencephalography (EEG) time series data face challenges related to cumbersome training times, regularization, and accurate reconstruction. To address these shortcomings, we present an ML filtration algorithm driven by a logistic covariance-targeted adversarial denoising autoencoder (TADA). We hypothesize that the expressivity of a targeted, correlation-driven convolutional autoencoder will enable effective time series filtration while minimizing compute requirements (e.g., runtime, model size). Furthermore, we expect that adversarial training with covariance rescaling will minimize signal degradation. To test this hypothesis, a TADA system prototype was trained and evaluated on the task of removing electromyographic (EMG) noise from EEG data in the EEGdenoiseNet dataset, which includes EMG and EEG data from 67 subjects. The TADA filter surpasses conventional signal filtration algorithms across quantitative metrics (Correlation Coefficient, Temporal RRMSE, Spectral RRMSE), and performs competitively against other deep learning architectures at a reduced model size of less than 400,000 trainable parameters. Further experimentation will be necessary to assess the viability of TADA on a wider range of deployment cases.	 | 当前基于机器学习（ML）的算法在过滤脑电图（EEG）时间序列数据时面临训练时间繁琐、正则化以及准确重构的挑战。为解决这些问题，我们提出了一种由逻辑关联目标对抗去噪自编码器（TADA）驱动的ML过滤算法。我们假设，一个以相关性为导向的目标卷积自编码器的表达能力将能够实现有效的时间序列过滤，同时尽可能减少计算需求（例如，运行时间、模型大小）。此外，我们预期使用相关性缩放的对抗训练将能够最小化信号失真。  为了测试这一假设，我们在EEGdenoiseNet数据集中训练并评估了TADA系统的原型，该数据集包含来自67名受试者的EMG和EEG数据。TADA过滤器在定量指标（相关系数、时域RRMSE、频谱RRMSE）上超越了传统的信号过滤算法，并且在模型参数少于40万的情况下，与其他深度学习架构竞争性地表现良好。进一步的实验将有助于评估TADA在更广泛部署情况下的可行性。
2501.04952	 | Open Problems in Machine Unlearning for AI Safety	 | Fazl Barez,Tingchen Fu,Ameya Prabhu,Stephen Casper,Amartya Sanyal,Adel Bibi,Aidan O'Gara,Robert Kirk,Ben Bucknall,Tim Fist,Luke Ong,Philip Torr,Kwok-Yan Lam,Robert Trager,David Krueger,Sören Mindermann,José Hernandez-Orallo,Mor Geva,Yarin Gal	 | 尽管机器遗忘在隐私和数据删除中显示出潜力，但在关键领域如网络安全中，它可能因难以有效管理和潜在的副作用而受限，因此需要更全面的安全框架来应对人工智能安全挑战。	 | As AI systems become more capable, widely deployed, and increasingly autonomous in critical areas such as cybersecurity, biological research, and healthcare, ensuring their safety and alignment with human values is paramount. Machine unlearning -- the ability to selectively forget or suppress specific types of knowledge -- has shown promise for privacy and data removal tasks, which has been the primary focus of existing research. More recently, its potential application to AI safety has gained attention. In this paper, we identify key limitations that prevent unlearning from serving as a comprehensive solution for AI safety, particularly in managing dual-use knowledge in sensitive domains like cybersecurity and chemical, biological, radiological, and nuclear (CBRN) safety. In these contexts, information can be both beneficial and harmful, and models may combine seemingly harmless information for harmful purposes -- unlearning this information could strongly affect beneficial uses. We provide an overview of inherent constraints and open problems, including the broader side effects of unlearning dangerous knowledge, as well as previously unexplored tensions between unlearning and existing safety mechanisms. Finally, we investigate challenges related to evaluation, robustness, and the preservation of safety features during unlearning. By mapping these limitations and open challenges, we aim to guide future research toward realistic applications of unlearning within a broader AI safety framework, acknowledging its limitations and highlighting areas where alternative approaches may be required.	 | 随着人工智能系统的能力不断增强，部署范围不断扩大，并在关键领域（如网络安全、生物研究和医疗保健）中越来越自主，确保这些系统的安全性和与人类价值观的契合变得至关重要。机器遗忘——能够有选择地忘记或抑制特定类型的知识——已在隐私和数据删除任务中显示出潜力，这些任务一直是现有研究的主要关注点。最近，其在人工智能安全方面的潜在应用引起了更多关注。在本文中，我们识别出机器遗忘难以成为全面解决人工智能安全问题的方法，尤其是在管理网络安全等敏感领域中的双重用途知识方面存在关键限制。在这种背景下，信息既可以是有益的，也可能是有害的，模型可能会将看似无害的信息组合起来用于有害目的——遗忘这些信息可能会显著影响其有益的应用。我们概述了固有的限制和开放问题，包括遗忘危险知识的更广泛的副作用，以及遗忘与现有安全机制之间之前未被探索的紧张关系。最后，我们探讨了与评估、鲁棒性和在遗忘过程中保持安全功能相关的挑战。通过绘制这些限制和开放挑战，我们旨在引导未来的研究朝着在更广泛的人工智能安全框架内实现机器遗忘的现实应用方向发展，同时承认其局限性，并指出需要替代方法的领域。
2501.04940	 | A New Perspective on Privacy Protection in Federated Learning with Granular-Ball Computing	 | Guannan Lai,Yihui Feng,Xin Yang,Xiaoyu Deng,Hao Yu,Shuyin Xia,Guoyin Wang,Tianrui Li	 | 粒度-球体联邦学习（GrBFL）是一种新的联邦学习框架，专门用于图像分类，通过将图像分割成具有最优粗粒度的多个区域并重构为图结构，有效保护隐私并提高效率。与传统方法相比，GrBFL 能够去除冗余信息并保留关键特征，实验结果表明其在多个任务上表现出色。	 | Federated Learning (FL) facilitates collaborative model training while prioritizing privacy by avoiding direct data sharing. However, most existing articles attempt to address challenges within the model's internal parameters and corresponding outputs, while neglecting to solve them at the input level. To address this gap, we propose a novel framework called Granular-Ball Federated Learning (GrBFL) for image classification. GrBFL diverges from traditional methods that rely on the finest-grained input data. Instead, it segments images into multiple regions with optimal coarse granularity, which are then reconstructed into a graph structure. We designed a two-dimensional binary search segmentation algorithm based on variance constraints for GrBFL, which effectively removes redundant information while preserving key representative features. Extensive theoretical analysis and experiments demonstrate that GrBFL not only safeguards privacy and enhances efficiency but also maintains robust utility, consistently outperforming other state-of-the-art FL methods. The code is available at https://github.com/AIGNLAI/GrBFL.	 | 联邦学习（FL）通过避免直接数据共享来促进协作模型训练，同时优先考虑隐私保护。然而，现有的大多数文章主要集中于解决模型内部参数及其相应输出的挑战，而忽略了在输入层面解决问题。为了解决这一差距，我们提出了一种名为粒度-球体联邦学习（GrBFL）的新框架，专门用于图像分类。GrBFL 与传统的依赖于最细粒度输入数据的方法不同，它将图像分割成具有最优粗粒度的多个区域，然后将这些区域重构为图结构。我们为 GrBFL 设计了一种基于方差约束的二维二分搜索分割算法，该算法能够有效去除冗余信息，同时保留关键的代表性特征。广泛的理论分析和实验表明，GrBFL 不仅能够保护隐私和提高效率，还能保持稳健的实用性，并且在与其他最先进的 FL 方法的比较中表现出色。代码已发布在 https://github.com/AIGNLAI/GrBFL。
2501.04916	 | SpecTf: Transformers Enable Data-Driven Imaging Spectroscopy Cloud Detection	 | Jake H. Lee,Michael Kiper,David R. Thompson,Philip G. Brodrick	 | SpecTf 是一种基于深度学习的云检测方法，仅利用光谱信息进行云检测，显著优于现有基准方法，并展示了固有的可解释性和在不同平台和仪器上的通用性。	 | Current and upcoming generations of visible-shortwave infrared (VSWIR) imaging spectrometers promise unprecedented capacity to quantify Earth System processes across the globe. However, reliable cloud screening remains a fundamental challenge for these instruments, where traditional spatial and temporal approaches are limited by cloud variability and limited temporal coverage. The Spectroscopic Transformer (SpecTf) addresses these challenges with a spectroscopy-specific deep learning architecture that performs cloud detection using only spectral information (no spatial or temporal data are required). By treating spectral measurements as sequences rather than image channels, SpecTf learns fundamental physical relationships without relying on spatial context. Our experiments demonstrate that SpecTf significantly outperforms the current baseline approach implemented for the EMIT instrument, and performs comparably with other machine learning methods with orders of magnitude fewer learned parameters. Critically, we demonstrate SpecTf's inherent interpretability through its attention mechanism, revealing physically meaningful spectral features the model has learned. Finally, we present SpecTf's potential for cross-instrument generalization by applying it to a different instrument on a different platform without modifications, opening the door to instrument agnostic data driven algorithms for future imaging spectroscopy tasks.	 | 当前和即将推出的可见-短波红外（VSWIR）成像光谱仪有望以前所未有的能力量化全球地球系统过程。然而，可靠地筛选云仍然是这些仪器面临的基本挑战，其中传统的空间和时间方法受到云变化和有限时间覆盖的限制。光谱变换器（SpecTf）通过一种专门针对光谱学的深度学习架构来解决这些挑战，该架构仅使用光谱信息进行云检测（不需要空间或时间数据）。通过将光谱测量视为序列而不是图像通道，SpecTf 在不依赖空间上下文的情况下学习基本的物理关系。我们的实验表明，SpecTf 显著优于目前为 EMIT 仪器实施的基准方法，并且在参数数量级更少的情况下与其他机器学习方法表现相当。关键的是，我们通过其注意力机制证明了 SpecTf 的固有可解释性，揭示了模型学习的物理上重要的光谱特征。最后，我们展示了 SpecTf 在不同平台上不同仪器上的通用潜力，无需修改即可应用，为未来的成像光谱学任务打开了仪器无关的数据驱动算法的大门。
2501.04897	 | Online Continual Learning: A Systematic Literature Review of Approaches, Challenges, and Benchmarks	 | Seyed Amir Bidaki,Amir Mohammadkhah,Kiyan Rezaee,Faeze Hassani,Sadegh Eskandari,Maziar Salahi,Mohammad M. Ghassemi	 | 该研究进行了首次全面的系统文献综述，分析了81种在线持续学习方法，识别了数百个组件和数据集，并指出了未来研究的方向，包括减少计算开销和提高在资源受限环境中的可扩展性。	 | Online Continual Learning (OCL) is a critical area in machine learning, focusing on enabling models to adapt to evolving data streams in real-time while addressing challenges such as catastrophic forgetting and the stability-plasticity trade-off. This study conducts the first comprehensive Systematic Literature Review (SLR) on OCL, analyzing 81 approaches, extracting over 1,000 features (specific tasks addressed by these approaches), and identifying more than 500 components (sub-models within approaches, including algorithms and tools). We also review 83 datasets spanning applications like image classification, object detection, and multimodal vision-language tasks. Our findings highlight key challenges, including reducing computational overhead, developing domain-agnostic solutions, and improving scalability in resource-constrained environments. Furthermore, we identify promising directions for future research, such as leveraging self-supervised learning for multimodal and sequential data, designing adaptive memory mechanisms that integrate sparse retrieval and generative replay, and creating efficient frameworks for real-world applications with noisy or evolving task boundaries. By providing a rigorous and structured synthesis of the current state of OCL, this review offers a valuable resource for advancing this field and addressing its critical challenges and opportunities. The complete SLR methodology steps and extracted data are publicly available through the provided link: https://github.com/kiyan-rezaee/ Systematic-Literature-Review-on-Online-Continual-Learning	 | 在线持续学习（OCL）是机器学习中的一个重要领域，主要关注使模型能够实时适应不断变化的数据流，同时解决灾难性遗忘和稳定-可塑性权衡等挑战。本研究首次进行了全面的系统文献综述（SLR），分析了81种方法，提取了超过1000个特征（这些方法所解决的具体任务），并识别了超过500个组件（方法中的子模型，包括算法和工具）。我们还回顾了83个数据集，这些数据集涵盖了图像分类、对象检测以及多模态视觉-语言任务等应用领域。我们的研究结果突出了关键挑战，包括减少计算开销、开发领域无关的解决方案以及在资源受限环境中提高可扩展性。此外，我们还指出了未来研究的潜在方向，例如利用自监督学习处理多模态和序列数据、设计结合稀疏检索和生成重演的自适应记忆机制，以及创建适用于实际应用的有效框架，这些框架能够处理有噪声或不断变化的任务边界。通过提供严谨和结构化的OCL当前状态综述，这项综述为推进该领域及其关键挑战和机遇提供了宝贵资源。完整的SLR方法步骤和提取数据已通过提供的链接公开发布：https://github.com/kiyan-rezaee/Systematic-Literature-Review-on-Online-Continual-Learning
2501.04896	 | Quantifying Itch and its Impact on Sleep Using Machine Learning and Radio Signals	 | Michail Ouroutzoglou,Mingmin Zhao,Joshua Hellerstein,Hariharan Rahul,Asima Badic,Brian S. Kim,Dina Katabi	 | 本文介绍了一种利用家用无线电设备和人工智能技术非侵入性地监测慢性瘙痒患者搔抓行为及其对睡眠质量影响的方法，研究结果显示该技术可行且准确，并揭示了搔抓与低睡眠质量之间的显著关联。	 | Chronic itch affects 13% of the US population, is highly debilitating, and underlies many medical conditions. A major challenge in clinical care and new therapeutics development is the lack of an objective measure for quantifying itch, leading to reliance on subjective measures like patients' self-assessment of itch severity. In this paper, we show that a home radio device paired with artificial intelligence (AI) can concurrently capture scratching and evaluate its impact on sleep quality by analyzing radio signals bouncing in the environment. The device eliminates the need for wearable sensors or skin contact, enabling monitoring of chronic itch over extended periods at home without burdening patients or interfering with their skin condition. To validate the technology, we conducted an observational clinical study of chronic pruritus patients, monitored at home for one month using both the radio device and an infrared camera. Comparing the output of the device to ground truth data from the camera demonstrates its feasibility and accuracy (ROC AUC = 0.997, sensitivity = 0.825, specificity = 0.997). The results reveal a significant correlation between scratching and low sleep quality, manifested as a reduction in sleep efficiency (R = 0.6, p < 0.001) and an increase in sleep latency (R = 0.68, p < 0.001). Our study underscores the potential of passive, long-term, at-home monitoring of chronic scratching and its sleep implications, offering a valuable tool for both clinical care of chronic itch patients and pharmaceutical clinical trials.	 | 慢性瘙痒影响美国人口的13%，具有极高的致残性，并且是许多医学条件的基础。在临床护理和新药物开发中面临的主要挑战是没有客观的瘙痒测量标准，导致依赖患者自我评估瘙痒严重程度等主观指标。在这篇文章中，我们展示了将家用无线电设备与人工智能（AI）结合使用，可以同时捕捉搔抓行为，并通过分析环境中的无线电信号反弹来评估其对睡眠质量的影响。该设备消除了穿戴传感器或皮肤接触的必要性，使得患者能够在家中长时间监测慢性瘙痒，而不会增加他们的负担或影响他们的皮肤状况。为了验证这项技术，我们对慢性瘙痒患者进行了为期一个月的居家观测临床研究，使用无线设备和红外摄像头进行监测。将设备输出与摄像头的真实数据进行比较，证明了其可行性和准确性（ROC AUC = 0.997，灵敏度 = 0.825，特异性 = 0.997）。研究结果揭示了搔抓与低睡眠质量之间存在显著相关性，表现为睡眠效率的降低（R = 0.6，p < 0.001）和睡眠潜伏期的增加（R = 0.68，p < 0.001）。我们的研究强调了被动、长时间居家监测慢性搔抓及其睡眠影响的潜力，为慢性瘙痒患者的临床护理以及制药临床试验提供了有价值的工具。
2501.04894	 | A Look into How Machine Learning is Reshaping Engineering Models: the Rise of Analysis Paralysis, Optimal yet Infeasible Solutions, and the Inevitable Rashomon Paradox	 | MZ Naser	 | 本文通过结构工程的视角探讨了机器学习在土木工程中应用时与传统规范之间的哲学紧张关系，并指出了采用机器学习可能带来的三个主要悖论：分析瘫痪、不可行的解决方案和拉氏门，强调了工程实践和教育中需要进行认识论上的调整。	 | The widespread acceptance of empirically derived codal provisions and equations in civil engineering stands in stark contrast to the skepticism facing machine learning (ML) models, despite their shared statistical foundations. This paper examines this philosophical tension through the lens of structural engineering and explores how integrating ML challenges traditional engineering philosophies and professional identities. Recent efforts have documented how ML enhances predictive accuracy, optimizes designs, and analyzes complex behaviors. However, one might also raise concerns about the diminishing role of human intuition and the interpretability of algorithms. To showcase this rarely explored front, this paper presents how ML can be successfully integrated into various engineering problems by means of formulation via deduction, induction, and abduction. Then, this paper identifies three principal paradoxes that could arise when adopting ML: analysis paralysis (increased prediction accuracy leading to a reduced understanding of physical mechanisms), infeasible solutions (optimization resulting in unconventional designs that challenge engineering intuition), and the Rashomon effect (where contradictions in explainability methods and physics arise). This paper concludes by addressing these paradoxes and arguing the need to rethink epistemological shifts in engineering and engineering education and methodologies to harmonize traditional principles with ML.	 | 在土木工程领域，基于经验的规范条文和方程得到了广泛接受，而相比之下，机器学习（ML）模型则受到了怀疑，尽管它们具有相同的统计基础。本文通过结构工程的视角探讨了这种哲学上的紧张关系，并探讨了如何将ML纳入工程实践对传统工程哲学和专业身份的挑战。最近的努力已经证明了ML如何提高预测准确性、优化设计和分析复杂行为。然而，人们也可能担心人类直觉作用的减小以及算法的可解释性。为了展示这一鲜少探讨的领域，本文将展示如何通过演绎、归纳和 abduction 方法成功地将ML整合到各种工程问题中。接着，本文识别出在采用ML时可能出现的三个主要悖论：分析瘫痪（预测准确性的提高导致对物理机制理解的减少）、不可行的解决方案（优化导致的非传统设计挑战工程直觉），以及拉氏门（不同解释方法和物理现象的矛盾）。最后，本文讨论了这些悖论，并强调需要重新思考工程及工程教育和方法中的认识论转变，以使传统原则与ML相协调。
2501.04879	 | Multilinear Tensor Low-Rank Approximation for Policy-Gradient Methods in Reinforcement Learning	 | Sergio Rozada,Hoi-To Wai,Antonio G. Marques	 | 本文提出了一种基于多线性映射的张量低秩策略方法，以高效估计强化学习中的策略参数，并通过理论保证和实验验证了该方法在降低计算和样本复杂性的同时，能够实现与传统方法相当甚至更好的性能。	 | Reinforcement learning (RL) aims to estimate the action to take given a (time-varying) state, with the goal of maximizing a cumulative reward function. Predominantly, there are two families of algorithms to solve RL problems: value-based and policy-based methods, with the latter designed to learn a probabilistic parametric policy from states to actions. Most contemporary approaches implement this policy using a neural network (NN). However, NNs usually face issues related to convergence, architectural suitability, hyper-parameter selection, and underutilization of the redundancies of the state-action representations (e.g. locally similar states). This paper postulates multi-linear mappings to efficiently estimate the parameters of the RL policy. More precisely, we leverage the PARAFAC decomposition to design tensor low-rank policies. The key idea involves collecting the policy parameters into a tensor and leveraging tensor-completion techniques to enforce low rank. We establish theoretical guarantees of the proposed methods for various policy classes and validate their efficacy through numerical experiments. Specifically, we demonstrate that tensor low-rank policy models reduce computational and sample complexities in comparison to NN models while achieving similar rewards.	 | 强化学习（RL）旨在根据给定的（时间变化的）状态估计采取的动作，目标是最大化累积奖励函数。主要存在两种解决RL问题的算法家族：基于值的方法和基于策略的方法，后者旨在从状态到动作学习一种概率参数化策略。大多数当代方法使用神经网络（NN）来实现这一策略。然而，神经网络通常面临收敛性问题、架构适应性、超参数选择和状态-动作表示冗余性（例如局部相似状态）的未充分利用等问题。本文提出多线性映射来高效估计RL策略的参数。更具体地说，我们利用PARAFAC分解设计张量低秩策略。关键思路是将策略参数集合成一个张量，并利用张量补全技术来保证低秩。我们为各种策略类别建立了所提出的算法的理论保证，并通过数值实验验证其有效性。具体而言，我们证明张量低秩策略模型在减少计算和样本复杂性的同时，能够实现类似甚至更好的奖励。
2501.04826	 | Intelligent Gradient Boosting Algorithms for Estimating Strength of Modified Subgrade Soil	 | Ismail B. Mustapha,Muyideen Abdulkareem,Shafaatunnur Hasan,Abideen Ganiyu,Hatem Nabus,Jin Chai Lee	 | 本研究利用分类增强（CatBoost）、极端梯度增强（XGBoost）和支持向量回归（SVR）三种机器学习技术，评估改良水化石灰活性稻壳灰（HARSH）下铺土壤特性的预测性能，结果表明XGBoost在预测CBR、UCS和R方面表现最佳，而SVR和CatBoost在某些特性上的预测也具有竞争力。	 | The performance of pavement under loading depends on the strength of the subgrade. However, experimental estimation of properties of pavement strengths such as California bearing ratio (CBR), unconfined compressive strength (UCS) and resistance value (R) are often tedious, time-consuming and costly, thereby inspiring a growing interest in machine learning based tools which are simple, cheap and fast alternatives. Thus, the potential application of two boosting techniques; categorical boosting (CatBoost) and extreme gradient boosting (XGBoost) and support vector regression (SVR), is similarly explored in this study for estimation of properties of subgrade soil modified with hydrated lime activated rice husk ash (HARSH). Using 121 experimental data samples of varying proportions of HARSH, plastic limit, liquid limit, plasticity index, clay activity, optimum moisture content, and maximum dry density as input for CBR, UCS and R estimation, four evaluation metrics namely coefficient of determination (R2), root mean squared error (RMSE), mean absolute error (MAE) and mean absolute percentage error (MAPE) are used to evaluate the models' performance. The results indicate that XGBoost outperformed CatBoost and SVR in estimating these properties, yielding R2 of 0.9994, 0.9995 and 0.9999 in estimating the CBR, UCS and R respectively. Also, SVR outperformed CatBoost in estimating the CBR and R with R2 of 0.9997 respectively. On the other hand, CatBoost outperformed SVR in estimating the UCS with R2 of 0.9994. Feature sensitivity analysis shows that the three machine learning techniques are unanimous that increasing HARSH proportion lead to values of the estimated properties respectively. A comparison with previous results also shows superiority of XGBoost in estimating subgrade properties.	 | 加载下铺性能取决于下铺层强度。然而，通过实验估算铺面强度的属性，如加州承载比（CBR）、无侧限抗压强度（UCS）和抗阻值（R），往往耗时、费力且成本高昂，因此激发了对基于机器学习的工具的兴趣，这些工具简单、经济且快速。因此，本研究探讨了两种增强技术——分类增强（CatBoost）和极端梯度增强（XGBoost）以及支持向量回归（SVR）应用于改良水化石灰活性稻壳灰（HARSH）下铺土壤特性估计的潜在应用。  使用121个不同比例HARSH的实验数据样本，以及塑限、液限、塑性指数、粘土活性、最佳含水量和最大干密度作为输入，来估计CBR、UCS和R，本研究使用四个评价指标：决定系数（R²）、均方根误差（RMSE）、平均绝对误差（MAE）和平均绝对百分比误差（MAPE）来评估模型的性能。结果表明，XGBoost在估计这些特性方面优于CatBoost和SVR，分别在估算CBR、UCS和R时获得了R²值为0.9994、0.9995和0.9999。同时，SVR在估算CBR和R时的表现优于CatBoost，分别获得了R²值为0.9997。另一方面，CatBoost在估算UCS时的表现优于SVR，其R²值为0.9994。特征敏感性分析表明，三种机器学习技术一致认为HARSH比例增加会使估计特性值增加。与以前的结果相比，XGBoost在估计下铺土特性方面的优越性也得到了体现。
2501.04817	 | Decentralised Resource Sharing in TinyML: Wireless Bilayer Gossip Parallel SGD for Collaborative Learning	 | Ziyuan Bao,Eiman Kanjo,Soumya Banerjee,Hasib-Al Rashid,Tinoosh Mohsenin	 | 本文提出了一种二层Gossip去中心化并行随机梯度下降（GD PSGD）框架，旨在解决边缘设备上部署去中心化联邦学习的挑战，如间歇性连接和有限通信范围。该框架通过分层通信结构和Gossip协议实现了高效模型聚合，在资源受限环境下有效支持了联邦学习任务，尤其是在非同质分布数据集上的表现。	 | With the growing computational capabilities of microcontroller units (MCUs), edge devices can now support machine learning models. However, deploying decentralised federated learning (DFL) on such devices presents key challenges, including intermittent connectivity, limited communication range, and dynamic network topologies. This paper proposes a novel framework, bilayer Gossip Decentralised Parallel Stochastic Gradient Descent (GD PSGD), designed to address these issues in resource-constrained environments. The framework incorporates a hierarchical communication structure using Distributed Kmeans (DKmeans) clustering for geographic grouping and a gossip protocol for efficient model aggregation across two layers: intra-cluster and inter-cluster. We evaluate the framework's performance against the Centralised Federated Learning (CFL) baseline using the MCUNet model on the CIFAR-10 dataset under IID and Non-IID conditions. Results demonstrate that the proposed method achieves comparable accuracy to CFL on IID datasets, requiring only 1.8 additional rounds for convergence. On Non-IID datasets, the accuracy loss remains under 8\% for moderate data imbalance. These findings highlight the framework's potential to support scalable and privacy-preserving learning on edge devices with minimal performance trade-offs.	 | 随着微控制器单元（MCUs）的计算能力不断提升，边缘设备现在可以支持机器学习模型。然而，在这些设备上部署去中心化的联邦学习（DFL）也带来了关键挑战，包括间歇性连接、有限的通信范围以及动态网络拓扑。本文提出了一个新颖的框架，二层Gossip去中心化并行随机梯度下降（GD PSGD），旨在在资源受限的环境中解决这些问题。该框架采用分层通信结构，使用分布式K均值（DKmeans）聚类进行地理分组，并采用Gossip协议在两层（簇内和簇间）实现高效模型聚合。我们使用MCUNet模型在CIFAR-10数据集上，分别在同质分布（IID）和非同质分布（Non-IID）条件下，将该框架的性能与中央化联邦学习（CFL）基线进行了评估。结果表明，该方法在同质分布数据集上的准确率与CFL相近，只需额外1.8个迭代轮次即可收敛。在非同质分布数据集上，对于中等程度的数据不平衡，准确率损失也保持在8%以下。这些发现表明，该框架在边缘设备上支持可扩展且隐私保护的学习具有潜力，且几乎没有性能折衷。
2501.04816	 | Probabilistic Skip Connections for Deterministic Uncertainty Quantification in Deep Neural Networks	 | Felix Jimenez,Matthias Katzfuss	 | 该研究提出了一种无需重新训练网络的方法，通过识别现有中间层并基于其特征向量构建概率模型（PSC），以进行不确定性量化和异常分布检测，该方法性能与现有需要修改训练过程的方法相当甚至更好。	 | Deterministic uncertainty quantification (UQ) in deep learning aims to estimate uncertainty with a single pass through a network by leveraging outputs from the network's feature extractor. Existing methods require that the feature extractor be both sensitive and smooth, ensuring meaningful input changes produce meaningful changes in feature vectors. Smoothness enables generalization, while sensitivity prevents feature collapse, where distinct inputs are mapped to identical feature vectors. To meet these requirements, current deterministic methods often retrain networks with spectral normalization. Instead of modifying training, we propose using measures of neural collapse to identify an existing intermediate layer that is both sensitive and smooth. We then fit a probabilistic model to the feature vector of this intermediate layer, which we call a probabilistic skip connection (PSC). Through empirical analysis, we explore the impact of spectral normalization on neural collapse and demonstrate that PSCs can effectively disentangle aleatoric and epistemic uncertainty. Additionally, we show that PSCs achieve uncertainty quantification and out-of-distribution (OOD) detection performance that matches or exceeds existing single-pass methods requiring training modifications. By retrofitting existing models, PSCs enable high-quality UQ and OOD capabilities without retraining.	 | 确定性不确定性量化（UQ）在深度学习中的目标是在网络中通过单次通过，利用网络特征提取器的输出来估计不确定性。现有的方法要求特征提取器既敏感又平滑，以确保有意义的输入变化会产生有意义的特征向量变化。平滑性有助于泛化，而敏感性则防止特征坍缩，即不同的输入被映射到相同的特征向量。为了满足这些要求，当前的确定性方法通常通过光谱标准化重新训练网络。我们不修改训练过程，而是提出使用神经坍缩的度量来识别一个现有的中间层，该中间层既敏感又平滑。然后，我们根据这个中间层的特征向量拟合一个概率模型，称之为概率跳过连接（PSC）。  通过实证分析，我们探讨了光谱标准化对神经坍缩的影响，并展示了PSC可以有效分离隐含不确定性与先验不确定性。此外，我们证明PSC能够实现与或超过现有需要修改训练过程的单次通过方法的不确定性量化和异常分布（OOD）检测性能。通过重新利用现有模型，PSC能够在无需重新训练的情况下提供高质量的UQ和OOD能力。
2501.04811	 | Fast, Fine-Grained Equivalence Checking for Neural Decompilers	 | Luke Dramko,Claire Le Goues,Edward J. Schwartz	 | 该研究提出了一种名为codealign的新技术，用以评估神经反编译器的效果，通过定义指令级的代码等价关系，展示神经反编译器预测的正确性，比现有方法更加详细和准确。	 | Neural decompilers are machine learning models that reconstruct the source code from an executable program. Critical to the lifecycle of any machine learning model is an evaluation of its effectiveness. However, existing techniques for evaluating neural decompilation models have substantial weaknesses, especially when it comes to showing the correctness of the neural decompiler's predictions. To address this, we introduce codealign, a novel instruction-level code equivalence technique designed for neural decompilers. We provide a formal definition of a relation between equivalent instructions, which we term an equivalence alignment. We show how codealign generates equivalence alignments, then evaluate codealign by comparing it with symbolic execution. Finally, we show how the information codealign provides-which parts of the functions are equivalent and how well the variable names match-is substantially more detailed than existing state-of-the-art evaluation metrics, which report unitless numbers measuring similarity.	 | 神经反编译器是机器学习模型，可以从可执行程序中重建源代码。任何机器学习模型生命周期的关键在于对其效果的评估。然而，现有用于评估神经反编译器模型的技术存在显著的不足，特别是在展示神经反编译器预测正确性方面。为解决这一问题，我们引入了codealign，这是一种专为神经反编译器设计的新颖的指令级代码等价技术。我们提供了等价指令之间关系的正式定义，我们称之为等价对齐。我们展示了codealign如何生成等价对齐，然后通过与符号执行进行比较来评估codealign。最后，我们展示了codealign提供的信息——哪些函数部分是等价的以及变量名匹配程度——比现有最先进的评估指标更为详细，后者仅报告衡量相似性的无单位数字。
2501.05450	 | Decentralized Diffusion Models	 | David McAllister,Matthew Tancik,Jiaming Song,Angjoo Kanazawa	 | 该研究提出了一种去中心化的扩散模型框架，通过在独立的集群或数据中心之间分散训练过程，减少了对高带宽集中式网络架构的依赖，从而降低了基础设施成本并提高了对局部GPU故障的抗性。与传统的中心化训练方法相比，该去中心化方法在多个“计算岛屿”上分散了训练负担，并在ImageNet和LAION美学数据集上取得了更好的性能。	 | Large-scale AI model training divides work across thousands of GPUs, then synchronizes gradients across them at each step. This incurs a significant network burden that only centralized, monolithic clusters can support, driving up infrastructure costs and straining power systems. We propose Decentralized Diffusion Models, a scalable framework for distributing diffusion model training across independent clusters or datacenters by eliminating the dependence on a centralized, high-bandwidth networking fabric. Our method trains a set of expert diffusion models over partitions of the dataset, each in full isolation from one another. At inference time, the experts ensemble through a lightweight router. We show that the ensemble collectively optimizes the same objective as a single model trained over the whole dataset. This means we can divide the training burden among a number of "compute islands," lowering infrastructure costs and improving resilience to localized GPU failures. Decentralized diffusion models empower researchers to take advantage of smaller, more cost-effective and more readily available compute like on-demand GPU nodes rather than central integrated systems. We conduct extensive experiments on ImageNet and LAION Aesthetics, showing that decentralized diffusion models FLOP-for-FLOP outperform standard diffusion models. We finally scale our approach to 24 billion parameters, demonstrating that high-quality diffusion models can now be trained with just eight individual GPU nodes in less than a week.	 | 以下是该摘要的中文翻译：  大型AI模型训练将工作分配给成千上万的GPU，然后在每一步中同步梯度。这产生了显著的网络负担，只有集中化的大型集群才能支持，从而增加了基础设施成本并加重了电力系统的压力。我们提出了一种去中心化的扩散模型框架，通过消除对高带宽集中式网络架构的依赖，可以在独立的集群或数据中心之间分散扩散模型的训练。我们的方法在数据集的不同分区上训练一组专家扩散模型，每个模型彼此完全隔离。在推理时，专家模型通过一个轻量级路由器进行联合。我们证明，联合后的专家模型集体优化了与在完整数据集上训练的单个模型相同的目标。这意味着我们可以将训练负担分散到多个“计算岛屿”上，降低基础设施成本并提高对局部GPU故障的抗性。去中心化的扩散模型使研究者能够利用更小、成本更低且更易于获取的计算资源，如按需GPU节点，而非集中式的集成系统。我们在ImageNet和LAION美学数据集上进行了广泛的实验，证明去中心化的扩散模型在FLOP对FLOP的基础上优于标准的扩散模型。最后，我们将这种方法扩展到240亿参数，证明现在只需八个单独的GPU节点即可在不到一周的时间内训练出高质量的扩散模型。
2501.05445	 | Consistent Flow Distillation for Text-to-3D Generation	 | Runjie Yan,Yinbo Chen,Xiaolong Wang	 | Consistent Flow Distillation (CFD) improves text-to-3D generation by ensuring consistent multi-view Gaussian noise on 3D objects, which guides the sampling process more effectively than traditional Score Distillation Sampling (SDS). This results in better visual quality and diversity in 3D generation compared to previous methods.	 | Score Distillation Sampling (SDS) has made significant strides in distilling image-generative models for 3D generation. However, its maximum-likelihood-seeking behavior often leads to degraded visual quality and diversity, limiting its effectiveness in 3D applications. In this work, we propose Consistent Flow Distillation (CFD), which addresses these limitations. We begin by leveraging the gradient of the diffusion ODE or SDE sampling process to guide the 3D generation. From the gradient-based sampling perspective, we find that the consistency of 2D image flows across different viewpoints is important for high-quality 3D generation. To achieve this, we introduce multi-view consistent Gaussian noise on the 3D object, which can be rendered from various viewpoints to compute the flow gradient. Our experiments demonstrate that CFD, through consistent flows, significantly outperforms previous methods in text-to-3D generation.	 | 分数蒸馏采样（Score Distillation Sampling，SDS）在将图像生成模型精简以用于3D生成方面取得了显著进展。然而，其最大似然性追求的行为往往会导致视觉质量下降和多样性降低，限制了其在3D应用中的效果。在这项工作中，我们提出了一致流蒸馏（Consistent Flow Distillation，CFD），以解决这些限制。我们首先利用扩散ODE或SDE采样过程的梯度来指导3D生成。从基于梯度的采样角度来看，我们发现不同视角下2D图像流的一致性对于高质量3D生成至关重要。为了实现这一点，我们在3D物体上引入了一致的多视角高斯噪声，可以从不同视角渲染以计算流的梯度。我们的实验表明，通过一致的流，CFD在文本到3D生成方面显著优于以前的方法。
2501.05439	 | From Simple to Complex Skills: The Case of In-Hand Object Reorientation	 | Haozhi Qi,Brent Yi,Mike Lambeta,Yi Ma,Roberto Calandra,Jitendra Malik	 | 该研究提出了一种利用低级技能解决复杂任务的分层策略，该策略能够在模拟和现实世界中实现物体再定向，并对分布外变化具有鲁棒性。此外，该系统还包括一种通用的对象姿态估计器，结合本体感觉信息、低级技能预测和控制误差来估计动态变化中的对象姿态。	 | Learning policies in simulation and transferring them to the real world has become a promising approach in dexterous manipulation. However, bridging the sim-to-real gap for each new task requires substantial human effort, such as careful reward engineering, hyperparameter tuning, and system identification. In this work, we present a system that leverages low-level skills to address these challenges for more complex tasks. Specifically, we introduce a hierarchical policy for in-hand object reorientation based on previously acquired rotation skills. This hierarchical policy learns to select which low-level skill to execute based on feedback from both the environment and the low-level skill policies themselves. Compared to learning from scratch, the hierarchical policy is more robust to out-of-distribution changes and transfers easily from simulation to real-world environments. Additionally, we propose a generalizable object pose estimator that uses proprioceptive information, low-level skill predictions, and control errors as inputs to estimate the object pose over time. We demonstrate that our system can reorient objects, including symmetrical and textureless ones, to a desired pose.	 | 在模拟中学习策略并将这些策略转移到现实世界已成为灵巧操作领域的一个有前途的方法。然而，为每个新任务跨越仿真实验室之间的鸿沟需要大量的手工努力，例如仔细的设计奖励机制、超参数调优和系统识别。在本工作中，我们提出了一种系统，该系统利用低级技能来解决更复杂任务中的这些挑战。具体而言，我们基于之前获得的旋转技能，引入了一种分层策略，用于手内物体的再定向。这种分层策略能够在环境反馈和低级技能策略本身提供的反馈的基础上学习选择执行哪些低级技能。相比于从头开始学习，分层策略对于分布外变化更具鲁棒性，并且可以从模拟环境轻松转移到现实世界环境。此外，我们还提出了一种通用的对象姿态估计器，该估计器使用本体感觉信息、低级技能预测和控制误差作为输入，以估计随时间变化的对象姿态。我们证明了该系统可以将包括对称和无纹理的对象再定向到所需姿态。
2501.05425	 | Entangled Mean Estimation in High-Dimensions	 | Ilias Diakonikolas,Daniel M. Kane,Sihan Liu,Thanasis Pittas	 | 该研究探讨了在高维子信号模型下估计公共均值的问题，设计了一种计算高效的算法，实现了信息论意义上的近最优误差，误差率为 \(f(α, N) + \sqrt{D/(αN)}\)，其中 \(f(α, N)\) 是一维问题的误差。算法通过迭代细化策略和拒绝采样程序逐步学习更准确的均值估计，并采用列表可解码学习等子程序来处理偏差问题。	 | We study the task of high-dimensional entangled mean estimation in the subset-of-signals model. Specifically, given $N$ independent random points $x_1,\ldots,x_N$ in $\mathbb{R}^D$ and a parameter $α\in (0, 1)$ such that each $x_i$ is drawn from a Gaussian with mean $μ$ and unknown covariance, and an unknown $α$-fraction of the points have identity-bounded covariances, the goal is to estimate the common mean $μ$. The one-dimensional version of this task has received significant attention in theoretical computer science and statistics over the past decades. Recent work [LY20; CV24] has given near-optimal upper and lower bounds for the one-dimensional setting. On the other hand, our understanding of even the information-theoretic aspects of the multivariate setting has remained limited.   In this work, we design a computationally efficient algorithm achieving an information-theoretically near-optimal error. Specifically, we show that the optimal error (up to polylogarithmic factors) is $f(α,N) + \sqrt{D/(αN)}$, where the term $f(α,N)$ is the error of the one-dimensional problem and the second term is the sub-Gaussian error rate. Our algorithmic approach employs an iterative refinement strategy, whereby we progressively learn more accurate approximations $\hat μ$ to $μ$. This is achieved via a novel rejection sampling procedure that removes points significantly deviating from $\hat μ$, as an attempt to filter out unusually noisy samples. A complication that arises is that rejection sampling introduces bias in the distribution of the remaining points. To address this issue, we perform a careful analysis of the bias, develop an iterative dimension-reduction strategy, and employ a novel subroutine inspired by list-decodable learning that leverages the one-dimensional result.	 | 我们研究了子信号模型下的高维纠缠均值估计问题。具体来说，给定 $N$ 个独立的随机点 $x_1, \ldots, x_N$ 在 $\mathbb{R}^D$ 中，以及一个参数 $α \in (0, 1)$，使得每个 $x_i$ 都是从均值为 $μ$、未知协方差的高斯分布中抽取的，且有未知的 $α$ 分数的点具有单位边界协方差，目标是估计公共均值 $μ$。一维版本的这一任务在过去几十年中在理论计算机科学和统计学中得到了广泛关注。最近的研究 [LY20; CV24] 给出了近最优的上界和下界。另一方面，我们对多变量设置的信息论方面的理解仍然有限。在这项工作中，我们设计了一个计算效率高的算法，实现了信息论意义上的近最优误差。具体来说，我们证明了最优误差（除了对数因子外）为 $f(α, N) + \sqrt{D/(αN)}$，其中 $f(α, N)$ 是一维问题的误差，第二项是亚高斯误差率。我们的算法采用了一种迭代细化策略，逐步学习更准确的 $μ$ 的近似值 $\hat{μ}$。这通过一种新颖的拒绝采样程序实现，该程序删除了与 $\hat{μ}$ 显著偏离的点，试图过滤掉异常噪声样本。一个出现的复杂问题是拒绝采样会引入剩余点分布的偏差。为了解决这一问题，我们仔细分析了偏差，开发了一种迭代降维策略，并采用了由一维结果启发的新型子程序，该子程序利用了列表可解码学习。
2501.05423	 | Using LLMs to Infer Non-Binary COVID-19 Sentiments of Chinese Micro-bloggers	 | Jerry Chongyi Hu,Mohammed Shahid Modi,Boleslaw K. Szymanski	 | 该研究通过分析新冠肺炎疫情期间中国微博平台上的情绪变化，探讨了公众情绪如何在公共卫生危机中转变，并揭示了社会事件和政府行动对公众意见的影响，填补了中国平台情绪分析的空白。	 | Studying public sentiment during crises is crucial for understanding how opinions and sentiments shift, resulting in polarized societies. We study Weibo, the most popular microblogging site in China, using posts made during the outbreak of the COVID-19 crisis. The study period includes the pre-COVID-19 stage, the outbreak stage, and the early stage of epidemic prevention. We use Llama 3 8B, a Large Language Model, to analyze users' sentiments on the platform by classifying them into positive, negative, sarcastic, and neutral categories. Analyzing sentiment shifts on Weibo provides insights into how social events and government actions influence public opinion. This study contributes to understanding the dynamics of social sentiments during health crises, fulfilling a gap in sentiment analysis for Chinese platforms. By examining these dynamics, we aim to offer valuable perspectives on digital communication's role in shaping society's responses during unprecedented global challenges.	 | 在危机期间研究公众情绪对于理解意见和情绪如何转变、导致社会极化具有重要意义。我们通过分析新冠肺炎疫情期间中国最流行的微博平台上的发布内容，研究了这一问题。研究时间段包括新冠肺炎疫情之前、疫情爆发期以及疫情初期的防控阶段。我们利用Llama 3 8B（一种大型语言模型）将用户在平台上的情绪分类为积极、消极、讽刺和中性类别，以此分析情绪的变化。分析微博上的情绪变化为我们提供了社会事件和政府行动如何影响公众意见的洞察。本研究有助于理解公共卫生危机期间社会情绪的动态，填补了对于中国平台情绪分析的空白。通过研究这些动态，我们旨在提供关于数字通信在塑造社会对前所未有的全球挑战的回应中所扮演角色的宝贵视角。
2501.05409	 | A Novel Pathology Foundation Model by Mayo Clinic, Charit\'e, and Aignostics	 | Maximilian Alber,Stephan Tietz,Jonas Dippel,Timo Milbich,Timothée Lesort,Panos Korfiatis,Moritz Krügener,Beatriz Perez Cancer,Neelay Shah,Alexander Möllers,Philipp Seegerer,Alexandra Carpen-Amarie,Kai Standvoss,Gabriel Dernbach,Edwin de Jong,Simon Schallenberg,Andreas Kunft,Helmut Hoffer von Ankershoffen,Gavin Schaeferle,Patrick Duffy,Matt Redlon,Philipp Jurmeister,David Horst,Lukas Ruff,Klaus-Robert Müller,Frederick Klauschen,Andrew Norgan	 | 研究人员介绍了一种基于RudolfV方法的新视觉基础模型，该模型在来自两家医疗机构的120万张组织病理学图像上训练，尽管参数量和训练数据规模不是最大，但在21个公开基准数据集上达到了最先进的性能。	 | Recent advances in digital pathology have demonstrated the effectiveness of foundation models across diverse applications. In this report, we present a novel vision foundation model based on the RudolfV approach. Our model was trained on a dataset comprising 1.2 million histopathology whole slide images, collected from two medical institutions: Mayo Clinic and Charité - Universtätsmedizin Berlin. Comprehensive evaluations show that our model achieves state-of-the-art performance across twenty-one public benchmark datasets, even though it is neither the largest model by parameter count nor by training dataset size.	 | 近年来，数字病理学领域的进展展示了基础模型在多种应用中的有效性。在此报告中，我们介绍了一种基于RudolfV方法的新视觉基础模型。该模型在来自两家医疗机构（梅奥诊所和Charité - 布鲁塞尔医科大学）的120万张组织病理学全切片图像数据集上进行了训练。全面的评估结果显示，尽管该模型的参数量和训练数据集规模都不是最大的，但它的性能在二十一个公开基准数据集上达到了最先进的水平。
2501.05408	 | TimeRL: Efficient Deep Reinforcement Learning with Polyhedral Dependence Graphs	 | Pedro F. Silvestre,Peter Pietzuch	 | TimeRL系统结合了即时执行的动态性和基于图形执行的优化与调度，通过引入递归张量的声明式编程模型和多面体依赖图（PDG）来优化深度强化学习（DRL）算法，从而显著提升了执行效率并减少了内存使用。	 | Modern deep learning (DL) workloads increasingly use complex deep reinforcement learning (DRL) algorithms that generate training data within the learning loop. This results in programs with several nested loops and dynamic data dependencies between tensors. While DL systems with eager execution support such dynamism, they lack the optimizations and smart scheduling of graph-based execution. Graph-based execution, however, cannot express dynamic tensor shapes, instead requiring the use of multiple static subgraphs. Either execution model for DRL thus leads to redundant computation, reduced parallelism, and less efficient memory management.   We describe TimeRL, a system for executing dynamic DRL programs that combines the dynamism of eager execution with the whole-program optimizations and scheduling of graph-based execution. TimeRL achieves this by introducing the declarative programming model of recurrent tensors, which allows users to define dynamic dependencies as intuitive recurrence equations. TimeRL translates recurrent tensors into a polyhedral dependence graph (PDG) with dynamic dependencies as symbolic expressions. Through simple PDG transformations, TimeRL applies whole-program optimizations, such as automatic vectorization, incrementalization, and operator fusion. The PDG also allows for the computation of an efficient program-wide execution schedule, which decides on buffer deallocations, buffer donations, and GPU/CPU memory swapping. We show that TimeRL executes current DRL algorithms up to 47$\times$ faster than existing DRL systems, while using 16$\times$ less GPU peak memory.	 | 现代深度学习（DL）工作负载越来越多地使用复杂的深度强化学习（DRL）算法，在学习循环中生成训练数据。这导致程序中存在多个嵌套循环和张量之间的动态数据依赖。虽然具备即时执行功能的DL系统支持这种动态性，但它们缺乏基于图形执行的优化和智能调度。然而，基于图形执行无法表达动态的张量形状，而是需要使用多个静态子图。要么执行模型对于DRL都会导致冗余计算、减少并行性和更不高效的内存管理。我们描述了TimeRL系统，这是一个结合了即时执行的动态性和基于图形执行的全程序优化和调度的系统。TimeRL通过引入递归张量的声明式编程模型来实现这一点，该模型允许用户通过直观的递归方程定义动态依赖关系。TimeRL将递归张量转换为多面体依赖图（PDG），其中动态依赖关系作为符号表达式。通过简单的PDG变换，TimeRL应用全程序优化，如自动向量化、增量化和操作符融合。PDG还允许计算一个高效的程序级执行调度，决定缓冲区回收、缓冲区捐赠以及GPU/CPU内存互换。我们展示了与现有DRL系统相比，TimeRL在执行当前DRL算法时快了最多47倍，同时使用了16倍少的GPU峰值内存。
2501.05387	 | Integrating Explainable AI for Effective Malware Detection in Encrypted Network Traffic	 | Sileshi Nibret Zeleke,Amsalu Fentie Jember,Mario Bochicchio	 | 该研究利用可解释的人工智能技术结合集成学习模型，通过多视图特征识别恶意网络流量，并在自定义数据集上达到了高准确率，通过Shapley值解释了模型决策的关键特征，提升了检测恶意加密流量的透明度和可靠性。	 | Encrypted network communication ensures confidentiality, integrity, and privacy between endpoints. However, attackers are increasingly exploiting encryption to conceal malicious behavior. Detecting unknown encrypted malicious traffic without decrypting the payloads remains a significant challenge. In this study, we investigate the integration of explainable artificial intelligence (XAI) techniques to detect malicious network traffic. We employ ensemble learning models to identify malicious activity using multi-view features extracted from various aspects of encrypted communication. To effectively represent malicious communication, we compiled a robust dataset with 1,127 unique connections, more than any other available open-source dataset, and spanning 54 malware families. Our models were benchmarked against the CTU-13 dataset, achieving performance of over 99% accuracy, precision, and F1-score. Additionally, the eXtreme Gradient Boosting (XGB) model demonstrated 99.32% accuracy, 99.53% precision, and 99.43% F1-score on our custom dataset. By leveraging Shapley Additive Explanations (SHAP), we identified that the maximum packet size, mean inter-arrival time of packets, and transport layer security version used are the most critical features for the global model explanation. Furthermore, key features were identified as important for local explanations across both datasets for individual traffic samples. These insights provide a deeper understanding of the model decision-making process, enhancing the transparency and reliability of detecting malicious encrypted traffic.	 | 加密网络通信确保了端点之间的保密性、完整性和隐私性。然而，攻击者越来越多地利用加密来隐藏恶意行为。在不解密载荷的情况下检测未知的加密恶意流量依然是一项重大挑战。在本研究中，我们探讨了将可解释的人工智能（XAI）技术应用于检测恶意网络流量的方法。我们使用集成学习模型，通过从加密通信的各个方面提取的多视图特征来识别恶意活动。为了有效表示恶意通信，我们构建了一个包含1,127个独特连接的稳健数据集，这是目前可用的开源数据集中数量最多的，涵盖了54个恶意软件家族。我们的模型在CTU-13数据集上进行了基准测试，实现了超过99%的准确率、精确率和F1分数。此外，极端梯度提升（XGB）模型在我们自定义的数据集上达到了99.32%的准确率、99.53%的精确率和99.43%的F1分数。通过利用Shapley加性解释（SHAP），我们发现最大数据包大小、数据包平均间隔时间和使用的传输层安全性版本是全局模型解释中最重要的特征。此外，对于两个数据集中的个别流量样本，关键特征被确定为本地解释中的重要特征。这些见解加深了我们对模型决策过程的理解，增强了检测恶意加密流量的透明度和可靠性。
2501.05368	 | Developing a Foundation of Vector Symbolic Architectures Using Category Theory	 | Nolan P Shaw,P Michael Furlong,Britt Anderson,Jeff Orchard	 | 文章探讨了向量符号架构（VSAs）作为神经网络的替代框架，通过将范畴论应用于VSAs，提出VSAs可以在Met（Lawvere度量空间范畴）上被理解为一种（除法）环，从而有望在机器学习和认知科学之间建立更紧密的联系。	 | At the risk of overstating the case, connectionist approaches to machine learning, i.e. neural networks, are enjoying a small vogue right now. However, these methods require large volumes of data and produce models that are uninterpretable to humans. An alternative framework that is compatible with neural networks and gradient-based learning, but explicitly models compositionality, is Vector Symbolic Architectures (VSAs). VSAs are a family of algebras on high-dimensional vector representations. They arose in cognitive science from the need to unify neural processing and the kind of symbolic reasoning that humans perform. While machine learning methods have benefited from category theoretical analyses, VSAs have not yet received similar treatment. In this paper, we present a first attempt at applying category theory to VSAs. Specifically, we conduct a brief literature survey demonstrating the lacking intersection of these two topics, provide a list of desiderata for VSAs, and propose that VSAs may be understood as a (division) rig in a category enriched over a monoid in Met (the category of Lawvere metric spaces). This final contribution suggests that VSAs may be generalised beyond current implementations. It is our hope that grounding VSAs in category theory will lead to more rigorous connections with other research, both within and beyond, learning and cognition.	 | 尽管夸大其词，但目前连接主义方法，即神经网络，在机器学习领域正流行起来。然而，这些方法需要大量的数据，并且生成的人们难以理解的模型。一种与神经网络和梯度学习兼容但明确建模组合性的替代框架是向量符号架构（VSAs）。VSAs是一系列基于高维向量表示的代数。它们起源于认知科学，目的是统一神经处理和人类进行的符号推理。虽然机器学习方法已经从范畴论分析中受益，但VSAs尚未受到类似处理。在这篇文章中，我们首次尝试将范畴论应用于VSAs。具体来说，我们进行了一篇简要的文献综述，展示了这两个领域之间的缺乏交集，列出了对VSAs的期望标准，并建议VSAs可以被理解为在Met（Lawvere度量空间范畴）上富化的范畴中的一种（除法）环。这一最终贡献表明，VSAs可能在当前实现之外得到更广泛的推广。我们希望将VSAs置于范畴论的基础上，能够与学习和认知领域的其他研究产生更严谨的联系，无论是内部还是外部。
2501.05336	 | Stream Aligner: Efficient Sentence-Level Alignment via Distribution Induction	 | Hantao Lou,Jiaming Ji,Kaile Wang,Yaodong Yang	 | 该研究提出了一种名为流式分布诱导对齐器（Stream Aligner）的新颖对齐方法，通过动态的句子级校正增强大型语言模型的性能，并减少了对额外模型能力的依赖，显著提升了推理能力和用户交互效率。实验表明，Stream Aligner 在多个任务上优于传统对齐方法，特别是在提高大型语言模型的帮助性和无害性方面效果显著。	 | The rapid advancement of large language models (LLMs) has led to significant improvements in their capabilities, but also to increased concerns about their alignment with human values and intentions. Current alignment strategies, including adaptive training and inference-time methods, have demonstrated potential in this area. However, these approaches still struggle to balance deployment complexity and capability across various tasks and difficulties. In this work, we introduce the Streaming Distribution Induce Aligner (Stream Aligner), a novel alignment paradigm that combines efficiency with enhanced performance in various tasks throughout the generation process. Stream Aligner achieves dynamic sentence-level correction by using a small model to learn the preferences of the suffix sentence, iteratively correcting the suffix sentence output by the upstream model, and then using the corrected sentence to replace the suffix sentence in subsequent generations. Compared to Aligner, our experiments demonstrate that Stream Aligner reduces reliance on the capabilities of additional models, enhances the reasoning abilities of LLMs, and decreases latency during user interaction. Specifically, Stream Aligner-2B model has achieved an improvement of 76.1% in helpfulness, 36.0% in harmlessness on the tested Llama2-70B-chat model, and Stream Aligner-8B has achieved an improvement of 3.5% on the math ability of the tested Llama3-70B-Instruct model.	 | 大型语言模型（LLMs）的迅速发展极大地提升了其能力，但也引发了对其与人类价值观和意图对齐的担忧。当前的对齐策略，包括适应性训练和推理时的方法，已在这一领域显示出潜力。然而，这些方法仍然难以在各种任务和困难中平衡部署复杂性和能力。在此项工作中，我们引入了流式分布诱导对齐器（Stream Aligner），这是一种结合了效率和在生成过程中增强各种任务性能的新颖对齐范式。Stream Aligner 通过使用一个小模型来学习后缀句子的偏好，迭代纠正上游模型输出的后缀句子，并使用纠正后的句子替换后续生成中的后缀句子，实现了动态的句子级校正。与传统的Aligner相比，我们的实验表明，Stream Aligner 减少了对额外模型能力的依赖，增强了LLMs的推理能力，并降低了用户交互期间的延迟。具体而言，Stream Aligner-2B 模型在测试的Llama2-70B-chat模型上实现了帮助性提升76.1%，无害性提升36.0%，而Stream Aligner-8B在测试的Llama3-70B-Instruct模型的数学能力上实现了3.5%的提升。
2501.05325	 | The explanation dialogues: an expert focus study to understand requirements towards explanations within the GDPR	 | Laura State,Alejandra Bringas Colmenarejo,Andrea Beretta,Salvatore Ruggieri,Franco Turini,Stephanie Law	 | 该研究通过“解释对话”揭示了法律专家对可解释人工智能（XAI）的期望和理解，特别是在欧盟通用数据保护条例（GDPR）下的信贷领域，并提出了针对XAI方法开发者的法律建议。	 | Explainable AI (XAI) provides methods to understand non-interpretable machine learning models. However, we have little knowledge about what legal experts expect from these explanations, including their legal compliance with, and value against European Union legislation. To close this gap, we present the Explanation Dialogues, an expert focus study to uncover the expectations, reasoning, and understanding of legal experts and practitioners towards XAI, with a specific focus on the European General Data Protection Regulation. The study consists of an online questionnaire and follow-up interviews, and is centered around a use-case in the credit domain. We extract both a set of hierarchical and interconnected codes using grounded theory, and present the standpoints of the participating experts towards XAI. We find that the presented explanations are hard to understand and lack information, and discuss issues that can arise from the different interests of the data controller and subject. Finally, we present a set of recommendations for developers of XAI methods, and indications of legal areas of discussion. Among others, recommendations address the presentation, choice, and content of an explanation, technical risks as well as the end-user, while we provide legal pointers to the contestability of explanations, transparency thresholds, intellectual property rights as well as the relationship between involved parties.	 | 可解释的人工智能（Explainable AI, XAI）提供了理解不可解释的机器学习模型的方法。然而，我们对法律专家期望这些解释的内容知之甚少，包括它们与欧盟立法的法律合规性和价值。为了填补这一空白，我们提出了“解释对话”（Explanation Dialogues），这是一种专家焦点研究，旨在揭示法律专家和从业人员对XAI的期望、推理和理解，特别是针对欧洲通用数据保护条例（GDPR）。这项研究包括在线问卷和后续访谈，聚焦于信贷领域的案例。我们使用扎根理论提取了一套层次化且相互连接的编码，展示了参与专家对XAI的立场。我们发现，提出的解释难以理解且缺乏信息，并讨论了数据控制者和主体之间不同利益可能引起的问题。最后，我们提出了对XAI方法开发者的建议，并指出了讨论的法律领域。其中，建议涉及解释的呈现、选择和内容、技术风险以及最终用户，同时提供了关于解释的可争议性、透明度门槛、知识产权以及相关方关系的法律指引。
2501.05313	 | Optimizing Distributed Deployment of Mixture-of-Experts Model Inference in Serverless Computing	 | Mengfan Liu,Wei Wang,Chuan Wu	 | 该研究探讨了在无服务器平台上部署混合专家模型（MoE）的优化方法，提出了一种多维ε贪婪搜索的贝叶斯优化框架，通过预测专家流行度、优化通信流水线和部署算法，实现了显著的成本降低和性能优化。实验结果表明，相比CPU集群和LambdaML，此设计在计费成本上分别降低了至少75.67%和43.41%，同时保持了满意的推理吞吐量。	 | With the advancement of serverless computing, running machine learning (ML) inference services over a serverless platform has been advocated, given its labor-free scalability and cost effectiveness. Mixture-of-Experts (MoE) models have been a dominant type of model architectures to enable large models nowadays, with parallel expert networks. Serving large MoE models on serverless computing is potentially beneficial, but has been underexplored due to substantial challenges in handling the skewed expert popularity and scatter-gather communication bottleneck in MoE model execution, for cost-efficient serverless MoE deployment and performance guarantee. We study optimized MoE model deployment and distributed inference serving on a serverless platform, that effectively predict expert selection, pipeline communication with model execution, and minimize the overall billed cost of serving MoE models. Especially, we propose a Bayesian optimization framework with multi-dimensional epsilon-greedy search to learn expert selections and optimal MoE deployment achieving optimal billed cost, including: 1) a Bayesian decision-making method for predicting expert popularity; 2) flexibly pipelined scatter-gather communication; and 3) an optimal model deployment algorithm for distributed MoE serving. Extensive experiments on AWS Lambda show that our designs reduce the billed cost of all MoE layers by at least 75.67% compared to CPU clusters while maintaining satisfactory inference throughput. As compared to LambdaML in serverless computing, our designs achieves 43.41% lower cost with a throughput decrease of at most 18.76%.	 | 随着无服务器计算的发展，将机器学习（ML）推理服务部署在无服务器平台上得到了倡导，这得益于其无需人工干预的可扩展性和成本效益。混合专家模型（MoE）已成为当今最主流的模型架构之一，它通过并行的专家网络来实现大型模型。在无服务器计算平台上部署大型MoE模型可能具有潜在的好处，但由于在MoE模型执行过程中处理专家受欢迎程度分布不均以及散集通信瓶颈等重大挑战，这种部署方式仍处于探索阶段。为了实现成本效益的无服务器MoE部署并确保性能，我们需要研究在无服务器平台上优化MoE模型的部署和分布式推理服务，以有效预测专家的选择、模型执行中的管道通信以及将总体计费成本降到最低。特别地，我们提出了一种多维ε贪婪搜索的贝叶斯优化框架，以学习专家选择和最优化的MoE部署，包括：1) 一种基于贝叶斯决策的专家流行度预测方法；2) 灵活的散集通信流水线；3) 一种用于分布式MoE推理服务的最佳模型部署算法。在AWS Lambda上的广泛实验表明，与CPU集群相比，我们的设计在所有MoE层的计费成本上至少降低了75.67%，同时保持了满意的推理吞吐量。与无服务器计算中的LambdaML相比，我们的设计在成本上降低了43.41%，并且吞吐量最多减少了18.76%。
2501.05309	 | Private Selection with Heterogeneous Sensitivities	 | Daniela Antonova,Allegra Laro,Audra McMillan,Lorenz Wolf	 | 该研究探讨了在不同ially私（DP）选择中得分和敏感度分布如何影响机制性能，并提出了一种根据得分和敏感性相关性选择的启发式方法，以及一个改进的广义指数机制（GEM），以适应不同情境下的需求，从而在多种设置中优于传统机制如报告噪声最大值（RNM）。	 | Differentially private (DP) selection involves choosing a high-scoring candidate from a finite candidate pool, where each score depends on a sensitive dataset. This problem arises naturally in a variety of contexts including model selection, hypothesis testing, and within many DP algorithms. Classical methods, such as Report Noisy Max (RNM), assume all candidates' scores are equally sensitive to changes in a single individual's data, but this often isn't the case. To address this, algorithms like the Generalised Exponential Mechanism (GEM) leverage variability in candidate sensitivities. However, we observe that while these algorithms can outperform RNM in some situations, they may underperform in others - they can even perform worse than random selection. In this work, we explore how the distribution of scores and sensitivities impacts DP selection mechanisms. In all settings we study, we find that there exists a mechanism that utilises heterogeneity in the candidate sensitivities that outperforms standard mechanisms like RNM. However, no single mechanism uniformly outperforms RNM. We propose using the correlation between the scores and sensitivities as the basis for deciding which DP selection mechanism to use. Further, we design a slight variant of GEM, modified GEM that generally performs well whenever GEM performs poorly. Relying on the correlation heuristic we propose combined GEM, which adaptively chooses between GEM and modified GEM and outperforms both in polarised settings.	 | 不同ially私（DP）选择涉及从有限的候选池中挑选出得分较高的候选人，其中每个得分依赖于一个敏感的数据集。这个问题在多种情境中自然出现，包括模型选择、假设检验以及许多DP算法中。传统的算法，如报告噪声最大值（RNM），假设所有候选人的得分对单一个体数据的变化具有相同的敏感性，但实际情况往往并非如此。为了应对这一问题，算法如广义指数机制（GEM）利用了候选敏感度的变异性。然而，我们观察到，在某些情况下，这些算法的表现优于RNM，但在其他情况下，它们的表现可能较差——甚至不如随机选择。在本项工作中，我们探讨了得分和敏感度分布如何影响DP选择机制。在所有我们研究的设置中，我们发现存在一种机制，该机制利用候选敏感性的异质性，优于标准机制如RNM。但没有一个机制在所有情况下都优于RNM。我们建议根据得分和敏感性的相关性来决定使用哪种DP选择机制。此外，我们设计了一个GEM的小变种，修改后的GEM，在GEM表现不佳时通常表现出色。依赖于相关性的启发式方法，我们提出了组合GEM，该方法在极端情况下能够自适应地选择GEM和修改后的GEM，从而在这些情境下优于两者。
2501.05281	 | Comparison Study: Glacier Calving Front Delineation in Synthetic Aperture Radar Images With Deep Learning	 | Nora Gourmelon,Konrad Heidler,Erik Loebel,Daniel Cheng,Julian Klink,Anda Dong,Fei Wu,Noah Maul,Moritz Koch,Marcel Dreier,Dakota Pyles,Thorsten Seehaus,Matthias Braun,Andreas Maier,Vincent Christlein	 | 该研究比较了多种深度学习系统从合成孔径雷达图像中自动提取海洋终端冰川消融前沿位置的能力，并发现最佳DL模型的精度仍低于人类标注者，表明现有DL系统需进一步改进以实现全面自动化监控。研究还指出了未来应探索的方向，包括视觉变换器、基础模型以及更多数据的采集和处理策略。	 | Calving front position variation of marine-terminating glaciers is an indicator of ice mass loss and a crucial parameter in numerical glacier models. Deep Learning (DL) systems can automatically extract this position from Synthetic Aperture Radar (SAR) imagery, enabling continuous, weather- and illumination-independent, large-scale monitoring. This study presents the first comparison of DL systems on a common calving front benchmark dataset. A multi-annotator study with ten annotators is performed to contrast the best-performing DL system against human performance. The best DL model's outputs deviate 221 m on average, while the average deviation of the human annotators is 38 m. This significant difference shows that current DL systems do not yet match human performance and that further research is needed to enable fully automated monitoring of glacier calving fronts. The study of Vision Transformers, foundation models, and the inclusion and processing strategy of more information are identified as avenues for future research.	 | 海洋终端冰川的消融前沿位置变化是冰质量损失的指示器，并且是数值冰川模型中的关键参数。深度学习（DL）系统可以从合成孔径雷达（SAR）图像中自动提取这一位置，从而实现连续的大规模监测，不受天气和光照条件的影响。本研究首次对多种DL系统在共同的消融前沿基准数据集上进行了比较。进行了多标注者研究，邀请了十位标注者以对比最佳DL系统与人类表现的差异。最佳DL模型输出的平均偏差为221米，而人类标注者的平均偏差为38米。这一显著差异表明当前的DL系统尚未达到人类的性能水平，需要进一步研究以实现消融前沿自动化的全面监控。该研究指出了视觉变换器、基础模型以及更多信息的采集和处理策略等未来研究的方向。
2501.05278	 | Off-Policy Evaluation and Counterfactual Methods in Dynamic Auction Environments	 | Ritam Guha,Nilavra Pathak	 | 该研究探讨了反事实估计器在动态拍卖环境中的应用，以加速资源分配策略的评估过程，减少A/B测试所需的时间和资源，并提高政策选择的准确性。通过利用反事实估计器，该研究旨在构建一个高级数据分析系统，以实时评估和优化资源分配策略。	 | Counterfactual estimators are critical for learning and refining policies using logged data, a process known as Off-Policy Evaluation (OPE). OPE allows researchers to assess new policies without costly experiments, speeding up the evaluation process. Online experimental methods, such as A/B tests, are effective but often slow, thus delaying the policy selection and optimization process.   In this work, we explore the application of OPE methods in the context of resource allocation in dynamic auction environments. Given the competitive nature of environments where rapid decision-making is crucial for gaining a competitive edge, the ability to quickly and accurately assess algorithmic performance is essential. By utilizing counterfactual estimators as a preliminary step before conducting A/B tests, we aim to streamline the evaluation process, reduce the time and resources required for experimentation, and enhance confidence in the chosen policies. Our investigation focuses on the feasibility and effectiveness of using these estimators to predict the outcomes of potential resource allocation strategies, evaluate their performance, and facilitate more informed decision-making in policy selection. Motivated by the outcomes of our initial study, we envision an advanced analytics system designed to seamlessly and dynamically assess new resource allocation strategies and policies.	 | 反事实估计器对于使用日志数据学习和改进策略至关重要，这一过程称为离策评估（Off-Policy Evaluation, OPE）。OPE 允许研究人员无需进行昂贵的实验即可评估新策略，从而加快评估过程。在线实验方法，如 A/B 测试，虽然有效但通常速度较慢，因此会延缓政策选择和优化过程。  在本工作中，我们探讨了 OPE 方法在动态拍卖环境中资源分配应用的可能性。由于在竞争激烈且快速决策至关重要的环境中，能够迅速且准确地评估算法性能至关重要。通过在进行 A/B 测试之前使用反事实估计器作为初步步骤，我们旨在简化评估过程，减少实验所需的时间和资源，并增强对所选政策的信心。我们的研究重点在于利用这些估计器预测潜在资源分配策略的成果、评估其性能，并促进更明智的政策选择决策。基于我们初步研究的成果，我们设想了一个高级数据分析系统，旨在无缝且动态地评估新的资源分配策略和政策。
2501.05269	 | CellViT++: Energy-Efficient and Adaptive Cell Segmentation and Classification Using Foundation Models	 | Fabian Hörst,Moritz Rempe,Helmut Becker,Lukas Heine,Julius Keyl,Jens Kleesiek	 | CellViT++ 是一种用于数字病理学通用细胞分割的框架，它利用视觉变换器与基础模型作为编码器来同时计算深层细胞特征和分割掩码，能够在少量标注数据下适应未见过的细胞类型，并且有效减少了碳足迹。	 | Digital Pathology is a cornerstone in the diagnosis and treatment of diseases. A key task in this field is the identification and segmentation of cells in hematoxylin and eosin-stained images. Existing methods for cell segmentation often require extensive annotated datasets for training and are limited to a predefined cell classification scheme. To overcome these limitations, we propose $\text{CellViT}^{\scriptscriptstyle ++}$, a framework for generalized cell segmentation in digital pathology. $\text{CellViT}^{\scriptscriptstyle ++}$ utilizes Vision Transformers with foundation models as encoders to compute deep cell features and segmentation masks simultaneously. To adapt to unseen cell types, we rely on a computationally efficient approach. It requires minimal data for training and leads to a drastically reduced carbon footprint. We demonstrate excellent performance on seven different datasets, covering a broad spectrum of cell types, organs, and clinical settings. The framework achieves remarkable zero-shot segmentation and data-efficient cell-type classification. Furthermore, we show that $\text{CellViT}^{\scriptscriptstyle ++}$ can leverage immunofluorescence stainings to generate training datasets without the need for pathologist annotations. The automated dataset generation approach surpasses the performance of networks trained on manually labeled data, demonstrating its effectiveness in creating high-quality training datasets without expert annotations. To advance digital pathology, $\text{CellViT}^{\scriptscriptstyle ++}$ is available as an open-source framework featuring a user-friendly, web-based interface for visualization and annotation. The code is available under https://github.com/TIO-IKIM/CellViT-plus-plus.	 | 数字病理学是疾病诊断和治疗的基础。这一领域的一个关键任务是在苏木精和伊红染色图像中识别和分割细胞。现有的细胞分割方法通常需要大量标注的数据集进行训练，并且局限于预定义的细胞分类方案。为了克服这些限制，我们提出了一种名为$\text{CellViT}^{\scriptscriptstyle ++}$的框架，用于数字病理学中的通用细胞分割。$\text{CellViT}^{\scriptscriptstyle ++}$利用视觉转换器与基础模型作为编码器来同时计算深层细胞特征和分割掩码。为了适应未见过的细胞类型，我们依赖于一种计算效率高的方法。这种方法只需要少量数据进行训练，并且大幅减少了碳足迹。我们在七个不同的数据集上展示了其出色的性能，这些数据集涵盖了广泛的细胞类型、器官和临床场景。该框架实现了卓越的零样本分割和数据高效的细胞类型分类。此外，我们证明了$\text{CellViT}^{\scriptscriptstyle ++}$可以通过免疫荧光染色生成训练数据集，而无需病理学家的标注。自动化数据集生成方法超越了在手动标注数据上训练的网络性能，证明了其在无需专家标注的情况下创建高质量训练数据集的有效性。为了推动数字病理学的发展，$\text{CellViT}^{\scriptscriptstyle ++}$作为开源框架提供，具有用户友好的、基于Web的界面用于可视化和标注。代码可在https://github.com/TIO-IKIM/CellViT-plus-plus找到。
2501.05260	 | Enhancing Plagiarism Detection in Marathi with a Weighted Ensemble of TF-IDF and BERT Embeddings for Low-Resource Language Processing	 | Atharva Mutsaddi,Aditya Choudhary	 | 本文提出了一种利用BERT句子嵌入和TF-IDF特征结合的方法，以提高马拉地语文本剽窃检测的准确性，并通过机器学习模型的集成来捕捉文本的统计、语义和句法特征。设计针对低资源语言的稳健基准剽窃检测系统变得至关重要，尤其是对于像马拉地语这样的非主流语言。	 | Plagiarism involves using another person's work or concepts without proper attribution, presenting them as original creations. With the growing amount of data communicated in regional languages such as Marathi -- one of India's regional languages -- it is crucial to design robust plagiarism detection systems tailored for low-resource languages. Language models like Bidirectional Encoder Representations from Transformers (BERT) have demonstrated exceptional capability in text representation and feature extraction, making them essential tools for semantic analysis and plagiarism detection. However, the application of BERT for low-resource languages remains under-explored, particularly in the context of plagiarism detection. This paper presents a method to enhance the accuracy of plagiarism detection for Marathi texts using BERT sentence embeddings in conjunction with Term Frequency-Inverse Document Frequency (TF-IDF) feature representation. This approach effectively captures statistical, semantic, and syntactic aspects of text features through a weighted voting ensemble of machine learning models.	 | 剽窃是指未经授权使用他人的作品或概念，并将其呈现为原创创作。随着越来越多的数据用地区语言（如印度的马拉地语）进行交流，设计针对低资源语言的 robust 基准剽窃检测系统变得至关重要。像双向编码器表示从.transformers（BERT）这样的语言模型在文本表示和特征提取方面展示了出色的能力，使其成为语义分析和剽窃检测的重要工具。然而，将BERT应用于低资源语言的领域仍然相对未被探索，尤其是在剽窃检测方面的应用。本文提出了一种方法，通过结合BERT句子嵌入和词频-逆文档频率（TF-IDF）特征表示来提高马拉地语文本剽窃检测的准确性。该方法通过机器学习模型的加权投票集成有效地捕捉了文本特征的统计、语义和句法方面。
2501.05234	 | Optimizing Estonian TV Subtitles with Semi-supervised Learning and LLMs	 | Artem Fedorchenko,Tanel Alumäe	 | 本文介绍了一种通过微调 Whisper 模型并结合迭代伪标签和大型语言模型后编辑来生成高质量爱沙尼亚同语言字幕的方法，实验结果表明这种方法能显著提高字幕质量，并且可以在实时应用中使用。	 | This paper presents an approach for generating high-quality, same-language subtitles for Estonian TV content. We fine-tune the Whisper model on human-generated Estonian subtitles and enhance it with iterative pseudo-labeling and large language model (LLM) based post-editing. Our experiments demonstrate notable subtitle quality improvement through pseudo-labeling with an unlabeled dataset. We find that applying LLM-based editing at test time enhances subtitle accuracy, while its use during training does not yield further gains. This approach holds promise for creating subtitle quality close to human standard and could be extended to real-time applications.	 | 本文介绍了一种生成高质量同语言字幕的方法，用于爱沙尼亚电视内容。我们通过对人工生成的爱沙尼亚字幕进行微调 Whisper 模型，并结合迭代伪标签和基于大型语言模型（LLM）的后编辑，提高了模型的性能。我们的实验表明，通过使用未标记的数据集进行伪标签，可以显著提高字幕质量。我们发现，在测试时应用基于 LLM 的编辑可以提高字幕的准确性，而在训练过程中使用 LLM 编辑则不会带来额外的收益。这种方法有望实现接近人工标准的字幕质量，并且可以扩展到实时应用中。
2501.05226	 | Light Transport-aware Diffusion Posterior Sampling for Single-View Reconstruction of 3D Volumes	 | Ludwic Leonard,Nils Thuerey,Ruediger Westermann	 | 该研究提出了一种在多重光散射条件下进行体素场单视图重建的技术，利用无条件扩散模型和新的单平面表示隐空间代码进行训练，并结合基于物理的可微体积渲染器优化重建结果，显著提高了单视图重建的精度。	 | We introduce a single-view reconstruction technique of volumetric fields in which multiple light scattering effects are omnipresent, such as in clouds. We model the unknown distribution of volumetric fields using an unconditional diffusion model trained on a novel benchmark dataset comprising 1,000 synthetically simulated volumetric density fields. The neural diffusion model is trained on the latent codes of a novel, diffusion-friendly, monoplanar representation. The generative model is used to incorporate a tailored parametric diffusion posterior sampling technique into different reconstruction tasks. A physically-based differentiable volume renderer is employed to provide gradients with respect to light transport in the latent space. This stands in contrast to classic NeRF approaches and makes the reconstructions better aligned with observed data. Through various experiments, we demonstrate single-view reconstruction of volumetric clouds at a previously unattainable quality.	 | 我们介绍了一种在多重光散射现象普遍存在的情况下（如云朵中）进行体素场单视图重建的技术。我们使用一个基于新型基准数据集训练的无条件扩散模型，该数据集包含1000个合成模拟的体素密度场，来建模未知的体素场分布。神经扩散模型是在一种新的、扩散友好的单平面表示的隐空间代码上进行训练的。生成模型被用于将定制的参数化扩散后验采样技术融入到不同的重建任务中。我们使用基于物理的可微体积渲染器来提供关于隐空间中光传输的梯度，这与经典的NeRF方法有所不同，使得重建结果更好地与观测数据对齐。通过各种实验，我们展示了单视图重建体素云朵的质量达到了以前无法达到的水平。
2501.05223	 | EVA-S2PLoR: A Secure Element-wise Multiplication Meets Logistic Regression on Heterogeneous Database	 | Tianle Tao,Shizhao Peng,Tianyu Mei,Shoumo Li,Haogang Zhu	 | 本文提出了一种高效的可验证安全双方逻辑回归框架（EVA-S2PLoR），通过新颖的安全位元乘法协议实现准确的非线性计算，并在精度上显著优于许多现有框架，特别是在sigmoid函数性能方面提高了约10个数量级。	 | Accurate nonlinear computation is a key challenge in privacy-preserving machine learning (PPML). Most existing frameworks approximate it through linear operations, resulting in significant precision loss. This paper proposes an efficient, verifiable and accurate security 2-party logistic regression framework (EVA-S2PLoR), which achieves accurate nonlinear function computation through a novel secure element-wise multiplication protocol and its derived protocols. Our framework primarily includes secure 2-party vector element-wise multiplication, addition to multiplication, reciprocal, and sigmoid function based on data disguising technology, where high efficiency and accuracy are guaranteed by the simple computation flow based on the real number domain and the few number of fixed communication rounds. We provide secure and robust anomaly detection through dimension transformation and Monte Carlo methods. EVA-S2PLoR outperforms many advanced frameworks in terms of precision (improving the performance of the sigmoid function by about 10 orders of magnitude compared to most frameworks) and delivers the best overall performance in secure logistic regression experiments.	 | 在隐私保护机器学习（PPML）中，准确的非线性计算是一个关键挑战。大多数现有框架通过线性操作进行近似，导致显著的精度损失。本文提出了一种高效的、可验证且准确的安全双方逻辑回归框架（EVA-S2PLoR），该框架通过一种新颖的安全位元乘法协议及其衍生协议实现准确的非线性函数计算。我们的框架主要基于数据伪装技术，包括安全的双向向量位元乘法、加法到乘法、倒数和sigmoid函数运算，其中高效的计算流程和少量固定的通信轮数保证了高效率和高精度。我们通过维度转换和蒙特卡洛方法提供安全且稳健的异常检测。EVA-S2PLoR在精度方面优于许多先进的框架（与大多数框架相比，其sigmoid函数性能提高了约10个数量级），并在安全逻辑回归实验中取得了最佳的整体性能。
2501.05207	 | CoDe: Communication Delay-Tolerant Multi-Agent Collaboration via Dual Alignment of Intent and Timeliness	 | Shoucheng Song,Youfang Lin,Sheng Han,Chang Yao,Hao Wu,Shuo Wang,Kai Lv	 | 本文定义了多智能体强化学习中两种通信延迟设置，并提出了一个名为CoDe的新框架，以处理通信延迟带来的挑战，该框架通过未来动作推断学习意图表示，并设计了一种意图和及时性双重对齐机制来增强异步消息融合，实验结果表明CoDe在不同延迟情况下均优于基线算法并表现出鲁棒性。	 | Communication has been widely employed to enhance multi-agent collaboration. Previous research has typically assumed delay-free communication, a strong assumption that is challenging to meet in practice. However, real-world agents suffer from channel delays, receiving messages sent at different time points, termed {\it{Asynchronous Communication}}, leading to cognitive biases and breakdowns in collaboration. This paper first defines two communication delay settings in MARL and emphasizes their harm to collaboration. To handle the above delays, this paper proposes a novel framework, Communication Delay-tolerant Multi-Agent Collaboration (CoDe). At first, CoDe learns an intent representation as messages through future action inference, reflecting the stable future behavioral trends of the agents. Then, CoDe devises a dual alignment mechanism of intent and timeliness to strengthen the fusion process of asynchronous messages. In this way, agents can extract the long-term intent of others, even from delayed messages, and selectively utilize the most recent messages that are relevant to their intent. Experimental results demonstrate that CoDe outperforms baseline algorithms in three MARL benchmarks without delay and exhibits robustness under fixed and time-varying delays.	 | 通信已被广泛用于增强多智能体的合作。此前的研究通常假定通信无延迟，这是一个在实践中难以满足的强假设。然而，现实世界中的智能体受到信道延迟的影响，它们在不同时间点接收到发送的消息，这被称为异步通信，导致认知偏见并破坏了合作。本文首先定义了多智能体强化学习(MARL)中的两种通信延迟设置，并强调了这些延迟对合作的伤害。为了处理上述延迟，本文提出了一个名为通信延迟容忍多智能体合作（CoDe）的新框架。首先，CoDe 通过未来动作推断学习一种意图表示，反映了智能体的稳定未来行为趋势。然后，CoDe 设计了一种意图和及时性的双重对齐机制来加强异步消息的融合过程。通过这种方式，智能体可以从延迟的消息中提取他人的长期意图，并有选择地利用与它们的意图最相关的最近的消息。实验结果表明，在无延迟和固定及时间变化延迟的情况下，CoDe 都优于基线算法，并且表现出鲁棒性。
2501.05204	 | Design and Control of a Bipedal Robotic Character	 | Ruben Grandia,Espen Knoop,Michael A. Hopkins,Georg Wiedebach,Jared Bishop,Steven Pickles,David Müller,Moritz Bächer	 | 该研究介绍了将富有表现力的艺术家主导动作与腿足机器人的稳健动态机动性相结合的新方法，通过基于强化学习的控制架构和直观的操作员界面，使机器人能够在娱乐应用中进行实时表演，从而产生可信的机器人角色并促进人机互动。	 | Legged robots have achieved impressive feats in dynamic locomotion in challenging unstructured terrain. However, in entertainment applications, the design and control of these robots face additional challenges in appealing to human audiences. This work aims to unify expressive, artist-directed motions and robust dynamic mobility for legged robots. To this end, we introduce a new bipedal robot, designed with a focus on character-driven mechanical features. We present a reinforcement learning-based control architecture to robustly execute artistic motions conditioned on command signals. During runtime, these command signals are generated by an animation engine which composes and blends between multiple animation sources. Finally, an intuitive operator interface enables real-time show performances with the robot. The complete system results in a believable robotic character, and paves the way for enhanced human-robot engagement in various contexts, in entertainment robotics and beyond.	 | 腿足机器人在复杂非结构化地形下的动态运动中已经取得了令人印象深刻的成就。然而，在娱乐应用中，设计和控制这些机器人的挑战不仅仅是技术上的，还需要能够吸引人类观众。本研究旨在将富有表现力的、艺术家主导的动作与腿足机器人的稳健动态机动性结合起来。为此，我们介绍了一种新的双足机器人，其设计重点在于角色驱动的机械特征。我们提出了一种基于强化学习的控制架构，以根据指令信号稳健地执行艺术动作。在运行时，这些指令信号由动画引擎生成，该引擎能够组合和混合多个动画源。最后，直观的操作员界面使机器人能够进行实时表演。整个系统最终产生了一个可信的机器人角色，并为娱乐机器人和其他领域中增强的人机互动铺平了道路。
2501.05190	 | RadioTransformer: Accurate Radio Map Construction and Coverage Prediction	 | Yuxuan Li,Cheng Zhang,Wen Wang,Yongming Huang	 | 本文提出了一种混合变压器-卷积模型RadioTransformer，用于提高射频地图和路径损耗预测的准确性，实验结果表明该模型比当前先进方法（SOTA）具有更好的预测性能，均方根误差（RMSE）降低了超过30%。	 | Radio map, or pathloss map prediction, is a crucial method for wireless network modeling and management. By leveraging deep learning to construct pathloss patterns from geographical maps, an accurate digital replica of the transmission environment could be established with less computational overhead and lower prediction error compared to traditional model-driven techniques. While existing state-of-the-art (SOTA) methods predominantly rely on convolutional architectures, this paper introduces a hybrid transformer-convolution model, termed RadioTransformer, to enhance the accuracy of radio map prediction. The proposed model features a multi-scale transformer-based encoder for efficient feature extraction and a convolution-based decoder for precise pixel-level image reconstruction. Simulation results demonstrate that the proposed scheme significantly improves prediction accuracy, and over a 30% reduction in root mean square error (RMSE) is achieved compared to typical SOTA approaches.	 | 无线网络建模和管理中，射频地图或路径损耗预测是一种关键方法。通过利用深度学习从地理地图中构建路径损耗模式，可以建立一个比传统模型驱动技术具有更低计算开销和预测误差的精确数字传输环境模型。尽管现有的先进方法（SOTA）大多依赖于卷积架构，本文引入了一种混合变压器-卷积模型，称为RadioTransformer，以提高射频地图预测的准确性。所提出的模型采用多尺度变压器编码器进行高效的特征提取，并使用基于卷积的解码器进行精确的像素级图像重建。仿真结果表明，所提出的方案显著提高了预测准确性，与典型的SOTA方法相比，均方根误差（RMSE）的降低幅度超过30%。
2501.05170	 | De-centering the (Traditional) User: Multistakeholder Evaluation of Recommender Systems	 | Robin Burke,Gediminas Adomavicius,Toine Bogers,Tommaso Di Noia,Dominik Kowald,Julia Neidhardt,Özlem Özgöbek,Maria Soledad Pera,Nava Tintarev,Jürgen Ziegler	 | 本文探讨了多利益相关方推荐系统的评估复杂性，强调了不仅需要考虑用户群体，还需考虑其他多个利益相关方（如生产者）的影响和偏好，并提供了理论与实践结合的具体应用案例和未来研究方向。	 | Multistakeholder recommender systems are those that account for the impacts and preferences of multiple groups of individuals, not just the end users receiving recommendations. Due to their complexity, evaluating these systems cannot be restricted to the overall utility of a single stakeholder, as is often the case of more mainstream recommender system applications. In this article, we focus our discussion on the intricacies of the evaluation of multistakeholder recommender systems. We bring attention to the different aspects involved in the evaluation of multistakeholder recommender systems - from the range of stakeholders involved (including but not limited to producers and consumers) to the values and specific goals of each relevant stakeholder. Additionally, we discuss how to move from theoretical principles to practical implementation, providing specific use case examples. Finally, we outline open research directions for the RecSys community to explore. We aim to provide guidance to researchers and practitioners about how to think about these complex and domain-dependent issues of evaluation in the course of designing, developing, and researching applications with multistakeholder aspects.	 | 多利益相关方推荐系统是指不仅要考虑最终接收推荐的用户群体的影响和偏好，还要考虑多个其他群体的影响和偏好。由于其复杂性，评估这些系统不能仅限于单一利益相关方的整体效用，这与主流推荐系统应用的情况有所不同。在本文中，我们将重点讨论多利益相关方推荐系统的评估复杂性。我们将关注多利益相关方推荐系统评估涉及的不同方面——从涉及的各种利益相关方（包括但不限于生产者和消费者）到每个相关利益相关方的价值观和具体目标。此外，我们还将讨论如何从理论原则过渡到实际实施，并提供具体的应用案例。最后，我们概述了推荐系统社区可以探索的开放研究方向。我们旨在为研究人员和实践者提供指导，以便他们在设计、开发和研究具有多利益相关方特点的应用程序时，能够考虑这些复杂且依赖于特定领域的评估问题。
2501.05113	 | Constrained Optimization of Charged Particle Tracking with Multi-Agent Reinforcement Learning	 | Tobias Kortus,Ralf Keidel,Nicolas R. Gauger,Jan Kieseler(for the Bergen pCT Collaboration)	 | 本文提出了一种具有分配约束的多agent强化学习方法，用于重建像素化粒子检测器中的粒子轨迹，通过最小化总散射量并解决线性分配问题来优化合作策略，实验结果表明该方法在模拟数据上比多种基线方法更为有效。引入成本裕度的约束增强了方法的优化和泛化能力，为基于RL的跟踪发展奠定了基础。	 | Reinforcement learning demonstrated immense success in modelling complex physics-driven systems, providing end-to-end trainable solutions by interacting with a simulated or real environment, maximizing a scalar reward signal. In this work, we propose, building upon previous work, a multi-agent reinforcement learning approach with assignment constraints for reconstructing particle tracks in pixelated particle detectors. Our approach optimizes collaboratively a parametrized policy, functioning as a heuristic to a multidimensional assignment problem, by jointly minimizing the total amount of particle scattering over the reconstructed tracks in a readout frame. To satisfy constraints, guaranteeing a unique assignment of particle hits, we propose a safety layer solving a linear assignment problem for every joint action. Further, to enforce cost margins, increasing the distance of the local policies predictions to the decision boundaries of the optimizer mappings, we recommend the use of an additional component in the blackbox gradient estimation, forcing the policy to solutions with lower total assignment costs. We empirically show on simulated data, generated for a particle detector developed for proton imaging, the effectiveness of our approach, compared to multiple single- and multi-agent baselines. We further demonstrate the effectiveness of constraints with cost margins for both optimization and generalization, introduced by wider regions with high reconstruction performance as well as reduced predictive instabilities. Our results form the basis for further developments in RL-based tracking, offering both enhanced performance with constrained policies and greater flexibility in optimizing tracking algorithms through the option for individual and team rewards.	 | 在建模复杂物理驱动系统方面，强化学习取得了巨大的成功，通过与模拟或真实环境的互动，提供了端到端可训练的解决方案，并通过最大化标量奖励信号来实现最大化目标。在本文中，我们在此前工作的基础上，提出了一种具有分配约束的多agent强化学习方法，用于重建像素化粒子检测器中的粒子轨迹。我们的方法通过联合最小化重建轨迹在读出帧中的总散射量，优化合作的参数化策略，该策略作为一个针对多维分配问题的启发式方法。为了满足约束条件，确保每个粒子击中点的唯一分配，我们提出了一个安全层，用于为每组动作解决线性分配问题。此外，为了增加局部策略预测与优化映射决策边界的距离，我们建议在黑盒梯度估计中增加一个额外组件，迫使策略趋向于具有更低总分配成本的解。通过在模拟数据上的实验，我们展示了该方法的有效性，这些数据是为用于质子成像的粒子探测器而生成的，与多种单agent和多agent基线方法相比，该方法更为有效。我们进一步证明了引入成本裕度的约束对于优化和泛化都有效，这使得重建性能较高的区域更加宽广，并减少了预测不稳定性。我们的结果为基于RL的跟踪进一步发展提供了基础，不仅提高了受约束策略的性能，还通过赋予个体和团队奖励的选项增强了优化跟踪算法的灵活性。
2501.05105	 | Robust Score Matching	 | Richard Schwank,Andrew McCormack,Mathias Drton	 | 本文开发了一种基于几何均值中位数的鲁棒评分匹配方法，能在观察数据被污染的情况下提供一致的参数估计，并特别适用于保持指数族模型凸性的非高斯图形模型。实验结果表明，该方法在无污染时与标准评分匹配估计效果相当，而在有污染时表现更优。	 | Proposed in Hyvärinen (2005), score matching is a parameter estimation procedure that does not require computation of distributional normalizing constants. In this work we utilize the geometric median of means to develop a robust score matching procedure that yields consistent parameter estimates in settings where the observed data has been contaminated. A special appeal of the proposed method is that it retains convexity in exponential family models. The new method is therefore particularly attractive for non-Gaussian, exponential family graphical models where evaluation of normalizing constants is intractable. Support recovery guarantees for such models when contamination is present are provided. Additionally, support recovery is studied in numerical experiments and on a precipitation dataset. We demonstrate that the proposed robust score matching estimator performs comparably to the standard score matching estimator when no contamination is present but greatly outperforms this estimator in a setting with contamination.	 | Hyvärinen (2005) 提出的评分匹配是一种不需要计算分布归一化常数的参数估计方法。在本文中，我们利用几何均值中位数来开发一种鲁棒的评分匹配方法，该方法能够在观测数据被污染的情况下提供一致的参数估计。所提出方法的一个特别之处在于，它在指数族模型中保持了凸性。因此，该新方法特别适用于非高斯、指数族图形模型，其中计算归一化常数是不可行的。当存在污染时，此类模型的支撑恢复保证被提供。此外，在数值实验和一个降水量数据集中研究了支撑恢复情况。我们证明，在没有污染的情况下，所提出的方法与标准评分匹配估计的效果相当，但在存在污染的情况下，该方法的性能远超标准方法。
2501.05097	 | A 1Mb mixed-precision quantized encoder for image classification and patch-based compression	 | Van Thien Nguyen,William Guicquero,Gilles Sicard	 | 本文展示了为图像处理设计的ASIC神经网络加速器可以应用于多种任务，如图像分类和压缩，并且只需要非常有限的硬件资源；通过优化权重和激活的量化及剪枝，以及提出自动调整量化因子和层共享位移规范化方法，实现了高效的硬件实现和性能提升。	 | Even if Application-Specific Integrated Circuits (ASIC) have proven to be a relevant choice for integrating inference at the edge, they are often limited in terms of applicability. In this paper, we demonstrate that an ASIC neural network accelerator dedicated to image processing can be applied to multiple tasks of different levels: image classification and compression, while requiring a very limited hardware. The key component is a reconfigurable, mixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and activation quantizations combined with convolutional layer structural pruning to lower hardware-related constraints (memory and computing). We introduce an automatic adaptation of linear symmetric quantizer scaling factors to perform quantized levels equalization, aiming at stabilizing quinary and ternary weights training. In addition, a proposed layer-shared Bit-Shift Normalization significantly simplifies the implementation of the hardware-expensive Batch Normalization. For a specific configuration in which the encoder design only requires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides, we also show that this quantized encoder can be used to compress image patch-by-patch while the reconstruction can performed remotely, by a dedicated full-frame decoder. This solution typically enables an end-to-end compression almost without any block artifacts, outperforming patch-based state-of-the-art techniques employing a patch-constant bitrate.	 | 即使应用特定集成电路（ASIC）已被证明是将推理集成到边缘的合适选择，它们在适用性方面通常也受到限制。在本文中，我们展示了专门为图像处理设计的ASIC神经网络加速器可以应用于不同级别的多种任务：图像分类和压缩，同时需要非常有限的硬件资源。关键组件是一个可配置的混合精度（3b/2b/1b）编码器，该编码器利用适当的权重和激活量化与卷积层结构剪枝相结合，以降低与硬件相关的约束（存储和计算）。我们介绍了一种自动调整线性对称量化器量化因子的方法，以实现量化级别的均衡，旨在稳定五进制和三进制权重的训练。此外，提出了一种层共享位移规范化方法，显著简化了成本高昂的批量规范化硬件的实现。对于一种特定的配置，在这种配置中，编码器设计只需要1Mb，对CIFAR-10的分类准确率达到87.5%。此外，我们还展示了这种量化编码器可以用于逐块压缩图像，而重建可以通过一个专门的全帧解码器在远程进行。此解决方案通常能够实现几乎无块状伪影的端到端压缩，从而超越了基于块的最新技术，这些技术使用基于块的恒定比特率。
2501.05089	 | Supervised Learning with Evolving Tasks and Performance Guarantees	 | Verónica Álvarez,Santiago Mazuelas,Jose A. Lozano	 | 本文提出了一种适用于多个监督学习场景的学习方法，能够适应任务随时间演变的情况，并提供了可计算的性能保证，实验结果表明该方法在多个场景中能提高性能并验证了性能保证的可靠性。	 | Multiple supervised learning scenarios are composed by a sequence of classification tasks. For instance, multi-task learning and continual learning aim to learn a sequence of tasks that is either fixed or grows over time. Existing techniques for learning tasks that are in a sequence are tailored to specific scenarios, lacking adaptability to others. In addition, most of existing techniques consider situations in which the order of the tasks in the sequence is not relevant. However, it is common that tasks in a sequence are evolving in the sense that consecutive tasks often have a higher similarity. This paper presents a learning methodology that is applicable to multiple supervised learning scenarios and adapts to evolving tasks. Differently from existing techniques, we provide computable tight performance guarantees and analytically characterize the increase in the effective sample size. Experiments on benchmark datasets show the performance improvement of the proposed methodology in multiple scenarios and the reliability of the presented performance guarantees.	 | 以下是该摘要的中文翻译：  多监督学习场景由一系列分类任务组成。例如，多任务学习和持续学习旨在学习固定或随时间增长的任务序列。现有的一系列任务学习技术针对特定场景进行了定制，缺乏对其他场景的适应性。此外，大多数现有技术假设序列中任务的顺序无关紧要。然而，通常情况下，序列中的任务在演变，即连续任务往往具有更高的相似性。本文提出了一种适用于多个监督学习场景的学习方法，能够适应演变中的任务。与现有技术不同，我们提供了可计算的性能保证，并从理论上分析了有效样本大小的增加。在基准数据集上的实验表明，在多个场景中，所提出的方法能提高性能，并且展示了所提出的性能保证的可靠性。
2501.05087	 | Enhanced Quantile Regression with Spiking Neural Networks for Long-Term System Health Prognostics	 | David J Poland	 | 本文提出了一种基于增强分位回归神经网络（EQRNN）的预测维护框架，用于工业机器人系统的故障预测，该框架通过双重计算阶段实现高效且准确的故障预警，并在实际测试中显著降低了系统故障和维护停机时间。	 | This paper presents a novel predictive maintenance framework centered on Enhanced Quantile Regression Neural Networks EQRNNs, for anticipating system failures in industrial robotics. We address the challenge of early failure detection through a hybrid approach that combines advanced neural architectures. The system leverages dual computational stages: first implementing an EQRNN optimized for processing multi-sensor data streams including vibration, thermal, and power signatures, followed by an integrated Spiking Neural Network SNN, layer that enables microsecond-level response times. This architecture achieves notable accuracy rates of 92.3\% in component failure prediction with a 90-hour advance warning window. Field testing conducted on an industrial scale with 50 robotic systems demonstrates significant operational improvements, yielding a 94\% decrease in unexpected system failures and 76\% reduction in maintenance-related downtimes. The framework's effectiveness in processing complex, multi-modal sensor data while maintaining computational efficiency validates its applicability for Industry 4.0 manufacturing environments.	 | 本文介绍了一种以增强分位回归神经网络（EQRNN）为中心的新型预测维护框架，用于工业机器人系统的故障预测。我们通过结合先进的神经架构来应对早期故障检测的挑战。该系统利用了双重计算阶段：首先采用优化用于处理包括振动、热和功率特征在内的多传感器数据流的EQRNN，然后是集成尖峰神经网络（SNN）层，该层能够实现微秒级的响应速度。该架构在90小时的提前预警窗口内实现了组件故障预测的92.3%的显著准确率。在工业规模的50个机器人系统上进行的现场测试显示，这一框架能够显著提高运营性能，将意外系统故障减少了94%，同时将与维护相关的停机时间减少了76%。该框架在处理复杂多模态传感器数据的同时保持计算效率，证明了其适用于第四次工业革命（Industry 4.0）制造环境的有效性。
2501.05085	 | End-to-End Deep Learning for Interior Tomography with Low-Dose X-ray CT	 | Yoseob Han,Dufan Wu,Kyungsang Kim,Quanzheng Li	 | 本文提出了一种基于双域卷积神经网络的方法来解决内部X射线计算机断层扫描中的耦合伪影问题，该方法能够分别处理低剂量CT和区域兴趣CT带来的噪声和截断问题，并优于传统的图像域深度学习方法。	 | Objective: There exist several X-ray computed tomography (CT) scanning strategies to reduce a radiation dose, such as (1) sparse-view CT, (2) low-dose CT, and (3) region-of-interest (ROI) CT (called interior tomography). To further reduce the dose, the sparse-view and/or low-dose CT settings can be applied together with interior tomography. Interior tomography has various advantages in terms of reducing the number of detectors and decreasing the X-ray radiation dose. However, a large patient or small field-of-view (FOV) detector can cause truncated projections, and then the reconstructed images suffer from severe cupping artifacts. In addition, although the low-dose CT can reduce the radiation exposure dose, analytic reconstruction algorithms produce image noise. Recently, many researchers have utilized image-domain deep learning (DL) approaches to remove each artifact and demonstrated impressive performances, and the theory of deep convolutional framelets supports the reason for the performance improvement. Approach: In this paper, we found that the image-domain convolutional neural network (CNN) is difficult to solve coupled artifacts, based on deep convolutional framelets. Significance: To address the coupled problem, we decouple it into two sub-problems: (i) image domain noise reduction inside truncated projection to solve low-dose CT problem and (ii) extrapolation of projection outside truncated projection to solve the ROI CT problem. The decoupled sub-problems are solved directly with a novel proposed end-to-end learning using dual-domain CNNs. Main results: We demonstrate that the proposed method outperforms the conventional image-domain deep learning methods, and a projection-domain CNN shows better performance than the image-domain CNNs which are commonly used by many researchers.	 | 目标：存在几种X射线计算机断层扫描（CT）扫描策略以减少辐射剂量，如（1）稀视角CT，（2）低剂量CT，和（3）区域兴趣（ROI）CT（称为内部CT）。为了进一步降低剂量，可以将稀视角和/或低剂量CT设置与内部CT一起应用。内部CT在减少探测器数量和减少X射线辐射剂量方面具有多种优势。然而，大型患者或小视野（FOV）探测器会导致投影截断，从而导致重建图像出现严重的凹陷伪影。此外，尽管低剂量CT可以减少辐射暴露剂量，但解析重建算法会产生图像噪声。最近，许多研究人员利用图像域深度学习（DL）方法去除每个伪影并展示了令人印象深刻的表现，而深度卷积小波理论支持性能提升的原因。方法：在本文中，我们发现基于深度卷积小波的图像域卷积神经网络（CNN）难以解决耦合伪影问题。意义：为了解决耦合问题，我们将之分解为两个子问题：（i）在截断投影内减少图像域噪声以解决低剂量CT问题；（ii）外推投影以解决ROI CT问题。分解后的子问题通过新的端到端学习方法直接使用双域CNN解决。主要结果：我们证明了所提出的方法优于传统的图像域深度学习方法，并且投影域CNN的性能优于许多研究人员常用的图像域CNN。
2501.05082	 | Comparison of Feature Learning Methods for Metadata Extraction from PDF Scholarly Documents	 | Zeyd Boukhers,Cong Yang	 | 该研究评估了多种特征学习和预测方法，旨在从具有高模板差异性的科学文档中提取元数据，以提高其可访问性并促进更广泛的应用，结果显示这些方法在提取元数据方面的准确性和效率，并提供了方法比较的实验结果和见解。	 | The availability of metadata for scientific documents is pivotal in propelling scientific knowledge forward and for adhering to the FAIR principles (i.e. Findability, Accessibility, Interoperability, and Reusability) of research findings. However, the lack of sufficient metadata in published documents, particularly those from smaller and mid-sized publishers, hinders their accessibility. This issue is widespread in some disciplines, such as the German Social Sciences, where publications often employ diverse templates. To address this challenge, our study evaluates various feature learning and prediction methods, including natural language processing (NLP), computer vision (CV), and multimodal approaches, for extracting metadata from documents with high template variance. We aim to improve the accessibility of scientific documents and facilitate their wider use. To support our comparison of these methods, we provide comprehensive experimental results, analyzing their accuracy and efficiency in extracting metadata. Additionally, we provide valuable insights into the strengths and weaknesses of various feature learning and prediction methods, which can guide future research in this field.	 | 科学文档的元数据可用性对于推动科学知识的发展至关重要，并且对于遵循研究发现的FAIR原则（即可查找性、可访问性、互操作性和可重用性）也至关重要。然而，特别是在小型和中型出版商的作品中，缺乏足够的元数据影响了其可访问性。这一问题在一些学科中尤为普遍，例如德国社会科学领域，其中出版物常常使用多种多样的模板。为了应对这一挑战，我们的研究评估了各种特征学习和预测方法，包括自然语言处理（NLP）、计算机视觉（CV）和多模态方法，以从具有高模板差异性的文档中提取元数据。我们旨在提高科学文档的可访问性，并促进其更广泛的应用。为了支持这些方法的比较，我们提供了全面的实验结果，分析它们在提取元数据方面的准确性和效率。此外，我们还提供了各种特征学习和预测方法的优势和劣势的宝贵见解，这些见解可指导该领域的未来研究。
2501.05076	 | TipSegNet: Fingertip Segmentation in Contactless Fingerprint Imaging	 | Laurenz Ruzicka,Bernhard Kohn,Clemens Heitzinger	 | 本文提出了一种名为TipSegNet的新型深度学习模型，能够在灰度手部图像中实现最先进的指尖分割性能，显著提高了无接触指纹识别系统的准确性和可靠性，平均交并比（mIoU）达到0.987，准确率达到0.999。	 | Contactless fingerprint recognition systems offer a hygienic, user-friendly, and efficient alternative to traditional contact-based methods. However, their accuracy heavily relies on precise fingertip detection and segmentation, particularly under challenging background conditions. This paper introduces TipSegNet, a novel deep learning model that achieves state-of-the-art performance in segmenting fingertips directly from grayscale hand images. TipSegNet leverages a ResNeXt-101 backbone for robust feature extraction, combined with a Feature Pyramid Network (FPN) for multi-scale representation, enabling accurate segmentation across varying finger poses and image qualities. Furthermore, we employ an extensive data augmentation strategy to enhance the model's generalizability and robustness. TipSegNet outperforms existing methods, achieving a mean Intersection over Union (mIoU) of 0.987 and an accuracy of 0.999, representing a significant advancement in contactless fingerprint segmentation. This enhanced accuracy has the potential to substantially improve the reliability and effectiveness of contactless biometric systems in real-world applications.	 | 无接触指纹识别系统提供了一种卫生、用户友好且高效的替代传统接触式方法的选择。然而，这些系统的准确性很大程度上依赖于在困难背景条件下精确的指尖检测和分割。本文介绍了一种名为TipSegNet的新型深度学习模型，该模型能够在灰度手部图像中直接实现最先进的指尖分割性能。TipSegNet利用ResNeXt-101骨干网络进行稳健的特征提取，并结合特征金字塔网络（FPN）进行多尺度表示，从而能够在不同手指姿态和图像质量下实现精确分割。此外，我们还采用了一种广泛的图像增强策略，以提高模型的泛化能力和鲁棒性。TipSegNet在现有方法中表现出色，平均交并比（mIoU）达到0.987，准确率达到0.999，这标志着在无接触指纹分割方面取得了显著进展。这种增强的准确性有望在实际应用中显著提高无接触生物识别系统的可靠性和有效性。
2501.05075	 | A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for General Industrial Process Tasks Based on Large Language Model	 | Shuo Tong,Han Liu,Runyuan Guo,Xueqiong Tian,Wenqing Wang,Ding Liu,Youmin Zhang	 | 本文提出了一种名为LLM-TKESS的框架，利用大型语言模型的强大能力来增强软传感器的建模，通过辅助变量序列编码器和两阶段微调策略，克服了传统软传感器在复杂数据和预测性能方面的限制，并通过实验验证了其在小样本条件下的优越性能。	 | Data-driven soft sensors (DDSS) have become mainstream methods for predicting key performance indicators in process industries. However, DDSS development requires complex and costly customized designs tailored to various tasks during the modeling process. Moreover, DDSS are constrained to a single structured data modality, limiting their ability to incorporate additional contextual knowledge. Furthermore, DDSSs' limited representation learning leads to weak predictive performance with scarce data. To address these challenges, we propose a general framework named LLM-TKESS (large language model for text-based knowledge-embedded soft sensing), harnessing the powerful general problem-solving capabilities, cross-modal knowledge transfer abilities, and few-shot capabilities of LLM for enhanced soft sensing modeling. Specifically, an auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLM's potential for capturing temporal relationships within series and spatial semantic relationships among auxiliary variables. Then, we propose a two-stage fine-tuning alignment strategy: in the first stage, employing parameter-efficient fine-tuning through autoregressive training adjusts LLM to rapidly accommodate process variable data, resulting in a soft sensing foundation model (SSFM). Subsequently, by training adapters, we adapt the SSFM to various downstream tasks without modifying its architecture. Then, we propose two text-based knowledge-embedded soft sensors, integrating new natural language modalities to overcome the limitations of pure structured data models. Furthermore, benefiting from LLM's pre-existing world knowledge, our model demonstrates outstanding predictive capabilities in small sample conditions. Using the thermal deformation of air preheater rotor as a case study, we validate through extensive experiments that LLM-TKESS exhibits outstanding performance.	 | 数据驱动的软传感器（DDSS）已成为过程工业中预测关键绩效指标的主要方法。然而，在建模过程中，DDSS的开发需要针对各种任务进行复杂且昂贵的定制设计。此外，DDSS受到单一结构化数据模态的限制，限制了它们整合额外上下文知识的能力。进一步而言，DDSS的有限表征学习导致在数据稀缺的情况下预测性能较弱。为解决这些挑战，我们提出了一种名为LLM-TKESS（大型语言模型用于基于文本的知识嵌入软传感）的一般框架。该框架利用了大型语言模型（LLM）强大的通用问题解决能力、跨模态知识迁移能力和少样本能力，以增强软传感建模。具体地，我们提出了一个辅助变量序列编码器（AVS编码器）来释放LLM捕捉时间序列内部时序关系以及辅助变量之间空间语义关系的潜力。然后，我们提出了两阶段微调对齐策略：在第一阶段，通过自回归训练进行参数高效微调，使LLM能够快速适应过程变量数据，从而形成一个软传感基础模型（SSFM）。随后，通过训练适配器，我们将SSFM适应各种下游任务，而不修改其架构。此外，我们提出了两种基于文本的知识嵌入软传感器，将新的自然语言模态集成进来，以克服纯结构化数据模型的局限性。得益于LLM预先存在的世界知识，我们的模型在小样本条件下表现出卓越的预测能力。以空气预热器转子的热变形为例，通过广泛的实验验证了LLM-TKESS表现出色。
2501.05068	 | D3RM: A Discrete Denoising Diffusion Refinement Model for Piano Transcription	 | Hounsu Kim,Taegyun Kwon,Juhan Nam	 | 本文提出了一种新的基于离散扩散模型的钢琴转谱架构，通过引入邻域注意力层和差异化的转换状态策略提高细化能力，并在MAESTRO数据集上取得了优于现有基于扩散模型和基线模型的F1分数。	 | Diffusion models have been widely used in the generative domain due to their convincing performance in modeling complex data distributions. Moreover, they have shown competitive results on discriminative tasks, such as image segmentation. While diffusion models have also been explored for automatic music transcription, their performance has yet to reach a competitive level. In this paper, we focus on discrete diffusion model's refinement capabilities and present a novel architecture for piano transcription. Our model utilizes Neighborhood Attention layers as the denoising module, gradually predicting the target high-resolution piano roll, conditioned on the finetuned features of a pretrained acoustic model. To further enhance refinement, we devise a novel strategy which applies distinct transition states during training and inference stage of discrete diffusion models. Experiments on the MAESTRO dataset show that our approach outperforms previous diffusion-based piano transcription models and the baseline model in terms of F1 score. Our code is available in https://github.com/hanshounsu/d3rm.	 | 扩散模型因其在建模复杂数据分布方面表现出色而广泛应用于生成领域。此外，扩散模型在诸如图像分割等判别任务上也表现出竞争力。尽管扩散模型在自动音乐转谱方面也得到了探索，但其性能尚未达到与传统方法竞争的水平。本文专注于离散扩散模型的细化能力，并提出了一种新的钢琴转谱架构。我们的模型利用了邻域注意力层作为去噪模块，逐步预测目标高分辨率的钢琴卷，条件是预训练声学模型的微调特征。为了进一步增强细化能力，我们设计了一种新策略，在离散扩散模型的训练和推理阶段应用不同的转换状态。在MAESTRO数据集上的实验表明，我们的方法在F1分数上优于之前的基于扩散的钢琴转谱模型和基线模型。我们的代码可在 https://github.com/hanshounsu/d3rm 获得。
2501.05058	 | Simultaneous emulation and downscaling with physically-consistent deep learning-based regional ocean emulators	 | Leonard Lupin-Jimenez,Moein Darman,Subhashis Hazarika,Tianning Wu,Michael Gray,Ruyoing He,Anthony Wong,Ashesh Chattopadhyay	 | 本文提出了一种基于深度学习的海洋模拟与降尺度框架，专注于墨西哥湾的高分辨率区域海洋模拟，能够在8公里空间分辨率下整合海洋表层变量，并通过物理约束生成模型将分辨率进一步降尺度至4公里，同时保持良好的短期技能和长期统计表现。	 | Building on top of the success in AI-based atmospheric emulation, we propose an AI-based ocean emulation and downscaling framework focusing on the high-resolution regional ocean over Gulf of Mexico. Regional ocean emulation presents unique challenges owing to the complex bathymetry and lateral boundary conditions as well as from fundamental biases in deep learning-based frameworks, such as instability and hallucinations. In this paper, we develop a deep learning-based framework to autoregressively integrate ocean-surface variables over the Gulf of Mexico at $8$ Km spatial resolution without unphysical drifts over decadal time scales and simulataneously downscale and bias-correct it to $4$ Km resolution using a physics-constrained generative model. The framework shows both short-term skills as well as accurate long-term statistics in terms of mean and variability.	 | 基于AI在大气模拟方面的成功，我们提出了一个专注于墨西哥湾高分辨率区域海洋的AI基海洋模拟与降尺度框架。区域海洋模拟面临着独特挑战，包括复杂的海底地形和侧向边界条件，以及基于深度学习框架的基本偏差，如不稳定性与幻觉。在本文中，我们开发了一种基于深度学习的方法，以自回归的方式在8公里的空间分辨率上整合墨西哥湾的海洋表层变量，同时在长达十年的时间尺度上避免不物理的漂移，并使用一个物理约束生成模型将其同时降尺度和偏差校正至4公里分辨率。该框架在短期技能和长期统计（包括均值和变异性）方面均显示出良好的表现。
2501.05057	 | LearningFlow: Automated Policy Learning Workflow for Urban Driving with Large Language Models	 | Zengqi Peng,Yubin Wang,Xu Han,Lei Zheng,Jun Ma	 | LearningFlow是一种创新的自动策略学习工作流，利用多个大型语言模型代理在城市驾驶中协作生成定制化的训练课程和奖励函数，从而显著减少对手动设计的依赖并提高样本效率。实验结果显示，LearningFlow在多种驾驶任务中表现出优越性能和稳健泛化能力。	 | Recent advancements in reinforcement learning (RL) demonstrate the significant potential in autonomous driving. Despite this promise, challenges such as the manual design of reward functions and low sample efficiency in complex environments continue to impede the development of safe and effective driving policies. To tackle these issues, we introduce LearningFlow, an innovative automated policy learning workflow tailored to urban driving. This framework leverages the collaboration of multiple large language model (LLM) agents throughout the RL training process. LearningFlow includes a curriculum sequence generation process and a reward generation process, which work in tandem to guide the RL policy by generating tailored training curricula and reward functions. Particularly, each process is supported by an analysis agent that evaluates training progress and provides critical insights to the generation agent. Through the collaborative efforts of these LLM agents, LearningFlow automates policy learning across a series of complex driving tasks, and it significantly reduces the reliance on manual reward function design while enhancing sample efficiency. Comprehensive experiments are conducted in the high-fidelity CARLA simulator, along with comparisons with other existing methods, to demonstrate the efficacy of our proposed approach. The results demonstrate that LearningFlow excels in generating rewards and curricula. It also achieves superior performance and robust generalization across various driving tasks, as well as commendable adaptation to different RL algorithms.	 | 近年来，强化学习（RL）的研究展示了在自动驾驶领域巨大的潜力。尽管前景广阔，但在复杂环境中的手动设计奖励函数以及低样本效率等挑战仍阻碍着安全有效的驾驶策略的发展。为应对这些挑战，我们提出了LearningFlow，这是一种针对城市驾驶的创新自动策略学习工作流。该框架利用多个大型语言模型（LLM）代理在整个RL训练过程中协作。LearningFlow包括课程序列生成过程和奖励生成过程，这两个过程协同工作以通过生成定制化的训练课程和奖励函数来引导RL策略。特别是，每个过程都由一个分析代理支持，该代理评估训练进度并为生成代理提供关键洞察。通过这些LLM代理的协作努力，LearningFlow能够自动跨多种复杂的驾驶任务学习策略，并显著减少对手动奖励函数设计的依赖，同时提高样本效率。我们在高保真CARLA模拟器中进行了全面的实验，并与其他现有方法进行了比较，以展示我们提出的方法的有效性。结果显示，LearningFlow在生成奖励和课程方面表现出色，实现了在各种驾驶任务中的优越性能和稳健泛化，并能够很好地适应不同的RL算法。
2501.05037	 | LongViTU: Instruction Tuning for Long-Form Video Understanding	 | Rujie Wu,Xiaojian Ma,Hai Ci,Yue Fan,Yuxuan Wang,Haozhe Zhao,Qing Li,Yizhou Wang	 | LongViTU是一个包含约121万对问答和900小时视频的自动生成数据集，旨在用于长视频理解，每对问答具有长期背景、丰富知识和明确时间戳标签等特点，被用作指令跟随基准数据集；进一步的有监督微调显著提高了模型在长视频理解任务中的性能，特别是对于不同分布的基准测试具有强大的泛化能力。	 | This paper introduce LongViTU, a large-scale (~121k QA pairs, ~900h videos), automatically generated dataset for long-form video understanding. We developed a systematic approach that organizes videos into a hierarchical tree structure and incorporates self-revision mechanisms to ensure high-quality QA pairs. Each QA pair in LongViTU features: 1) long-term context (average certificate length of 4.6 minutes); 2) rich knowledge and condensed reasoning (commonsense, causality, planning, etc.); and 3) explicit timestamp labels for relevant events. LongViTU also serves as a benchmark for instruction following in long-form and streaming video understanding. We evaluate the open-source state-of-the-art long video understanding model, LongVU, and the commercial model, Gemini-1.5-Pro, on our benchmark. They achieve GPT-4 scores of 49.9 and 52.3, respectively, underscoring the substantial challenge posed by our benchmark. Further supervised fine-tuning (SFT) on LongVU led to performance improvements of 12.0% on our benchmark, 2.2% on the in-distribution (ID) benchmark EgoSchema, 1.0%, 2.2% and 1.2% on the out-of-distribution (OOD) benchmarks VideoMME (Long), WorldQA and OpenEQA, respectively. These outcomes demonstrate LongViTU's high data quality and robust OOD generalizability.	 | 本文介绍了LongViTU，这是一个大规模的自动生成数据集，包含约121万对问答（QA）和约900小时的视频，旨在用于长视频理解。我们开发了一种系统的方法，将视频组织成层次树状结构，并引入自我修订机制以确保高质量的问答对。LongViTU中的每一对问答具备以下特点：1）长期背景（平均证书长度为4.6分钟）；2）丰富的知识和浓缩的推理（常识、因果关系、规划等）；3）与相关事件的明确时间戳标签。此外，LongViTU还作为长视频和流式视频理解中的指令跟随基准数据集。我们对开源的先进长视频理解模型LongVU和商用模型Gemini-1.5-Pro进行了评估。在我们的基准测试中，它们分别获得了GPT-4评分的49.9和52.3，突显了我们基准测试带来的重大挑战。进一步的有监督微调（SFT）对LongVU的性能改进在我们的基准测试中提升了12.0%，在同分布（ID）基准测试EgoSchema中提升了2.2%，在三个不同分布（OOD）基准测试VideoMME（长视频）、WorldQA和OpenEQA中分别提升了1.0%、2.2%和1.2%。这些结果表明，LongViTU的数据质量高且具有强大的OOD泛化能力。
2501.05034	 | Towards Fingerprint Mosaicking Artifact Detection: A Self-Supervised Deep Learning Approach	 | Laurenz Ruzicka,Alexander Spenke,Stephan Bergmann,Gerd Nolden,Bernhard Kohn,Clemens Heitzinger	 | 本文提出了一种基于深度学习的方法，利用自监督学习在大规模未标记指纹数据上训练模型，有效检测和评分指纹图像中的马赛克artifact，提高生物识别系统的准确性和可靠性。	 | Fingerprint mosaicking, which is the process of combining multiple fingerprint images into a single master fingerprint, is an essential process in modern biometric systems. However, it is prone to errors that can significantly degrade fingerprint image quality. This paper proposes a novel deep learning-based approach to detect and score mosaicking artifacts in fingerprint images. Our method leverages a self-supervised learning framework to train a model on large-scale unlabeled fingerprint data, eliminating the need for manual artifact annotation. The proposed model effectively identifies mosaicking errors, achieving high accuracy on various fingerprint modalities, including contactless, rolled, and pressed fingerprints and furthermore proves to be robust to different data sources. Additionally, we introduce a novel mosaicking artifact score to quantify the severity of errors, enabling automated evaluation of fingerprint images. By addressing the challenges of mosaicking artifact detection, our work contributes to improving the accuracy and reliability of fingerprint-based biometric systems.	 | 指纹马赛克是一种将多个指纹图像合成一个主指纹图像的过程，在现代生物识别系统中是必不可少的。然而，它容易出现错误，这可能会显著降低指纹图像的质量。本文提出了一种基于深度学习的方法来检测和评分指纹图像中的马赛克 artifacts。我们的方法利用自监督学习框架在大规模未标记的指纹数据上训练模型，从而消除了手动标注 artifacts 的需求。所提出的模型能够有效识别马赛克错误，并在各种指纹模态（包括非接触式、卷曲和压印指纹）上实现了高精度，且对不同的数据源具有鲁棒性。此外，我们引入了一种新的马赛克 artifact 评分方法来量化错误的严重程度，使指纹图像的自动评估成为可能。通过解决马赛克 artifact 检测的挑战，我们的工作有助于提高基于指纹的生物识别系统的准确性和可靠性。
2501.05031	 | ECBench: Can Multi-modal Foundation Models Understand the Egocentric World? A Holistic Embodied Cognition Benchmark	 | Ronghao Dang,Yuqian Yuan,Wenqi Zhang,Yifei Xin,Boqiang Zhang,Long Li,Liuyi Wang,Qinyang Zeng,Xin Li,Lidong Bing	 | ECBench 是一种系统性的基准工具，旨在评估大型视觉-语言模型在具身认知方面的能力，特别是通过多样化的场景视频和全面的问题格式；它采用细致的人工注释和多轮问题筛选策略确保高质量，并利用 ECEval 系统进行公平合理的评估。	 | The enhancement of generalization in robots by large vision-language models (LVLMs) is increasingly evident. Therefore, the embodied cognitive abilities of LVLMs based on egocentric videos are of great interest. However, current datasets for embodied video question answering lack comprehensive and systematic evaluation frameworks. Critical embodied cognitive issues, such as robotic self-cognition, dynamic scene perception, and hallucination, are rarely addressed. To tackle these challenges, we propose ECBench, a high-quality benchmark designed to systematically evaluate the embodied cognitive abilities of LVLMs. ECBench features a diverse range of scene video sources, open and varied question formats, and 30 dimensions of embodied cognition. To ensure quality, balance, and high visual dependence, ECBench uses class-independent meticulous human annotation and multi-round question screening strategies. Additionally, we introduce ECEval, a comprehensive evaluation system that ensures the fairness and rationality of the indicators. Utilizing ECBench, we conduct extensive evaluations of proprietary, open-source, and task-specific LVLMs. ECBench is pivotal in advancing the embodied cognitive capabilities of LVLMs, laying a solid foundation for developing reliable core models for embodied agents. All data and code are available at https://github.com/Rh-Dang/ECBench.	 | 通过大型视觉-语言模型（LVLMs）增强机器人的一般化能力日益明显。因此，基于自我中心视频的LVLMs的具身认知能力引起了极大的兴趣。然而，现有的具身视频问答数据集缺乏全面和系统的评估框架。诸如机器人自我认知、动态场景感知和幻觉等关键的具身认知问题很少得到解决。为了解决这些挑战，我们提出了一种名为ECBench的高质量基准，旨在系统性地评估LVLMs的具身认知能力。ECBench具有多样化的场景视频来源、开放且多样的问题格式以及30个维度的具身认知。为了确保高质量、平衡性和高视觉依赖性，ECBench采用了不依赖类别的细致人工注释和多轮问题筛选策略。此外，我们还引入了ECEval，这是一种全面的评估系统，确保评估指标的公平性和合理性。利用ECBench，我们对专有、开源和特定任务的LVLMs进行了广泛的评估。ECBench对提升LVLMs的具身认知能力至关重要，为开发可靠的具身代理核心模型奠定了坚实基础。所有数据和代码可在https://github.com/Rh-Dang/ECBench获取。
2501.05014	 | UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation	 | Oleg Sautenkov,Yasheerah Yaqoot,Artem Lykov,Muhammad Ahsan Mustafa,Grik Tadevosyan,Aibek Akhmetkazy,Miguel Altamirano Cabrera,Mikhail Martynov,Sausar Karaf,Dzmitry Tsetserukou	 | UAV-VLA系统结合卫星影像处理、视觉语言模型和GPT技术，允许用户通过简单的文本请求生成飞行路径和行动计划，提升空中操作的效率和便捷性；该系统在轨迹生成和目标定位方面表现出色，轨迹长度差异为22%，平均误差为34.22米。	 | The UAV-VLA (Visual-Language-Action) system is a tool designed to facilitate communication with aerial robots. By integrating satellite imagery processing with the Visual Language Model (VLM) and the powerful capabilities of GPT, UAV-VLA enables users to generate general flight paths-and-action plans through simple text requests. This system leverages the rich contextual information provided by satellite images, allowing for enhanced decision-making and mission planning. The combination of visual analysis by VLM and natural language processing by GPT can provide the user with the path-and-action set, making aerial operations more efficient and accessible. The newly developed method showed the difference in the length of the created trajectory in 22% and the mean error in finding the objects of interest on a map in 34.22 m by Euclidean distance in the K-Nearest Neighbors (KNN) approach.	 | UAV-VLA（视觉-语言-行动）系统是一种旨在与空中机器人进行沟通的工具。通过将卫星影像处理与视觉语言模型（VLM）以及GPT的强大处理能力相结合，UAV-VLA使用户能够通过简单的文本请求生成通用的飞行路径和行动计划。该系统利用丰富的卫星图像提供的上下文信息，增强了决策制定和任务规划。VLM的视觉分析和GPT的自然语言处理相结合，可以为用户提供路径和行动方案，从而使空中操作更加高效和便捷。新发展的方法在K-最近邻（KNN）方法中，生成的轨迹长度差异为22%，在欧几里得距离下，找到地图上感兴趣的物体的平均误差为34.22米。
2501.05007	 | Quantum-enhanced causal discovery for a small number of samples	 | Yota Maeda,Ken Arai,Yu Tanaka,Yu Terada,Hiroshi Ueno,Hiroyuki Tezuka	 | 该研究提出了一种无需假设底层模型结构的新量子彼得-克拉克（qPC）算法，用于从任意分布数据中发现因果关系，并通过实验验证了其在小样本量下的优越性能和对假阳性的有效控制。该量子算法在传统方法难以应对的场景中提供了更稳健和精确的因果推理，展示了量子计算在因果发现领域的潜在应用价值。	 | The discovery of causal relationships from observed data has attracted significant interest from disciplines such as economics, social sciences, epidemiology, and biology. In practical applications, considerable knowledge of the underlying systems is often unavailable, and real data are often associated with nonlinear causal structures, which make the direct use of most conventional causality analysis methods difficult. This study proposes a novel quantum Peter-Clark (qPC) algorithm for causal discovery that does not assume any underlying model structures. Based on the independence conditional tests in a class of reproducing kernel Hilbert spaces characterized by quantum circuits, the proposed qPC algorithm can explore causal relationships from the observed data drawn from arbitrary distributions. We conducted systematic experiments on fundamental graph parts of causal structures, demonstrating that the qPC algorithm exhibits a significantly better performance, particularly with smaller sample sizes compared to its classical counterpart. Furthermore, we proposed a novel optimization approach based on Kernel Target Alignment (KTA) for determining hyperparameters of quantum kernels. This method effectively reduced the risk of false positives in causal discovery, enabling more reliable inference. Our theoretical and experimental results demonstrate that the proposed quantum algorithm can empower classical algorithms for robust and accurate inference in causal discovery, supporting them in regimes where classical algorithms typically fail. Additionally, the effectiveness of this method was validated using the Boston Housing dataset as a real-world application. These findings demonstrate the new potential of quantum circuit-based causal discovery methods in addressing practical challenges, particularly in small-sample scenarios where traditional approaches have shown limitations.	 | 从观测数据中发现因果关系的研究引起了经济学、社会科学、流行病学和生物学等领域的广泛关注。在实际应用中，底层系统的信息往往不完全可用，真实数据通常与非线性因果结构相关，这使得大多数传统因果分析方法的直接使用变得困难。本研究提出了一种无需假设任何底层模型结构的新量子彼得-克拉克（qPC）算法，用于因果发现。基于由量子电路定义的一类再生核希尔伯特空间中的条件独立性检验，所提出的qPC算法可以从任意分布的数据中探索因果关系。我们对因果结构的基本图部分进行了系统实验，结果显示qPC算法在小样本数量下表现显著优于其经典对应物。此外，我们还基于核目标对齐（KTA）提出了一种新的优化方法，用于确定量子核的超参数。这种方法有效地降低了因果发现中假阳性风险，使推理更加可靠。我们的理论和实验结果表明，提出的量子算法能够使经典算法在因果发现中实现稳健和精确的推理，在经典算法通常失败的领域提供支持。此外，我们通过使用波士顿房价数据集作为实际应用验证了该方法的有效性。这些发现展示了基于量子电路的因果发现方法在应对实际挑战方面的全新潜力，特别是在传统方法在小样本场景中表现出局限性的领域。
2501.04982	 | CuRLA: Curriculum Learning Based Deep Reinforcement Learning for Autonomous Driving	 | Bhargava Uppuluri,Anjel Patel,Neil Mehta,Sridhar Kamath,Pratyush Chakraborty	 | 该研究提出了一种结合深度强化学习和课程学习的方法，利用Proximal Policy Optimization (PPO)代理和Variational Autoencoder (VAE)在CARLA模拟器中学习安全驾驶，以提高自动驾驶代理在复杂环境中的适应性和可靠性，同时增强其安全性能。	 | In autonomous driving, traditional Computer Vision (CV) agents often struggle in unfamiliar situations due to biases in the training data. Deep Reinforcement Learning (DRL) agents address this by learning from experience and maximizing rewards, which helps them adapt to dynamic environments. However, ensuring their generalization remains challenging, especially with static training environments. Additionally, DRL models lack transparency, making it difficult to guarantee safety in all scenarios, particularly those not seen during training. To tackle these issues, we propose a method that combines DRL with Curriculum Learning for autonomous driving. Our approach uses a Proximal Policy Optimization (PPO) agent and a Variational Autoencoder (VAE) to learn safe driving in the CARLA simulator. The agent is trained using two-fold curriculum learning, progressively increasing environment difficulty and incorporating a collision penalty in the reward function to promote safety. This method improves the agent's adaptability and reliability in complex environments, and understand the nuances of balancing multiple reward components from different feedback signals in a single scalar reward function. Keywords: Computer Vision, Deep Reinforcement Learning, Variational Autoencoder, Proximal Policy Optimization, Curriculum Learning, Autonomous Driving.	 | 在自动驾驶领域，传统的计算机视觉（CV）代理往往在不熟悉的情境下因训练数据中的偏差而难以应对。深度强化学习（DRL）代理通过从经验中学习并最大化奖励来解决这一问题，这有助于它们适应动态环境。然而，确保其泛化能力仍然具有挑战性，尤其是在静态训练环境中。此外，DRL模型的透明度较低，这使得在所有场景中保证安全变得困难，特别是那些在训练中未见过的场景。为了解决这些问题，我们提出了一种将DRL与课程学习相结合的方法用于自动驾驶。我们的方法使用Proximal Policy Optimization（PPO）代理和Variational Autoencoder（VAE）在CARLA模拟器中学习安全驾驶。代理通过两阶段的课程学习进行训练，逐步增加环境难度，并在奖励函数中引入碰撞惩罚以促进安全性。这种方法提高了代理在复杂环境中的适应性和可靠性，并能够理解从不同反馈信号中平衡多种奖励组件的细微差别。关键词：计算机视觉，深度强化学习，变分自编码器，Proximal Policy Optimization，课程学习，自动驾驶。
2501.04971	 | Self-Adaptive Ising Machines for Constrained Optimization	 | Corentin Delacour	 | 自适应伊斯丁机器通过逐步塑造其能量景观来解决带有约束的优化问题，与传统的伊斯丁机器相比，能够在较少的样本数下找到更好的解决方案，特别是在处理二次背包问题等具有线性约束的伊斯丁问题时表现出色。	 | Ising machines (IM) are physics-inspired alternatives to von Neumann architectures for solving hard optimization tasks. By mapping binary variables to coupled Ising spins, IMs can naturally solve unconstrained combinatorial optimization problems such as finding maximum cuts in graphs. However, despite their importance in practical applications, constrained problems remain challenging to solve for IMs that require large quadratic energy penalties to ensure the correspondence between energy ground states and constrained optimal solutions. To relax this requirement, we propose a self-adaptive IM that iteratively shapes its energy landscape using a Lagrange relaxation of constraints and avoids prior tuning of penalties. Using a probabilistic-bit (p-bit) IM emulated in software, we benchmark our algorithm with multidimensional knapsack problems (MKP) and quadratic knapsack problems (QKP), the latter being an Ising problem with linear constraints. For QKP with 300 variables, the proposed algorithm finds better solutions than state-of-the-art IMs such as Fujitsu's Digital Annealer and requires 7,500x fewer samples. Our results show that adapting the energy landscape during the search can speed up IMs for constrained optimization.	 | 以下是该摘要的中文翻译：  伊斯丁机器（IM）是受物理启发的替代冯·诺伊曼架构，用于解决困难的优化任务。通过将二进制变量映射到耦合的伊斯丁自旋中，IM可以直接解决诸如图中最大割之类的不受约束的组合优化问题。然而，尽管它们在实际应用中具有重要意义，但对于需要通过大量二次能量罚分来确保能量基态与约束最优点之间的对应关系的约束问题，IM仍然难以解决。为了缓解这一要求，我们提出了一种自适应IM，它通过拉格朗日松弛约束来逐步塑造其能量景观，并避免预先调整罚分。我们使用软件模拟的概率位（p-bit）IM，将该算法与多维背包问题（MKP）和二次背包问题（QKP）进行了基准测试，后者是一个具有线性约束的伊斯丁问题。对于具有300个变量的QKP，所提出的算法找到了比富士通的“数字调制器”等最先进的IM更好的解决方案，并且需要少7500倍的样本数。我们的结果表明，在搜索过程中适应能量景观可以加快IM在约束优化中的速度。
2501.04961	 | Demystifying Domain-adaptive Post-training for Financial LLMs	 | Zixuan Ke,Yifei Ming,Xuan-Phi Nguyen,Caiming Xiong,Shafiq Joty	 | FINDAP 是一种系统性方法，旨在研究金融领域大型语言模型的适应性后训练，通过确定目标领域所需的能力并设计相应的评估体系，提出了一种新颖的偏好数据蒸馏方法，有效提升了模型在金融任务上的性能。	 | Domain-adaptive post-training of large language models (LLMs) has emerged as a promising approach for specialized domains such as medicine and finance. However, significant challenges remain in identifying optimal adaptation criteria and training strategies across varying data and model configurations. To address these challenges, we introduce FINDAP, a systematic and fine-grained investigation into domain-adaptive post-training of LLMs for the finance domain. Our approach begins by identifying the core capabilities required for the target domain and designing a comprehensive evaluation suite aligned with these needs. We then analyze the effectiveness of key post-training stages, including continual pretraining, instruction tuning, and preference alignment. Building on these insights, we propose an effective training recipe centered on a novel preference data distillation method, which leverages process signals from a generative reward model. The resulting model, Llama-Fin, achieves state-of-the-art performance across a wide range of financial tasks. Our analysis also highlights how each post-training stage contributes to distinct capabilities, uncovering specific challenges and effective solutions, providing valuable insights for domain adaptation of LLMs. Project page: https://github.com/SalesforceAIResearch/FinDap	 | 大型语言模型（LLMs）在医学和金融等特定领域的适应性后训练已展现出有前景的方法。然而，仍然存在在不同数据和模型配置下确定最佳适应标准和训练策略的重大挑战。为了解决这些挑战，我们引入了FINDAP，这是一种系统而精细的探索，旨在研究金融领域的LLMs适应性后训练。我们的方法首先确定了目标领域所需的核心能力，并设计了一套与这些需求对齐的全面评估套件。我们随后分析了关键后训练阶段的有效性，包括持续预训练、指令微调和偏好对齐。基于这些见解，我们提出了一种基于新颖的偏好数据蒸馏方法的有效训练配方，该方法利用生成奖励模型中的过程信号。由此产生的模型Llama-Fin在广泛范围的金融任务上达到了最先进的性能。我们的分析还强调了每个后训练阶段如何贡献不同的能力，揭示了特定的挑战和有效的解决方案，为LLMs的领域适应提供了宝贵的见解。项目页面：https://github.com/SalesforceAIResearch/FinDap
2501.04946	 | Non-asymptotic analysis of the performance of the penalized least trimmed squares in sparse models	 | Yijun Zuo	 | 本文首次建立了基于最小trimmed平方（LTS）估计器的有限样本（非渐近）误差界，以高概率进行估计和预测，特别适用于高维稀疏模型的正则化回归场景。	 | The least trimmed squares (LTS) estimator is a renowned robust alternative to the classic least squares estimator and is popular in location, regression, machine learning, and AI literature. Many studies exist on LTS, including its robustness, computation algorithms, extension to non-linear cases, asymptotics, etc. The LTS has been applied in the penalized regression in a high-dimensional real-data sparse-model setting where dimension $p$ (in thousands) is much larger than sample size $n$ (in tens, or hundreds). In such a practical setting, the sample size $n$ often is the count of sub-population that has a special attribute (e.g. the count of patients of Alzheimer's, Parkinson's, Leukemia, or ALS, etc.) among a population with a finite fixed size N. Asymptotic analysis assuming that $n$ tends to infinity is not practically convincing and legitimate in such a scenario. A non-asymptotic or finite sample analysis will be more desirable and feasible.   This article establishes some finite sample (non-asymptotic) error bounds for estimating and predicting based on LTS with high probability for the first time.	 | 最小trimmed平方（LTS）估计器是经典最小平方估计器的一种著名的稳健替代方案，在位置估计、回归分析、机器学习和人工智能文献中颇受欢迎。关于LTS的研究很多，包括它的稳健性、计算算法、非线性情况下的扩展、渐近性质等。LTS在高维稀疏模型的正则化回归中得到了应用，其中维度 $p$（以千计）远大于样本量 $n$（以十或百计）。在这样一个实际的情景中，样本量 $n$ 往往是具有某种特殊属性的子人群数量（例如阿尔茨海默病、帕金森病、白血病或ALS等患者的数量），而在一个固定大小为 $N$ 的总体中。当 $n$ 趋于无穷时的渐近分析在这样的场景中并不实际且可信。因此，进行非渐近或有限样本分析会更为合适和可行。  本文首次建立了基于LTS的有限样本（非渐近）误差界，以高概率估计和预测。
2501.04914	 | From Mesh Completion to AI Designed Crown	 | Golriz Hosseinimanesh,Farnoosh Ghadiri,Francois Guibault,Farida Cheriet,Julia Keren	 | 该研究提出了一种名为Dental Mesh Completion (DMC)的端到端深度学习方法，以自动化牙冠设计过程，通过生成基于点云上下文的牙冠网格，显著减少了手动调整的需要。实验结果显示，DMC方法在多个数据集上的查耳默斯距离仅为0.062，表现出色。	 | Designing a dental crown is a time-consuming and labor intensive process. Our goal is to simplify crown design and minimize the tediousness of making manual adjustments while still ensuring the highest level of accuracy and consistency. To this end, we present a new end- to-end deep learning approach, coined Dental Mesh Completion (DMC), to generate a crown mesh conditioned on a point cloud context. The dental context includes the tooth prepared to receive a crown and its surroundings, namely the two adjacent teeth and the three closest teeth in the opposing jaw. We formulate crown generation in terms of completing this point cloud context. A feature extractor first converts the input point cloud into a set of feature vectors that represent local regions in the point cloud. The set of feature vectors is then fed into a transformer to predict a new set of feature vectors for the missing region (crown). Subsequently, a point reconstruction head, followed by a multi-layer perceptron, is used to predict a dense set of points with normals. Finally, a differentiable point-to-mesh layer serves to reconstruct the crown surface mesh. We compare our DMC method to a graph-based convolutional neural network which learns to deform a crown mesh from a generic crown shape to the target geometry. Extensive experiments on our dataset demonstrate the effectiveness of our method, which attains an average of 0.062 Chamfer Distance.The code is available at:https://github.com/Golriz-code/DMC.gi	 | 设计牙冠是一个耗时且劳动密集型的过程。我们的目标是简化牙冠设计过程，减少手动调整的繁琐性，同时仍确保最高水平的准确性和一致性。为此，我们提出了一种新的端到端深度学习方法，称为牙冠网格完成（Dental Mesh Completion，DMC），以生成一个基于点云上下文的牙冠网格。牙冠的上下文包括准备接收牙冠的牙齿及其周围环境，即相邻的两颗牙齿和下颌对面的三颗最接近的牙齿。我们将牙冠生成表达为完成这个点云上下文。首先，特征提取器将输入点云转换为一组特征向量，这些特征向量表示点云中的局部区域。然后将这一组特征向量输入到变换器中，预测缺失区域（牙冠）的新一组特征向量。随后，使用一个点重建头部和一个多层感知器，预测密集点集及其法线。最后，一个可微分的点到网格层用于重建牙冠表面网格。我们将DMC方法与一种基于图的卷积神经网络进行了比较，该网络学习将通用牙冠形状变形为目标几何形状的牙冠网格。我们在数据集上的广泛实验表明，我们的方法非常有效，平均测得的查耳默斯距离（Chamfer Distance）为0.062。代码可以在以下地址获得：https://github.com/Golriz-code/DMC.gi
2501.04903	 | Towards understanding the bias in decision trees	 | Nathan Phelps,Daniel J. Lizotte,Douglas G. Woolford	 | 本研究挑战了在不平衡数据学习中机器学习模型偏向多数类的普遍信念，指出决策树在特定条件下可能偏向少数类，并证明了这一现象在只有一个阳性案例的数据集和多个阳性案例时都存在，这对随机森林等基于树的模型有重要影响。	 | There is a widespread and longstanding belief that machine learning models are biased towards the majority (or negative) class when learning from imbalanced data, leading them to neglect or ignore the minority (or positive) class. In this study, we show that this belief is not necessarily correct for decision trees, and that their bias can actually be in the opposite direction. Motivated by a recent simulation study that suggested that decision trees can be biased towards the minority class, our paper aims to reconcile the conflict between that study and decades of other works. First, we critically evaluate past literature on this problem, finding that failing to consider the data generating process has led to incorrect conclusions about the bias in decision trees. We then prove that, under specific conditions related to the predictors, decision trees fit to purity and trained on a dataset with only one positive case are biased towards the minority class. Finally, we demonstrate that splits in a decision tree are also biased when there is more than one positive case. Our findings have implications on the use of popular tree-based models, such as random forests.	 | 在不平衡数据学习过程中，人们普遍且长期认为机器学习模型会偏向多数类（或负面类），从而忽视或忽略少数类（或正面类）。本研究展示了这一信念并不一定适用于决策树，其偏差方向实际上可能是相反的。受到最近一项模拟研究的启发，该研究表明决策树可能偏向少数类，我们的论文旨在解决这一研究与其他数十年工作的冲突。首先，我们对过去关于这一问题的研究进行了批判性评估，发现忽视数据生成过程导致了对决策树偏差的错误结论。然后，我们在特定条件下证明，当决策树根据纯洁度进行拟合并训练于只有一个阳性案例的数据集时，决策树是偏向少数类的。最后，我们展示了当有多个阳性案例时，决策树的分割也存在偏差。我们的发现对广泛使用的基于树的模型，如随机森林，具有重要的影响。
2501.04898	 | Optimality and Adaptivity of Deep Neural Features for Instrumental Variable Regression	 | Juno Kim,Dimitri Meunier,Arthur Gretton,Taiji Suzuki,Zhu Li	 | 该研究对深层特征工具变量（DFIV）回归进行了收敛性分析，证明了在目标结构函数位于Besov空间时，DFIV算法达到了最小最大最优的学习率。此外，研究还表明DFIV作为一种数据自适应算法，在处理具有低空间同质性目标函数和第一阶段样本数据效率方面优于固定特征（核或筛）IV方法。	 | We provide a convergence analysis of deep feature instrumental variable (DFIV) regression (Xu et al., 2021), a nonparametric approach to IV regression using data-adaptive features learned by deep neural networks in two stages. We prove that the DFIV algorithm achieves the minimax optimal learning rate when the target structural function lies in a Besov space. This is shown under standard nonparametric IV assumptions, and an additional smoothness assumption on the regularity of the conditional distribution of the covariate given the instrument, which controls the difficulty of Stage 1. We further demonstrate that DFIV, as a data-adaptive algorithm, is superior to fixed-feature (kernel or sieve) IV methods in two ways. First, when the target function possesses low spatial homogeneity (i.e., it has both smooth and spiky/discontinuous regions), DFIV still achieves the optimal rate, while fixed-feature methods are shown to be strictly suboptimal. Second, comparing with kernel-based two-stage regression estimators, DFIV is provably more data efficient in the Stage 1 samples.	 | 我们提供了一种深层特征工具变量（DFIV）回归（Xu等，2021）的收敛性分析，这是一种利用由两阶段深度神经网络学习的数据自适应特征进行工具变量（IV）回归的非参数方法。我们证明了在目标结构函数位于Besov空间时，DFIV算法达到了最小最大最优的学习率。在标准的非参数IV假设下，并且在条件分布的光滑性上增加了一个额外的假设，以控制第一阶段的难度，从而证明了这一点。进一步地，我们展示了作为一种数据自适应算法，DFIV在两种方式上优于固定特征（核或筛）IV方法。首先，当目标函数具有低空间同质性（即它既有光滑区域也有尖锐或不连续区域）时，DFIV仍然能够达到最优率，而固定特征方法则显示出严格次优。其次，与基于核的两阶段回归估计器相比，DFIV在第一阶段样本中的数据效率得到了证明上的更高。
2501.04882	 | Reach Measurement, Optimization and Frequency Capping In Targeted Online Advertising Under k-Anonymity	 | Yuan Gao,Mu Qiao	 | 本文探讨了在确保用户$k$-匿名性（一种隐私保护模型）的前提下，如何进行触达度测量与优化，提出了一种概率折扣方法来实现广告效果与用户隐私之间的平衡，并通过实验评估了这些建议的有效性。	 | The growth in the use of online advertising to foster brand awareness over recent years is largely attributable to the ubiquity of social media. One pivotal technology contributing to the success of online brand advertising is frequency capping, a mechanism that enables marketers to control the number of times an ad is shown to a specific user. However, the very foundation of this technology is being scrutinized as the industry gravitates towards advertising solutions that prioritize user privacy. This paper delves into the issue of reach measurement and optimization within the context of $k$-anonymity, a privacy-preserving model gaining traction across major online advertising platforms. We outline how to report reach within this new privacy landscape and demonstrate how probabilistic discounting, a probabilistic adaptation of traditional frequency capping, can be employed to optimize campaign performance. Experiments are performed to assess the trade-off between user privacy and the efficacy of online brand advertising. Notably, we discern a significant dip in performance as long as privacy is introduced, yet this comes with a limited additional cost for advertising platforms to offer their users more privacy.	 | 近年来，随着在线广告在提升品牌知名度方面的使用日益普遍，这很大程度上归功于社交媒体的广泛普及。一种对在线品牌广告成功起关键作用的技术是频率封顶（frequency capping），它使营销人员能够控制广告在特定用户面前展示的次数。然而，这种技术的基础正受到质疑，因为行业正转向优先考虑用户隐私的广告解决方案。本文探讨了在$k$-匿名性（一种获得广泛认可的隐私保护模型）的背景下，如何进行触达度测量与优化的问题。我们详细阐述了如何在这一新的隐私环境中报告触达度，并展示了如何利用概率折扣（一种概率性的频率封顶方法）来优化广告活动的表现。通过实验评估了用户隐私与在线品牌广告效果之间的权衡。值得注意的是，我们发现只要引入隐私，广告效果就会显著下降，但为用户提供更多隐私的机会成本相对较小。
2501.04881	 | Geophysical inverse problems with measurement-guided diffusion models	 | Matteo Ravasi	 | 本文讨论了两种采样算法——扩散后验采样（DPS）和伪逆引导扩散模型（PGDM）——在解决地球物理逆问题中的应用，通过在地震插值和地震反演中的数值实验表明，PGDM 在合成数据集和实地数据集上表现更优且计算成本更低。	 | Solving inverse problems with the reverse process of a diffusion model represents an appealing avenue to produce highly realistic, yet diverse solutions from incomplete and possibly noisy measurements, ultimately enabling uncertainty quantification at scale. However, because of the intractable nature of the score function of the likelihood term (i.e., $\nabla_{\mathbf{x}_t} p(\mathbf{y} | \mathbf{x}_t)$), various samplers have been proposed in the literature that use different (more or less accurate) approximations of such a gradient to guide the diffusion process towards solutions that match the observations. In this work, I consider two sampling algorithms recently proposed under the name of Diffusion Posterior Sampling (DPS) and Pseudo-inverse Guided Diffusion Model (PGDM), respectively. In DSP, the guidance term used at each step of the reverse diffusion process is obtained by applying the adjoint of the modeling operator to the residual obtained from a one-step denoising estimate of the solution. On the other hand, PGDM utilizes a pseudo-inverse operator that originates from the fact that the one-step denoised solution is not assumed to be deterministic, rather modeled as a Gaussian distribution. Through an extensive set of numerical examples on two geophysical inverse problems (namely, seismic interpolation and seismic inversion), I show that two key aspects for the success of any measurement-guided diffusion process are: i) our ability to re-parametrize the inverse problem such that the sought after model is bounded between -1 and 1 (a pre-requisite for any diffusion model); ii) the choice of the training dataset used to learn the implicit prior that guides the reverse diffusion process. Numerical examples on synthetic and field datasets reveal that PGDM outperforms DPS in both scenarios at limited additional cost.	 | 利用扩散模型的逆过程解决逆问题是一种生成高度逼真且多样化的解决方案的有效途径，特别是在从不完整且可能含有噪声的测量中最终进行大规模不确定性量化时。然而，由于似然项分数函数的不可解性质（即 $\nabla_{\mathbf{x}_t} p(\mathbf{y} | \mathbf{x}_t)$），文献中提出了多种采样算法，这些算法使用不同（更准确或不太准确）的该梯度的近似值来引导扩散过程，以达到与观测结果相匹配的解。在本文中，我考虑了两种最近分别被称为扩散后验采样（DPS）和伪逆引导扩散模型（PGDM）的采样算法。  在DPS中，逆扩散过程中的引导项是在从一步去噪估计的解中获得残差后，通过应用建模算子的伴随算子计算得到的。另一方面，PGDM 使用一个伪逆算子，这是因为一步去噪解不被视为确定性的，而是建模为高斯分布。通过在两个地球物理逆问题（即地震插值和地震反演）上的大量数值示例，我展示了任何测量引导的扩散过程成功的两个关键方面：i) 我们将逆问题重新参数化的能力，使得所求模型被限制在-1和1之间（任何扩散模型的前提条件）；ii) 用于训练引导逆向扩散过程的隐式先验的训练数据集的选择。数值示例表明，在合成数据集和实地数据集上，PGDM 在这两种情况下均优于DPS，且成本较低。
2501.04880	 | Leveraging Log Probabilities in Language Models to Forecast Future Events	 | Tommaso Soru,Jim Marshall	 | 本文介绍了一种使用大型语言模型进行AI驱动的前瞻性分析新方法，并利用多步概率估计生成了15个不同主题的预测，结果显示该方法在准确性上显著优于随机猜测和广用AI系统。	 | In the constantly changing field of data-driven decision making, accurately predicting future events is crucial for strategic planning in various sectors. The emergence of Large Language Models (LLMs) marks a significant advancement in this area, offering advanced tools that utilise extensive text data for prediction. In this industry paper, we introduce a novel method for AI-driven foresight using LLMs. Building on top of previous research, we employ data on current trends and their trajectories for generating forecasts on 15 different topics. Subsequently, we estimate their probabilities via a multi-step approach based on log probabilities. We show we achieve a Brier score of 0.186, meaning a +26% improvement over random chance and a +19% improvement over widely-available AI systems.	 | 在数据驱动决策不断变化的领域中，准确预测未来事件对于各个行业的战略规划至关重要。大型语言模型（LLMs）的出现标志着这一领域的重大进展，它们利用大量文本数据提供了先进的预测工具。在本行业论文中，我们介绍了一种使用LLMs进行AI驱动前瞻性分析的新方法。基于先前的研究，我们利用当前趋势及其轨迹的数据，生成15个不同主题的预测。随后，我们通过基于对数概率的多步方法估计这些预测的概率。结果显示，我们的Brier得分为0.186，这意味着相比于随机猜测，我们提高了26%的准确性，相比广为使用的AI系统提高了19%的准确性。
2501.04871	 | RieszBoost: Gradient Boosting for Riesz Regression	 | Kaitlyn J. Lee,Alejandro Schuler	 | 本文提出了一种新的梯度提升算法，可以直接估计Riesz代表元，适用于表格数据，为Riesz回归提供了一种灵活、非参数且计算效率高的方法。通过模拟研究，该方法在多种函数中的表现优于或至少与间接估计技术相当，提供了一种用户友好且稳健的因果量估计解决方案。	 | Answering causal questions often involves estimating linear functionals of conditional expectations, such as the average treatment effect or the effect of a longitudinal modified treatment policy. By the Riesz representation theorem, these functionals can be expressed as the expected product of the conditional expectation of the outcome and the Riesz representer, a key component in doubly robust estimation methods. Traditionally, the Riesz representer is estimated indirectly by deriving its explicit analytical form, estimating its components, and substituting these estimates into the known form (e.g., the inverse propensity score). However, deriving or estimating the analytical form can be challenging, and substitution methods are often sensitive to practical positivity violations, leading to higher variance and wider confidence intervals. In this paper, we propose a novel gradient boosting algorithm to directly estimate the Riesz representer without requiring its explicit analytical form. This method is particularly suited for tabular data, offering a flexible, nonparametric, and computationally efficient alternative to existing methods for Riesz regression. Through simulation studies, we demonstrate that our algorithm performs on par with or better than indirect estimation techniques across a range of functionals, providing a user-friendly and robust solution for estimating causal quantities.	 | 回答因果问题通常涉及估计条件期望的线性函数，例如平均治疗效应或纵向修改治疗政策的效果。通过Riesz表示定理，这些函数可以表示为结果的条件期望与Riesz代表元的期望乘积，Riesz代表元是双稳健估计方法中的关键组成部分。传统上，Riesz代表元通常通过推导其显式分析形式、估计其各个组成部分，然后将这些估计值代入已知形式（例如逆倾向得分）间接地进行估计。然而，推导或估计显式的分析形式可能具有挑战性，而替代方法往往对实际的正性违反问题敏感，导致方差增加和置信区间变宽。本文中，我们提出了一种新的梯度提升算法，可以直接估计Riesz代表元，而不需要其显式分析形式。该方法特别适用于表格数据，为Riesz回归提供了灵活的、非参数的、计算效率高的替代方法。通过模拟研究，我们展示了我们的算法在多种函数中具有与或优于间接估计技术的表现，提供了一种用户友好且稳健的因果量估计解决方案。
2501.04870	 | Deep Transfer $Q$-Learning for Offline Non-Stationary Reinforcement Learning	 | Jinhang Chai,Elynn Chen,Jianqing Fan	 | 本文探讨了在商业和医疗领域的动态决策场景中，如何利用非平稳有限时期马尔可夫决策过程模型进行迁移学习，提出了一种新的“加权目标程序”来构建“可迁移的RL样本”，并提出了“迁移深度$Q^*$学习”算法，该算法能够在神经网络逼近中提供理论保证并改善决策性能。	 | In dynamic decision-making scenarios across business and healthcare, leveraging sample trajectories from diverse populations can significantly enhance reinforcement learning (RL) performance for specific target populations, especially when sample sizes are limited. While existing transfer learning methods primarily focus on linear regression settings, they lack direct applicability to reinforcement learning algorithms. This paper pioneers the study of transfer learning for dynamic decision scenarios modeled by non-stationary finite-horizon Markov decision processes, utilizing neural networks as powerful function approximators and backward inductive learning. We demonstrate that naive sample pooling strategies, effective in regression settings, fail in Markov decision processes.To address this challenge, we introduce a novel ``re-weighted targeting procedure'' to construct ``transferable RL samples'' and propose ``transfer deep $Q^*$-learning'', enabling neural network approximation with theoretical guarantees. We assume that the reward functions are transferable and deal with both situations in which the transition densities are transferable or nontransferable. Our analytical techniques for transfer learning in neural network approximation and transition density transfers have broader implications, extending to supervised transfer learning with neural networks and domain shift scenarios. Empirical experiments on both synthetic and real datasets corroborate the advantages of our method, showcasing its potential for improving decision-making through strategically constructing transferable RL samples in non-stationary reinforcement learning contexts.	 | 在商业和医疗领域的动态决策场景中，利用来自多种人群的样本轨迹可以显著提高特定目标人群的强化学习（RL）性能，尤其是在样本数量有限的情况下。尽管现有的迁移学习方法主要集中在线性回归设置上，但它们缺乏直接应用于强化学习算法的能力。本文首次探讨了利用非平稳有限时期马尔可夫决策过程模型的动态决策场景中的迁移学习问题，使用神经网络作为强大的函数逼近器，并结合反向归纳学习。我们证明了在回归设置中有效的简单样本聚类策略，在马尔可夫决策过程中是无效的。为了解决这一挑战，我们引入了一种新的“加权目标程序”来构建“可迁移的RL样本”，并提出了“迁移深度$Q^*$学习”，使得神经网络逼近具有理论保证。我们假设奖励函数是可迁移的，并处理转移密度可转移或不可转移的情况。我们对神经网络逼近中的迁移学习以及转移密度转移的分析技术具有更广泛的意义，扩展到使用神经网络的监督迁移学习和领域迁移场景。在合成数据集和真实数据集上的实证实验表明了我们方法的优势，展示了其在非平稳强化学习环境中通过战略性构建可迁移的RL样本来改进决策的潜力。
2501.04845	 | Intelligent experiments through real-time AI: Fast Data Processing and Autonomous Detector Control for sPHENIX and future EIC detectors	 | J. Kvapil(1),G. Borca-Tasciuc(2),H. Bossi(3),K. Chen(4),Y. Chen(4),Y. Corrales Morales(3),H. Da Costa(1),C. Da Silva(1),C. Dean(3),J. Durham(1),S. Fu(5),C. Hao(6),P. Harris(3),O. Hen(3),H. Jheng(3),Y. Lee(3),P. Li(6),X. Li(1),Y. Lin(1),M. X. Liu(1),V. Loncar(3),J. P. Mitrevski(8),A. Olvera(5),M. L. Purschke(7),J. S. Renck(1),G. Roland(3),J. Schambach(9),Z. Shi(1),N. Tran(8),N. Wuerfel(10),B. Xu(6),D. Yu(11),H. Zhang(6) ((1) Los Alamos National Laboratory, (2) Rensselaer Polytechnic Institute, (3) Massachusetts Institute of Technology, (4) Central China Normal University, (5) University of North Texas, (6) Georgia Institute of Technology, (7) Brookhaven National Laboratory, (8) Fermilab, (9) Oak Ridge National Laboratory, (10) University of Michigan, (11) New Jersey Institute of Technology)	 | 该项目由美国能源部核物理AI-机器学习倡议启动，旨在通过智能流技术和图神经网络处理高通量数据，解决高能核实验中的数据处理挑战，特别是实时识别sPHENIX实验中的稀有重味事件，未来还可应用于EIC实验。	 | This R\&D project, initiated by the DOE Nuclear Physics AI-Machine Learning initiative in 2022, leverages AI to address data processing challenges in high-energy nuclear experiments (RHIC, LHC, and future EIC). Our focus is on developing a demonstrator for real-time processing of high-rate data streams from sPHENIX experiment tracking detectors. The limitations of a 15 kHz maximum trigger rate imposed by the calorimeters can be negated by intelligent use of streaming technology in the tracking system. The approach efficiently identifies low momentum rare heavy flavor events in high-rate p+p collisions (3MHz), using Graph Neural Network (GNN) and High Level Synthesis for Machine Learning (hls4ml). Success at sPHENIX promises immediate benefits, minimizing resources and accelerating the heavy-flavor measurements. The approach is transferable to other fields. For the EIC, we develop a DIS-electron tagger using Artificial Intelligence - Machine Learning (AI-ML) algorithms for real-time identification, showcasing the transformative potential of AI and FPGA technologies in high-energy nuclear and particle experiments real-time data processing pipelines.	 | 以下是该研究开发项目的中文翻译：  --- 2022年由美国能源部核物理AI-机器学习倡议启动的该项目利用AI解决高能核实验（RHIC、LHC和未来EIC）中的数据处理挑战。我们重点关注开发一个用于实时处理sPHENIX实验追踪探测器高通量数据流的演示系统。通过智能使用流技术，可以克服由 calorimeters 限制的15 kHz 最大触发率。该方法利用图神经网络（GNN）和高层次合成（hls4ml）高效识别高通量p+p碰撞（3MHz）中的低动量稀有重味事件。在sPHENIX的成功应用将立即带来资源节约和加速重味测量的好处。该方法具有跨领域的可转移性。对于EIC，我们开发了一个 DIS 电子标记器，使用AI-ML算法进行实时识别，展示了AI和FPGA技术在高能核物理和粒子实验实时数据处理管道中的变革潜力。
2501.04831	 | Quantum Hybrid Support Vector Machines for Stress Detection in Older Adults	 | Md Saif Hassan Onim,Travis S. Humble,Himanshu Thapliyal	 | 本研究提出了一种使用量子混合支持向量机进行异常检测的压力监测技术，通过穿戴智能手表收集的数据，旨在预防和诊断老年人的认知障碍，结果显示量子机器学习在少量特征情况下能提高准确性并具有更高的召回率。	 | Stress can increase the possibility of cognitive impairment and decrease the quality of life in older adults. Smart healthcare can deploy quantum machine learning to enable preventive and diagnostic support. This work introduces a unique technique to address stress detection as an anomaly detection problem that uses quantum hybrid support vector machines. With the help of a wearable smartwatch, we mapped baseline sensor reading as normal data and stressed sensor reading as anomaly data using cortisol concentration as the ground truth. We have used quantum computing techniques to explore the complex feature spaces with kernel-based preprocessing. We illustrate the usefulness of our method by doing experimental validation on 40 older adults with the help of the TSST protocol. Our findings highlight that using a limited number of features, quantum machine learning provides improved accuracy compared to classical methods. We also observed that the recall value using quantum machine learning is higher compared to the classical method. The higher recall value illustrates the potential of quantum machine learning in healthcare, as missing anomalies could result in delayed diagnostics or treatment.	 | 压力可以增加老年人认知障碍的可能性，并降低其生活质量。智能医疗可以通过部署量子机器学习来提供预防性和诊断性的支持。本研究引入了一种独特的技术，将压力检测视为异常检测问题，并使用量子混合支持向量机。借助可穿戴智能手表，我们将基线传感器读数映射为正常数据，将压力传感器读数映射为异常数据，以皮质醇浓度作为参考标准。我们使用量子计算技术通过核预处理来探索复杂的特征空间。我们通过在40名老年人中使用TSST协议进行实验验证来说明我们方法的有效性。我们的研究表明，与经典方法相比，即使使用少量特征，量子机器学习也能提供改进的准确性。我们还观察到，使用量子机器学习的召回率高于经典方法。更高的召回率表明量子机器学习在医疗保健领域的潜力，因为漏检异常可能导致诊断或治疗延迟。
2501.04794	 | A Steerable Deep Network for Model-Free Diffusion MRI Registration	 | Gianfranco Cortes,Baba C. Vemuri	 | 本文提出了一种基于深度学习的框架，用于配准未处理的扩散磁共振成像数据，该框架在位置-方向空间中表述配准问题，并使用$\mathsf{SE}(3)$-等变UNet生成速度场，从而保留未处理dMRI域的几何属性，实验结果表明该方法在人类连接组项目dMRI数据上具有竞争力的表现。	 | Nonrigid registration is vital to medical image analysis but remains challenging for diffusion MRI (dMRI) due to its high-dimensional, orientation-dependent nature. While classical methods are accurate, they are computationally demanding, and deep neural networks, though efficient, have been underexplored for nonrigid dMRI registration compared to structural imaging. We present a novel, deep learning framework for model-free, nonrigid registration of raw diffusion MRI data that does not require explicit reorientation. Unlike previous methods relying on derived representations such as diffusion tensors or fiber orientation distribution functions, in our approach, we formulate the registration as an equivariant diffeomorphism of position-and-orientation space. Central to our method is an $\mathsf{SE}(3)$-equivariant UNet that generates velocity fields while preserving the geometric properties of a raw dMRI's domain. We introduce a new loss function based on the maximum mean discrepancy in Fourier space, implicitly matching ensemble average propagators across images. Experimental results on Human Connectome Project dMRI data demonstrate competitive performance compared to state-of-the-art approaches, with the added advantage of bypassing the overhead for estimating derived representations. This work establishes a foundation for data-driven, geometry-aware dMRI registration directly in the acquisition space.	 | 非刚性配准在医学图像分析中至关重要，但由于扩散磁共振成像（dMRI）的高维和方向依赖性，这一过程仍然具有挑战性。尽管经典方法准确，但它们在计算上非常耗时，而深度神经网络虽然效率高，但在非刚性dMRI配准方面与结构图像相比相对未被充分探索。我们提出了一种新的基于深度学习的框架，用于配准未处理的扩散磁共振成像数据，该框架不需要显式的方向重定向。与依赖于衍生物表示（如扩散张量或纤维方向分布函数）的先前方法不同，在我们的方法中，我们将配准问题表述为位置-方向空间中的等变微分同胚。我们方法的核心是一个 $\mathsf{SE}(3)$-等变UNet，该网络生成速度场同时保持未处理的dMRI域的几何属性。我们引入了一种基于Fourier空间最大均值差异的新损失函数，隐式地在图像间匹配群体平均传播器。在人类连接组项目dMRI数据上的实验结果表明，与最先进的方法相比，我们的方法具有竞争力的表现，还具有绕过衍生物表示估计的额外优势。这项工作为直接在采集空间中进行数据驱动的几何感知dMRI配准奠定了基础。
2501.04784	 | Leveraging Registers in Vision Transformers for Robust Adaptation	 | Srikar Yellapragada,Kowshik Thopalli,Vivek Narayanaswamy,Wesam Sakla,Yang Liu,Yamen Mubarka,Dimitris Samaras,Jayaraman J. Thiagarajan	 | 本文提出了一种将特殊CLS令牌嵌入与平均池化的寄存器嵌入结合的方法，以提高视觉变换器在异常分布场景中的泛化能力和异常拒绝性能，实验结果表明该方法能够提升2-4%的OOD准确率并降低2-3%的假阳性率，且无需额外的计算开销。	 | Vision Transformers (ViTs) have shown success across a variety of tasks due to their ability to capture global image representations. Recent studies have identified the existence of high-norm tokens in ViTs, which can interfere with unsupervised object discovery. To address this, the use of "registers" which are additional tokens that isolate high norm patch tokens while capturing global image-level information has been proposed. While registers have been studied extensively for object discovery, their generalization properties particularly in out-of-distribution (OOD) scenarios, remains underexplored. In this paper, we examine the utility of register token embeddings in providing additional features for improving generalization and anomaly rejection. To that end, we propose a simple method that combines the special CLS token embedding commonly employed in ViTs with the average-pooled register embeddings to create feature representations which are subsequently used for training a downstream classifier. We find that this enhances OOD generalization and anomaly rejection, while maintaining in-distribution (ID) performance. Extensive experiments across multiple ViT backbones trained with and without registers reveal consistent improvements of 2-4\% in top-1 OOD accuracy and a 2-3\% reduction in false positive rates for anomaly detection. Importantly, these gains are achieved without additional computational overhead.	 | 视觉变换器（ViTs）因其能够捕捉全局图像表示而在多种任务中显示出成功。最近的研究发现，ViTs 中存在高范数令牌，这些令牌会干扰无监督的物体发现。为解决这一问题，提出了“寄存器”这一附加令牌的概念，寄存器可以隔离高范数拼贴令牌同时捕捉全局图像级信息。虽然寄存器在物体发现方面得到了广泛研究，但在异常分布（OOD）场景中的泛化能力仍然未被充分探索。本文中，我们研究了寄存器令牌嵌入在提供额外特征以提高泛化能力和异常拒绝方面的有效性。为此，我们提出了一种简单的方法，将ViTs 中常用的特殊CLS令牌嵌入与平均池化的寄存器嵌入相结合，以创建用于训练下游分类器的特征表示。我们发现，这种方法在提升OOD泛化能力和异常拒绝方面表现出色，并且保持了在分布内（ID）性能。在多个ViT主干网络（有和没有寄存器）进行的广泛实验中，我们发现这在顶级OOD准确率上提高了2-4%，在异常检测中的假阳性率降低了2-3%，而且这些增益无需额外的计算开销。
2501.04762	 | Efficient and Responsible Adaptation of Large Language Models for Robust and Equitable Top-k Recommendations	 | Kirandeep Kaur,Manya Chadha,Vinayak Gupta,Chirag Shah	 | 该研究提出了一种混合任务分配框架，旨在通过公平地服务所有用户群体来提升推荐系统的社会福祉。该框架通过识别并优先处理弱用户和不活跃用户的历史交互，结合多种推荐算法和大型语言模型，有效减少了弱用户数量并提高了模型对子群体的鲁棒性，同时控制了成本增加。	 | Conventional recommendation systems (RSs) are typically optimized to enhance performance metrics uniformly across all training samples, inadvertently overlooking the needs of diverse user populations. The performance disparity among various populations can harm the model's robustness to sub-populations due to the varying user properties. While large language models (LLMs) show promise in enhancing RS performance, their practical applicability is hindered by high costs, inference latency, and degraded performance on long user queries. To address these challenges, we propose a hybrid task allocation framework designed to promote social good by equitably serving all user groups. By adopting a two-phase approach, we promote a strategic assignment of tasks for efficient and responsible adaptation of LLMs. Our strategy works by first identifying the weak and inactive users that receive a suboptimal ranking performance by RSs. Next, we use an in-context learning approach for such users, wherein each user interaction history is contextualized as a distinct ranking task. We evaluate our hybrid framework by incorporating eight different recommendation algorithms and three different LLMs -- both open and close-sourced. Our results on three real-world datasets show a significant reduction in weak users and improved robustness to subpopulations without disproportionately escalating costs.	 | 传统的推荐系统（RS）通常被优化为在所有训练样本上均匀提升性能指标，无意间忽视了多样用户群体的需求。各种人群之间的性能差异可能会损害模型对子群体的鲁棒性，因为用户的属性是变化的。虽然大型语言模型（LLMs）在提升RS性能方面表现出潜力，但其实际应用受到高成本、推理延迟以及在处理长用户查询时性能下降的阻碍。为解决这些挑战，我们提出了一种混合任务分配框架，旨在通过公平地服务所有用户群体促进社会福祉。通过采用两阶段方法，我们促进了对LLMs的有效和负责任的适应。我们的策略首先通过RS识别那些接收不到最优排名性能的弱用户和不活跃用户。接下来，我们使用上下文学习方法为这些用户进行任务分配，其中每个用户的交互历史被作为独立的排名任务进行处理。我们通过将八个不同的推荐算法和三种不同的LLMs（开源和闭源）整合到我们的混合框架中来评估该框架。在三个真实世界的数据集上的结果表明，我们的框架显著减少了弱用户数量，并提高了对子群体的鲁棒性，而成本并未不成比例地增加。
2501.04757	 | DAREK -- Distance Aware Error for Kolmogorov Networks	 | Masoud Ataei,Mohammad Javad Khojasteh,Vikas Dhiman	 | 本文提出了Kolmogorov Arnold 网络（KANs）的距离感知误差边界，称为DAREK，通过推广Newton多项式的误差边界并应用于样条的嵌套复合，改进了先前估计的宽松性和非距离感知性。通过从稀疏激光扫描数据估计物体形状，验证了该方法的效率和准确性，显示出比蒙特卡洛方法更快且能可靠地包含真实障碍物形状的误差边界。	 | In this paper, we provide distance-aware error bounds for Kolmogorov Arnold Networks (KANs). We call our new error bounds estimator DAREK -- Distance Aware Error for Kolmogorov networks. Z. Liu et al. provide error bounds, which may be loose, lack distance-awareness, and are defined only up to an unknown constant of proportionality. We review the error bounds for Newton's polynomial, which is then generalized to an arbitrary spline, under Lipschitz continuity assumptions. We then extend these bounds to nested compositions of splines, arriving at error bounds for KANs. We evaluate our method by estimating an object's shape from sparse laser scan points. We use KAN to fit a smooth function to the scans and provide error bounds for the fit. We find that our method is faster than Monte Carlo approaches, and that our error bounds enclose the true obstacle shape reliably.	 | 在本文中，我们提供了Kolmogorov Arnold 网络（KANs）的距离感知误差边界。我们将我们新的误差边界估计器称为DAREK——距离感知误差的Kolmogorov网络。Z. Liu等提供了误差边界，这些边界可能过于宽松、缺乏距离感知性，并且仅定义为未知比例常数的上界。我们回顾了Newton多项式的误差边界，然后在Lipschitz连续假设下将其推广到任意样条。然后我们将这些边界扩展到样条的嵌套复合，从而得到KANs的误差边界。我们通过从稀疏激光扫描点估计物体的形状来评估我们的方法。我们使用KAN拟合扫描数据，并提供拟合的误差边界。我们发现，我们的方法比蒙特卡洛方法更快，并且我们的误差边界能够可靠地包含真实的障碍物形状。
2501.04750	 | Efficient License Plate Recognition in Videos Using Visual Rhythm and Accumulative Line Analysis	 | Victor Nascimento Ribeiro,Nina S. T. Hirata	 | 本文提出两种新方法，能够在单帧图像中高效识别车牌信息，从而显著降低计算需求；第一种方法利用视觉节奏从视频生成时空间图像，第二种方法采用累积线分析算法进行单线视频处理，两者均使用YOLO进行车牌检测和CNN进行OCR，实验结果显示处理速度比传统逐帧方法快三倍。	 | Video-based Automatic License Plate Recognition (ALPR) involves extracting vehicle license plate text information from video captures. Traditional systems typically rely heavily on high-end computing resources and utilize multiple frames to recognize license plates, leading to increased computational overhead. In this paper, we propose two methods capable of efficiently extracting exactly one frame per vehicle and recognizing its license plate characters from this single image, thus significantly reducing computational demands. The first method uses Visual Rhythm (VR) to generate time-spatial images from videos, while the second employs Accumulative Line Analysis (ALA), a novel algorithm based on single-line video processing for real-time operation. Both methods leverage YOLO for license plate detection within the frame and a Convolutional Neural Network (CNN) for Optical Character Recognition (OCR) to extract textual information. Experiments on real videos demonstrate that the proposed methods achieve results comparable to traditional frame-by-frame approaches, with processing speeds three times faster.	 | 基于视频的自动车牌识别（Automatic License Plate Recognition, ALPR）涉及从视频捕获中提取车辆车牌文字信息。传统的系统通常依赖高性能的计算资源，并利用多个帧来识别车牌，导致计算开销增加。本文提出两种方法，能够在每辆车辆上高效地提取单帧，并从该单帧图像中识别车牌字符，从而显著降低计算需求。第一种方法使用视觉节奏（Visual Rhythm, VR）从视频生成时空间图像，第二种方法采用累积线分析（Accumulative Line Analysis, ALA），这是一种基于单线视频处理的新颖算法，适用于实时操作。两种方法均利用YOLO进行帧内的车牌检测，并使用卷积神经网络（Convolutional Neural Network, CNN）进行光学字符识别（Optical Character Recognition, OCR）以提取文本信息。实验表明，所提出的方法在处理速度上比传统的逐帧方法快三倍，同时能达到相似的效果。
2501.04734	 | Generative Style Transfer for MRI Image Segmentation: A Case of Glioma Segmentation in Sub-Saharan Africa	 | Rancy Chepchirchir,Jill Sunday,Raymond Confidence,Dong Zhang,Talha Chaudhry,Udunna C. Anazodo,Kendi Muchungi,Yujing Zou	 | 该研究在撒哈拉以南非洲地区开发了一种基于深度学习的脑肿瘤分割方法，通过领域适应、模型比较和数据增强策略，提高了脑肿瘤预测的准确性，并探讨了在该地区应用机器学习技术的潜力。	 | In Sub-Saharan Africa (SSA), the utilization of lower-quality Magnetic Resonance Imaging (MRI) technology raises questions about the applicability of machine learning methods for clinical tasks. This study aims to provide a robust deep learning-based brain tumor segmentation (BraTS) method tailored for the SSA population using a threefold approach. Firstly, the impact of domain shift from the SSA training data on model efficacy was examined, revealing no significant effect. Secondly, a comparative analysis of 3D and 2D full-resolution models using the nnU-Net framework indicates similar performance of both the models trained for 300 epochs achieving a five-fold cross-validation score of 0.93. Lastly, addressing the performance gap observed in SSA validation as opposed to the relatively larger BraTS glioma (GLI) validation set, two strategies are proposed: fine-tuning SSA cases using the GLI+SSA best-pretrained 2D fullres model at 300 epochs, and introducing a novel neural style transfer-based data augmentation technique for the SSA cases. This investigation underscores the potential of enhancing brain tumor prediction within SSA's unique healthcare landscape.	 | 在撒哈拉以南非洲（SSA），低质量的磁共振成像（MRI）技术的使用引发了关于机器学习方法在临床任务中的适用性问题。本研究旨在通过三重方法为SSA人群提供一种稳健的基于深度学习的脑肿瘤分割（BraTS）方法。首先，研究了SSA训练数据的领域转移对模型效果的影响，结果显示没有显著影响。其次，使用nnU-Net框架对3D和2D全分辨率模型进行了比较分析，显示两种模型经过300个周期的训练后，5折交叉验证得分为0.93，性能相当。最后，针对SSA验证集与相对较大的BraTS胶质母细胞瘤（GLI）验证集之间观察到的性能差距，提出了两种策略：使用GLI+SSA最优预训练2D全分辨率模型在300个周期进行微调SSA案例，以及引入一种新颖的基于神经风格迁移的数据增强技术以改善SSA案例的表现。这项研究强调了在SSA独特的医疗保健环境中增强脑肿瘤预测的潜力。
2501.04733	 | AI-Driven Reinvention of Hydrological Modeling for Accurate Predictions and Interpretation to Transform Earth System Modeling	 | Cuihui Xia,Lei Yue,Deliang Chen,Yuyang Li,Hongqiang Yang,Ancheng Xue,Zhiqiang Li,Qing He,Guoqing Zhang,Dambaru Ballab Kattel,Lei Lei,Ming Zhou	 | HydroTrace 是一种算法驱动、无需数据输入的模型，在喜马拉雅高原等挑战性区域表现出色，实现了98%的纳什-斯图夫斯效率，并能有效解释复杂的水文行为。该模型通过高级注意力机制捕捉时空变化，结合大语言模型的应用，提高了预测准确性和可解释性，成为地球系统建模的变革性工具。	 | Traditional equation-driven hydrological models often struggle to accurately predict streamflow in challenging regional Earth systems like the Tibetan Plateau, while hybrid and existing algorithm-driven models face difficulties in interpreting hydrological behaviors. This work introduces HydroTrace, an algorithm-driven, data-agnostic model that substantially outperforms these approaches, achieving a Nash-Sutcliffe Efficiency of 98% and demonstrating strong generalization on unseen data. Moreover, HydroTrace leverages advanced attention mechanisms to capture spatial-temporal variations and feature-specific impacts, enabling the quantification and spatial resolution of streamflow partitioning as well as the interpretation of hydrological behaviors such as glacier-snow-streamflow interactions and monsoon dynamics. Additionally, a large language model (LLM)-based application allows users to easily understand and apply HydroTrace's insights for practical purposes. These advancements position HydroTrace as a transformative tool in hydrological and broader Earth system modeling, offering enhanced prediction accuracy and interpretability.	 | 传统的方程驱动型水文模型在像喜马拉雅高原这样具有挑战性的区域地球系统中往往难以准确预测径流。而混合型和现有算法驱动的模型则在解释水文行为方面遇到困难。本研究提出了HydroTrace——一种算法驱动、不依赖数据的模型，该模型显著优于现有方法，实现了98%的纳什-斯图夫斯效率，并在未见过的数据上表现出强大的泛化能力。此外，HydroTrace 利用高级注意力机制捕捉时空变化和特征特定影响，能够量化和空间解析径流分配，解释如冰川-积雪-径流相互作用和季风动态等水文行为。另外，基于大语言模型（LLM）的应用使得用户能够轻松理解和应用HydroTrace的洞察以实际应用。这些进步使HydroTrace成为水文学和更广泛地球系统建模中的一个变革性工具，提供了更高的预测准确性和可解释性。
2501.04724	 | Guiding Treatment Strategies: The Role of Adjuvant Anti-Her2 Neu Therapy and Skin/Nipple Involvement in Local Recurrence-Free Survival in Breast Cancer Patients	 | Joe Omatoi,Abdul M Mohammed,Dennis Trujillo	 | 本研究利用线性非高斯无环模型（LiNGAM）从观察性患者数据中提取因果关系，发现辅助抗-Her2神经治疗能显著延长局部复发无病生存期，而皮肤/乳头受累则缩短了这一时间，强调了该疗法对Her2阳性患者的治疗重要性。	 | This study explores how causal inference models, specifically the Linear Non-Gaussian Acyclic Model (LiNGAM), can extract causal relationships between demographic factors, treatments, conditions, and outcomes from observational patient data, enabling insights beyond correlation. Unlike traditional randomized controlled trials (RCTs), which establish causal relationships within narrowly defined populations, our method leverages broader observational data, improving generalizability. Using over 40 features in the Duke MRI Breast Cancer dataset, we found that Adjuvant Anti-Her2 Neu Therapy increased local recurrence-free survival by 169 days, while Skin/Nipple involvement reduced it by 351 days. These findings highlight the therapy's importance for Her2-positive patients and the need for targeted interventions for high-risk cases, informing personalized treatment strategies.	 | 本研究探讨了因果推断模型，特别是线性非高斯无环模型（LiNGAM），如何从观察性的患者数据中提取与人口统计因素、治疗、条件和结果之间的因果关系，从而获得超越相关性的见解。与传统的随机对照试验（RCTs）仅在严格定义的人群中建立因果关系不同，我们的方法利用了更广泛的观察数据，提高了适用范围。通过使用杜克MRI乳腺癌数据集中超过40个特征，我们发现辅助抗-Her2神经治疗使局部复发无病生存期增加了169天，而皮肤/乳头受累则减少了这一时间，达到了351天。这些发现强调了该疗法对于Her2阳性患者的重要性，并指出了针对高风险病例需要采取更精准干预的必要性，从而为个性化治疗策略提供信息。
2501.04721	 | A Shape-Based Functional Index for Objective Assessment of Pediatric Motor Function	 | Shashwat Kumar,Arafat Rahman,Robert Gutierrez,Sarah Livermon,Allison N. McCrady,Silvia Blemker,Rebecca Scharf,Anuj Srivastava,Laura E. Barnes	 | 该研究利用可穿戴传感器和数据驱动的方法，客观评估了19名杜兴氏肌营养不良症患者、9名脊髓性肌萎缩症患者及13名对照者的日常活动中的运动功能，发现DMD和SMA群体的运动功能与健康对照者相当，但SMA患者表现出更大的运动不对称性，并提出了一个新的运动功能指数来追踪疾病进展和治疗效果。	 | Clinical assessments for neuromuscular disorders, such as Spinal Muscular Atrophy (SMA) and Duchenne Muscular Dystrophy (DMD), continue to rely on subjective measures to monitor treatment response and disease progression. We introduce a novel method using wearable sensors to objectively assess motor function during daily activities in 19 patients with DMD, 9 with SMA, and 13 age-matched controls. Pediatric movement data is complex due to confounding factors such as limb length variations in growing children and variability in movement speed. Our approach uses Shape-based Principal Component Analysis to align movement trajectories and identify distinct kinematic patterns, including variations in motion speed and asymmetry. Both DMD and SMA cohorts have individuals with motor function on par with healthy controls. Notably, patients with SMA showed greater activation of the motion asymmetry pattern. We further combined projections on these principal components with partial least squares (PLS) to identify a covariation mode with a canonical correlation of r = 0.78 (95% CI: [0.34, 0.94]) with muscle fat infiltration, the Brooke score (a motor function score), and age-related degenerative changes, proposing a novel motor function index. This data-driven method can be deployed in home settings, enabling better longitudinal tracking of treatment efficacy for children with neuromuscular disorders.	 | 神经肌肉疾病的临床评估，如脊髓性肌萎缩症（SMA）和杜兴氏肌营养不良症（DMD），仍然依赖于主观指标来监测治疗反应和疾病进展。我们介绍了一种新的方法，使用可穿戴传感器客观地评估19名DMD患者、9名SMA患者和13名同龄对照者在日常活动中运动功能。儿童运动数据由于生长儿童的肢体长度差异和运动速度的变异性而变得复杂。我们的方法使用基于形状的主成分分析（PCA）对运动轨迹进行对齐，并识别出包括运动速度和不对称性的不同运动学模式。DMD和SMA群体中都有与健康对照者相当的运动功能。值得注意的是，SMA患者的运动功能显示了更大的运动不对称性。我们进一步通过部分最小二乘法（PLS）结合主成分投影，识别出与肌肉脂肪浸润、Brooke评分（一种运动功能评分）和年龄相关退行性变化相关联的协变模式，其典型相关性为r = 0.78（95%CI：[0.34, 0.94]），提出了一种新的运动功能指数。这种数据驱动的方法可以在家庭环境中应用，有助于更好地追踪患有神经肌肉疾病的儿童的治疗效果。
2501.04712	 | Pressing Intensity: An Intuitive Measure for Pressing in Soccer	 | Joris Bekkers	 | 该研究引入了一种创新框架，利用位置跟踪数据和Spearman梯形控制模型，通过计算“拦截时间”并转化为概率值来量化压迫强度，从而为教练和分析师提供动态的、直观的压迫情况分析。该方法不仅提高了压迫衡量的精确性，还推动了现代足球分析能力的发展。	 | Pressing is a fundamental defensive strategy in football, characterized by applying pressure on the ball owning team to regain possession. Despite its significance, existing metrics for measuring pressing often lack precision or comprehensive consideration of positional data, player movement and speed. This research introduces an innovative framework for quantifying pressing intensity, leveraging advancements in positional tracking data and components from Spearman's Pitch Control model. Our method integrates player velocities, movement directions, and reaction times to compute the time required for a defender to intercept an attacker or the ball. This time-to-intercept measure is then transformed into probabilistic values using a logistic function, enabling dynamic and intuitive analysis of pressing situations at the individual frame level. the model captures how every player's movement influences pressure on the field, offering actionable insights for coaches, analysts, and decision-makers. By providing a robust and intepretable metric, our approach facilitates the identification of pressing strategies, advanced situational analyses, and the derivation of metrics, advancing the analytical capabilities for modern football.	 | 以下是该摘要的中文翻译：  压迫是一种基本的防守策略，其特点是在对方控球时对其施加压力以重新夺回球权。尽管它非常重要，但现有的压迫衡量指标往往缺乏精确性，或者没有全面考虑位置数据、球员移动和速度。本研究引入了一种创新的框架，以量化压迫强度，利用位置跟踪数据的进步和 Spearman 梯形控制模型的组件。我们的方法将球员的速度、移动方向和反应时间整合在一起，以计算防守方拦截进攻方或球所需的时间。然后，使用逻辑函数将这种“拦截时间”转换为概率值，从而实现对压迫情况在每一帧的动态和直观分析。该模型捕捉到每位球员的移动如何影响场上的压迫，为教练、分析师和决策者提供了可操作的见解。通过提供一个稳健且可解释的指标，我们的方法有助于识别压迫策略、高级情境分析，并提出新的指标，推动现代足球分析能力的发展。
