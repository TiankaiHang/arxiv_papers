| ID | Title | Authors | Summary (zh) | Abstract (en) | Abstract (zh) | 
| --- | --- | --- | --- | --- | --- |
2501.09019	 | Ouroboros-Diffusion: Exploring Consistent Content Generation in Tuning-free Long Video Diffusion	 | Jingyuan Chen,Fuchen Long,Jie An,Zhaofan Qiu,Ting Yao,Jiebo Luo,Tao Mei	 | Ouroboros-Diffusion 是一种增强的视频去噪框架，通过引入潜在采样技术和主体感知的跨帧注意力机制，提升了结构和内容的一致性，从而生成连贯的长视频。该方法在保持主体一致性、运动平滑性和时序一致性方面表现优越。	 | The first-in-first-out (FIFO) video diffusion, built on a pre-trained text-to-video model, has recently emerged as an effective approach for tuning-free long video generation. This technique maintains a queue of video frames with progressively increasing noise, continuously producing clean frames at the queue's head while Gaussian noise is enqueued at the tail. However, FIFO-Diffusion often struggles to keep long-range temporal consistency in the generated videos due to the lack of correspondence modeling across frames. In this paper, we propose Ouroboros-Diffusion, a novel video denoising framework designed to enhance structural and content (subject) consistency, enabling the generation of consistent videos of arbitrary length. Specifically, we introduce a new latent sampling technique at the queue tail to improve structural consistency, ensuring perceptually smooth transitions among frames. To enhance subject consistency, we devise a Subject-Aware Cross-Frame Attention (SACFA) mechanism, which aligns subjects across frames within short segments to achieve better visual coherence. Furthermore, we introduce self-recurrent guidance. This technique leverages information from all previous cleaner frames at the front of the queue to guide the denoising of noisier frames at the end, fostering rich and contextual global information interaction. Extensive experiments of long video generation on the VBench benchmark demonstrate the superiority of our Ouroboros-Diffusion, particularly in terms of subject consistency, motion smoothness, and temporal consistency.	 | 基于预训练的文本转视频模型构建的先进先出（FIFO）视频扩散技术，最近已成为一种无调参的长视频生成的有效方法。该技术维护一个以逐级增加噪声的视频帧队列，连续在队列头部生成干净的帧，同时在队列尾部加入高斯噪声。然而，由于缺乏帧间对应性建模，FIFO-Diffusion 在生成的视频中往往难以保持长时间范围内的时序一致性。本文中，我们提出了Ouroboros-Diffusion，一种新颖的视频去噪框架，旨在增强结构和内容（主体）的一致性，从而使生成任意长度的连贯视频成为可能。具体而言，我们在队列尾部引入了一种新的潜在采样技术，以提高结构一致性，确保帧之间的视觉平滑过渡。为了增强主体一致性，我们设计了一种基于主体感知的跨帧注意力机制（SACFA），该机制在短段落内对齐跨帧的主体，以实现更好的视觉连贯性。此外，我们引入了自我递归引导。该技术利用队列前端所有干净帧的信息来引导队列末端较脏帧的去噪过程，促进丰富的全局上下文信息交互。在VBench基准上的长期视频生成实验表明，我们的Ouroboros-Diffusion在主体一致性、运动平滑性和时序一致性方面表现出明显优势。
2501.09012	 | Multimodal LLMs Can Reason about Aesthetics in Zero-Shot	 | Ruixiang Jiang,Changwen Chen	 | 该研究首次利用多模态大语言模型（MLLMs）的推理能力评估艺术作品的美学，并构建了MM-StyleBench数据集；研究发现特定于艺术的任务分解和具体语言可以提升MLLMs的推理能力，用于美学评估。	 | We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability shall be elicited to evaluate the aesthetics of artworks. To facilitate this investigation, we construct MM-StyleBench, a novel high-quality dataset for benchmarking artistic stylization. We then develop a principled method for human preference modeling and perform a systematic correlation analysis between MLLMs' responses and human preference. Our experiments reveal an inherent hallucination issue of MLLMs in art evaluation, associated with response subjectivity. ArtCoT is proposed, demonstrating that art-specific task decomposition and the use of concrete language boost MLLMs' reasoning ability for aesthetics. Our findings offer valuable insights into MLLMs for art and can benefit a wide range of downstream applications, such as style transfer and artistic image generation. Code available at https://github.com/songrise/MLLM4Art.	 | 我们首次探讨了如何通过多模态大语言模型（MLLMs）的推理能力来评估艺术作品的美学。为促进这一研究，我们构建了MM-StyleBench，这是一个高质量的新颖数据集，用于评估艺术风格化。然后，我们开发了一种原理性的方法来建模人类偏好，并进行了MLLMs回应与人类偏好之间的系统相关性分析。我们的实验揭示了MLLMs在艺术评估中固有的幻觉问题，与回应的主观性相关。我们提出了ArtCoT，证明了特定于艺术的任务分解和使用具体语言可以提升MLLMs的推理能力，以用于美学评估。我们的发现为艺术领域的MLLMs提供了宝贵的见解，并可应用于多种下游应用，如风格迁移和艺术图像生成。代码可在https://github.com/songrise/MLLM4Art 获取。
2501.09008	 | SimGen: A Diffusion-Based Framework for Simultaneous Surgical Image and Segmentation Mask Generation	 | Aditya Bhat,Rupak Bose,Chinedu Innocent Nwoye,Nicolas Padoy	 | SimGen 是一种基于 DDPM 框架和残差 U-Net 的扩散模型，能够同时生成高质量的手术图像及其分割掩码，显著提高了手术应用、模拟和教育的精确度，且优于现有基准模型，在多个公开数据集上验证了其有效性。	 | Acquiring and annotating surgical data is often resource-intensive, ethical constraining, and requiring significant expert involvement. While generative AI models like text-to-image can alleviate data scarcity, incorporating spatial annotations, such as segmentation masks, is crucial for precision-driven surgical applications, simulation, and education. This study introduces both a novel task and method, SimGen, for Simultaneous Image and Mask Generation. SimGen is a diffusion model based on the DDPM framework and Residual U-Net, designed to jointly generate high-fidelity surgical images and their corresponding segmentation masks. The model leverages cross-correlation priors to capture dependencies between continuous image and discrete mask distributions. Additionally, a Canonical Fibonacci Lattice (CFL) is employed to enhance class separability and uniformity in the RGB space of the masks. SimGen delivers high-fidelity images and accurate segmentation masks, outperforming baselines across six public datasets assessed on image and semantic inception distance metrics. Ablation study shows that the CFL improves mask quality and spatial separation. Downstream experiments suggest generated image-mask pairs are usable if regulations limit human data release for research. This work offers a cost-effective solution for generating paired surgical images and complex labels, advancing surgical AI development by reducing the need for expensive manual annotations.	 | 获取和标注手术数据往往资源密集、伦理限制且需要大量专家参与。虽然生成式AI模型，如文本转图像，可以缓解数据稀缺问题，但将空间注释，如分割掩码，纳入其中对于精确驱动的手术应用、模拟和教育至关重要。本研究引入了一项新的任务和方法——SimGen，用于同时生成图像和掩码。SimGen 是基于 DDPM 框架和残差 U-Net 的扩散模型，旨在同时生成高质量的手术图像及其相应的分割掩码。该模型利用交叉相关先验来捕捉连续图像和离散掩码分布之间的依赖关系。此外，还使用了标准斐波那契格子（CFL）来增强掩码在 RGB 空间的类别可分性和均匀性。SimGen 生成的图像和准确的分割掩码在六个公开数据集上均优于基准模型，这些数据集在图像和语义启动距离指标上进行了评估。消融研究显示，CFL 提高了掩码质量和空间分离度。下游实验表明，如果法规限制了人类数据的发布用于研究，则生成的图像-掩码对可以使用。这项工作提供了一种经济高效的解决方案，用于生成配对的手术图像和复杂标签，通过减少昂贵的手动注释需求，促进了手术AI的发展。
2501.08994	 | RepVideo: Rethinking Cross-Layer Representation for Video Generation	 | Chenyang Si,Weichen Fan,Zhengyao Lv,Ziqi Huang,Yu Qiao,Ziwei Liu	 | 本文研究了扩散模型在视频生成中的应用，并提出了一种名为RepVideo的增强表示框架，通过累积相邻层的特征来提高时间一致性和语义表达能力，从而生成更高质量的视频。	 | Video generation has achieved remarkable progress with the introduction of diffusion models, which have significantly improved the quality of generated videos. However, recent research has primarily focused on scaling up model training, while offering limited insights into the direct impact of representations on the video generation process. In this paper, we initially investigate the characteristics of features in intermediate layers, finding substantial variations in attention maps across different layers. These variations lead to unstable semantic representations and contribute to cumulative differences between features, which ultimately reduce the similarity between adjacent frames and negatively affect temporal coherence. To address this, we propose RepVideo, an enhanced representation framework for text-to-video diffusion models. By accumulating features from neighboring layers to form enriched representations, this approach captures more stable semantic information. These enhanced representations are then used as inputs to the attention mechanism, thereby improving semantic expressiveness while ensuring feature consistency across adjacent frames. Extensive experiments demonstrate that our RepVideo not only significantly enhances the ability to generate accurate spatial appearances, such as capturing complex spatial relationships between multiple objects, but also improves temporal consistency in video generation.	 | 视频生成在扩散模型的引入下取得了显著的进步，这极大地提高了生成视频的质量。然而，最近的研究主要集中在扩大模型训练的规模上，而对直接作用于视频生成过程的表示特性的影响则提供了有限的见解。在本文中，我们首先探究了中间层特征的特性，发现不同层的注意力图存在显著差异。这些差异导致了语义表示的不稳定，并且累积了特征之间的差异，最终减少了相邻帧之间的相似性，从而负面影响了时间一致性。为了解决这一问题，我们提出了RepVideo，这是一种增强表示框架，适用于文本到视频的扩散模型。通过将相邻层的特征累积起来形成丰富的表示，该方法能够捕获更稳定的语义信息。这些增强表示随后被用作注意力机制的输入，从而提高语义的表达能力，并确保相邻帧之间的特征一致性。大量的实验表明，我们的RepVideo不仅能显著提高生成准确的空域外观的能力，如捕捉多个对象之间的复杂空间关系，还能提高视频生成的时间一致性。
2501.08983	 | CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities	 | Haozhe Xie,Zhaoxi Chen,Fangzhou Hong,Ziwei Liu	 | CityDreamer4D是一种专门用于生成无界4D城市的组合生成模型，通过分离动态和静态对象并使用不同类型的神经场来生成4D城市中的物体，从而解决4D城市生成的挑战。该模型结合了交通场景生成器和无界布局生成器，提供了先进的4D城市生成性能，并支持多种下游应用。	 | 3D scene generation has garnered growing attention in recent years and has made significant progress. Generating 4D cities is more challenging than 3D scenes due to the presence of structurally complex, visually diverse objects like buildings and vehicles, and heightened human sensitivity to distortions in urban environments. To tackle these issues, we propose CityDreamer4D, a compositional generative model specifically tailored for generating unbounded 4D cities. Our main insights are 1) 4D city generation should separate dynamic objects (e.g., vehicles) from static scenes (e.g., buildings and roads), and 2) all objects in the 4D scene should be composed of different types of neural fields for buildings, vehicles, and background stuff. Specifically, we propose Traffic Scenario Generator and Unbounded Layout Generator to produce dynamic traffic scenarios and static city layouts using a highly compact BEV representation. Objects in 4D cities are generated by combining stuff-oriented and instance-oriented neural fields for background stuff, buildings, and vehicles. To suit the distinct characteristics of background stuff and instances, the neural fields employ customized generative hash grids and periodic positional embeddings as scene parameterizations. Furthermore, we offer a comprehensive suite of datasets for city generation, including OSM, GoogleEarth, and CityTopia. The OSM dataset provides a variety of real-world city layouts, while the Google Earth and CityTopia datasets deliver large-scale, high-quality city imagery complete with 3D instance annotations. Leveraging its compositional design, CityDreamer4D supports a range of downstream applications, such as instance editing, city stylization, and urban simulation, while delivering state-of-the-art performance in generating realistic 4D cities.	 | 近年来，三维场景生成受到了越来越多的关注，并取得了显著的进步。生成4D城市比生成3D场景更具挑战性，因为建筑物和车辆等结构性复杂、视觉多样化的对象的存在，以及人类对城市环境中失真的高度敏感性。为了解决这些问题，我们提出了一种名为CityDreamer4D的组合生成模型，专门用于生成无界4D城市。我们的主要见解包括：1）4D城市的生成应将动态对象（如车辆）与静态场景（如建筑物和道路）分开，2）4D场景中的所有对象应由不同类型的神经场组成，分别为建筑物、车辆和背景物体。具体而言，我们提出了交通场景生成器和无界布局生成器，通过高度紧凑的BEV表示来生成动态交通场景和静态城市布局。4D城市中的物体通过结合背景物体和实例对象的神经场生成，针对背景物体和实例的不同特性，神经场采用定制的生成哈希网格和周期位置嵌入作为场景参数化。此外，我们还提供了一系列城市生成数据集，包括OSM、GoogleEarth和CityTopia。OSM数据集提供了多种真实世界的城市布局，而Google Earth和CityTopia数据集则提供了大规模、高质量的城市图像，并附带3D实例注释。借助其组合设计，CityDreamer4D支持一系列下游应用，如实例编辑、城市风格化和城市模拟，同时在生成逼真的4D城市方面表现出最先进的性能。
2501.08982	 | CityLoc: 6 DoF Localization of Text Descriptions in Large-Scale Scenes with Gaussian Representation	 | Qi Ma,Runyi Yang,Bin Ren,Ender Konukoglu,Luc Van Gool,Danda Pani Paudel	 | 本文提出了一种基于扩散的架构，用于生成条件化的相机姿态分布，以定位大规模3D场景中的文本描述，并通过3D高斯点积细化姿态分布，从而在五个大规模数据集上优于现有方法。	 | Localizing text descriptions in large-scale 3D scenes is inherently an ambiguous task. This nonetheless arises while describing general concepts, e.g. all traffic lights in a city.   To facilitate reasoning based on such concepts, text localization in the form of distribution is required. In this paper, we generate the distribution of the camera poses conditioned upon the textual description.   To facilitate such generation, we propose a diffusion-based architecture that conditionally diffuses the noisy 6DoF camera poses to their plausible locations.   The conditional signals are derived from the text descriptions, using the pre-trained text encoders. The connection between text descriptions and pose distribution is established through pretrained Vision-Language-Model, i.e. CLIP. Furthermore, we demonstrate that the candidate poses for the distribution can be further refined by rendering potential poses using 3D Gaussian splatting, guiding incorrectly posed samples towards locations that better align with the textual description, through visual reasoning.   We demonstrate the effectiveness of our method by comparing it with both standard retrieval methods and learning-based approaches. Our proposed method consistently outperforms these baselines across all five large-scale datasets. Our source code and dataset will be made publicly available.	 | 在大规模3D场景中定位文本描述是一个固有的模糊任务。这种模糊性在描述一般概念时尤其明显，例如一个城市中的所有交通信号灯。为了基于这些概念进行推理，需要一种以分布形式呈现的文本定位方法。在本文中，我们根据文本描述生成条件化的相机姿态分布。  为了促进这种生成，我们提出了一种基于扩散的架构，该架构在文本描述的条件下对噪声态的6自由度（6DoF）相机姿态进行扩散，使其定位在合理位置。条件信号来自预训练的文本编码器。文本描述与姿态分布之间的联系通过预训练的视觉-语言模型（如CLIP）建立。此外，我们展示了通过使用3D高斯点积进行渲染，可以进一步细化分布中的候选姿态，通过视觉推理将错误的姿态引导到更好地与文本描述对齐的位置。  我们通过与标准检索方法和基于学习的方法进行对比，展示了我们方法的有效性。我们的方法在五个大规模数据集上都优于这些基线。我们的源代码和数据集将公开提供。
2501.08962	 | An analysis of data variation and bias in image-based dermatological datasets for machine learning classification	 | Francisco Mauro,Emanoel Thyago,Othon Vinicius,Rodrigo Abreu,Kelvin Cunha,José Gabriel,Rafael Barros,Thales Bezerra,Manoel Henriques,Natalia Lopes,Érico Moutinho,Jéssica Guido,Tsang Ing Ren,Paulo Borba	 | 该研究探讨了人工智能算法在辅助医疗决策中的应用，特别是如何通过迁移学习减少临床图像与显微皮肤镜图像之间的数据差距，以提高模型在低分辨率临床图像上的分类性能。研究评估了不同数据分布对模型预测的影响，并提出了一种结合两种数据集的方法，以提高模型的最终准确率。	 | AI algorithms have become valuable in aiding professionals in healthcare. The increasing confidence obtained by these models is helpful in critical decision demands. In clinical dermatology, classification models can detect malignant lesions on patients' skin using only RGB images as input. However, most learning-based methods employ data acquired from dermoscopic datasets on training, which are large and validated by a gold standard. Clinical models aim to deal with classification on users' smartphone cameras that do not contain the corresponding resolution provided by dermoscopy. Also, clinical applications bring new challenges. It can contain captures from uncontrolled environments, skin tone variations, viewpoint changes, noises in data and labels, and unbalanced classes. A possible alternative would be to use transfer learning to deal with the clinical images. However, as the number of samples is low, it can cause degradations on the model's performance; the source distribution used in training differs from the test set. This work aims to evaluate the gap between dermoscopic and clinical samples and understand how the dataset variations impact training. It assesses the main differences between distributions that disturb the model's prediction. Finally, from experiments on different architectures, we argue how to combine the data from divergent distributions, decreasing the impact on the model's final accuracy.	 | 人工智能算法在辅助医疗专业人员方面变得越来越有价值。这些模型获得的不断增加的信心有助于关键决策需求。在临床皮肤科中，分类模型可以仅使用RGB图像作为输入来检测患者皮肤上的恶性病变。然而，大多数基于学习的方法在训练时使用的是经过金标准验证的显微皮肤镜数据集，这些数据集规模庞大。临床模型旨在处理用户智能手机摄像头上的分类，这些摄像头无法提供显微皮肤镜提供的相应分辨率。此外，临床应用带来了新的挑战。这些应用可能包含来自非受控环境的截图、皮肤色调变化、视角变化、数据和标签噪声以及类别不平衡。一种可能的替代方案是使用迁移学习来处理临床图像。然而，由于样本数量较少，这可能导致模型性能下降；用于训练的源分布与测试集不同。本研究旨在评估显微皮肤镜样本与临床样本之间的差距，并理解数据集变化如何影响训练。它评估了影响模型预测的主要分布差异。最后，通过不同架构的实验，我们提出了如何结合不同分布的数据，以减少对模型最终准确率的影响。
2501.08931	 | Visual WetlandBirds Dataset: Bird Species Identification and Behavior Recognition in Videos	 | Javier Rodriguez-Juan,David Ortiz-Perez,Manuel Benavent-Lledo,David Mulero-Pérez,Pablo Ruiz-Ponce,Adrian Orihuela-Torres,Jose Garcia-Rodriguez,Esther Sebastián-González	 | 该研究推出了首个专门用于鸟类行为检测和物种分类的细粒度视频数据集，填补了现有鸟类视频数据集的空白，并展示了在鸟类行为识别和物种分类任务上的基线结果。	 | The current biodiversity loss crisis makes animal monitoring a relevant field of study. In light of this, data collected through monitoring can provide essential insights, and information for decision-making aimed at preserving global biodiversity. Despite the importance of such data, there is a notable scarcity of datasets featuring videos of birds, and none of the existing datasets offer detailed annotations of bird behaviors in video format. In response to this gap, our study introduces the first fine-grained video dataset specifically designed for bird behavior detection and species classification. This dataset addresses the need for comprehensive bird video datasets and provides detailed data on bird actions, facilitating the development of deep learning models to recognize these, similar to the advancements made in human action recognition. The proposed dataset comprises 178 videos recorded in Spanish wetlands, capturing 13 different bird species performing 7 distinct behavior classes. In addition, we also present baseline results using state of the art models on two tasks: bird behavior recognition and species classification.	 | 当前的生物多样性丧失危机使得动物监测成为一个重要的研究领域。鉴于这一点，通过监测收集的数据可以提供重要的见解和信息，用于旨在保护全球生物多样性的决策。尽管这些数据非常重要，但现有的鸟类视频数据集相对稀缺，而且没有现有的数据集提供视频格式下鸟类行为的详细注释。为应对这一缺口，我们的研究首次推出了一个专门用于鸟类行为检测和物种分类的细粒度视频数据集。该数据集满足了全面的鸟类视频数据集需求，并提供了鸟类行为的详细数据，促进了通过深度学习模型识别这些行为的发展，类似于人类行为识别技术的进步。本数据集包括在西班牙湿地拍摄的178个视频，记录了13种不同鸟类进行的7种不同行为类。此外，我们还展示了使用最新模型在两个任务上的基线结果：鸟类行为识别和物种分类。
2501.08924	 | Learning Joint Denoising, Demosaicing, and Compression from the Raw Natural Image Noise Dataset	 | Benoit Brummer,Christophe De Vleeschouwer	 | 本文介绍了Raw Natural Image Noise Dataset（RawNIND），一个旨在支持跨传感器和图像开发流程去噪模型开发的多样化原始图像集合，并提出了两种新的去噪方法，均优于传统方法，且集成去噪与压缩可显著提高率失真性能和计算效率。	 | This paper introduces the Raw Natural Image Noise Dataset (RawNIND), a diverse collection of paired raw images designed to support the development of denoising models that generalize across sensors, image development workflows, and styles. Two denoising methods are proposed: one operates directly on raw Bayer data, leveraging computational efficiency, while the other processes linear RGB images for improved generalization to different sensors, with both preserving flexibility for subsequent development. Both methods outperform traditional approaches which rely on developed images. Additionally, the integration of denoising and compression at the raw data level significantly enhances rate-distortion performance and computational efficiency. These findings suggest a paradigm shift toward raw data workflows for efficient and flexible image processing.	 | 本文介绍了Raw Natural Image Noise Dataset（RawNIND），这是一个多样化的配对原始图像集合，旨在支持跨传感器、图像开发流程和风格的去噪模型开发。文中提出了两种去噪方法：一种直接作用于原始Bayer数据，利用计算效率的优势，另一种则处理线性RGB图像，以提高对不同传感器的通用性，两者均保持了后续开发的灵活性。这两种方法均优于依赖于已开发图像的传统方法。此外，将去噪和压缩集成到原始数据级别，显著提高了率失真性能和计算效率。这些发现表明，向原始数据流程转变是实现高效和灵活图像处理的范式转变。
2501.08912	 | Empowering Agricultural Insights: RiceLeafBD - A Novel Dataset and Optimal Model Selection for Rice Leaf Disease Diagnosis through Transfer Learning Technique	 | Sadia Afrin Rimi,Md. Jalal Uddin Chowdhury,Rifat Abdullah,Iftekhar Ahmed,Mahrima Akter Mim,Mohammad Shoaib Rahman	 | 该研究针对孟加拉国水稻产量下降的问题，提出并评估了多个深度学习和迁移学习模型，旨在精确识别影响水稻叶片的疾病，从而为提高水稻产量提供技术支持。通过使用自定义数据集和轻量级CNN及预训练模型，研究实现了91.5%的高识别准确率。	 | The number of people living in this agricultural nation of ours, which is surrounded by lush greenery, is growing on a daily basis. As a result of this, the level of arable land is decreasing, as well as residential houses and industrial factories. The food crisis is becoming the main threat for us in the upcoming days. Because on the one hand, the population is increasing, and on the other hand, the amount of food crop production is decreasing due to the attack of diseases. Rice is one of the most significant cultivated crops since it provides food for more than half of the world's population. Bangladesh is dependent on rice (Oryza sativa) as a vital crop for its agriculture, but it faces a significant problem as a result of the ongoing decline in rice yield brought on by common diseases. Early disease detection is the main difficulty in rice crop cultivation. In this paper, we proposed our own dataset, which was collected from the Bangladesh field, and also applied deep learning and transfer learning models for the evaluation of the datasets. We elaborately explain our dataset and also give direction for further research work to serve society using this dataset. We applied a light CNN model and pre-trained InceptionNet-V2, EfficientNet-V2, and MobileNet-V2 models, which achieved 91.5% performance for the EfficientNet-V2 model of this work. The results obtained assaulted other models and even exceeded approaches that are considered to be part of the state of the art. It has been demonstrated by this study that it is possible to precisely and effectively identify diseases that affect rice leaves using this unbiased datasets. After analysis of the performance of different models, the proposed datasets are significant for the society for research work to provide solutions for decreasing rice leaf disease.	 | 以下是该摘要的中文翻译：  在我国这个被茂密绿树环绕的农业国家中，人口数量正在日益增长。这导致可耕地面积减少，同时住宅和工业厂房也在增加。食品危机正逐渐成为我们未来面临的主要威胁。一方面，人口在增加；另一方面，由于疾病侵袭，粮食作物产量却在下降。水稻是最重要的栽培作物之一，因为它能为全球超过一半的人口提供食物。孟加拉国依赖水稻（Oryza sativa）作为其农业中的重要作物，但由于持续的水稻产量下降，孟加拉国面临着重大问题。在水稻作物种植中，早期疾病检测是主要难题。本文中，我们提出了一个从孟加拉国实地收集的自定义数据集，并应用了深度学习和迁移学习模型对数据集进行评估。我们详细解释了该数据集，并为后续研究工作提供了方向，以利用该数据集服务社会。我们应用了轻量级CNN模型以及预训练的InceptionNet-V2、EfficientNet-V2和MobileNet-V2模型，这些模型在EfficientNet-V2模型上实现了91.5%的性能。通过这项研究，我们证明了可以使用这些无偏数据集精确且有效地识别影响水稻叶片的疾病。通过对不同模型性能的分析，提出的数据集对于研究工作具有重要意义，旨在为减少水稻叶片疾病提供解决方案。
2501.08910	 | Lights, Camera, Matching: The Role of Image Illumination in Fair Face Recognition	 | Gabriella Pangelinan,Grace Bezold,Haiyu Wu,Michael C. King,Kevin W. Bowyer	 | 本研究通过调整面部亮度来减少高加索和非洲裔美国女性配对图像间的相似度分数差异，发现基于亮度分布进行平衡可最多降低57.6%的d'值，显著提高两种人群面部识别的准确率。	 | Facial brightness is a key image quality factor impacting face recognition accuracy differentials across demographic groups. In this work, we aim to decrease the accuracy gap between the similarity score distributions for Caucasian and African American female mated image pairs, as measured by d' between distributions. To balance brightness across demographic groups, we conduct three experiments, interpreting brightness in the face skin region either as median pixel value or as the distribution of pixel values. Balancing based on median brightness alone yields up to a 46.8% decrease in d', while balancing based on brightness distribution yields up to a 57.6% decrease. In all three cases, the similarity scores of the individual distributions improve, with mean scores maximally improving 5.9% for Caucasian females and 3.7% for African American females.	 | 面部亮度是影响不同人口群体之间面部识别准确率差异的关键图像质量因素。在本研究中，我们旨在减少高加索和非洲裔美国女性配对图像之间的相似度分数分布差异，测量方法为d'值。为了在不同人口群体之间平衡亮度，我们进行了三项实验，将面部皮肤区域的亮度解释为单个像素值的中值或像素值的分布。仅基于中值亮度进行平衡可将d'值最多降低46.8%，而基于亮度分布进行平衡则最多可降低57.6%。在所有三种情况下，个体分布的相似度分数都得到了改进，高加索女性的平均评分最多提高了5.9%，非洲裔美国女性的平均评分最多提高了3.7%。
2501.08900	 | Enhanced Multi-Scale Cross-Attention for Person Image Generation	 | Hao Tang,Ling Shao,Nicu Sebe,Luc Van Gool	 | 本文提出了一种新颖的基于交叉注意力机制的生成对抗网络（XingGAN），用于个人图像生成任务，通过两个专门的生成分支捕捉人物的外观和形状，并引入了多尺度交叉注意力和增强注意力模块来提升性能，实验结果表明该方法在多个公开数据集上优于现有的GAN方法，并且训练和推断速度更快。	 | In this paper, we propose a novel cross-attention-based generative adversarial network (GAN) for the challenging person image generation task. Cross-attention is a novel and intuitive multi-modal fusion method in which an attention/correlation matrix is calculated between two feature maps of different modalities. Specifically, we propose the novel XingGAN (or CrossingGAN), which consists of two generation branches that capture the person's appearance and shape, respectively. Moreover, we propose two novel cross-attention blocks to effectively transfer and update the person's shape and appearance embeddings for mutual improvement. This has not been considered by any other existing GAN-based image generation work. To further learn the long-range correlations between different person poses at different scales and sub-regions, we propose two novel multi-scale cross-attention blocks. To tackle the issue of independent correlation computations within the cross-attention mechanism leading to noisy and ambiguous attention weights, which hinder performance improvements, we propose a module called enhanced attention (EA). Lastly, we introduce a novel densely connected co-attention module to fuse appearance and shape features at different stages effectively. Extensive experiments on two public datasets demonstrate that the proposed method outperforms current GAN-based methods and performs on par with diffusion-based methods. However, our method is significantly faster than diffusion-based methods in both training and inference.	 | 在本文中，我们提出了一种新颖的基于交叉注意力的生成对抗网络（GAN），用于具有挑战性的个人图像生成任务。交叉注意力是一种新颖且直观的多模态融合方法，其中在不同模态的两个特征图之间计算注意力/相关矩阵。具体来说，我们提出了新颖的XingGAN（或CrossingGAN），它由两个分别捕捉人物外观和形状的生成分支组成。此外，我们提出了两个新颖的交叉注意力块，以有效传递和更新人物的形状和外观嵌入，从而实现相互提升。这在任何现有的基于GAN的图像生成工作中都没有被考虑过。为了进一步学习不同尺度和子区域的不同人体姿态之间的长程相关性，我们提出了两个新颖的多尺度交叉注意力块。为了解决交叉注意力机制内部独立相关计算导致注意力权重嘈杂且模棱两可的问题，这妨碍了性能提升，我们提出了一种名为增强注意力（EA）的模块。最后，我们引入了一种新颖的密集连接联合注意力模块，以在不同阶段有效地融合外观和形状特征。在两个公开数据集上的大量实验表明，所提出的方法在性能上优于当前的基于GAN的方法，并且与基于扩散的方法相当。然而，我们的方法在训练和推断时明显比基于扩散的方法更快。
2501.08885	 | Feature-based One-For-All: A Universal Framework for Heterogeneous Knowledge Distillation	 | Jhe-Hao Lin,Yi Yao,Chan-Feng Hsu,Hongxia Xie,Hong-Han Shuai,Wen-Huang Cheng	 | 本文提出了一种基于特征的“一揽子”（FOFA）知识蒸馏框架，用于在不同模型架构间进行特征蒸馏，通过提示调优模块和区域感知注意力机制解决架构差异问题，实验结果表明该方法在CIFAR、ImageNet和COCO数据集上表现出优越性。	 | Knowledge distillation (KD) involves transferring knowledge from a pre-trained heavy teacher model to a lighter student model, thereby reducing the inference cost while maintaining comparable effectiveness. Prior KD techniques typically assume homogeneity between the teacher and student models. However, as technology advances, a wide variety of architectures have emerged, ranging from initial Convolutional Neural Networks (CNNs) to Vision Transformers (ViTs), and Multi-Level Perceptrons (MLPs). Consequently, developing a universal KD framework compatible with any architecture has become an important research topic. In this paper, we introduce a feature-based one-for-all (FOFA) KD framework to enable feature distillation across diverse architecture. Our framework comprises two key components. First, we design prompt tuning blocks that incorporate student feedback, allowing teacher features to adapt to the student model's learning process. Second, we propose region-aware attention to mitigate the view mismatch problem between heterogeneous architecture. By leveraging these two modules, effective distillation of intermediate features can be achieved across heterogeneous architectures. Extensive experiments on CIFAR, ImageNet, and COCO demonstrate the superiority of the proposed method.	 | 知识蒸馏（KD）涉及将预训练的大型教师模型的知识转移到较小的学生模型中，从而降低推理成本同时保持类似的有效性。之前的KD技术通常假设教师和学生模型之间的一致性。然而，随着技术的发展，各种各样的架构已经涌现，从最初的卷积神经网络（CNNs）到视觉变换器（ViTs）和多层感知器（MLPs）。因此，开发一种与任何架构兼容的通用KD框架已经成为一个重要研究课题。在本文中，我们介绍了一种基于特征的“一揽子”（FOFA）KD框架，以实现不同架构之间的特征蒸馏。我们的框架包括两个关键组成部分。首先，我们设计了提示调优模块，该模块结合了学生反馈，使得教师特征能够适应学生模型的学习过程。其次，我们提出了区域感知注意力，以减轻异构架构之间的视角不对齐问题。通过利用这两个模块，可以在异构架构之间实现有效的中间特征蒸馏。在CIFAR、ImageNet和COCO上的广泛实验表明，所提出的方法具有优越性。
2501.08861	 | Generative Planning with 3D-vision Language Pre-training for End-to-End Autonomous Driving	 | Tengpeng Li,Hanli Wang,Xianfei Li,Wenlong Liao,Tao He,Pai Peng	 | 提出了一种名为GPVL的生成规划与三维视觉语言预训练模型，用于实现自主驾驶的端到端学习，该模型通过三维视觉语言预训练模块和跨模态语言模型解决了视觉理解、决策推理和场景泛化等挑战，并在nuScenes数据集上表现出色。	 | Autonomous driving is a challenging task that requires perceiving and understanding the surrounding environment for safe trajectory planning. While existing vision-based end-to-end models have achieved promising results, these methods are still facing the challenges of vision understanding, decision reasoning and scene generalization. To solve these issues, a generative planning with 3D-vision language pre-training model named GPVL is proposed for end-to-end autonomous driving. The proposed paradigm has two significant aspects. On one hand, a 3D-vision language pre-training module is designed to bridge the gap between visual perception and linguistic understanding in the bird's eye view. On the other hand, a cross-modal language model is introduced to generate holistic driving decisions and fine-grained trajectories with perception and navigation information in an auto-regressive manner. Experiments on the challenging nuScenes dataset demonstrate that the proposed scheme achieves excellent performances compared with state-of-the-art methods. Besides, the proposed GPVL presents strong generalization ability and real-time potential when handling high-level commands in various scenarios. It is believed that the effective, robust and efficient performance of GPVL is crucial for the practical application of future autonomous driving systems. Code is available at https://github.com/ltp1995/GPVL	 | 自主驾驶是一项具有挑战性的任务，需要感知和理解周围环境以进行安全的路径规划。尽管现有的基于视觉的端到端模型已经取得了显著成果，但这些方法仍然面临视觉理解、决策推理和场景泛化的挑战。为了解决这些问题，提出了一种名为GPVL的生成规划与三维视觉语言预训练模型，以实现端到端的自主驾驶。该提出的范式有两个重要方面。一方面，设计了一个三维视觉语言预训练模块，以在鸟瞰视角中弥合视觉感知与语言理解之间的差距。另一方面，引入了一种跨模态语言模型，通过感知和导航信息的自回归方式生成全面的驾驶决策和细粒度的轨迹。在具有挑战性的nuScenes数据集上的实验表明，所提出的方案在与最新方法相比时表现出色。此外，所提出的GPVL在处理各种场景中的高级命令时具备强大的泛化能力和实时潜力。相信GPVL的有效、稳健和高效性能对于未来自主驾驶系统的实际应用至关重要。相关代码可在https://github.com/ltp1995/GPVL获取。
2501.08837	 | MANTA: Diffusion Mamba for Efficient and Effective Stochastic Long-Term Dense Anticipation	 | Olga Zatsarynna,Emad Bahrami,Yazan Abu Farha,Gianpiero Francesca,Juergen Gall	 | 该研究解决了长时间范围内的动作预测问题，设计了MANTA网络以有效建模长序列的同时保持线性复杂度，并在多个数据集上取得了最先进的性能。	 | Our work addresses the problem of stochastic long-term dense anticipation. The goal of this task is to predict actions and their durations several minutes into the future based on provided video observations. Anticipation over extended horizons introduces high uncertainty, as a single observation can lead to multiple plausible future outcomes. To address this uncertainty, stochastic models are designed to predict several potential future action sequences. Recent work has further proposed to incorporate uncertainty modelling for observed frames by simultaneously predicting per-frame past and future actions in a unified manner. While such joint modelling of actions is beneficial, it requires long-range temporal capabilities to connect events across distant past and future time points. However, the previous work struggles to achieve such a long-range understanding due to its limited and/or sparse receptive field. To alleviate this issue, we propose a novel MANTA (MAmba for ANTicipation) network. Our model enables effective long-term temporal modelling even for very long sequences while maintaining linear complexity in sequence length. We demonstrate that our approach achieves state-of-the-art results on three datasets - Breakfast, 50Salads, and Assembly101 - while also significantly improving computational and memory efficiency.	 | 我们的研究解决了随机长期密集预见的问题。本任务的目标是在给定视频观察的基础上，预测几分钟后的动作及其持续时间。在更长的时间范围内进行预见引入了高不确定性，因为单个观察可能会导致多种可能的未来结果。为了应对这种不确定性，我们设计了随机模型来预测多个潜在的未来动作序列。最近的工作还进一步提出，通过同时在统一框架中预测每一帧的过去和未来动作来纳入观测帧的不确定性建模。虽然这种动作的联合建模是有益的，但它需要长时间范围的时间能力，以连接过去和未来时间点之间的事件。然而，先前的工作由于其有限的或稀疏的感受野，在实现这种长期理解方面遇到了困难。为了解决这一问题，我们提出了一种新的MANTA（MAmba for ANTicipation）网络。我们的模型即使对于非常长的序列也能有效地进行长期时间建模，同时保持序列长度的线性复杂度。我们展示了我们的方法在 Breakfast、50Salads 和 Assembly101 三个数据集上达到了最先进的结果，同时显著提高了计算和内存效率。
2501.08816	 | IDEA: Image Description Enhanced CLIP-Adapter	 | Zhipeng Ye,Feng Jiang,Qiufeng Wang,Kaizhu Huang,Jiaqi Huang	 | 该研究提出了一种图像描述增强的CLIP-适配器（IDEA）方法，通过结合图像的视觉特征和文本描述来提升少样本图像分类任务的性能，并在多个数据集上取得了与最先进的模型相当甚至更好的结果。进一步改进后的可训练IDEA（T-IDEA）则在11个数据集上达到了SOTA结果，并发布了生成了1,637,795个图像-文本对的“IMD-11”数据集。	 | CLIP (Contrastive Language-Image Pre-training) has attained great success in pattern recognition and computer vision. Transferring CLIP to downstream tasks (e.g. zero- or few-shot classification) is a hot topic in multimodal learning. However, current studies primarily focus on either prompt learning for text or adapter tuning for vision, without fully exploiting the complementary information and correlations among image-text pairs. In this paper, we propose an Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to few-shot image classification tasks. This method captures fine-grained features by leveraging both visual features and textual descriptions of images. IDEA is a training-free method for CLIP, and it can be comparable to or even exceeds state-of-the-art models on multiple tasks. Furthermore, we introduce Trainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable components (i.e., a projector and a learnable latent space), further enhancing the model's performance and achieving SOTA results on 11 datasets. As one important contribution, we employ the Llama model and design a comprehensive pipeline to generate textual descriptions for images of 11 datasets, resulting in a total of 1,637,795 image-text pairs, named "IMD-11". Our code and data are released at https://github.com/FourierAI/IDEA.	 | CLIP（对比语言-图像预训练）在模式识别和计算机视觉领域取得了巨大成功。将CLIP应用于下游任务（如零样本或少样本分类）是多模态学习中的热点话题。然而，当前的研究主要集中在文本的提示学习或视觉的适配调整上，而没有充分利用图像-文本配对之间的互补信息和相关性。本文提出了一种图像描述增强的CLIP-适配器（IDEA）方法，以适应少样本图像分类任务。该方法通过利用图像的视觉特征和文本描述来捕捉细粒度特征。IDEA是一种无需训练的方法，可以在多个任务上与最先进的模型相媲美，甚至超越它们。此外，我们引入了可训练的IDEA（T-IDEA），它通过添加两个轻量级可学习组件（即投影器和可学习的潜在空间）扩展了IDEA，进一步提高了模型的性能，并在11个数据集上达到了SOTA结果。作为一个重要贡献，我们使用Llama模型并设计了一个全面的生成管道，为11个数据集的图像生成文本描述，生成了共计1,637,795个图像-文本对，称为“IMD-11”。我们的代码和数据已发布在https://github.com/FourierAI/IDEA。
2501.08815	 | Human Pose-Constrained UV Map Estimation	 | Matej Suchanek,Miroslav Purkrabek,Jiri Matas	 | 该研究提出了一种新的方法PC-CSE，通过整合估计的2D人体姿态来增强像素到顶点的分配，从而提高UV图的全局连贯性和解剖学合理性，评估结果显示该方法在DensePose COCO上实现了稳定改进。	 | UV map estimation is used in computer vision for detailed analysis of human posture or activity. Previous methods assign pixels to body model vertices by comparing pixel descriptors independently, without enforcing global coherence or plausibility in the UV map. We propose Pose-Constrained Continuous Surface Embeddings (PC-CSE), which integrates estimated 2D human pose into the pixel-to-vertex assignment process. The pose provides global anatomical constraints, ensuring that UV maps remain coherent while preserving local precision. Evaluation on DensePose COCO demonstrates consistent improvement, regardless of the chosen 2D human pose model. Whole-body poses offer better constraints by incorporating additional details about the hands and feet. Conditioning UV maps with human pose reduces invalid mappings and enhances anatomical plausibility. In addition, we highlight inconsistencies in the ground-truth annotations.	 | UV图估计在计算机视觉中用于详细分析人类姿势或活动。之前的方法通过独立比较像素描述符将像素分配给身体模型顶点，而不强制UV图的全局连贯性和合理性。我们提出了Pose-Constrained Continuous Surface Embeddings (PC-CSE)，它将估计的2D人体姿态整合到像素到顶点的分配过程中。人体姿态提供了全局解剖约束，确保UV图保持连贯性，同时保留局部精度。在DensePose COCO上的评估表明，无论选择哪种2D人体姿态模型，都能实现一致的改进。全身姿态通过包含关于手和脚的额外细节提供了更好的约束。使用人体姿态条件化UV图可以减少无效映射并增强解剖学合理性。此外，我们还指出了地面真实标注中的不一致性。
2501.08807	 | Multi-visual modality micro drone-based structural damage detection	 | Isaac Osei Agyemanga,Liaoyuan Zeng,Jianwen Chena,Isaac Adjei-Mensah,Daniel Acheampong	 | 研究提出了一种名为DetectorX的鲁棒框架，结合微型无人机进行结构损伤检测，通过引入茎块和螺旋池化技术增强了物体检测器的准确性和适应性，在多个实验中表现出良好的性能和鲁棒性。	 | Accurate detection and resilience of object detectors in structural damage detection are important in ensuring the continuous use of civil infrastructure. However, achieving robustness in object detectors remains a persistent challenge, impacting their ability to generalize effectively. This study proposes DetectorX, a robust framework for structural damage detection coupled with a micro drone. DetectorX addresses the challenges of object detector robustness by incorporating two innovative modules: a stem block and a spiral pooling technique. The stem block introduces a dynamic visual modality by leveraging the outputs of two Deep Convolutional Neural Network (DCNN) models. The framework employs the proposed event-based reward reinforcement learning to constrain the actions of a parent and child DCNN model leading to a reward. This results in the induction of two dynamic visual modalities alongside the Red, Green, and Blue (RGB) data. This enhancement significantly augments DetectorX's perception and adaptability in diverse environmental situations. Further, a spiral pooling technique, an online image augmentation method, strengthens the framework by increasing feature representations by concatenating spiraled and average/max pooled features. In three extensive experiments: (1) comparative and (2) robustness, which use the Pacific Earthquake Engineering Research Hub ImageNet dataset, and (3) field-experiment, DetectorX performed satisfactorily across varying metrics, including precision (0.88), recall (0.84), average precision (0.91), mean average precision (0.76), and mean average recall (0.73), compared to the competing detectors including You Only Look Once X-medium (YOLOX-m) and others. The study's findings indicate that DetectorX can provide satisfactory results and demonstrate resilience in challenging environments.	 | 结构损伤检测中物体检测器的准确检测和鲁棒性对于确保民用基础设施的连续使用至关重要。然而，实现物体检测器的鲁棒性仍然是一个持续的挑战，影响其有效泛化的能力。本研究提出了一种名为DetectorX的鲁棒框架，结合了微型无人机进行结构损伤检测。DetectorX通过引入两个创新模块——茎块和螺旋池化技术，来解决物体检测器的鲁棒性问题。茎块通过利用两个深度卷积神经网络（DCNN）模型的输出引入动态视觉模态。该框架采用提出的基于事件的奖励强化学习来约束父母和子DCNN模型的行为，从而产生奖励。这一过程使DetectorX在红、绿、蓝（RGB）数据之外还产生了两种动态视觉模态，显著增强了其在不同环境情况下的感知和适应能力。此外，螺旋池化技术作为一种在线图像增强方法，通过连接螺旋池化和平均/最大池化特征，增强了框架的特征表示能力。在三项广泛的实验中，使用太平洋地震工程研究枢纽的ImageNet数据集进行（1）比较和（2）鲁棒性实验，以及（3）现场实验，DetectorX在多种指标（包括精确度0.88、召回率0.84、平均精确度0.91、平均平均精确度0.76和平均平均召回率0.73）上表现良好，与You Only Look Once X-medium（YOLOX-m）和其他竞品检测器相比。研究结果表明，DetectorX可以提供满意的结果，并在挑战性环境中表现出良好的鲁棒性。
2501.08799	 | Exploring ChatGPT for Face Presentation Attack Detection in Zero and Few-Shot in-Context Learning	 | Alain Komaty,Hatef Otroshi Shahreza,Anjith George,Sebastien Marcel	 | 研究展示了GPT-4o在面部欺骗检测任务中的潜力，特别是在少量示例学习场景中表现出色，并具备一定的推理能力，但零样本任务中仍受限于专门的PAD系统。	 | This study highlights the potential of ChatGPT (specifically GPT-4o) as a competitive alternative for Face Presentation Attack Detection (PAD), outperforming several PAD models, including commercial solutions, in specific scenarios. Our results show that GPT-4o demonstrates high consistency, particularly in few-shot in-context learning, where its performance improves as more examples are provided (reference data). We also observe that detailed prompts enable the model to provide scores reliably, a behavior not observed with concise prompts. Additionally, explanation-seeking prompts slightly enhance the model's performance by improving its interpretability. Remarkably, the model exhibits emergent reasoning capabilities, correctly predicting the attack type (print or replay) with high accuracy in few-shot scenarios, despite not being explicitly instructed to classify attack types. Despite these strengths, GPT-4o faces challenges in zero-shot tasks, where its performance is limited compared to specialized PAD systems. Experiments were conducted on a subset of the SOTERIA dataset, ensuring compliance with data privacy regulations by using only data from consenting individuals. These findings underscore GPT-4o's promise in PAD applications, laying the groundwork for future research to address broader data privacy concerns and improve cross-dataset generalization. Code available here: https://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad	 | 本研究突显了ChatGPT（特别是GPT-4o）作为面部欺骗检测（Face Presentation Attack Detection, PAD）竞争性替代方案的潜力，在特定场景中其表现优于包括商业解决方案在内的多个PAD模型。我们的结果显示，GPT-4o 在少量示例上下文学习中表现出高度的一致性，随着提供更多的示例（参考数据），其性能会有所提升。我们还观察到，详细提示可以使模型更可靠地提供评分，而简洁提示则不具备这种行为。此外，寻求解释的提示能略微提升模型的性能，提高其可解释性。值得注意的是，该模型表现出新兴的推理能力，在少量示例场景中正确预测了攻击类型（打印或重放），尽管模型并未被明确指示分类攻击类型。尽管具备这些优点，GPT-4o 在零样本任务中面临挑战，其性能受限于专门的PAD系统。实验在SOTERIA数据集的部分子集上进行，通过仅使用同意个体的数据，确保符合数据隐私法规。这些发现突显了GPT-4o 在PAD应用中的潜力，并为未来研究解决更广泛的数据隐私问题和提高跨数据集泛化能力奠定了基础。相关代码可在此获取：https://gitlab.idiap.ch/bob/bob.paper.wacv2025_chatgpt_face_pad
2501.08771	 | Admitting Ignorance Helps the Video Question Answering Models to Answer	 | Haopeng Li,Tom Drummond,Mingming Gong,Mohammed Bennamoun,Qiuhong Ke	 | 该研究针对现有视频问答模型易出现的虚假相关性问题，提出了一种新的训练框架，通过引入干预问题和位移、扰动等技术，促使模型在面对未知情况时承认自己的局限性，从而提高模型的性能。实验结果表明，仅通过少量结构修改，该框架就能显著提升VideoQA模型的表现。	 | Significant progress has been made in the field of video question answering (VideoQA) thanks to deep learning and large-scale pretraining. Despite the presence of sophisticated model structures and powerful video-text foundation models, most existing methods focus solely on maximizing the correlation between answers and video-question pairs during training. We argue that these models often establish shortcuts, resulting in spurious correlations between questions and answers, especially when the alignment between video and text data is suboptimal. To address these spurious correlations, we propose a novel training framework in which the model is compelled to acknowledge its ignorance when presented with an intervened question, rather than making guesses solely based on superficial question-answer correlations. We introduce methodologies for intervening in questions, utilizing techniques such as displacement and perturbation, and design frameworks for the model to admit its lack of knowledge in both multi-choice VideoQA and open-ended settings. In practice, we integrate a state-of-the-art model into our framework to validate its effectiveness. The results clearly demonstrate that our framework can significantly enhance the performance of VideoQA models with minimal structural modifications.	 | 在深度学习和大规模预训练的推动下，视频问答（VideoQA）领域取得了显著进展。尽管存在复杂的模型结构和强大的视频-文本基础模型，现有的大多数方法仍然仅仅在训练过程中着力于最大化答案与视频-问题对之间的相关性。我们认为，这些模型往往建立了一些捷径，导致问题和答案之间出现虚假的相关性，尤其是在视频和文本数据对齐不佳的情况下。为了应对这些虚假的相关性，我们提出了一种新的训练框架，在这种框架中，当模型面对干预后的提问时，它被强迫承认自己的无知，而不是仅仅基于表面的问题-答案相关性进行猜测。我们引入了干预问题的方法，使用诸如位移和扰动等技术，并设计了模型在多选和开放式提问情境下承认自己知识不足的框架。在实践中，我们将一个最先进的模型整合到我们的框架中以验证其有效性。结果表明，我们的框架仅通过少量的结构修改就能显著提升VideoQA模型的性能。
2501.08763	 | Few-Shot Learner Generalizes Across AI-Generated Image Detection	 | Shiyu Wu,Jing Liu,Jing Li,Yequan Wang	 | 我们提出了一种新型的Few-Shot Detector（FSD），它可以利用少量样本学习专门的度量空间来有效地区分未见过的假图像，从而在GenImage数据集上提高了7.4%的平均ACC，并在未见过的图像中更好地捕捉到了类别内部的共同特征。	 | Current fake image detectors trained on large synthetic image datasets perform satisfactorily on limited studied generative models. However, they suffer a notable performance decline over unseen models. Besides, collecting adequate training data from online generative models is often expensive or infeasible. To overcome these issues, we propose Few-Shot Detector (FSD), a novel AI-generated image detector which learns a specialized metric space to effectively distinguish unseen fake images by utilizing very few samples. Experiments show FSD achieves state-of-the-art performance by $+7.4\%$ average ACC on GenImage dataset. More importantly, our method is better capable of capturing the intra-category common features in unseen images without further training.	 | 当前，基于大型合成图像数据集训练的假图像检测器在有限研究的生成模型上表现良好，但在未见过的模型上则表现出明显的性能下降。此外，从在线生成模型收集足够的训练数据往往成本高昂或不现实。为了解决这些问题，我们提出了一种新型的Few-Shot Detector（FSD），这是一种利用少量样本学习专门度量空间的AI生成图像检测器，可以有效地区分未见过的假图像。实验结果显示，FSD在GenImage数据集上的平均ACC提高了7.4%，达到最佳性能。更重要的是，我们的方法在未见过的图像中更好地捕捉到了类别内部的共同特征，无需进一步训练。
2501.08712	 | Self-supervised Transformation Learning for Equivariant Representations	 | Jaemyung Yu,Jaehyun Choi,Dong-Jae Lee,HyeongGwon Hong,Junmo Kim	 | 自我监督变换学习（STL）通过使用图像对中的变换表示来学习不变和变换敏感的特征，从而在多种分类和检测任务中超越了现有方法，并且能够处理复杂的变换，展示了良好的适应性和鲁棒性。	 | Unsupervised representation learning has significantly advanced various machine learning tasks. In the computer vision domain, state-of-the-art approaches utilize transformations like random crop and color jitter to achieve invariant representations, embedding semantically the same inputs despite transformations. However, this can degrade performance in tasks requiring precise features, such as localization or flower classification. To address this, recent research incorporates equivariant representation learning, which captures transformation-sensitive information. However, current methods depend on transformation labels and thus struggle with interdependency and complex transformations. We propose Self-supervised Transformation Learning (STL), replacing transformation labels with transformation representations derived from image pairs. The proposed method ensures transformation representation is image-invariant and learns corresponding equivariant transformations, enhancing performance without increased batch complexity. We demonstrate the approach's effectiveness across diverse classification and detection tasks, outperforming existing methods in 7 out of 11 benchmarks and excelling in detection. By integrating complex transformations like AugMix, unusable by prior equivariant methods, this approach enhances performance across tasks, underscoring its adaptability and resilience. Additionally, its compatibility with various base models highlights its flexibility and broad applicability. The code is available at https://github.com/jaemyung-u/stl.	 | 无监督表示学习在各种机器学习任务中取得了显著进展。在计算机视觉领域，最先进的方法利用随机裁剪和色域抖动等变换来实现不变表示，即使在变换下也能嵌入语义相同的输入。然而，这种方法在需要精确特征的任务中（如定位或花卉分类）可能会降低性能。为了解决这一问题，最近的研究引入了变换敏感的表示学习，它可以捕捉到变换的敏感信息。然而，当前的方法依赖于变换标签，因此难以处理复杂的依赖性和变换。我们提出了自我监督变换学习（STL），用图像对中获得的变换表示来替代变换标签。该方法确保变换表示是图像不变的，并学习相应的变换敏感性，从而在不增加批处理复杂性的情况下提升性能。我们通过在各种分类和检测任务中的应用展示了该方法的有效性，在11个基准中有7个超越了现有方法，并在检测任务中表现尤为出色。通过整合先前变换敏感方法无法处理的复杂变换（如AugMix），该方法在任务中提升了性能，证明了其适应性和鲁棒性。此外，该方法与多种基础模型的兼容性展示了其灵活性和广泛适用性。代码可以在https://github.com/jaemyung-u/stl 获取。
2501.08682	 | RealVVT: Towards Photorealistic Video Virtual Try-on via Spatio-Temporal Consistency	 | Siqi Li,Zhengkai Jiang,Jiawei Zhou,Zhihong Liu,Xiaowei Chi,Haoqian Wang	 | RealVVT框架通过引入衣物与时间一致性策略、空间一致性机制以及姿势引导的长视频虚拟试穿技术，有效解决了视频序列中虚拟试穿的一致性和真实性问题，显著提升了现有模型的性能。	 | Virtual try-on has emerged as a pivotal task at the intersection of computer vision and fashion, aimed at digitally simulating how clothing items fit on the human body. Despite notable progress in single-image virtual try-on (VTO), current methodologies often struggle to preserve a consistent and authentic appearance of clothing across extended video sequences. This challenge arises from the complexities of capturing dynamic human pose and maintaining target clothing characteristics. We leverage pre-existing video foundation models to introduce RealVVT, a photoRealistic Video Virtual Try-on framework tailored to bolster stability and realism within dynamic video contexts. Our methodology encompasses a Clothing & Temporal Consistency strategy, an Agnostic-guided Attention Focus Loss mechanism to ensure spatial consistency, and a Pose-guided Long Video VTO technique adept at handling extended video sequences.Extensive experiments across various datasets confirms that our approach outperforms existing state-of-the-art models in both single-image and video VTO tasks, offering a viable solution for practical applications within the realms of fashion e-commerce and virtual fitting environments.	 | 虚拟试穿已成为计算机视觉与时尚交汇领域的重要任务，旨在通过数字方式模拟衣物在人体上的搭配效果。尽管在单张图像虚拟试穿（VTO）上取得了显著进展，但当前的方法往往难以在整个视频序列中保持服装的一致性和真实感。这一挑战源于捕捉动态人体姿态和保持目标服装特性上的复杂性。我们利用现有的视频基础模型，引入了RealVVT框架，该框架旨在增强动态视频环境中的稳定性和真实性。我们的方法包括衣物与时间一致性策略、无约束引导注意力聚焦损失机制以确保空间一致性，以及姿势引导的长视频虚拟试穿技术，该技术适用于处理长视频序列。通过对多个数据集的广泛实验表明，我们的方法在单张图像和视频虚拟试穿任务中均优于现有最先进的模型，为电子商务中的服装和虚拟试衣环境提供了可行的解决方案。
2501.08676	 | FlexiClip: Locality-Preserving Free-Form Character Animation	 | Anant Khandelwal	 | FlexiClip 通过引入时间雅可比量和概率流微分方程来解决剪贴画动画中的时间一致性和几何完整性问题，从而实现平滑且无缝的运动过渡。该方法结合了空间和时间建模，并与预训练的视频扩散模型集成，以生成高质量的剪贴画动画，适用于快速运动和非刚性变形的复杂场景。	 | Animating clipart images with seamless motion while maintaining visual fidelity and temporal coherence presents significant challenges. Existing methods, such as AniClipart, effectively model spatial deformations but often fail to ensure smooth temporal transitions, resulting in artifacts like abrupt motions and geometric distortions. Similarly, text-to-video (T2V) and image-to-video (I2V) models struggle to handle clipart due to the mismatch in statistical properties between natural video and clipart styles. This paper introduces FlexiClip, a novel approach designed to overcome these limitations by addressing the intertwined challenges of temporal consistency and geometric integrity. FlexiClip extends traditional Bézier curve-based trajectory modeling with key innovations: temporal Jacobians to correct motion dynamics incrementally, continuous-time modeling via probability flow ODEs (pfODEs) to mitigate temporal noise, and a flow matching loss inspired by GFlowNet principles to optimize smooth motion transitions. These enhancements ensure coherent animations across complex scenarios involving rapid movements and non-rigid deformations. Extensive experiments validate the effectiveness of FlexiClip in generating animations that are not only smooth and natural but also structurally consistent across diverse clipart types, including humans and animals. By integrating spatial and temporal modeling with pre-trained video diffusion models, FlexiClip sets a new standard for high-quality clipart animation, offering robust performance across a wide range of visual content. Project Page: https://creative-gen.github.io/flexiclip.github.io/	 | 在保持视觉保真度和时间连贯性的同时，使剪贴画图像实现平滑且无缝的运动动画面临着重大挑战。现有方法，例如AniClipart，虽然能够有效地建模空间变形，但往往无法确保平滑的时间过渡，从而导致诸如突兀的运动和几何失真等伪影。同样，文本转视频（T2V）和图像转视频（I2V）模型由于自然视频和剪贴画风格在统计特性的不匹配问题，难以处理剪贴画。本文介绍了一种名为FlexiClip的创新方法，旨在通过解决时间一致性和几何完整性交织的问题来克服这些限制。FlexiClip通过引入时间雅可比量来逐步纠正运动动力学，通过概率流微分方程（pfODEs）进行连续时间建模以减轻时间噪声，并采用受GFlowNet原则启发的流动匹配损失来优化平滑的运动过渡。这些增强确保在涉及快速运动和非刚性变形的复杂场景中实现连贯的动画。广泛的经验验证表明，FlexiClip在生成既平滑自然又在不同类型的剪贴画中结构一致的动画方面非常有效，包括人类和动物。通过将空间和时间建模与预训练的视频扩散模型集成，FlexiClip设定了高质量剪贴画动画的新标准，并在各种视觉内容中表现出良好的性能。项目页面：https://creative-gen.github.io/flexiclip.github.io/
2501.08665	 | A Survey on Facial Image Privacy Preservation in Cloud-Based Services	 | Chen Chen,Mengyuan Sun,Xueluan Gong,Yanjiao Chen,Qian Wang	 | 本文综述了当前用于保护云服务中面部图像隐私的方法，将其分为图像模糊化保护和对抗扰动保护两大类，并分析了这两种方法的有效性，指出了现有研究的不足并提出了未来的研究方向。	 | Facial recognition models are increasingly employed by commercial enterprises, government agencies, and cloud service providers for identity verification, consumer services, and surveillance. These models are often trained using vast amounts of facial data processed and stored in cloud-based platforms, raising significant privacy concerns. Users' facial images may be exploited without their consent, leading to potential data breaches and misuse. This survey presents a comprehensive review of current methods aimed at preserving facial image privacy in cloud-based services. We categorize these methods into two primary approaches: image obfuscation-based protection and adversarial perturbation-based protection. We provide an in-depth analysis of both categories, offering qualitative and quantitative comparisons of their effectiveness. Additionally, we highlight unresolved challenges and propose future research directions to improve privacy preservation in cloud computing environments.	 | 面部识别模型越来越多地被商业企业、政府机构和云服务提供商用于身份验证、消费者服务和监控。这些模型通常使用大量的面部数据进行训练，并在云平台中处理和存储，这引发了重大的隐私问题。用户的面部图像可能在未经其同意的情况下被利用，从而导致数据泄露和滥用。本文综述了当前旨在保护云服务中面部图像隐私的方法。我们将这些方法分为两大类：基于图像模糊化的保护和基于对抗扰动的保护。我们对这两类方法进行了深入分析，并提供了它们有效性的定性和定量比较。此外，我们还指出了未解决的挑战，并提出了未来研究方向，以改善云计算环境中的隐私保护。
2501.08659	 | BRIGHT-VO: Brightness-Guided Hybrid Transformer for Visual Odometry with Multi-modality Refinement Module	 | Dongzhihan Wang,Yang Yang,Liang Xu	 | BrightVO是一种基于Transformer的视觉里程计方法，通过前端的视觉特征提取和后端的多模态精炼模块（结合IMU数据）优化位姿估计，特别是在低光照条件下性能显著提升。该研究还引入了一个合成的低光数据集KiC4R，并在KITTI基准测试中达到了最先进的性能，开源代码可在GitHub上获取。	 | Visual odometry (VO) plays a crucial role in autonomous driving, robotic navigation, and other related tasks by estimating the position and orientation of a camera based on visual input. Significant progress has been made in data-driven VO methods, particularly those leveraging deep learning techniques to extract image features and estimate camera poses. However, these methods often struggle in low-light conditions because of the reduced visibility of features and the increased difficulty of matching keypoints. To address this limitation, we introduce BrightVO, a novel VO model based on Transformer architecture, which not only performs front-end visual feature extraction, but also incorporates a multi-modality refinement module in the back-end that integrates Inertial Measurement Unit (IMU) data. Using pose graph optimization, this module iteratively refines pose estimates to reduce errors and improve both accuracy and robustness. Furthermore, we create a synthetic low-light dataset, KiC4R, which includes a variety of lighting conditions to facilitate the training and evaluation of VO frameworks in challenging environments. Experimental results demonstrate that BrightVO achieves state-of-the-art performance on both the KiC4R dataset and the KITTI benchmarks. Specifically, it provides an average improvement of 20% in pose estimation accuracy in normal outdoor environments and 259% in low-light conditions, outperforming existing methods. For widespread use and further development, the research work is fully open-source at https://github.com/Anastasiawd/BrightVO.	 | 视觉里程计（VO）在无人驾驶、机器人导航以及其他相关任务中扮演着重要角色，它通过基于视觉输入来估计摄像机的位置和方向。数据驱动的VO方法取得了显著进展，特别是那些利用深度学习技术提取图像特征和估计摄像机姿态的方法。然而，这些方法在低光条件下常常遇到困难，因为特征的可见度较低，关键点之间的匹配也更加困难。为了解决这一局限性，我们引入了基于Transformer架构的BrightVO，这是一种新颖的VO模型，不仅能够在前端执行视觉特征提取，还在后端集成了多模态精炼模块，该模块整合了加速度计和陀螺仪（IMU）数据。通过位姿图优化，该模块逐步精炼位姿估计，从而减少误差并提高准确性和鲁棒性。此外，我们创建了一个合成的低光数据集KiC4R，包含各种光照条件，以促进在具有挑战性的环境中训练和评估VO框架。实验结果表明，BrightVO在KiC4R数据集和KITTI基准测试上都达到了最先进的性能。具体而言，在正常户外环境中，它在姿态估计准确性方面平均提高了20%，在低光条件下提高了259%，超过了现有方法。为了广泛使用和进一步发展，该研究工作已完全开源，可以在https://github.com/Anastasiawd/BrightVO找到。
2501.08654	 | StereoGen: High-quality Stereo Image Generation from a Single Image	 | Xianqi Wang,Hao Yang,Gangwei Xu,Junda Cheng,Min Lin,Yong Deng,Jinliang Zang,Yurui Chen,Xin Yang	 | 本文提出了一种新的立体图像生成管道StereoGen，通过利用单目深度估计模型生成伪深度图并采用微调扩散填补模型来恢复背景，从而生成高质量的右图像。实验表明，基于该管道训练的模型在零样本泛化方面达到了最先进的效果。	 | State-of-the-art supervised stereo matching methods have achieved amazing results on various benchmarks. However, these data-driven methods suffer from generalization to real-world scenarios due to the lack of real-world annotated data. In this paper, we propose StereoGen, a novel pipeline for high-quality stereo image generation. This pipeline utilizes arbitrary single images as left images and pseudo disparities generated by a monocular depth estimation model to synthesize high-quality corresponding right images. Unlike previous methods that fill the occluded area in warped right images using random backgrounds or using convolutions to take nearby pixels selectively, we fine-tune a diffusion inpainting model to recover the background. Images generated by our model possess better details and undamaged semantic structures. Besides, we propose Training-free Confidence Generation and Adaptive Disparity Selection. The former suppresses the negative effect of harmful pseudo ground truth during stereo training, while the latter helps generate a wider disparity distribution and better synthetic images. Experiments show that models trained under our pipeline achieve state-of-the-art zero-shot generalization results among all published methods. The code will be available upon publication of the paper.	 | 最新的监督立体匹配方法在各种基准测试上取得了令人惊叹的结果。然而，这些数据驱动的方法由于缺乏真实世界的标注数据，在应用于现实场景时存在泛化能力不足的问题。在本文中，我们提出了一种新的立体图像生成管道——StereoGen。此管道利用任意单张图像作为左图像，并通过单目深度估计模型生成伪深度图来合成高质量的右图像。与之前的方法不同，这些方法在合成的右图像中填补遮挡区域时使用随机背景或通过卷积选择性地利用邻近像素，我们采用了微调扩散填补模型来恢复背景。生成的图像具有更好的细节和未受损的语义结构。此外，我们还提出了无需训练的置信度生成和自适应视差选择。前者在立体匹配训练过程中抑制有害伪真实标签的负面影响，后者有助于生成更广泛的视差分布和更好的合成图像。实验表明，在我们提出的管道下训练的模型在所有已发布的方法中实现了最先进的零样本泛化效果。代码将在论文发表后提供。
2501.08649	 | Joint Learning of Depth and Appearance for Portrait Image Animation	 | Xinya Ji,Gaspard Zoss,Prashanth Chandran,Lingchen Yang,Xun Cao,Barbara Solenthaler,Derek Bradley	 | 本文提出了一种同时学习肖像图像的视觉外观和深度的新方法，采用端到端的扩散范式，并通过引入参考网络和通道扩展的扩散主干来有效生成高质量的三维输出，适用于多种下游应用。	 | 2D portrait animation has experienced significant advancements in recent years. Much research has utilized the prior knowledge embedded in large generative diffusion models to enhance high-quality image manipulation. However, most methods only focus on generating RGB images as output, and the co-generation of consistent visual plus 3D output remains largely under-explored. In our work, we propose to jointly learn the visual appearance and depth simultaneously in a diffusion-based portrait image generator. Our method embraces the end-to-end diffusion paradigm and introduces a new architecture suitable for learning this conditional joint distribution, consisting of a reference network and a channel-expanded diffusion backbone. Once trained, our framework can be efficiently adapted to various downstream applications, such as facial depth-to-image and image-to-depth generation, portrait relighting, and audio-driven talking head animation with consistent 3D output.	 | 近年来，二维肖像动画经历了显著的进步。许多研究利用大型生成扩散模型中嵌入的先验知识来提升高质量图像的处理效果。然而，大多数方法仅专注于生成RGB图像作为输出，而视觉一致性和3D输出的共同生成却仍然很大程度上未被探索。在我们的工作中，我们提出了一种在基于扩散的肖像图像生成器中同时学习视觉外观和深度的方法。我们的方法采用了端到端的扩散范式，并引入了一种适合学习这种条件联合分布的新架构，该架构包括一个参考网络和一个通道扩展的扩散主干。经过训练后，我们的框架可以高效地适应各种下游应用，如面部深度到图像生成、图像到深度生成、肖像重新光照以及与一致3D输出的音频驱动的谈话头动画。
2501.08643	 | MonSter: Marry Monodepth to Stereo Unleashes Power	 | Junda Cheng,Longliang Liu,Gangwei Xu,Xianqi Wang,Zhaoxing Zhang,Yong Deng,Jinliang Zang,Yurui Chen,Zhipeng Cai,Xin Yang	 | MonSter 是一种结合单目深度估计和立体匹配互补优势的新方法，通过迭代相互提升来处理病态区域，从而在多个常用排行榜上取得优异性能，特别是在ETH3D数据集上显著优于现有方法。	 | Stereo matching recovers depth from image correspondences. Existing methods struggle to handle ill-posed regions with limited matching cues, such as occlusions and textureless areas. To address this, we propose MonSter, a novel method that leverages the complementary strengths of monocular depth estimation and stereo matching. MonSter integrates monocular depth and stereo matching into a dual-branch architecture to iteratively improve each other. Confidence-based guidance adaptively selects reliable stereo cues for monodepth scale-shift recovery. The refined monodepth is in turn guides stereo effectively at ill-posed regions. Such iterative mutual enhancement enables MonSter to evolve monodepth priors from coarse object-level structures to pixel-level geometry, fully unlocking the potential of stereo matching. As shown in Fig.1, MonSter ranks 1st across five most commonly used leaderboards -- SceneFlow, KITTI 2012, KITTI 2015, Middlebury, and ETH3D. Achieving up to 49.5% improvements (Bad 1.0 on ETH3D) over the previous best method. Comprehensive analysis verifies the effectiveness of MonSter in ill-posed regions. In terms of zero-shot generalization, MonSter significantly and consistently outperforms state-of-the-art across the board. The code is publicly available at: https://github.com/Junda24/MonSter.	 | 立体匹配可以从图像对应关系中恢复深度。现有方法在处理匹配线索有限的病态区域，如遮挡和纹理缺失区域时表现不佳。为了解决这个问题，我们提出了一种名为MonSter的新方法，该方法利用单目深度估计和立体匹配的互补优势。MonSter将单目深度和立体匹配整合到一个双分支架构中，通过迭代相互提升。基于置信度的指导选择可靠的立体匹配线索来恢复单目深度的尺度变化。经过细化的单目深度随后在病态区域有效地指导立体匹配。这种迭代的相互提升使MonSter能够从粗略的对象级结构进化到像素级几何结构，完全释放了立体匹配的潜力。如图1所示，MonSter在SceneFlow、KITTI 2012、KITTI 2015、Middlebury和ETH3D这五个常用排行榜中均排名首位，相较于之前的最佳方法，在ETH3D上的性能提高了高达49.5%，即Bad 1.0。全面的分析验证了MonSter在病态区域的有效性。在零样本泛化方面，MonSter在各方面显著且一致地优于最先进的方法。代码已在https://github.com/Junda24/MonSter上公开。
2501.08639	 | Detecting Wildfire Flame and Smoke through Edge Computing using Transfer Learning Enhanced Deep Learning Models	 | Giovanny Vazquez,Shengjie Zhai,Mei Yang	 | 本研究利用迁移学习提升边缘计算设备上野火烟雾和火焰检测的性能，并通过实验验证了其在提高检测精度和泛化能力方面的有效性，但发现级联迁移学习并未对边缘计算的关键指标产生显著改善。研究表明，在缺乏硬件加速的情况下，YOLOv5n仍然是一个高效的模型，处理速度远超其最新版本YOLOv11n。	 | Autonomous unmanned aerial vehicles (UAVs) integrated with edge computing capabilities empower real-time data processing directly on the device, dramatically reducing latency in critical scenarios such as wildfire detection. This study underscores Transfer Learning's (TL) significance in boosting the performance of object detectors for identifying wildfire smoke and flames, especially when trained on limited datasets, and investigates the impact TL has on edge computing metrics. With the latter focusing how TL-enhanced You Only Look Once (YOLO) models perform in terms of inference time, power usage, and energy consumption when using edge computing devices. This study utilizes the Aerial Fire and Smoke Essential (AFSE) dataset as the target, with the Flame and Smoke Detection Dataset (FASDD) and the Microsoft Common Objects in Context (COCO) dataset serving as source datasets. We explore a two-stage cascaded TL method, utilizing D-Fire or FASDD as initial stage target datasets and AFSE as the subsequent stage. Through fine-tuning, TL significantly enhances detection precision, achieving up to 79.2% mean Average Precision (mAP@0.5), reduces training time, and increases model generalizability across the AFSE dataset. However, cascaded TL yielded no notable improvements and TL alone did not benefit the edge computing metrics evaluated. Lastly, this work found that YOLOv5n remains a powerful model when lacking hardware acceleration, finding that YOLOv5n can process images nearly twice as fast as its newer counterpart, YOLO11n. Overall, the results affirm TL's role in augmenting the accuracy of object detectors while also illustrating that additional enhancements are needed to improve edge computing performance.	 | 自主无人机（UAV）集成了边缘计算能力，能够直接在设备上进行实时数据处理，大幅减少了如野火检测等关键场景中的延迟。本研究强调了迁移学习（TL）在提升识别野火烟雾和火焰的对象检测器性能方面的重要性，尤其是在训练数据有限的情况下，并探讨了TL对边缘计算指标的影响。后者关注的是TL增强的You Only Look Once (YOLO) 模型在使用边缘计算设备时的推理时间、能耗和能量消耗等方面的性能。本研究利用Aerial Fire and Smoke Essential (AFSE) 数据集作为目标，将Flame and Smoke Detection Dataset（FASDD）和Microsoft Common Objects in Context（COCO）数据集作为源数据集。我们探索了一种两阶段级联的迁移学习方法，使用D-Fire或FASDD作为初始阶段的目标数据集，AFSE作为后续阶段。通过微调，TL显著提高了检测精度，实现了高达79.2%的平均精度（mAP@0.5），减少了训练时间，并提高了模型在AFSE数据集上的泛化能力。然而，级联的TL并未带来显著的改进，仅TL本身也没有对评估的边缘计算指标产生正面影响。最后，本研究发现，在缺乏硬件加速的情况下，YOLOv5n仍然是一个强大的模型，并发现YOLOv5n处理图像的速度几乎是其较新的版本YOLO11n的两倍。总体而言，结果证实了TL在增强对象检测器准确性方面的作用，同时也表明还需要进一步改进以提升边缘计算性能。
2501.08609	 | Computerized Assessment of Motor Imitation for Distinguishing Autism in Video (CAMI-2DNet)	 | Kaleab A. Kinfu,Carolina Pacheco,Alice D. Sperry,Deana Crocetti,Bahar Tunçgenç,Stewart H. Mostofsky,René Vidal	 | 本文提出了一种名为CAMI-2DNet的深度学习方法，用于自动评估视频数据中的运动模仿能力，该方法能够在不需要数据标准化、清洗和注释的情况下，从视频中解耦出运动编码，并通过计算运动编码相似性来区分自闭症谱系障碍（ASC）个体与典型个体。	 | Motor imitation impairments are commonly reported in individuals with autism spectrum conditions (ASCs), suggesting that motor imitation could be used as a phenotype for addressing autism heterogeneity. Traditional methods for assessing motor imitation are subjective, labor-intensive, and require extensive human training. Modern Computerized Assessment of Motor Imitation (CAMI) methods, such as CAMI-3D for motion capture data and CAMI-2D for video data, are less subjective. However, they rely on labor-intensive data normalization and cleaning techniques, and human annotations for algorithm training. To address these challenges, we propose CAMI-2DNet, a scalable and interpretable deep learning-based approach to motor imitation assessment in video data, which eliminates the need for data normalization, cleaning and annotation. CAMI-2DNet uses an encoder-decoder architecture to map a video to a motion encoding that is disentangled from nuisance factors such as body shape and camera views. To learn a disentangled representation, we employ synthetic data generated by motion retargeting of virtual characters through the reshuffling of motion, body shape, and camera views, as well as real participant data. To automatically assess how well an individual imitates an actor, we compute a similarity score between their motion encodings, and use it to discriminate individuals with ASCs from neurotypical (NT) individuals. Our comparative analysis demonstrates that CAMI-2DNet has a strong correlation with human scores while outperforming CAMI-2D in discriminating ASC vs NT children. Moreover, CAMI-2DNet performs comparably to CAMI-3D while offering greater practicality by operating directly on video data and without the need for ad-hoc data normalization and human annotations.	 | 自闭症谱系条件（ASCs）个体常常报告存在运动模仿障碍，这提示运动模仿可能被用作解决自闭症异质性的表型。传统的运动模仿评估方法具有主观性、劳动密集型且需要大量的人类培训。现代计算机化运动模仿评估方法（如用于运动捕捉数据的CAMI-3D和用于视频数据的CAMI-2D），虽然相对主观性较少，但仍依赖于劳动密集型的数据标准化和清洗技术，以及人类注释来进行算法训练。为了解决这些挑战，我们提出了一种名为CAMI-2DNet的可扩展且可解释的基于深度学习的方法，用于视频数据中的运动模仿评估。CAMI-2DNet消除了数据标准化、清洗和注释的需要，采用编码解码架构将视频映射到从身体形态和摄像视角等干扰因素中解耦的运动编码。为了学习一个解耦的表示，我们利用运动重定向生成的合成数据（通过重新排列运动、身体形态和摄像视角来虚拟角色的运动），以及真实参与者数据。为了自动评估个体模仿演员的能力，我们计算了其运动编码之间的相似性分数，并使用该分数来辨别ASC个体与神经典型（NT）个体。我们的对比分析表明，CAMI-2DNet与人类评分有很强的相关性，且在区分ASC儿童与NT儿童方面优于CAMI-2D。此外，CAMI-2DNet在性能上与CAMI-3D相当，但在直接在视频数据上运行且无需进行特定的数据标准化和人类注释的情况下更具实用性。
2501.08605	 | PACF: Prototype Augmented Compact Features for Improving Domain Adaptive Object Detection	 | Chenguang Liu,Yongchao Feng,Yanan Zhang,Qingjie Liu,Yunhong Wang	 | 本文提出了一种原型增强紧凑特征（PACF）框架，通过校准目标区域提议网络特征的分布并降低类条件分布的方差，有效解决了目标检测器在新领域应用时性能下降的问题。该框架通过原型交叉熵损失和互惠正则化策略来提高特征的紧凑性和可区分性，从而实现更好的跨域泛化能力。	 | In recent years, there has been significant advancement in object detection. However, applying off-the-shelf detectors to a new domain leads to significant performance drop, caused by the domain gap. These detectors exhibit higher-variance class-conditional distributions in the target domain than that in the source domain, along with mean shift. To address this problem, we propose the Prototype Augmented Compact Features (PACF) framework to regularize the distribution of intra-class features. Specifically, we provide an in-depth theoretical analysis on the lower bound of the target features-related likelihood and derive the prototype cross entropy loss to further calibrate the distribution of target RoI features. Furthermore, a mutual regularization strategy is designed to enable the linear and prototype-based classifiers to learn from each other, promoting feature compactness while enhancing discriminability. Thanks to this PACF framework, we have obtained a more compact cross-domain feature space, within which the variance of the target features' class-conditional distributions has significantly decreased, and the class-mean shift between the two domains has also been further reduced. The results on different adaptation settings are state-of-the-art, which demonstrate the board applicability and effectiveness of the proposed approach.	 | 近年来，目标检测方面取得了显著的进步。然而，将现成的目标检测器应用于新的领域会导致性能显著下降，这是由于领域差距的存在。这些检测器在目标域中表现出比源域中更高的类条件分布方差，并伴随均值偏移。为了解决这个问题，我们提出了一种原型增强紧凑特征（PACF）框架来规范类内特征的分布。具体而言，我们对目标特征相关似然性下界进行了深入的理论分析，并推导出原型交叉熵损失，进一步校准目标RoI特征的分布。此外，我们设计了一种互惠正则化策略，使线性分类器和原型分类器能够相互学习，从而提高特征的紧凑性和可区分性。由于PACF框架，我们得到了一个更加紧凑的跨域特征空间，在该空间中，目标特征的类条件分布的方差显著降低，两个领域之间的类均值偏移也进一步减少。在不同适应设置上的结果处于最新水平，这表明所提出的方法具有广泛的应用性和有效性。
2501.08604	 | Watermarking in Diffusion Model: Gaussian Shading with Exact Diffusion Inversion via Coupled Transformations (EDICT)	 | Krishna Panthi	 | 本文提出了一种通过结合耦合变换框架（EDICT）来改进高斯阴影水印技术的方法，通过精确逆映射和交替去噪与加噪操作，提高了水印恢复的保真度，并在标准数据集上取得了统计显著的提升。这是首次将EDICT与高斯阴影水印结合，为实现更鲁棒和高保真度的水印嵌入和提取提供了新的研究途径。	 | This paper introduces a novel approach to enhance the performance of Gaussian Shading, a prevalent watermarking technique, by integrating the Exact Diffusion Inversion via Coupled Transformations (EDICT) framework. While Gaussian Shading traditionally embeds watermarks in a noise latent space, followed by iterative denoising for image generation and noise addition for watermark recovery, its inversion process is not exact, leading to potential watermark distortion. We propose to leverage EDICT's ability to derive exact inverse mappings to refine this process. Our method involves duplicating the watermark-infused noisy latent and employing a reciprocal, alternating denoising and noising scheme between the two latents, facilitated by EDICT. This allows for a more precise reconstruction of both the image and the embedded watermark. Empirical evaluation on standard datasets demonstrates that our integrated approach yields a slight, yet statistically significant improvement in watermark recovery fidelity. These results highlight the potential of EDICT to enhance existing diffusion-based watermarking techniques by providing a more accurate and robust inversion mechanism. To the best of our knowledge, this is the first work to explore the synergy between EDICT and Gaussian Shading for digital watermarking, opening new avenues for research in robust and high-fidelity watermark embedding and extraction.	 | 本文介绍了一种通过结合耦合变换框架（EDICT）来增强高斯阴影（Gaussian Shading）水印技术性能的新方法。传统上，高斯阴影将水印嵌入噪声潜空间，然后通过迭代去噪生成图像，并通过添加噪声恢复水印。然而，其逆过程并不精确，可能导致水印失真。我们提出利用EDICT能够推导出精确逆映射的能力来改进这一过程。我们的方法包括复制嵌入水印的噪声潜空间，并在两个潜空间之间交替进行去噪和加噪操作，借助EDICT实现这一过程。这使得能够更精确地重建图像和嵌入的水印。在标准数据集上的实验证明，我们的集成方法在水印恢复保真度方面表现出轻微但统计显著的提升。这些结果突显了EDICT在增强基于扩散的水印技术中的潜力，提供了一个更准确和稳健的逆变换机制。据我们所知，这是首次探索EDICT与高斯阴影在数字水印中的协同作用，为实现高鲁棒性和高保真度的水印嵌入和提取开辟了新的研究途径。
2501.08580	 | Densely Connected Parameter-Efficient Tuning for Referring Image Segmentation	 | Jiaqi Huang,Zunnan Xu,Ting Liu,Yong Liu,Haonan Han,Kehong Yuan,Xiu Li	 | 本文提出了一种参数高效的调优框架DETRIS，通过在每一层与所有前层之间建立密集连接来增强低秩视觉特征传播，实现有效的跨模态特征交互，适用于非对齐编码器，并仅需更新少量骨干网络参数便在多项基准测试中超越了最先进的方法。	 | In the domain of computer vision, Parameter-Efficient Tuning (PET) is increasingly replacing the traditional paradigm of pre-training followed by full fine-tuning. PET is particularly favored for its effectiveness in large foundation models, as it streamlines transfer learning costs and optimizes hardware utilization. However, the current PET methods are mainly designed for single-modal optimization. While some pioneering studies have undertaken preliminary explorations, they still remain at the level of aligned encoders (e.g., CLIP) and lack exploration of misaligned encoders. These methods show sub-optimal performance with misaligned encoders, as they fail to effectively align the multimodal features during fine-tuning. In this paper, we introduce DETRIS, a parameter-efficient tuning framework designed to enhance low-rank visual feature propagation by establishing dense interconnections between each layer and all preceding layers, which enables effective cross-modal feature interaction and adaptation to misaligned encoders. We also suggest using text adapters to improve textual features. Our simple yet efficient approach greatly surpasses state-of-the-art methods with 0.9% to 1.8% backbone parameter updates, evaluated on challenging benchmarks. Our project is available at \url{https://github.com/jiaqihuang01/DETRIS}.	 | 在计算机视觉领域，参数高效微调（Parameter-Efficient Tuning, PET）正逐渐替代传统的预训练后完全微调的模式。PET 特别适用于大型基础模型，因为它简化了迁移学习的成本，并优化了硬件利用。然而，当前的 PET 方法主要针对单模优化设计。尽管一些开创性研究已经进行了初步探索，但这些方法仍停留在对齐编码器（例如 CLIP）的水平，缺乏对非对齐编码器的探索。这些方法在使用非对齐编码器时表现出次优性能，因为它们无法有效地在微调过程中对齐多模态特征。在本文中，我们介绍了一种参数高效的调优框架 DETRIS，该框架通过在每一层与所有前层之间建立密集的连接来增强低秩视觉特征传播，从而实现有效的跨模态特征交互并适应非对齐编码器。我们还建议使用文本适配器来改善文本特征。我们的简单而有效的方法在挑战性基准测试中超越了最先进的方法，仅需更新 0.9% 至 1.8% 的骨干网络参数。我们的项目可以在 \url{https://github.com/jiaqihuang01/DETRIS} 获取。
2501.08577	 | Scalable and High-Quality Neural Implicit Representation for 3D Reconstruction	 | Leyuan Yang,Bailin Deng,Juyong Zhang	 | 本文提出了一种基于SDF的神经隐式表示方法，通过将物体或场景表示为多个具有重叠区域的独立局部SDF的融合来提高重建精度和规模，从而解决现有方法的不足。该方法包括构建局部辐射场分布、邻近局部SDF的相对姿态注册以及SDF融合三个关键步骤，展现了高保真度和可扩展性的优点。	 | Various SDF-based neural implicit surface reconstruction methods have been proposed recently, and have demonstrated remarkable modeling capabilities. However, due to the global nature and limited representation ability of a single network, existing methods still suffer from many drawbacks, such as limited accuracy and scale of the reconstruction. In this paper, we propose a versatile, scalable and high-quality neural implicit representation to address these issues. We integrate a divide-and-conquer approach into the neural SDF-based reconstruction. Specifically, we model the object or scene as a fusion of multiple independent local neural SDFs with overlapping regions. The construction of our representation involves three key steps: (1) constructing the distribution and overlap relationship of the local radiance fields based on object structure or data distribution, (2) relative pose registration for adjacent local SDFs, and (3) SDF blending. Thanks to the independent representation of each local region, our approach can not only achieve high-fidelity surface reconstruction, but also enable scalable scene reconstruction. Extensive experimental results demonstrate the effectiveness and practicality of our proposed method.	 | 近年来，基于SDF的神经隐式表面重建方法得到了广泛研究，并展现了出色的建模能力。然而，由于单个网络的全局特性和有限的表示能力，现有方法仍存在许多不足之处，如重建精度和规模的限制。本文提出了一种通用、可扩展且高质量的神经隐式表示方法，以解决这些问题。我们将分而治之的方法集成到基于SDF的神经网络重建中。具体来说，我们将物体或场景表示为多个具有重叠区域的独立局部SDF的融合。我们的表示方法包含三个关键步骤：（1）基于物体结构或数据分布构建局部辐射场的分布及其重叠关系；（2）邻近局部SDF的相对姿态注册；（3）SDF融合。由于每个局部区域的独立表示，我们的方法不仅可以实现高保真度的表面重建，还可以实现可扩展的场景重建。大量的实验结果证明了我们所提出方法的有效性和实用性。
2501.08553	 | DynamicFace: High-Quality and Consistent Video Face Swapping using Composable 3D Facial Priors	 | Runqi Wang,Sijie Xu,Tianyao He,Yang Chen,Wei Zhu,Dejia Song,Nemo Chen,Xu Tang,Yao Hu	 | DynamicFace是一种利用扩散模型和可插拔时间层实现视频面部交换的新方法，通过引入四种细粒度面部条件并结合Face Former和ReferenceNet，实现了高质量、高准确性的面部交换，同时保留了身份和表情细节。	 | Face swapping transfers the identity of a source face to a target face while retaining the attributes like expression, pose, hair, and background of the target face. Advanced face swapping methods have achieved attractive results. However, these methods often inadvertently transfer identity information from the target face, compromising expression-related details and accurate identity. We propose a novel method DynamicFace that leverages the power of diffusion model and plug-and-play temporal layers for video face swapping. First, we introduce four fine-grained face conditions using 3D facial priors. All conditions are designed to be disentangled from each other for precise and unique control. Then, we adopt Face Former and ReferenceNet for high-level and detailed identity injection. Through experiments on the FF++ dataset, we demonstrate that our method achieves state-of-the-art results in face swapping, showcasing superior image quality, identity preservation, and expression accuracy. Besides, our method could be easily transferred to video domain with temporal attention layer. Our code and results will be available on the project page: https://dynamic-face.github.io/	 | 面部交换将源人脸的身份转移到目标人脸上，同时保留目标人脸的表情、姿态、发型和背景等属性。高级面部交换方法已经取得了令人瞩目的成果。然而，这些方法往往无意中将目标人脸的身份信息转移到了源人脸上，影响了表情相关细节和准确的身份识别。我们提出了一种名为DynamicFace的新方法，该方法利用扩散模型和可插拔的时间层来实现视频面部交换。首先，我们引入了四种细粒度的面部条件，使用3D面部先验知识构建。所有条件都被设计为彼此分离，以便实现精确且独特的控制。然后，我们采用Face Former和ReferenceNet进行高层次和详细的身份注入。通过在FF++数据集上的实验，我们证明了我们的方法在面部交换方面达到了最先进的水平，展示了更好的图像质量、身份保留和表情准确性。此外，我们的方法可以很容易地扩展到视频领域，使用时间注意力层。我们的代码和结果将在项目页面上公开：https://dynamic-face.github.io/
2501.08549	 | The Devil is in Temporal Token: High Quality Video Reasoning Segmentation	 | Sitong Gong,Yunzhi Zhuge,Lu Zhang,Zongxin Yang,Pingping Zhang,Huchuan Lu	 | VRS-HQ是一种端到端的视频推理分割方法，利用多模态大型语言模型将时空特征注入到层次化标记中，通过时空动态聚合和标记驱动的关键帧选择策略实现高效的局部和全局信息捕捉与关键帧分割，显著提高了视频分割的性能。	 | Existing methods for Video Reasoning Segmentation rely heavily on a single special token to represent the object in the keyframe or the entire video, inadequately capturing spatial complexity and inter-frame motion. To overcome these challenges, we propose VRS-HQ, an end-to-end video reasoning segmentation approach that leverages Multimodal Large Language Models (MLLMs) to inject rich spatiotemporal features into hierarchical tokens.Our key innovations include a Temporal Dynamic Aggregation (TDA) and a Token-driven Keyframe Selection (TKS). Specifically, we design frame-level <SEG> and temporal-level <TAK> tokens that utilize MLLM's autoregressive learning to effectively capture both local and global information. Subsequently, we apply a similarity-based weighted fusion and frame selection strategy, then utilize SAM2 to perform keyframe segmentation and propagation. To enhance keyframe localization accuracy, the TKS filters keyframes based on SAM2's occlusion scores during inference. VRS-HQ achieves state-of-the-art performance on ReVOS, surpassing VISA by 5.9%/12.5%/9.1% in J&F scores across the three subsets. These results highlight the strong temporal reasoning and segmentation capabilities of our method. Code and model weights will be released at VRS-HQ.	 | 现有用于视频推理分割的方法高度依赖于单一特殊标记来表示关键帧中的对象或整个视频中的对象，这不足以捕捉空间复杂性和帧间运动。为了解决这些挑战，我们提出了VRS-HQ，这是一种端到端的视频推理分割方法，利用多模态大型语言模型（MLLM）将丰富的时空特征注入到层次化标记中。我们的关键技术创新包括时空动态聚合（TDA）和标记驱动的关键帧选择（TKS）。具体来说，我们设计了帧级的<SEG>和时间级的<TAK>标记，利用MLLM的自回归学习有效捕捉局部和全局信息。随后，我们应用基于相似性的加权融合和帧选择策略，然后使用SAM2执行关键帧分割和传播。为了提高关键帧定位的准确性，TKS在推理过程中根据SAM2的遮挡分数筛选关键帧。VRS-HQ在ReVOS上实现了最先进的性能，J&F分数在三个子集中分别超越VISA 5.9%/12.5%/9.1%。这些结果突显了我们方法的强大时空推理和分割能力。代码和模型权重将在VRS-HQ发布。
2501.08545	 | Comprehensive Subjective and Objective Evaluation Method for Text-generated Video	 | Zelu Qi,Ping Shi,Shuqi Wang,Zhaoyang Zhang,Zefeng Ying,Da Pan	 | 本文构建了一个大规模基准数据集T2VEval-Bench，包含来自12个模型的1,783个生成视频，用于评估文本生成视频的质量，并开发了T2VEval模型，该模型通过注意力融合模块和渐进式训练策略在多个指标上达到了最先进的性能。	 | Recent text-to-video (T2V) technology advancements, as demonstrated by models such as Gen3, Pika, and Sora, have significantly broadened its applicability and popularity. This progress has created a growing demand for accurate quality assessment metrics to evaluate the perceptual quality of text-generated videos and optimize video generation models. However, assessing the quality of text-generated videos remains challenging due to the presence of highly complex distortions, such as unnatural actions and phenomena that defy human cognition. To address these challenges, we constructed a large-scale benchmark dataset for \textbf{T}ext-generated \textbf{V}ideo \textbf{eval}uation, \textbf{T2VEval-Bench}, comprising 148 textual words and 1,783 videos generated by 12 models. During the subjective evaluation, we collected five key scores: overall impression, video quality, aesthetic quality, realness, and text-video consistency. For objective evaluation, we developed the \textbf{T2VEval} model, which assesses videos across three branches: quality, authenticity, and consistency. Using an attention-based fusion module, T2VEval effectively integrates features from each branch and predicts scores with the aid of a large oracle model. Additionally, we implemented a progressive training strategy, enabling each branch to learn targeted knowledge while maintaining synergy with the others. Experimental results demonstrate that T2VEval achieves state-of-the-art performance across multiple metrics. The dataset and code will be open-sourced upon completion of the follow-up work.	 | 近年来，如Gen3、Pika和Sora等模型所展示的文本转视频（Text-to-Video, T2V）技术的进步，极大地拓宽了其适用范围和受欢迎程度。这些进步催生了对准确的质量评估指标的需求，以评估由文本生成的视频的感知质量并优化视频生成模型。然而，由于存在复杂的扭曲现象，如不自然的动作和违背人类认知的现象，评价文本生成视频的质量仍然是一个挑战。为解决这些挑战，我们构建了一个大规模的基准数据集，命名为T2VEval-Bench，其中包括来自12个模型的148个文本描述和1,783个生成的视频。在主观评估过程中，我们收集了五个关键评分：总体印象、视频质量、美学质量、真实感以及文本与视频的一致性。在客观评估方面，我们开发了T2VEval模型，它在质量、真实性和一致性三个方面评估视频。利用基于注意力的融合模块，T2VEval有效地整合了每个分支的特征，并借助大型先验模型预测评分。此外，我们实施了一种渐进式训练策略，使每个分支能够学习有针对性的知识，同时保持与其他分支的协同作用。实验结果表明，T2VEval在多个指标上达到了最先进的性能。数据集和代码将在后续工作完成后开源。
2501.08514	 | Multimodal Fake News Video Explanation Generation	 | Lizhi Chen,Zhong Qian,Peifeng Li,Qiaoming Zhu	 | 本文提出了虚假新闻视频解释（FNVE）问题，旨在生成自然语言解释以揭示多模态新闻的真实情况，并通过新的多模态数据集FakeNVE和基于BART的自回归解码器进行了实验验证，结果显示该方法在多个评估指标上表现良好。	 | Multi-modal explanation involves the assessment of the veracity of a variety of different content, and relies on multiple information modalities to comprehensively consider the relevance and consistency between modalities. Most existing fake news video detection methods focus on improving accuracy while ignoring the importance of providing explanations. In this paper, we propose a novel problem - Fake News Video Explanation (FNVE) - Given a multimodal news containing both video and caption text, we aim to generate natural language explanations to reveal the truth of predictions. To this end, we develop FakeNVE, a new dataset of explanations for truthfully multimodal posts, where each explanation is a natural language (English) sentence describing the attribution of a news thread. We benchmark FakeNVE by using a multimodal transformer-based architecture. Subsequently, a BART-based autoregressive decoder is used as the generator. Empirical results show compelling results for various baselines (applicable to FNVE) across multiple evaluation metrics. We also perform human evaluation on explanation generation, achieving high scores for both adequacy and fluency.	 | 多模态解释涉及对多种不同类型内容的真伪评估，并依赖多种信息模态综合考虑不同模态之间的相关性和一致性。目前大多数虚假新闻视频检测方法主要关注提高准确率，而忽视了解释的重要性。本文提出了一个新颖的问题——虚假新闻视频解释（FNVE）：给定包含视频和标题文本的多模态新闻，我们的目标是生成自然语言解释以揭示预测的真实情况。为了解决这个问题，我们开发了 FakeNVE，这是一个新的多模态帖子解释数据集，其中每个解释是一个描述新闻线程归属的自然语言（英语）句子。我们通过使用多模态转换器架构对 FakeNVE 进行基准测试。随后，我们使用基于 BART 的自回归解码器作为生成器。实验结果表明，在多个评估指标上，FNVE 的各种基线方法都取得了令人信服的结果。此外，我们还进行了人工评估以测试解释生成的效果，结果显示解释的充分性和流畅性都得到了高度评价。
2501.08505	 | Yuan: Yielding Unblemished Aesthetics Through A Unified Network for Visual Imperfections Removal in Generated Images	 | Zhenyu Yu,Chee Seng Chan	 | Yuan是一种新型框架，能够独立自主地修正文本到图像合成中的视觉缺陷，通过生成精确的掩膜并使用先进的修补模块，提高了AI生成图像的质量和应用范围。	 | Generative AI presents transformative potential across various domains, from creative arts to scientific visualization. However, the utility of AI-generated imagery is often compromised by visual flaws, including anatomical inaccuracies, improper object placements, and misplaced textual elements. These imperfections pose significant challenges for practical applications. To overcome these limitations, we introduce \textit{Yuan}, a novel framework that autonomously corrects visual imperfections in text-to-image synthesis. \textit{Yuan} uniquely conditions on both the textual prompt and the segmented image, generating precise masks that identify areas in need of refinement without requiring manual intervention -- a common constraint in previous methodologies. Following the automated masking process, an advanced inpainting module seamlessly integrates contextually coherent content into the identified regions, preserving the integrity and fidelity of the original image and associated text prompts. Through extensive experimentation on publicly available datasets such as ImageNet100 and Stanford Dogs, along with a custom-generated dataset, \textit{Yuan} demonstrated superior performance in eliminating visual imperfections. Our approach consistently achieved higher scores in quantitative metrics, including NIQE, BRISQUE, and PI, alongside favorable qualitative evaluations. These results underscore \textit{Yuan}'s potential to significantly enhance the quality and applicability of AI-generated images across diverse fields.	 | 生成式AI在各个领域都展现出变革性的潜力，从创意艺术到科学可视化。然而，AI生成的图像常常因为视觉缺陷而失去实用性，包括解剖不准确、物体放置不当以及文本元素错位等问题。这些缺陷为实际应用带来了巨大挑战。为克服这些局限，我们提出了一种名为“Yuan”的新型框架，能够独立自主地修正文本到图像合成中的视觉缺陷。Yuan的独特之处在于它既依赖文本提示，又依赖分割图像，生成精确的掩膜来识别需要改进的区域，而无需人工干预——这是过去方法中常见的限制。在自动掩膜处理之后，一个先进的修补模块无缝地将上下文一致的内容融入到已识别的区域中，从而保持原始图像和相关文本提示的完整性和真实性。通过在ImageNet100和斯坦福小狗等公开数据集以及自动生成的数据集上进行广泛实验，Yuan在消除视觉缺陷方面表现出了优越的性能。我们的方法在定量指标（如NIQE、BRISQUE和PI）和定性评估中均取得了更高分数。这些结果表明，Yuan有潜力显著提高AI生成图像的质量和应用范围，使其在各个领域得到更广泛的应用。
2501.08504	 | SuperSAM: Crafting a SAM Supernetwork via Structured Pruning and Unstructured Parameter Prioritization	 | Waqwoya Abebe,Sadegh Jafari,Sixing Yu,Akash Dutta,Jan Strube,Nathan R. Tallent,Luanzheng Guo,Pablo Munoz,Ali Jannesari	 | 本文提出了一种针对视觉变换器架构的单次搜索（one-shot）神经架构搜索方法，通过将通用图像掩模分割模型SAM转换为共享权重的SuperSAM超级网络，并结合逐层结构化剪枝和参数优先级化来自动化搜索空间设计，生成的子网络在大小上比原始模型小30-70%，但性能更优。	 | Neural Architecture Search (NAS) is a powerful approach of automating the design of efficient neural architectures. In contrast to traditional NAS methods, recently proposed one-shot NAS methods prove to be more efficient in performing NAS. One-shot NAS works by generating a singular weight-sharing supernetwork that acts as a search space (container) of subnetworks. Despite its achievements, designing the one-shot search space remains a major challenge. In this work we propose a search space design strategy for Vision Transformer (ViT)-based architectures. In particular, we convert the Segment Anything Model (SAM) into a weight-sharing supernetwork called SuperSAM. Our approach involves automating the search space design via layer-wise structured pruning and parameter prioritization. While the structured pruning applies probabilistic removal of certain transformer layers, parameter prioritization performs weight reordering and slicing of MLP-blocks in the remaining layers. We train supernetworks on several datasets using the sandwich rule. For deployment, we enhance subnetwork discovery by utilizing a program autotuner to identify efficient subnetworks within the search space. The resulting subnetworks are 30-70% smaller in size compared to the original pre-trained SAM ViT-B, yet outperform the pretrained model. Our work introduces a new and effective method for ViT NAS search-space design.	 | 神经架构搜索（NAS）是一种自动化设计高效神经架构的强大方法。与传统的NAS方法相比，最近提出的单次搜索（one-shot）NAS方法在执行NAS方面更为高效。单次搜索通过生成一个共享权重的超级网络，该超级网络作为子网络的搜索空间（容器）来工作。尽管取得了这些成就，设计单次搜索的空间仍然是一个主要挑战。在本文中，我们提出了一种针对基于视觉变换器（ViT）架构的搜索空间设计策略。具体而言，我们将通用图像掩模分割模型（SAM）转换为一个共享权重的超级网络，称为SuperSAM。我们的方法通过逐层结构化剪枝和参数优先级化来自动化搜索空间设计。结构化剪枝通过概率性地移除某些变换器层来应用，而参数优先级化则对剩余层中的MLP块进行权重重排序和切片操作。我们使用三明治规则在多个数据集上训练超级网络。部署时，我们利用程序自动调优器来识别搜索空间中的高效子网络，从而增强子网络的发现能力。生成的子网络在大小上比原始预训练的SAM ViT-B小30-70%，但其性能优于预训练模型。我们的工作引入了一种新的、有效的ViT NAS搜索空间设计方法。
2501.08490	 | FLAVARS: A Multimodal Foundational Language and Vision Alignment Model for Remote Sensing	 | Isaac Corley,Simone Fobi Nsutezo,Anthony Ortiz,Caleb Robinson,Rahul Dodhia,Juan M. Lavista Ferres,Peyman Najafirad	 | FLAVARS是一种结合对比学习和掩蔽建模的预训练方法，通过对比位置编码实现地理空间对齐，在仅基于视觉的任务如KNN分类和语义分割上显著优于基线方法，尤其在SpaceNet1数据集上mIOU提高了6%，同时保留了零样本分类能力。	 | Remote sensing imagery is dense with objects and contextual visual information. There is a recent trend to combine paired satellite images and text captions for pretraining performant encoders for downstream tasks. However, while contrastive image-text methods like CLIP enable vision-language alignment and zero-shot classification ability, vision-only downstream performance tends to degrade compared to image-only pretraining, such as MAE. In this paper, we propose FLAVARS, a pretraining method that combines the best of both contrastive learning and masked modeling, along with geospatial alignment via contrastive location encoding. We find that FLAVARS significantly outperforms a baseline of SkyCLIP for vision-only tasks such as KNN classification and semantic segmentation, +6\% mIOU on SpaceNet1, while retaining the ability to perform zero-shot classification, unlike MAE pretrained methods.	 | 遥感影像富含目标和上下文视觉信息。最近的趋势是结合配对的卫星图像和文本描述来进行预训练，以提高下游任务的高性能编码器。然而，尽管像CLIP这样的对比图像-文本方法能够实现视觉-语言对齐和零样本分类能力，但仅基于视觉的下游性能往往会低于仅基于图像的预训练方法，例如MAE。在本文中，我们提出了一种名为FLAVARS的预训练方法，该方法结合了对比学习和掩蔽建模的最佳之处，并通过对比位置编码实现了地理空间对齐。我们发现，FLAVARS在仅基于视觉的任务，如KNN分类和语义分割上显著优于基线SkyCLIP，SpaceNet1数据集上的mIOU提高了6%。此外，FLAVARS保留了零样本分类的能力，而MAE预训练方法则不具备这一能力。
2501.08471	 | Benchmarking Classical, Deep, and Generative Models for Human Activity Recognition	 | Md Meem Hossain, TheAnh Han,Safina Showkat Ara,Zia Ush Shamszaman	 | 本文评估了经典机器学习模型、深度学习架构和受限玻尔兹曼机在五种关键基准数据集上的性能，结果发现卷积神经网络在所有数据集上表现最佳，尤其是伯克利MHAD数据集；同时，经典模型如随机森林在小数据集上表现良好，而基于RBM的模型在特征学习方面显示出了显著的潜力。	 | Human Activity Recognition (HAR) has gained significant importance with the growing use of sensor-equipped devices and large datasets. This paper evaluates the performance of three categories of models : classical machine learning, deep learning architectures, and Restricted Boltzmann Machines (RBMs) using five key benchmark datasets of HAR (UCI-HAR, OPPORTUNITY, PAMAP2, WISDM, and Berkeley MHAD). We assess various models, including Decision Trees, Random Forests, Convolutional Neural Networks (CNN), and Deep Belief Networks (DBNs), using metrics such as accuracy, precision, recall, and F1-score for a comprehensive comparison. The results show that CNN models offer superior performance across all datasets, especially on the Berkeley MHAD. Classical models like Random Forest do well on smaller datasets but face challenges with larger, more complex data. RBM-based models also show notable potential, particularly for feature learning. This paper offers a detailed comparison to help researchers choose the most suitable model for HAR tasks.	 | 人类活动识别（HAR）随着传感器装备设备的增多和大数据集的出现而变得越来越重要。本文评估了三类模型在五种关键基准数据集上的性能：经典机器学习模型、深度学习架构以及受限玻尔兹曼机（RBM）。我们使用准确率、精确率、召回率和F1分数等指标评估了包括决策树、随机森林、卷积神经网络（CNN）和深度信念网络（DBN）在内的各种模型。结果表明，CNN模型在所有数据集上表现最佳，尤其是在伯克利MHAD数据集上表现尤为突出。经典模型如随机森林在较小的数据集上表现良好，但在更大、更复杂的数据集上面临挑战。基于RBM的模型也显示出显著的潜力，特别是在特征学习方面。本文通过详细的比较帮助研究人员选择最适合HAR任务的模型。
2501.08470	 | Detecting Contextual Anomalies by Discovering Consistent Spatial Regions	 | Zhengye Yang,Richard J. Radke	 | 该研究提出了一种使用高斯混合模型聚类联合对象属性来发现具有相似对象级活动区域的方法，以实现视频异常检测，在街道场景数据集中达到了最先进的性能，且参数量比竞争模型少得多。此外，该模型还能生成高分辨率的正常性图，帮助人类操作员理解异常检测结果。	 | We describe a method for modeling spatial context to enable video anomaly detection. The main idea is to discover regions that share similar object-level activities by clustering joint object attributes using Gaussian mixture models. We demonstrate that this straightforward approach, using orders of magnitude fewer parameters than competing models, achieves state-of-the-art performance in the challenging spatial-context-dependent Street Scene dataset. As a side benefit, the high-resolution discovered regions learned by the model also provide explainable normalcy maps for human operators without the need for any pre-trained segmentation model.	 | 我们描述了一种用于建模空间上下文以实现视频异常检测的方法。主要思想是通过使用高斯混合模型聚类联合对象属性来发现具有相似对象级活动的区域。我们证明了这种方法仅使用比竞争模型少几个数量级的参数，在具有挑战性的空间上下文依赖的街道场景数据集中实现了最先进的性能。作为额外的好处，模型学习到的高分辨率发现区域还可以为人类操作员提供可解释的正常性图，而无需任何预先训练的分割模型。
2501.08465	 | Predicting Performance of Object Detection Models in Electron Microscopy Using Random Forests	 | Ni Li,Ryan Jacobs,Matthew Lynch,Vidit Agrawal,Kevin Field,Dane Morgan	 | 该研究提出了一种基于随机森林回归模型的方法，用于评估深度学习对象检测模型在透射电子显微镜图像中检测缺陷的性能，并能够快速预测新未标记图像上的F1分数，显示出良好的稳定性和相关性，为模型在不同应用中的适用性和可靠性评估提供了重要信息。	 | Quantifying prediction uncertainty when applying object detection models to new, unlabeled datasets is critical in applied machine learning. This study introduces an approach to estimate the performance of deep learning-based object detection models for quantifying defects in transmission electron microscopy (TEM) images, focusing on detecting irradiation-induced cavities in TEM images of metal alloys. We developed a random forest regression model that predicts the object detection F1 score, a statistical metric used to evaluate the ability to accurately locate and classify objects of interest. The random forest model uses features extracted from the predictions of the object detection model whose uncertainty is being quantified, enabling fast prediction on new, unlabeled images. The mean absolute error (MAE) for predicting F1 of the trained model on test data is 0.09, and the $R^2$ score is 0.77, indicating there is a significant correlation between the random forest regression model predicted and true defect detection F1 scores. The approach is shown to be robust across three distinct TEM image datasets with varying imaging and material domains. Our approach enables users to estimate the reliability of a defect detection and segmentation model predictions and assess the applicability of the model to their specific datasets, providing valuable information about possible domain shifts and whether the model needs to be fine-tuned or trained on additional data to be maximally effective for the desired use case.	 | 将对象检测模型应用于新未标记数据集时，量化预测不确定性在应用机器学习中至关重要。本研究提出了一种方法，用于估算基于深度学习的对象检测模型在评估透射电子显微镜（TEM）图像中缺陷时的性能，重点关注检测金属合金TEM图像中的辐照诱导空洞。我们开发了一个随机森林回归模型，用于预测对象检测的F1分数，这是一个用于评估准确定位和分类感兴趣对象能力的统计指标。随机森林模型使用了要量化的对象检测模型预测结果的特征，从而能够在新未标记图像上进行快速预测。训练模型在测试数据上预测F1的平均绝对误差（MAE）为0.09，R²得分为0.77，表明随机森林回归模型预测的F1分数与真实缺陷检测F1分数之间存在显著的相关性。该方法在三个不同TEM图像数据集上显示出了稳健性，这些数据集在成像和材料领域有所不同。我们的方法使用户能够估计缺陷检测和分割模型预测的可靠性，并评估模型对特定数据集的适用性，提供有关可能领域转移以及是否需要对模型进行微调或训练更多数据以最大程度地提高其在特定应用场景中的效果的有价值信息。
2501.08460	 | Towards Zero-Shot & Explainable Video Description by Reasoning over Graphs of Events in Space and Time	 | Mihai Masala,Marius Leordeanu	 | 本文提出了一种基于时空事件的新方法，旨在通过可解释和程序化的方式连接最先进的基于学习的视觉和语言模型，以生成视频的连贯描述，并通过标准指标和LLM-as-a-Jury方法验证了其有效性。	 | In the current era of Machine Learning, Transformers have become the de facto approach across a variety of domains, such as computer vision and natural language processing. Transformer-based solutions are the backbone of current state-of-the-art methods for language generation, image and video classification, segmentation, action and object recognition, among many others. Interestingly enough, while these state-of-the-art methods produce impressive results in their respective domains, the problem of understanding the relationship between vision and language is still beyond our reach. In this work, we propose a common ground between vision and language based on events in space and time in an explainable and programmatic way, to connect learning-based vision and language state of the art models and provide a solution to the long standing problem of describing videos in natural language. We validate that our algorithmic approach is able to generate coherent, rich and relevant textual descriptions on videos collected from a variety of datasets, using both standard metrics (e.g. Bleu, ROUGE) and the modern LLM-as-a-Jury approach.	 | 在当前的机器学习时代，变换器已成为各种领域的事实上的方法，包括计算机视觉和自然语言处理。基于变换器的解决方案是当前语言生成、图像和视频分类、分割、动作和对象识别等许多先进方法的核心。有趣的是，尽管这些最先进的方法在其各自领域中产生了令人印象深刻的结果，但视觉与语言之间的关系理解问题仍然超出了我们的能力范围。在本文中，我们提出了一种基于时空事件的视觉与语言之间的共同点，以一种可解释和程序化的方式连接基于学习的视觉和语言最先进的模型，并提供一种自然语言描述视频的长期解决方案。我们验证了我们的算法方法能够通过使用标准指标（例如BLEU、ROUGE）和现代的LLM-as-a-Jury方法，在来自多种数据集的视频中生成连贯、丰富且相关性的文本描述。
2501.08453	 | Vchitect-2.0: Parallel Transformer for Scaling Up Video Diffusion Models	 | Weichen Fan,Chenyang Si,Junhao Song,Zhenyu Yang,Yinan He,Long Zhuo,Ziqi Huang,Ziyue Dong,Jingwen He,Dongwei Pan,Yi Wang,Yuming Jiang,Yaohui Wang,Peng Gao,Xinyuan Chen,Hengjie Li,Dahua Lin,Yu Qiao,Ziwei Liu	 | Vchitect-2.0 是一种并行变压器架构，通过引入多模态扩散块和高效的训练框架，实现大规模文本到视频的生成，同时保证视频质量和时间连贯性，并在分布式系统上高效训练长视频序列。此外，该系统还采用了改进的数据处理管道，创建了高质量的大型训练数据集，广泛测试表明其在视频质量和训练效率上优于现有方法。	 | We present Vchitect-2.0, a parallel transformer architecture designed to scale up video diffusion models for large-scale text-to-video generation. The overall Vchitect-2.0 system has several key designs. (1) By introducing a novel Multimodal Diffusion Block, our approach achieves consistent alignment between text descriptions and generated video frames, while maintaining temporal coherence across sequences. (2) To overcome memory and computational bottlenecks, we propose a Memory-efficient Training framework that incorporates hybrid parallelism and other memory reduction techniques, enabling efficient training of long video sequences on distributed systems. (3) Additionally, our enhanced data processing pipeline ensures the creation of Vchitect T2V DataVerse, a high-quality million-scale training dataset through rigorous annotation and aesthetic evaluation. Extensive benchmarking demonstrates that Vchitect-2.0 outperforms existing methods in video quality, training efficiency, and scalability, serving as a suitable base for high-fidelity video generation.	 | 我们提出了 Vchitect-2.0，这是一种并行变压器架构，旨在扩展视频扩散模型，用于大规模文本到视频生成。Vchitect-2.0 系统的整体设计具有几个关键特征。   (1) 通过引入一种新颖的多模态扩散块，我们的方法实现了文本描述与生成的视频帧之间的一致对齐，同时保持序列内的时间连贯性。   (2) 为了解决内存和计算瓶颈，我们提出了一种高效的训练框架，该框架结合了混合并行和其他内存减少技术，使其能够在分布式系统上高效地训练长视频序列。   (3) 此外，我们改进的数据处理管道确保通过严格的注释和审美评估创建了 Vchitect T2V DataVerse，这是一个高质量的大规模训练数据集。   广泛的基准测试表明，Vchitect-2.0 在视频质量、训练效率和可扩展性方面优于现有方法，适合作为高保真视频生成的基础。
2501.08446	 | Poseidon: A ViT-based Architecture for Multi-Frame Pose Estimation with Adaptive Frame Weighting and Multi-Scale Feature Fusion	 | Cesare Davide Pace,Alessandro Marco De Nunzio,Claudio De Stefano,Francesco Fontanella,Mario Molinara	 | Poseidon是一种新颖的多帧姿态估计架构，通过整合自适应帧加权、多尺度特征融合和交叉注意力模块，提高了复杂连续动作的时空动态估计准确性。该方法在PoseTrack21和PoseTrack18数据集上分别实现了88.3和87.8的mAP得分，超越了现有方法。	 | Human pose estimation, a vital task in computer vision, involves detecting and localising human joints in images and videos. While single-frame pose estimation has seen significant progress, it often fails to capture the temporal dynamics for understanding complex, continuous movements. We propose Poseidon, a novel multi-frame pose estimation architecture that extends the ViTPose model by integrating temporal information for enhanced accuracy and robustness to address these limitations. Poseidon introduces key innovations: (1) an Adaptive Frame Weighting (AFW) mechanism that dynamically prioritises frames based on their relevance, ensuring that the model focuses on the most informative data; (2) a Multi-Scale Feature Fusion (MSFF) module that aggregates features from different backbone layers to capture both fine-grained details and high-level semantics; and (3) a Cross-Attention module for effective information exchange between central and contextual frames, enhancing the model's temporal coherence. The proposed architecture improves performance in complex video scenarios and offers scalability and computational efficiency suitable for real-world applications. Our approach achieves state-of-the-art performance on the PoseTrack21 and PoseTrack18 datasets, achieving mAP scores of 88.3 and 87.8, respectively, outperforming existing methods.	 | 人体姿态估计是计算机视觉中一个重要的任务，涉及在图像和视频中检测和定位人体关节。虽然单帧姿态估计取得了显著进展，但往往无法捕捉到复杂连续动作的时空动态。我们提出了一种名为Poseidon的新颖多帧姿态估计架构，该架构通过整合时空信息来增强准确性并提高对这些限制的鲁棒性。Poseidon引入了关键创新：(1) 一种自适应帧加权（AFW）机制，该机制根据帧的相关性动态优先级排序，确保模型集中于最相关信息；(2) 一种多尺度特征融合（MSFF）模块，该模块从不同骨干层聚合特征，以捕捉细粒度细节和高层次语义；以及(3) 一种交叉注意力模块，用于有效交换中心帧和上下文帧之间的信息，增强模型的时空一致性。提出的架构在复杂视频场景中提高了性能，并提供了适用于现实世界应用的可扩展性和计算效率。我们的方法在PoseTrack21和PoseTrack18数据集上达到了最先进的性能，分别实现了88.3和87.8的mAP得分，优于现有方法。
2501.08443	 | Instruction-Guided Fusion of Multi-Layer Visual Features in Large Vision-Language Models	 | Xu Li,Yi Zheng,Haotian Chen,Xiaolei Chen,Yuxuan Liang,Chenghang Lai	 | 该研究探讨了多模态视觉语言模型中不同编码器层次的视觉特征在各种任务中的表现，发现统一融合效果不佳，提出了一种基于指令的动态视觉聚合器，能够根据文本指令灵活整合多层特征，显著提高了模型性能。	 | Large Vision-Language Models (LVLMs) have achieved significant success in multimodal tasks by combining pre-trained vision encoders and large language models. However, current LVLMs mainly rely on features from the final layers of the vision encoder, neglecting complementary information in shallower layers. While recent methods have explored multi-layer features, they are often task-agnostic. We investigate the contributions of visual features from different encoder layers across 18 benchmarks and 6 task categories. Our results show that multi-layer features provide complementary strengths with varying task dependencies, and uniform fusion performs suboptimally. Based on these findings, we propose an instruction-guided vision aggregator that dynamically integrates multi-layer features based on textual instructions, without increasing the number of visual tokens. Extensive evaluations show superior performance, and analysis reveals the dominance of mid-to-high-level features in semantic tasks and the critical role of low-level features in fine-grained perception. This work provides valuable insights into the adaptive use of hierarchical visual features in LVLMs, advancing more flexible multimodal systems.	 | 大型多模态视觉语言模型（Large Vision-Language Models, LVLMs）通过结合预训练的视觉编码器和大规模语言模型，在多模态任务中取得了显著的成功。然而，当前的LVLMs主要依赖视觉编码器最终层的特征，忽略了较浅层提供的互补信息。虽然近期的方法已经探索了多层特征，但这些方法往往是任务无关的。我们对18个基准和6个任务类别中的不同编码器层次的视觉特征做出了贡献进行了研究。实验结果表明，多层特征提供了不同的任务依赖性，统一融合效果不佳。基于这些发现，我们提出了一种基于指令的视觉聚合器，该聚合器可以根据文本指令动态地整合多层特征，而不增加视觉令牌的数量。广泛的评估结果显示了优越的性能，分析揭示了中高层特征在语义任务中的主导作用和低层特征在细粒度感知中的关键作用。这项工作为LVLMs中层次视觉特征的适应性使用提供了宝贵见解，推动了更加灵活的多模态系统的发展。
2501.08440	 | FARE: A Deep Learning-Based Framework for Radar-based Face Recognition and Out-of-distribution Detection	 | Sabri Mustafa Kahya,Boran Hamdi Sivrikaya,Muhammet Sami Yavuz,Eckehard Steinbach	 | 本文提出了一种结合短距离FMCW雷达生成的Range-Doppler和微Range-Doppler图像的面部识别和离群值检测流水线方法，并通过两阶段训练实现了高精度，ID分类准确率为99.30%，OOD检测AUROC为96.91%。	 | In this work, we propose a novel pipeline for face recognition and out-of-distribution (OOD) detection using short-range FMCW radar. The proposed system utilizes Range-Doppler and micro Range-Doppler Images. The architecture features a primary path (PP) responsible for the classification of in-distribution (ID) faces, complemented by intermediate paths (IPs) dedicated to OOD detection. The network is trained in two stages: first, the PP is trained using triplet loss to optimize ID face classification. In the second stage, the PP is frozen, and the IPs-comprising simple linear autoencoder networks-are trained specifically for OOD detection. Using our dataset generated with a 60 GHz FMCW radar, our method achieves an ID classification accuracy of 99.30% and an OOD detection AUROC of 96.91%.	 | 在本文中，我们提出了一种新的基于短距离FMCW雷达的面部识别和离群值检测（OOD检测）的流水线方法。所提出的系统利用了Range-Doppler和微Range-Doppler图像。该架构包括一个主路径（PP），负责辨别分布内（ID）面部的分类，以及专用于OOD检测的中间路径（IPs）。该网络分为两个阶段进行训练：首先，使用三元组损失（triplet loss）训练PP以优化ID面部分类。在第二阶段，PP被冻结，而IPs（包含简单的线性自动编码网络）专门用于OOD检测的训练。使用60 GHz FMCW雷达生成的数据集，我们的方法实现了99.30%的ID分类准确率和96.91%的OOD检测AUROC。
2501.08415	 | Cross-Modal Transferable Image-to-Video Attack on Video Quality Metrics	 | Georgii Gotin,Ekaterina Shumitskaya,Anastasia Antsiferova,Dmitriy Vatolin	 | 本文提出了一种跨模态攻击方法IC2VQA，该方法利用图像和视频的底层特征相似性，通过在具有CLIP模块的白盒IQA模型上生成对抗扰动来攻击黑盒VQA模型，从而揭示了现代VQA模型的脆弱性。实验表明，IC2VQA在黑盒VQA模型攻击中具有较高的成功率，且在相同条件下比现有策略更有效。	 | Recent studies have revealed that modern image and video quality assessment (IQA/VQA) metrics are vulnerable to adversarial attacks. An attacker can manipulate a video through preprocessing to artificially increase its quality score according to a certain metric, despite no actual improvement in visual quality. Most of the attacks studied in the literature are white-box attacks, while black-box attacks in the context of VQA have received less attention. Moreover, some research indicates a lack of transferability of adversarial examples generated for one model to another when applied to VQA. In this paper, we propose a cross-modal attack method, IC2VQA, aimed at exploring the vulnerabilities of modern VQA models. This approach is motivated by the observation that the low-level feature spaces of images and videos are similar. We investigate the transferability of adversarial perturbations across different modalities; specifically, we analyze how adversarial perturbations generated on a white-box IQA model with an additional CLIP module can effectively target a VQA model. The addition of the CLIP module serves as a valuable aid in increasing transferability, as the CLIP model is known for its effective capture of low-level semantics. Extensive experiments demonstrate that IC2VQA achieves a high success rate in attacking three black-box VQA models. We compare our method with existing black-box attack strategies, highlighting its superiority in terms of attack success within the same number of iterations and levels of attack strength. We believe that the proposed method will contribute to the deeper analysis of robust VQA metrics.	 | 近年来的研究揭示，现代图像和视频质量评估（IQA/VQA）指标容易受到对抗攻击的影响。攻击者可以通过预处理技术操纵视频，使其在某个指标上的质量得分虚假提高，尽管其视觉质量实际上并没有提升。文献中大多数研究的攻击都是白盒攻击，而关于视频质量评估（VQA）的黑盒攻击则较少受到关注。此外，一些研究表明，针对一个模型生成的对抗样本并不容易在另一个模型上转移应用于VQA。在本文中，我们提出了一种跨模态攻击方法IC2VQA，旨在探索现代VQA模型的脆弱性。这一方法的动机是基于观察到图像和视频的底层特征空间相似。我们探讨了不同模态之间的对抗扰动的转移性；具体来说，我们分析了在具有额外CLIP模块的白盒IQA模型上生成的对抗扰动如何有效地针对VQA模型。CLIP模块的添加提高了转移性，因为CLIP模型以其有效捕捉低级语义而闻名。大量的实验证明，IC2VQA在攻击三个黑盒VQA模型时具有较高的成功率。我们将我们的方法与现有的黑盒攻击策略进行比较，突显了其在在相同迭代次数和攻击强度下具有更高的攻击成功率。我们认为，提出的方法将有助于对鲁棒VQA指标进行更深入的分析。
2501.08408	 | Leveraging 2D Masked Reconstruction for Domain Adaptation of 3D Pose Estimation	 | Hansoo Park,Chanwoo Kim,Jihyeon Kim,Hoseong Cho,Nhat Nguyen Bao Truong,Taehwan Kim,Seungryul Baek	 | 本文提出了一种无监督领域适应框架，利用掩码图像建模（MIM）来实现3D姿态估计，并通过前景为中心的重建和注意力正则化提高未标记数据的使用效果，从而在多种数据集上取得了最先进的准确率。	 | RGB-based 3D pose estimation methods have been successful with the development of deep learning and the emergence of high-quality 3D pose datasets. However, most existing methods do not operate well for testing images whose distribution is far from that of training data. However, most existing methods do not operate well for testing images whose distribution is far from that of training data. This problem might be alleviated by involving diverse data during training, however it is non-trivial to collect such diverse data with corresponding labels (i.e. 3D pose). In this paper, we introduced an unsupervised domain adaptation framework for 3D pose estimation that utilizes the unlabeled data in addition to labeled data via masked image modeling (MIM) framework. Foreground-centric reconstruction and attention regularization are further proposed to increase the effectiveness of unlabeled data usage. Experiments are conducted on the various datasets in human and hand pose estimation tasks, especially using the cross-domain scenario. We demonstrated the effectiveness of ours by achieving the state-of-the-art accuracy on all datasets.	 | 基于RGB的3D姿态估计方法随着深度学习的发展和高质量3D姿态数据集的出现而取得了成功。然而，大多数现有的方法在测试图像的分布与训练数据分布差异较大的情况下表现不佳。这个问题可能通过在训练过程中引入多样化的数据来缓解，然而收集这样的多样化数据及其对应的标签（即3D姿态）是相当具有挑战性的。在本文中，我们提出了一种无监督领域适应框架，该框架利用未标记的数据以及标记的数据通过掩码图像建模（MIM）框架来实现3D姿态估计。我们进一步提出前景为中心的重建和注意力正则化，以提高未标记数据的使用效果。我们在人体和手部姿态估计任务的各种数据集上进行了实验，特别是使用跨域场景。通过在所有数据集上达到最先进的准确率，我们证明了该方法的有效性。
2501.08361	 | Weight Averaging for Out-of-Distribution Generalization and Few-Shot Domain Adaptation	 | Shijian Xu	 | 该研究通过引入梯度相似性作为损失正则化项来增强权重平均（WA）技术的模型多样性，从而提高离分布泛化的性能，并提出将WA与敏锐性意识最小化（SAM）结合以解决少样本域适应问题。实验结果表明，这种结合方法在多个数据集上显著提高了离分布泛化和少样本域适应的准确率。	 | Empirical risk minimization (ERM) is not robust to changes in the distribution of data. When the distribution of test data is different from that of training data, the problem is known as out-of-distribution generalization. Recently, two techniques have been developed for addressing out-of-distribution generalization in computer vision: weight averaging (WA) and sharpness-aware minimization (SAM). WA involves training multiple models with different hyperparameters and then averaging the weights of these models, which can significantly improve out-of-distribution generalization performance. SAM optimizes a neural network to find minima in flat regions, which have been proven to perform well under distribution shifts. While these techniques have made great progress, there is still room for improvement and further exploration. In this thesis, we propose increasing the model diversity in WA explicitly by introducing gradient similarity as a loss regularizer to further improve out-of-distribution generalization performance. We also propose combining WA and SAM to solve the problem of few-shot domain adaptation. Our extensive experiments on digits datasets (MNIST, SVHN, USPS, MNIST-M) and other domain adaptation datasets (VLCS, PACS) show that combining WA and SAM leads to improved out-of-distribution generalization performance and significantly increases few-shot domain adaptation accuracy.	 | 经验风险最小化（ERM）对数据分布的变化不够稳健。当测试数据的分布与训练数据的分布不同，这个问题称为离分布泛化。最近，在计算机视觉领域开发了两种技术来解决离分布泛化问题：权重平均（WA）和敏锐性意识最小化（SAM）。WA涉及训练多个具有不同超参数的模型，然后平均这些模型的权重，这可以显著提高离分布泛化的性能。SAM通过优化神经网络以找到平坦区域的极小值来进行优化，这些平坦区域在分布变化下表现良好。尽管这些技术已经取得了很大的进步，但仍有改进和进一步探索的空间。在本论文中，我们提出通过引入梯度相似性作为损失正则化项来显式增加WA中的模型多样性，进一步提高离分布泛化性能。我们还提出结合WA和SAM来解决少样本域适应问题。我们在数字数据集（MNIST、SVHN、USPS、MNIST-M）和其他域适应数据集（VLCS、PACS）上的大量实验表明，结合WA和SAM能够提高离分布泛化性能，并显著提高少样本域适应的准确率。
2501.08347	 | SCOT: Self-Supervised Contrastive Pretraining For Zero-Shot Compositional Retrieval	 | Bhavin Jawade,Joao V. B. Soares,Kapil Thadani,Deen Dayal Mohan,Amir Erfan Eshratifar,Benjamin Culpepper,Paloma de Juan,Srirangaraj Setlur,Venu Govindaraju	 | 这项研究提出了SCOT（自我监督组成训练）策略，通过结合大规模图像-文本配对数据集和大型语言模型的生成能力，以对比方式预训练嵌入组合网络，从而提高组成图像检索的性能，特别是在零样本设置下超越了现有方法。	 | Compositional image retrieval (CIR) is a multimodal learning task where a model combines a query image with a user-provided text modification to retrieve a target image. CIR finds applications in a variety of domains including product retrieval (e-commerce) and web search. Existing methods primarily focus on fully-supervised learning, wherein models are trained on datasets of labeled triplets such as FashionIQ and CIRR. This poses two significant challenges: (i) curating such triplet datasets is labor intensive; and (ii) models lack generalization to unseen objects and domains. In this work, we propose SCOT (Self-supervised COmpositional Training), a novel zero-shot compositional pretraining strategy that combines existing large image-text pair datasets with the generative capabilities of large language models to contrastively train an embedding composition network. Specifically, we show that the text embedding from a large-scale contrastively-pretrained vision-language model can be utilized as proxy target supervision during compositional pretraining, replacing the target image embedding. In zero-shot settings, this strategy surpasses SOTA zero-shot compositional retrieval methods as well as many fully-supervised methods on standard benchmarks such as FashionIQ and CIRR.	 | 组成图像检索（CIR）是一种多模态学习任务，其中模型将查询图像与用户提供的文本修改结合起来检索目标图像。CIR 在产品检索（电子商务）和网络搜索等多个领域都有应用。现有的方法主要集中在完全监督学习上，即在标记的三元组数据集（如 FashionIQ 和 CIRR）上训练模型。这种方法存在两个主要挑战：(i) 筛选这样的三元组数据集非常耗时；(ii) 模型缺乏对未见对象和领域的泛化能力。在这项工作中，我们提出了一种新颖的自监督组成预训练策略 SCOT（自我监督组成训练），该策略结合了现有的大规模图像-文本配对数据集和大型语言模型的生成能力，以对比方式训练嵌入组合网络。具体而言，我们展示了一种大规模对比预训练的视觉-语言模型的文本嵌入可以在组成预训练过程中用作代理目标监督，替代目标图像嵌入。在零样本设置中，这种策略在标准基准如 FashionIQ 和 CIRR 上超越了现有的顶级零样本组成检索方法以及许多完全监督方法。
2501.09001	 | Vision Foundation Models for Computed Tomography	 | Suraj Pai(1 and 2 and 3),Ibrahim Hadzic(1 and 2 and 3),Dennis Bontempi(1 and 2 and 3),Keno Bressem(4 and 5),Benjamin H. Kann(1 and 3),Andriy Fedorov(6),Raymond H. Mak(1 and 3),Hugo J. W. L. Aerts(1 and 2 and 3 and 6) ((1) Artificial Intelligence in Medicine (AIM) Program, Mass General Brigham, Harvard Medical School, (2) Radiology and Nuclear Medicine, CARIM &amp; GROW, Maastricht University, (3) Department of Radiation Oncology, Brigham and Women's Hospital, Dana-Farber Cancer Institute, Harvard Medical School, (4) Department of Diagnostic and Interventional Radiology, Technical University of Munich, School of Medicine and Health, Klinikum rechts der Isar, TUM University Hospital, (5) Department of Cardiovascular Radiology and Nuclear Medicine, Technical University of Munich, School of Medicine and Health, German Heart Center, TUM University Hospital, (6) Department of Radiology, Brigham and Women's Hospital, Dana-Farber Cancer Institute, Harvard Medical School)	 | CT-FM 是一种大规模预训练的 3D 图像基础模型，专为放射学任务设计，在多种任务中表现出色，包括全身和肿瘤分割、头部 CT 优先处理、医学图像检索和语义理解，优于现有最佳模型。此外，CT-FM 还具备良好的鲁棒性，并能够识别和聚类解剖区域，通过开源模型权重、代码和数据，促进放射学中更加适应、可靠和可解释的 AI 解决方案。	 | Foundation models (FMs) have shown transformative potential in radiology by performing diverse, complex tasks across imaging modalities. Here, we developed CT-FM, a large-scale 3D image-based pre-trained model designed explicitly for various radiological tasks. CT-FM was pre-trained using 148,000 computed tomography (CT) scans from the Imaging Data Commons through label-agnostic contrastive learning. We evaluated CT-FM across four categories of tasks, namely, whole-body and tumor segmentation, head CT triage, medical image retrieval, and semantic understanding, showing superior performance against state-of-the-art models. Beyond quantitative success, CT-FM demonstrated the ability to cluster regions anatomically and identify similar anatomical and structural concepts across scans. Furthermore, it remained robust across test-retest settings and indicated reasonable salient regions attached to its embeddings. This study demonstrates the value of large-scale medical imaging foundation models and by open-sourcing the model weights, code, and data, aims to support more adaptable, reliable, and interpretable AI solutions in radiology.	 | 基础模型（FMs）在放射学领域展现了 transformative 的潜力，能够跨多种成像方式执行多种复杂任务。在这里，我们开发了 CT-FM，这是一种大规模基于3D图像的预训练模型，专为各种放射学任务设计。CT-FM 通过无标签对比学习，在来自 Imaging Data Commons 的 148,000 张计算机断层扫描（CT）切片上进行了预训练。我们对 CT-FM 进行了四类任务的评估，包括全身和肿瘤分割、头部 CT 优先处理、医学图像检索以及语义理解，显示出了优于现有最佳模型的性能。除了定量上的成功，CT-FM 还展示了其能够根据解剖区域进行聚类，并在切片之间识别相似的解剖和结构概念的能力。此外，它在多次测试中保持了鲁棒性，并且其嵌入中附带了显著的解剖区域。这项研究展示了大规模医学影像基础模型的价值，并通过开源模型权重、代码和数据，旨在支持在放射学中更加适应、可靠和可解释的 AI 解决方案。
2501.08902	 | Multi-View Transformers for Airway-To-Lung Ratio Inference on Cardiac CT Scans: The C4R Study	 | Sneha N. Naik,Elsa D. Angelini,Eric A. Hoffman,Elizabeth C. Oelsner,R. Graham Barr,Benjamin M. Smith,Andrew F. Laine	 | 该研究提出了一种基于注意力机制的多视图 Swin 变former 网络，从心脏CT图像中推断全肺气道树腔隙与肺体积的比率（ALR），并证实其在准确性与重现性上优于现有方法，为研究ALR与严重COVID-19及长期后遗症的关系提供了新途径。	 | The ratio of airway tree lumen to lung size (ALR), assessed at full inspiration on high resolution full-lung computed tomography (CT), is a major risk factor for chronic obstructive pulmonary disease (COPD). There is growing interest to infer ALR from cardiac CT images, which are widely available in epidemiological cohorts, to investigate the relationship of ALR to severe COVID-19 and post-acute sequelae of SARS-CoV-2 infection (PASC). Previously, cardiac scans included approximately 2/3 of the total lung volume with 5-6x greater slice thickness than high-resolution (HR) full-lung (FL) CT. In this study, we present a novel attention-based Multi-view Swin Transformer to infer FL ALR values from segmented cardiac CT scans. For the supervised training we exploit paired full-lung and cardiac CTs acquired in the Multi-Ethnic Study of Atherosclerosis (MESA). Our network significantly outperforms a proxy direct ALR inference on segmented cardiac CT scans and achieves accuracy and reproducibility comparable with a scan-rescan reproducibility of the FL ALR ground-truth.	 | 在全肺高分辨率计算机断层扫描（HR-FL CT）上，于全 Inspiratory 时评估气道树腔隙与肺体积的比率（ALR）是慢性阻塞性肺疾病（COPD）的重要风险因素。随着越来越多的兴趣集中在从心脏CT图像中推断ALR，而心脏CT图像在流行病学队列中广泛可用，以研究ALR与严重 COVID-19 及 SARS-CoV-2 感染的长期后遗症（PASC）之间的关系。此前，心脏扫描包括大约2/3的总肺容积，并且其切片厚度比高分辨率（HR）全肺（FL）CT 大5-6倍。在本研究中，我们提出了一种基于注意力机制的多视图 Swin 变former 网络，从分割的心脏CT扫描中推断出全肺ALR值。在监督训练中，我们利用 Multi-Ethnic 研究动脉硬化（MESA）研究中获取的配对的全肺和心脏CT扫描。我们网络在分割心脏CT扫描中的推断ALR上显著优于现有的替代直接推断方法，并达到了与全肺ALR真实值的扫描-再扫描再现性相当的准确性和重现性。
2501.08841	 | Exploring Task-Level Optimal Prompts for Visual In-Context Learning	 | Yan Zhu,Huan Ma,Changqing Zhang	 | 本文发现大多数测试样本在相同的提示下可以获得最佳性能，因此提出任务级别提示以减少寻找提示的成本，通过两种高效方法搜索任务级别提示，实验表明这种方法能以最小成本达到最佳的视觉在上下文学习（VICL）性能。	 | With the development of Vision Foundation Models (VFMs) in recent years, Visual In-Context Learning (VICL) has become a better choice compared to modifying models in most scenarios. Different from retraining or fine-tuning model, VICL does not require modifications to the model's weights or architecture, and only needs a prompt with demonstrations to teach VFM how to solve tasks. Currently, significant computational cost for finding optimal prompts for every test sample hinders the deployment of VICL, as determining which demonstrations to use for constructing prompts is very costly. In this paper, however, we find a counterintuitive phenomenon that most test samples actually achieve optimal performance under the same prompts, and searching for sample-level prompts only costs more time but results in completely identical prompts. Therefore, we propose task-level prompting to reduce the cost of searching for prompts during the inference stage and introduce two time-saving yet effective task-level prompt search strategies. Extensive experimental results show that our proposed method can identify near-optimal prompts and reach the best VICL performance with a minimal cost that prior work has never achieved.	 | 随着近年来视觉基础模型（VFMs）的发展，视觉在上下文学习（VICL）在大多数场景中已成为比修改模型更好的选择。与重新训练或微调模型不同，VICL 不需要对模型的权重或架构进行修改，只需提供一个带有示范的提示，即可教会 VFMs 如何解决任务。目前，为每个测试样本找到最优提示所需的大量计算成本阻碍了 VICL 的部署，因为确定用于构建提示的示范是极其昂贵的。然而，在本文中，我们发现了一个反直觉的现象，即大多数测试样本实际上在相同的提示下可以实现最佳性能，寻找样本级别的提示只会花费更多时间，但结果是完全相同的提示。因此，我们提出任务级别提示以减少推理阶段寻找提示的成本，并引入了两种节省时间且有效的方法来搜索任务级别提示。广泛实验结果表明，我们提出的方法可以识别接近最优的提示，并以先前工作从未实现的最小成本达到最佳的 VICL 性能。
2501.08828	 | MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents	 | Kuicai Dong,Yujing Chang,Xin Deik Goh,Dexun Li,Ruiming Tang,Yong Liu	 | 该研究提出了一个多模态文档检索基准MMDocIR，包含页面级和布局级检索任务，并通过实验展示了视觉信息在多模态文档检索中的关键作用及其对系统性能的提升。	 | Multi-modal document retrieval is designed to identify and retrieve various forms of multi-modal content, such as figures, tables, charts, and layout information from extensive documents. Despite its significance, there is a notable lack of a robust benchmark to effectively evaluate the performance of systems in multi-modal document retrieval. To address this gap, this work introduces a new benchmark, named as MMDocIR, encompassing two distinct tasks: page-level and layout-level retrieval. The former focuses on localizing the most relevant pages within a long document, while the latter targets the detection of specific layouts, offering a more fine-grained granularity than whole-page analysis. A layout can refer to a variety of elements such as textual paragraphs, equations, figures, tables, or charts. The MMDocIR benchmark comprises a rich dataset featuring expertly annotated labels for 1,685 questions and bootstrapped labels for 173,843 questions, making it a pivotal resource for advancing multi-modal document retrieval for both training and evaluation. Through rigorous experiments, we reveal that (i) visual retrievers significantly outperform their text counterparts, (ii) MMDocIR train set can effectively benefit the training process of multi-modal document retrieval and (iii) text retrievers leveraging on VLM-text perform much better than those using OCR-text. These findings underscores the potential advantages of integrating visual elements for multi-modal document retrieval.	 | 多模态文档检索旨在识别和检索各类多模态内容，如图表、表格、图表和布局信息，从广泛的文档中进行检索。尽管其重要性不言而喻，但在多模态文档检索领域却缺乏一个可靠的基准来有效评估系统性能。为填补这一空白，这项工作引入了一个新的基准，命名为MMDocIR，包含了两个不同的任务：页面级检索和布局级检索。前者关注于在长文档中定位最相关的页面，而后者则旨在检测特定布局，提供比整页分析更精细的粒度。一个布局可以包含文本段落、方程式、图表、表格或图表等多种元素。MMDocIR基准数据集包含1,685个问题的专家标注标签和173,843个问题的 bootstrap 标注标签，成为推动多模态文档检索训练和评估的关键资源。通过严格的实验，我们发现：(i) 视觉检索器明显优于其文本对应物；(ii) MMDocIR的训练集可以有效提升多模态文档检索的训练过程；(iii) 利用 VLM-text 的文本检索器表现远优于使用 OCR-text 的检索器。这些发现强调了在多模态文档检索中整合视觉元素的潜在优势。
2501.08819	 | Boosting Diffusion Guidance via Learning Degradation-Aware Models for Blind Super Resolution	 | Shao-Hao Lu,Ren Wang,Ching-Chun Huang,Wei-Chen Chiu	 | 本文提出了一种退化感知模型，该模型可以在不依赖已知退化内核的情况下实现盲超分辨率，同时通过输入扰动和引导标量技术进一步提升性能，实验结果表明该方法在盲超分辨率基准测试中优于现有最佳方法。	 | Recently, diffusion-based blind super-resolution (SR) methods have shown great ability to generate high-resolution images with abundant high-frequency detail, but the detail is often achieved at the expense of fidelity. Meanwhile, another line of research focusing on rectifying the reverse process of diffusion models (i.e., diffusion guidance), has demonstrated the power to generate high-fidelity results for non-blind SR. However, these methods rely on known degradation kernels, making them difficult to apply to blind SR. To address these issues, we introduce degradation-aware models that can be integrated into the diffusion guidance framework, eliminating the need to know degradation kernels. Additionally, we propose two novel techniques input perturbation and guidance scalar to further improve our performance. Extensive experimental results show that our proposed method has superior performance over state-of-the-art methods on blind SR benchmarks	 | 最近，基于扩散的盲超分辨率（SR）方法展示了生成具有丰富高频细节的高分辨率图像的强大能力，但这些细节往往是以牺牲保真度为代价的。与此同时，另一条研究路线专注于纠正扩散模型的反过程（即扩散引导），展示了生成非盲超分辨率高保真结果的能力。然而，这些方法依赖于已知的退化内核，这使得它们难以应用于盲超分辨率。为了解决这些问题，我们引入了退化感知模型，这些模型可以集成到扩散引导框架中，从而消除对退化内核的了解需求。此外，我们提出了两种新颖的技术——输入扰动和引导标量，以进一步提高我们的性能。大量实验结果表明，我们提出的方法在盲超分辨率基准测试中优于现有最先进的方法。
2501.08717	 | $\texttt{InfoHier}$: Hierarchical Information Extraction via Encoding and Embedding	 | Tianru Zhang,Li Ju,Prashant Singh,Salman Toor	 | InfoHier是一个结合自监督学习（SSL）和层次聚类（HC）的框架，旨在学习既能捕捉复杂模式又能反映多层次关系的稳健表示，从而提高聚类和表示学习的性能。	 | Analyzing large-scale datasets, especially involving complex and high-dimensional data like images, is particularly challenging. While self-supervised learning (SSL) has proven effective for learning representations from unlabelled data, it typically focuses on flat, non-hierarchical structures, missing the multi-level relationships present in many real-world datasets. Hierarchical clustering (HC) can uncover these relationships by organizing data into a tree-like structure, but it often relies on rigid similarity metrics that struggle to capture the complexity of diverse data types. To address these we envision $\texttt{InfoHier}$, a framework that combines SSL with HC to jointly learn robust latent representations and hierarchical structures. This approach leverages SSL to provide adaptive representations, enhancing HC's ability to capture complex patterns. Simultaneously, it integrates HC loss to refine SSL training, resulting in representations that are more attuned to the underlying information hierarchy. $\texttt{InfoHier}$ has the potential to improve the expressiveness and performance of both clustering and representation learning, offering significant benefits for data analysis, management, and information retrieval.	 | 分析大规模数据集，尤其是在涉及复杂和高维数据，如图像的情况下，特别具有挑战性。虽然自监督学习（SSL）在从未标记数据中学习表示方面非常有效，但它通常侧重于平坦、非层次结构的结构，而忽略了许多实际数据集中存在的多层次关系。层次聚类（HC）可以通过将数据组织成树状结构来发现这些关系，但它往往依赖于刚性相似性度量，这些度量难以捕捉多种数据类型的复杂性。为了解决这些问题，我们设想了一个名为$\texttt{InfoHier}$的框架，它可以将SSL与HC结合起来，以联合学习稳健的潜在表示和层次结构。这种方法利用SSL提供自适应表示，增强HC捕捉复杂模式的能力。同时，它还结合HC损失来细化SSL训练，从而产生更加符合潜在信息层次的表示。$\texttt{InfoHier}$有可能提高聚类和表示学习的表达能力和性能，为数据分析、管理和信息检索提供显著优势。
2501.08672	 | GS-LIVO: Real-Time LiDAR, Inertial, and Visual Multi-sensor Fused Odometry with Gaussian Mapping	 | Sheng Hong,Chunran Zheng,Yishu Shen,Changze Li,Fu Zhang,Tong Qin,Shaojie Shen	 | 本文提出了一种基于激光雷达-惯性-视觉传感器配置的实时高斯基同时定位与地图构建（SLAM）系统，通过使用递归八叉树组织的哈希索引体素构建全局高斯地图，并利用迭代错误状态卡尔曼滤波器进行多传感器融合，显著减少了GPU计算和内存消耗，实现了在英伟达Jetson Orin NX平台上的实时性能。	 | In recent years, 3D Gaussian splatting (3D-GS) has emerged as a novel scene representation approach. However, existing vision-only 3D-GS methods often rely on hand-crafted heuristics for point-cloud densification and face challenges in handling occlusions and high GPU memory and computation consumption. LiDAR-Inertial-Visual (LIV) sensor configuration has demonstrated superior performance in localization and dense mapping by leveraging complementary sensing characteristics: rich texture information from cameras, precise geometric measurements from LiDAR, and high-frequency motion data from IMU. Inspired by this, we propose a novel real-time Gaussian-based simultaneous localization and mapping (SLAM) system. Our map system comprises a global Gaussian map and a sliding window of Gaussians, along with an IESKF-based odometry. The global Gaussian map consists of hash-indexed voxels organized in a recursive octree, effectively covering sparse spatial volumes while adapting to different levels of detail and scales. The Gaussian map is initialized through multi-sensor fusion and optimized with photometric gradients. Our system incrementally maintains a sliding window of Gaussians, significantly reducing GPU computation and memory consumption by only optimizing the map within the sliding window. Moreover, we implement a tightly coupled multi-sensor fusion odometry with an iterative error state Kalman filter (IESKF), leveraging real-time updating and rendering of the Gaussian map. Our system represents the first real-time Gaussian-based SLAM framework deployable on resource-constrained embedded systems, demonstrated on the NVIDIA Jetson Orin NX platform. The framework achieves real-time performance while maintaining robust multi-sensor fusion capabilities. All implementation algorithms, hardware designs, and CAD models will be publicly available.	 | 近年来，三维高斯散射（3D-GS）方法已经发展成为一种新的场景表示方法。然而，现有的仅基于视觉的3D-GS方法往往依赖于手工设计的启发式方法进行点云稀疏化，并且在处理遮挡和高GPU内存及计算消耗方面面临挑战。激光雷达-惯性-视觉（LIV）传感器配置通过利用互补的传感特性显示出卓越的表现：摄像机提供的丰富纹理信息、激光雷达提供的精确几何测量以及惯性测量单元（IMU）提供的高频运动数据。受到这一启发，我们提出了一种新颖的实时高斯基同时定位与地图构建（SLAM）系统。我们的地图系统由全局高斯地图和滑动窗口中的高斯体构成，并配有基于IESKF（迭代错误状态卡尔曼滤波器）的里程计。全局高斯地图由递归八叉树组织的哈希索引体素构成，能够有效覆盖稀疏的空间体积并适应不同的细节和缩放级别。高斯地图通过多传感器融合初始化并使用光度梯度进行优化。我们的系统增量维护一个滑动窗口中的高斯体，通过仅优化滑动窗口内的地图显著减少了GPU计算和内存消耗。此外，我们实现了一种紧密耦合的多传感器融合里程计，使用迭代错误状态卡尔曼滤波器（IESKF），利用高斯地图的实时更新和渲染。我们的系统代表了第一个可在资源受限的嵌入式系统上部署的实时高斯基SLAM框架，已在英伟达Jetson Orin NX平台上进行了演示。该框架实现了实时性能，同时保持了多传感器融合的鲁棒性。所有实施算法、硬件设计和CAD模型都将公开。
2501.08667	 | TimeFlow: Longitudinal Brain Image Registration and Aging Progression Analysis	 | Bailiang Jian,Jiazhen Pan,Yitong Li,Fabian Bongratz,Ruochen Li,Daniel Rueckert,Benedikt Wiestler,Christian Wachinger	 | TimeFlow 是一种创新的框架，利用时空条件 U-Net 架构实现精确的纵向 MRI 对齐，并通过未来图像预测促进前瞻性分析，其性能优于现有方法，并支持有效区分神经退行性疾病和健康老化。	 | Predicting future brain states is crucial for understanding healthy aging and neurodegenerative diseases. Longitudinal brain MRI registration, a cornerstone for such analyses, has long been limited by its inability to forecast future developments, reliance on extensive, dense longitudinal data, and the need to balance registration accuracy with temporal smoothness. In this work, we present \emph{TimeFlow}, a novel framework for longitudinal brain MRI registration that overcomes all these challenges. Leveraging a U-Net architecture with temporal conditioning inspired by diffusion models, TimeFlow enables accurate longitudinal registration and facilitates prospective analyses through future image prediction. Unlike traditional methods that depend on explicit smoothness regularizers and dense sequential data, TimeFlow achieves temporal consistency and continuity without these constraints. Experimental results highlight its superior performance in both future timepoint prediction and registration accuracy compared to state-of-the-art methods. Additionally, TimeFlow supports novel biological brain aging analyses, effectively differentiating neurodegenerative conditions from healthy aging. It eliminates the need for segmentation, thereby avoiding the challenges of non-trivial annotation and inconsistent segmentation errors. TimeFlow paves the way for accurate, data-efficient, and annotation-free prospective analyses of brain aging and chronic diseases.	 | 预测未来的脑部状态对于理解健康老化和神经退行性疾病至关重要。纵向脑部MRI对齐，这是此类分析的基础，长期以来一直受到无法预测未来发展、依赖大量密集的纵向数据以及需要平衡对齐准确性和时间平滑性的限制。在本文中，我们提出了一种名为 \emph{TimeFlow} 的创新框架，以克服这些所有挑战。TimeFlow 利用了受扩散模型启发的时空条件U-Net架构，实现了精确的纵向对齐，并通过未来图像预测促进了前瞻性分析。与传统方法依赖显式的平滑正则化和密集序列数据不同，TimeFlow 在没有这些约束的情况下实现了时间一致性与连续性。实验结果表明，在未来时间点预测和对齐精度方面，TimeFlow 的性能优于当今最先进的方法。此外，TimeFlow 还支持新颖的生物脑老化分析，有效区分神经退行性疾病和健康老化。它消除了对分割的需求，从而避免了非平凡注释和分割误差不一致的问题。TimeFlow 为脑老化和慢性疾病的准确、数据高效且无需标注的前瞻性分析铺平了道路。
2501.08662	 | Product of Gaussian Mixture Diffusion Model for non-linear MRI Inversion	 | Laurenz Nagler,Martin Zach,Thomas Pock	 | 本文提出了一种轻量级的产品高斯混合扩散模型，用于磁共振成像重建，该模型结合了经典平滑性先验以估计线圈敏感度，从而提供快速推理的同时保持对对比度异常数据和采样轨迹的鲁棒性，并且计算后验期望和像素级方差更为方便。	 | Diffusion models have recently shown remarkable results in magnetic resonance imaging reconstruction. However, the employed networks typically are black-box estimators of the (smoothed) prior score with tens of millions of parameters, restricting interpretability and increasing reconstruction time. Furthermore, parallel imaging reconstruction algorithms either rely on off-line coil sensitivity estimation, which is prone to misalignment and restricting sampling trajectories, or perform per-coil reconstruction, making the computational cost proportional to the number of coils. To overcome this, we jointly reconstruct the image and the coil sensitivities using the lightweight, parameter-efficient, and interpretable product of Gaussian mixture diffusion model as an image prior and a classical smoothness priors on the coil sensitivities. The proposed method delivers promising results while allowing for fast inference and demonstrating robustness to contrast out-of-distribution data and sampling trajectories, comparable to classical variational penalties such as total variation. Finally, the probabilistic formulation allows the calculation of the posterior expectation and pixel-wise variance.	 | 扩散模型最近在磁共振成像重建方面展示了惊人的成果。然而，常用的网络通常是具有数百万参数的黑盒前项评分估计器，这限制了其可解释性并增加了重建时间。此外，平行成像重建算法要么依赖于离线线圈敏感度估计，这种估计容易产生对齐错误并限制采样轨迹，要么对每个线圈进行独立重建，使计算成本与线圈数量成正比。为克服这些限制，我们利用轻量级、参数高效的、可解释的产品高斯混合扩散模型作为图像先验，并结合经典平滑性先验作为线圈敏感度的先验，共同重建图像和线圈敏感度。所提出的方法在提供快速推理的同时，展示了对对比度异常数据和采样轨迹的强大鲁棒性，与传统的变分惩罚（如全变差）相当。最后，概率模型形式允许计算后验期望和像素级方差。
2501.08629	 | Self-Organizing Edge Computing Distribution Framework for Visual SLAM	 | Jussi Kalliola,Lauri Suomela,Sergio Moreschini,David Hästbacka	 | 本文提出了一种新的边缘辅助SLAM框架，能够在网络设备之间实现自主组织的完全分布式SLAM执行，或在单个设备上独立执行，该架构设计目标是设备无关、对网络故障具有鲁棒性，并对核心SLAM系统的影响最小。实验结果表明，该框架在准确性和资源利用率方面与整体方法相当，同时能够实现协作执行。	 | Localization within a known environment is a crucial capability for mobile robots. Simultaneous Localization and Mapping (SLAM) is a prominent solution to this problem. SLAM is a framework that consists of a diverse set of computational tasks ranging from real-time tracking to computation-intensive map optimization. This combination can present a challenge for resource-limited mobile robots. Previously, edge-assisted SLAM methods have demonstrated promising real-time execution capabilities by offloading heavy computations while performing real-time tracking onboard. However, the common approach of utilizing a client-server architecture for offloading is sensitive to server and network failures. In this article, we propose a novel edge-assisted SLAM framework capable of self-organizing fully distributed SLAM execution across a network of devices or functioning on a single device without connectivity. The architecture consists of three layers and is designed to be device-agnostic, resilient to network failures, and minimally invasive to the core SLAM system. We have implemented and demonstrated the framework for monocular ORB SLAM3 and evaluated it in both fully distributed and standalone SLAM configurations against the ORB SLAM3. The experiment results demonstrate that the proposed design matches the accuracy and resource utilization of the monolithic approach while enabling collaborative execution.	 | 在已知环境中的定位是移动机器人的一项关键能力。同时定位与建图（Simultaneous Localization and Mapping，简称SLAM）是解决这一问题的主流方法。SLAM是一种包含从实时追踪到密集计算地图优化等多种计算任务的框架。这种组合对资源有限的移动机器人来说可能颇具挑战性。此前，边缘辅助SLAM方法通过卸载密集计算并实现实时追踪，已经展示了出色的实时执行能力。然而，利用客户端-服务器架构进行卸载的常见方法对于服务器和网络故障较为敏感。本文中，我们提出了一种新的边缘辅助SLAM框架，该框架能够在网络设备之间实现自主组织的完全分布式SLAM执行，或者在单个设备上独立执行，无需连接网络。该架构由三层组成，设计目标是设备无关、对网络故障具有鲁棒性，并对核心SLAM系统的影响最小。我们已经为单目ORB SLAM3实现了并验证了该框架，并在完全分布式和独立执行的SLAM配置中与ORB SLAM3进行了对比测试。实验结果表明，所提出的设计在准确性和资源利用率方面与整体方法相当，同时能够实现协作执行。
2501.08593	 | Image-to-Force Estimation for Soft Tissue Interaction in Robotic-Assisted Surgery Using Structured Light	 | Jiayin Wang,Mingfeng Yao,Yanran Wei,Xiaoyu Guo,Ayong Zheng,Weidong Zhao	 | 该研究提出了一种基于视觉的触觉交互力反馈方案，利用结构光投影和图像到力的神经网络，通过内窥镜立体相机捕获的图像来估计软组织交互力，验证了该方法在不同硬度硅材料上的有效性。	 | For Minimally Invasive Surgical (MIS) robots, accurate haptic interaction force feedback is essential for ensuring the safety of interacting with soft tissue. However, most existing MIS robotic systems cannot facilitate direct measurement of the interaction force with hardware sensors due to space limitations. This letter introduces an effective vision-based scheme that utilizes a One-Shot structured light projection with a designed pattern on soft tissue coupled with haptic information processing through a trained image-to-force neural network. The images captured from the endoscopic stereo camera are analyzed to reconstruct high-resolution 3D point clouds for soft tissue deformation. Based on this, a modified PointNet-based force estimation method is proposed, which excels in representing the complex mechanical properties of soft tissue. Numerical force interaction experiments are conducted on three silicon materials with different stiffness. The results validate the effectiveness of the proposed scheme.	 | 对于微创手术（MIS）机器人而言，准确的触觉交互力反馈对于确保与软组织安全交互至关重要。然而，由于空间限制，大多数现有的MIS机器人系统无法通过硬件传感器直接测量交互力。本信介绍了一种有效的基于视觉的方案，该方案利用带有设计图案的一次成像结构光投影，并通过训练的图像到力神经网络处理触觉信息。通过内窥镜立体相机捕获的图像被分析以重建高分辨率的3D点云，用于软组织变形分析。在此基础上，我们提出了一种修改后的基于PointNet的力估计方法，该方法在表示软组织的复杂力学特性方面表现出色。在三种不同刚度的硅材料上进行了数值力交互实验。结果验证了所提出方案的有效性。
2501.08585	 | A Systematic Review of Machine Learning Methods for Multimodal EEG Data in Clinical Application	 | Siqi Zhao(1),Wangyang Li(1),Xiru Wang(1),Stevie Foglia(2),Hongzhao Tan(1),Bohan Zhang(1),Ameer Hamoodi(2),Aimee Nelson(2 and 3),Zhen Gao(1 and 2) ((1) WBooth School of Engineering Practice and Technology, McMaster University, Hamilton, Ontario Canada, (2) School of Biomedical Engineering, McMaster University, Hamilton, Ontario, Canada, (3) Department of Kinesiology, McMaster University, Hamilton, Ontario, Canada)	 | 这篇系统文献综述探讨了多模态脑电图（EEG）数据在临床应用中的机器学习（ML）和深度学习（DL）模型，共纳入16篇研究，展示了多模态EEG在神经精神疾病、癫痫检测、自闭症谱系障碍和睡眠阶段分类等领域的应用，并提高了临床诊断的准确性。	 | Machine learning (ML) and deep learning (DL) techniques have been widely applied to analyze electroencephalography (EEG) signals for disease diagnosis and brain-computer interfaces (BCI). The integration of multimodal data has been shown to enhance the accuracy of ML and DL models. Combining EEG with other modalities can improve clinical decision-making by addressing complex tasks in clinical populations. This systematic literature review explores the use of multimodal EEG data in ML and DL models for clinical applications. A comprehensive search was conducted across PubMed, Web of Science, and Google Scholar, yielding 16 relevant studies after three rounds of filtering. These studies demonstrate the application of multimodal EEG data in addressing clinical challenges, including neuropsychiatric disorders, neurological conditions (e.g., seizure detection), neurodevelopmental disorders (e.g., autism spectrum disorder), and sleep stage classification. Data fusion occurred at three levels: signal, feature, and decision levels. The most commonly used ML models were support vector machines (SVM) and decision trees. Notably, 11 out of the 16 studies reported improvements in model accuracy with multimodal EEG data. This review highlights the potential of multimodal EEG-based ML models in enhancing clinical diagnostics and problem-solving.	 | 机器学习（ML）和深度学习（DL）技术被广泛应用于分析脑电图（EEG）信号以进行疾病诊断和脑-机接口（BCI）。多模态数据的整合已被证明可以提高ML和DL模型的准确性。将EEG与其他模态数据结合可以提高临床决策的准确性，通过解决临床人群中的复杂任务。本系统文献综述探讨了在临床应用中使用多模态EEG数据的ML和DL模型。在PubMed、Web of Science和Google Scholar上进行了全面的文献搜索，经过三轮筛选后，共获得了16篇相关研究。这些研究展示了多模态EEG数据在解决临床挑战中的应用，包括神经精神疾病、神经学状况（如癫痫检测）、神经发育障碍（如自闭症谱系障碍）和睡眠阶段分类。数据融合发生在三个层次上：信号级、特征级和决策级。最常用的ML模型是支持向量机（SVM）和决策树。值得注意的是，16篇研究中有11篇报告了使用多模态EEG数据的模型准确性有所提高。本综述强调了基于多模态EEG的ML模型在提高临床诊断和问题解决方面的潜力。
2501.08575	 | GOTLoc: General Outdoor Text-based Localization Using Scene Graph Retrieval with OpenStreetMap	 | Donghwi Jung,Keonwoo Kim,Seong-Woo Kim	 | GOTLoc是一种新型的鲁棒定位方法，能够在无GPS信号的户外环境中运行，通过利用基于文本描述和地图生成的场景图进行比较来实现定位，解决了现有基于文本定位方法中点云地图扩展性和数据存储的挑战，同时在准确性、存储空间和处理时间方面表现出色。	 | We propose GOTLoc, a robust localization method capable of operating even in outdoor environments where GPS signals are unavailable. The method achieves this robust localization by leveraging comparisons between scene graphs generated from text descriptions and maps. Existing text-based localization studies typically represent maps as point clouds and identify the most similar scenes by comparing embeddings of text and point cloud data. However, point cloud maps have limited scalability as it is impractical to pre-generate maps for all outdoor spaces. Furthermore, their large data size makes it challenging to store and utilize them directly on actual robots. To address these issues, GOTLoc leverages compact data structures, such as scene graphs, to store spatial information, enabling individual robots to carry and utilize large amounts of map data. Additionally, by utilizing publicly available map data, such as OpenStreetMap, which provides global information on outdoor spaces, we eliminate the need for additional effort to create custom map data. For performance evaluation, we utilized the KITTI360Pose dataset in conjunction with corresponding OpenStreetMap data to compare the proposed method with existing approaches. Our results demonstrate that the proposed method achieves accuracy comparable to algorithms relying on point cloud maps. Moreover, in city-scale tests, GOTLoc required significantly less storage compared to point cloud-based methods and completed overall processing within a few seconds, validating its applicability to real-world robotics. Our code is available at https://github.com/donghwijung/GOTLoc.	 | 我们提出了一种名为GOTLoc的鲁棒定位方法，能够在没有GPS信号的户外环境中运行。该方法通过利用基于文本描述和地图生成的场景图之间的比较，实现了鲁棒定位。现有的基于文本的定位研究通常将地图表示为点云，并通过比较文本和点云数据的嵌入来识别最相似的场景。然而，点云地图的扩展性有限，因为无法预先生成所有户外空间的地图。此外，其庞大的数据量使得直接存储和在实际机器人上利用这些数据变得具有挑战性。为了解决这些问题，GOTLoc利用紧凑的数据结构，如场景图，来存储空间信息，使每个机器人能够携带和利用大量地图数据。此外，通过利用如OpenStreetMap等公开可用的地图数据，这些数据提供了户外空间的全球信息，从而避免了需要额外努力创建自定义地图数据。为了性能评估，我们利用了KITTI360Pose数据集以及相应的OpenStreetMap数据，将所提出的方法与现有方法进行了比较。结果表明，所提出的方法在准确性上与依赖点云地图的算法相当。此外，在城市规模的测试中，GOTLoc所需存储空间显著少于基于点云的方法，并且整体处理时间在几秒钟内完成，验证了其在实际机器人领域的适用性。我们的代码可以在https://github.com/donghwijung/GOTLoc获取。
2501.08506	 | Exploring the Efficacy of Meta-Learning: Unveiling Superior Data Diversity Utilization of MAML Over Pre-training	 | Kavita Selva,Satita Vittayaareekul,Brando Miranda	 | 研究发现测试集准确率与数据集多样性之间存在正相关性，表明数据多样性对视觉模型性能有显著影响，并且这种关系在多个流行数据集和不同模型配置中得到了验证。	 | Currently, data and model size dominate the narrative in the training of super-large, powerful models. However, there has been a lack of exploration on the effect of other attributes of the training dataset on model performance. We hypothesize that dataset diversity can impact the performance of vision models. Our study shows positive correlations between test set accuracy and data diversity, providing an argument for furthering the research of dataset attributes beyond size. We analyzed pre-training and model-agnostic meta-learning methods on twelve popular visual datasets (e.g., Omniglot, CIFAR-FS, Aircraft) and five model configurations, including MAML variants with different numbers of inner gradient steps and supervised learning. We show moderate to strong positive correlations (R-squared: 0.15-0.42) between accuracy and data diversity and weaker but significant correlations (R-squared: ~0.2) between loss and diversity. These findings support our hypothesis and demonstrate a promising way for a deeper exploration of how formal data diversity influences model performance. This initial study highlights the potential of (Task2Vec) data diversity as a valuable measure in the rapidly evolving field of large-scale learning and emphasizes that understanding the dataset is key to building more powerful and generalizable models.	 | 当前，数据和模型大小在超大规模、强大模型的训练中占据主导地位。然而，对于训练数据集的其他属性对模型性能的影响还缺乏探索。我们假设数据集多样性可以影响视觉模型的性能。我们的研究显示测试集准确率与数据多样性之间存在正相关性，这为超越数据集大小进一步研究数据集属性提供了论据。我们对十二个流行的视觉数据集（如 Omniglot、CIFAR-FS、Aircraft）和五种模型配置进行了分析，包括带有不同数量内部梯度步骤的不同 MAML 变体以及监督学习。结果显示，准确率与数据多样性的相关性从中等到很强（R²: 0.15-0.42），损失与多样性的相关性较弱但显著（R²: ~0.2）。这些发现支持了我们的假设，并展示了进一步探索正式数据多样性如何影响模型性能的可能性途径。初步研究表明，（Task2Vec）数据多样性作为衡量指标具有潜在价值，在快速发展的大规模学习领域具有重要意义，并强调理解数据集是构建更强大和通用模型的关键。
2501.08495	 | Automotive Elevation Mapping with Interferometric Synthetic Aperture Radar	 | Leyla A. Kabuli,Griffin Foster	 | 该研究展示了如何利用合成孔径雷达（SAR）和干涉测量SAR（InSAR）技术，即使在低分辨率的车载雷达系统上，也能实现三维空间的精确定位和建图，适用于城市和农业环境，并通过专门的信号处理方案生成点云，支持自主驾驶的精细感知决策。	 | Radar is a low-cost and ubiquitous automotive sensor, but is limited by array resolution and sensitivity when performing direction of arrival analysis. Synthetic Aperture Radar (SAR) is a class of techniques to improve azimuth resolution and sensitivity for radar. Interferometric SAR (InSAR) can be used to extract elevation from the variations in phase measurements in SAR images. Utilizing InSAR we show that a typical, low-resolution radar array mounted on a vehicle can be used to accurately localize detections in 3D space for both urban and agricultural environments. We generate point clouds in each environment by combining InSAR with a signal processing scheme tailored to automotive driving. This low-compute approach allows radar to be used as a primary sensor to map fine details in complex driving environments, and be used to make autonomous perception decisions.	 | 雷达是一种低成本且广泛应用的汽车传感器，但在进行到达方向分析时受限于阵列分辨率和灵敏度。合成孔径雷达（SAR）是一类技术，用于提高雷达的方位分辨率和灵敏度。干涉测量SAR（InSAR）可以通过分析SAR图像中相位测量的变化来提取高度。利用InSAR，我们展示了，即使是在车辆上安装的普通低分辨率雷达阵列，也能准确地将检测结果在三维空间中定位，适用于城市和农业环境。通过将InSAR与专门针对汽车驾驶设计的信号处理方案结合，我们生成了每个环境中的点云。这种低计算量的方法允许雷达作为主要传感器，在复杂驾驶环境中进行精细细节的建图，并用于自主感知决策。
2501.08411	 | BiDepth Multimodal Neural Network: Bidirectional Depth Deep Learning Arcitecture for Spatial-Temporal Prediction	 | Sina Ehsani,Fenglian Pan,Qingpei Hu,Jian Liu	 | 本文提出了一种双向深度调制的双深度多模态神经网络（BDMNN），以更好地整合时间深度信息和保持空间上下文，从而提高时空信息的预测准确性。实验结果表明，该方法在城市交通预测和降雨量预报中显著提升了预测准确性，分别降低了12%的均方误差和提高了15%，且未增加额外的计算资源需求。	 | Accurate prediction of spatial-temporal (ST) information in dynamic systems, such as urban mobility and weather patterns, is a crucial yet challenging problem. The complexity stems from the intricate interplay between spatial proximity and temporal relevance, where both long-term trends and short-term fluctuations are present in convoluted patterns. Existing approaches, including traditional statistical methods and conventional neural networks, may provide inaccurate results due to the lack of an effective mechanism that simultaneously incorporates information at variable temporal depths while maintaining spatial context, resulting in a trade-off between comprehensive long-term historical analysis and responsiveness to short-term new information. To bridge this gap, this paper proposes the BiDepth Multimodal Neural Network (BDMNN) with bidirectional depth modulation that enables a comprehensive understanding of both long-term seasonality and short-term fluctuations, adapting to the complex ST context. Case studies with real-world public data demonstrate significant improvements in prediction accuracy, with a 12% reduction in Mean Squared Error for urban traffic prediction and a 15% improvement in rain precipitation forecasting compared to state-of-the-art benchmarks, without demanding extra computational resources.	 | 动态系统（如城市交通和天气模式）中的时空（ST）信息准确预测是一个关键但极具挑战性的问题。这种复杂性源于空间接近性和时间相关性的复杂交互，其中既存在长期趋势，又存在短期波动，它们交织在一起形成了复杂模式。现有的方法，包括传统的统计方法和传统的神经网络，可能会因为缺乏一种同时整合变量时间深度信息和保持空间上下文的有效机制而提供不准确的结果，从而导致对长期历史分析的全面性和对短期新信息的响应之间的权衡。为了解决这一问题，本文提出了一种双向深度调制的双深度多模态神经网络（BDMNN），使其能够全面理解长期季节性和短期波动，适应复杂的时空上下文。通过使用现实世界公开数据的实际案例研究，证明了预测准确性有了显著提升，城市交通预测的均方误差减少了12%，降雨量预报提高了15%，并且没有增加额外的计算资源需求。
2501.08370	 | 3D Gaussian Splatting with Normal Information for Mesh Extraction and Improved Rendering	 | Meenakshi Krishnan,Liam Fowl,Ramani Duraiswami	 | 该研究提出了一种新的正则化方法，利用从高斯估计的符号距离函数梯度来提高可微分3D高斯点渲染的质量，并同时提取精确的表面网格，从而改善了视频生成、动画、AR-VR和游戏等应用的图像真实度。实验结果表明，该方法在多个数据集上的表现优于其他不牺牲网格质量的方法。	 | Differentiable 3D Gaussian splatting has emerged as an efficient and flexible rendering technique for representing complex scenes from a collection of 2D views and enabling high-quality real-time novel-view synthesis. However, its reliance on photometric losses can lead to imprecisely reconstructed geometry and extracted meshes, especially in regions with high curvature or fine detail. We propose a novel regularization method using the gradients of a signed distance function estimated from the Gaussians, to improve the quality of rendering while also extracting a surface mesh. The regularizing normal supervision facilitates better rendering and mesh reconstruction, which is crucial for downstream applications in video generation, animation, AR-VR and gaming. We demonstrate the effectiveness of our approach on datasets such as Mip-NeRF360, Tanks and Temples, and Deep-Blending. Our method scores higher on photorealism metrics compared to other mesh extracting rendering methods without compromising mesh quality.	 | 不同的可微分3D高斯点渲染技术已成为一种高效且灵活的方法，用于从2D视图集合中表示复杂场景，并实现高质量的实时新视角合成。然而，这种方法依赖于光度损失，可能导致在高曲率或细部区域几何重建不精确和提取的网格不准确。为此，我们提出了一种新的正则化方法，使用从高斯估计的符号距离函数梯度，以提高渲染质量并同时提取表面网格。这种正则化的法线监督有助于更好地渲染和网格重建，这对于视频生成、动画、AR-VR和游戏等下游应用至关重要。我们在Mip-NeRF360、Tanks and Temples和Deep-Blending等数据集上展示了我们方法的有效性。与其他不牺牲网格质量的网格提取渲染方法相比，我们的方法在图像真实度指标上得分更高。
2501.08352	 | A Preliminary Survey of Semantic Descriptive Model for Images	 | Chengxi Yan,Jie Jian,Yang Li	 | 本研究利用北京故宫博物院的古代中国画收藏，结合图象学理论，开发了一种新的术语提取和映射工作流程，构建了一个综合的语义模型，以支持艺术知识组织和文化内涵探索。	 | Considering the lack of a unified framework for image description and deep cultural analysis at the subject level in the field of Ancient Chinese Paintings (ACP), this study utilized the Beijing Palace Museum's ACP collections to develop a semantic model integrating the iconological theory with a new workflow for term extraction and mapping. Our findings underscore the model's effectiveness. SDM can be used to support further art-related knowledge organization and cultural exploration of ACPs.	 | 考虑到古代中国画（ACP）领域在图像描述和深层次文化分析方面缺乏统一的框架，特别是在主题层面，本研究利用北京故宫博物院的ACP收藏，结合图象学理论，开发了一种新的术语提取和映射工作流程，构建了一个综合的语义模型。我们的研究结果强调了该模型的有效性。语义模型（SDM）可以用于进一步支持与艺术相关的知识组织，并探索ACP的文化内涵。
2501.08334	 | High-throughput digital twin framework for predicting neurite deterioration using MetaFormer attention	 | Kuanren Qian,Genesis Omana Suarez,Toshihiko Nambara,Takahisa Kanekiyo,Yongjie Jessica Zhang	 | 本文介绍了一种高通量数字孪生框架，结合合成数据生成、实验图像和机器学习模型，用于建模神经发育障碍（NDDs）相关的神经突变，旨在通过预测潜在实验结果来指导实验决策，降低成本并提高研究效率。	 | Neurodevelopmental disorders (NDDs) cover a variety of conditions, including autism spectrum disorder, attention-deficit/hyperactivity disorder, and epilepsy, which impair the central and peripheral nervous systems. Their high comorbidity and complex etiologies present significant challenges for accurate diagnosis and effective treatments. Conventional clinical and experimental studies are time-intensive, burdening research progress considerably. This paper introduces a high-throughput digital twin framework for modeling neurite deteriorations associated with NDDs, integrating synthetic data generation, experimental images, and machine learning (ML) models. The synthetic data generator utilizes an isogeometric analysis (IGA)-based phase field model to capture diverse neurite deterioration patterns such as neurite retraction, atrophy, and fragmentation while mitigating the limitations of scarce experimental data. The ML model utilizes MetaFormer-based gated spatiotemporal attention architecture with deep temporal layers and provides fast predictions. The framework effectively captures long-range temporal dependencies and intricate morphological transformations with average errors of 1.9641% and 6.0339% for synthetic and experimental neurite deterioration, respectively. Seamlessly integrating simulations, experiments, and ML, the digital twin framework can guide researchers to make informed experimental decisions by predicting potential experimental outcomes, significantly reducing costs and saving valuable time. It can also advance our understanding of neurite deterioration and provide a scalable solution for exploring complex neurological mechanisms, contributing to the development of targeted treatments.	 | 神经发育障碍（NDDs）涵盖多种疾病，包括自闭症谱系障碍、注意缺陷多动障碍和癫痫，这些疾病会影响中枢和周围神经系统。它们的高共病率和复杂的病因给准确诊断和有效治疗带来了巨大挑战。传统的临床和实验研究耗时较长，严重影响了研究的进展。本文介绍了一种高通量数字孪生框架，用于建模与NDDs相关的神经突变，该框架整合了合成数据生成、实验图像和机器学习（ML）模型。合成数据生成器利用基于等几何分析（IGA）的相场模型来捕捉多样化的神经突变化学模式，如神经突回缩、萎缩和碎片化，从而缓解稀缺实验数据的局限性。ML模型采用基于MetaFormer的时空门控注意力架构，具有深层时间层，提供快速预测。该框架有效地捕捉了长程时间依赖性和复杂的形态学变化，对于合成和实验神经突变的平均误差分别为1.9641%和6.0339%。无缝整合模拟、实验和ML，数字孪生框架可以指导研究人员通过预测潜在实验结果来做出明智的实验决策，显著降低成本并节省宝贵时间。它还可以帮助我们更好地理解神经突变，提供探索复杂神经机制的可扩展方案，从而促进针对性治疗方法的发展。
