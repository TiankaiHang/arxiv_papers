| ID | Title | Authors | Summary (zh) | Abstract (en) | Abstract (zh) | 
| --- | --- | --- | --- | --- | --- |
2501.03222	 | Characterizing the Accuracy-Communication-Privacy Trade-off in Distributed Stochastic Convex Optimization	 | Sudeep Salgia,Nikola Pavlovic,Yuejie Chi,Qing Zhao	 | 本文研究了在分布式环境中进行不同ially私密的随机凸优化（DP-SCO）的问题，探讨了准确度、通信量和隐私保护之间的权衡，并通过一个新颖的下界和新分布式DP-SCO算法建立了匹配的结果，为分布式DP-SCO的权衡提供了完整的刻画。	 | We consider the problem of differentially private stochastic convex optimization (DP-SCO) in a distributed setting with $M$ clients, where each of them has a local dataset of $N$ i.i.d. data samples from an underlying data distribution. The objective is to design an algorithm to minimize a convex population loss using a collaborative effort across $M$ clients, while ensuring the privacy of the local datasets. In this work, we investigate the accuracy-communication-privacy trade-off for this problem. We establish matching converse and achievability results using a novel lower bound and a new algorithm for distributed DP-SCO based on Vaidya's plane cutting method. Thus, our results provide a complete characterization of the accuracy-communication-privacy trade-off for DP-SCO in the distributed setting.	 | 我们考虑在具有 \(M\) 个客户端的分布式环境中进行不同ially私密的随机凸优化（DP-SCO）的问题，其中每个客户端都拥有一组来自底层数据分布的 \(N\) 个独立同分布的数据样本。目标是在 \(M\) 个客户端的合作努力下，最小化一个凸的总体损失函数，同时确保本地数据集的隐私。在本文中，我们探讨了该问题的准确度-通信-隐私权衡。我们通过一个新颖的下界和基于 Vaidya 平面切割方法的新分布式 DP-SCO 算法，建立了匹配的反向结果和实现结果。因此，我们的结果为分布式环境中 DP-SCO 的准确度-通信-隐私权衡提供了完整的刻画。
2501.03190	 | Multimodal Machine Learning Can Predict Videoconference Fluidity and Enjoyment	 | Andrew Chang,Viswadruth Akkaraju,Ray McFadden Cogliano,David Poeppel,Dustin Freeman	 | 本研究利用多模态机器学习技术从音频、面部表情和身体运动中预测视频会议中的负面体验时刻，并取得了较高准确性，表明多模态信号能有效预测对话流畅度和愉悦度；同时，该工作为改善视频会议用户体验提供了新的研究方向和方法。	 | Videoconferencing is now a frequent mode of communication in both professional and informal settings, yet it often lacks the fluidity and enjoyment of in-person conversation. This study leverages multimodal machine learning to predict moments of negative experience in videoconferencing. We sampled thousands of short clips from the RoomReader corpus, extracting audio embeddings, facial actions, and body motion features to train models for identifying low conversational fluidity, low enjoyment, and classifying conversational events (backchanneling, interruption, or gap). Our best models achieved an ROC-AUC of up to 0.87 on hold-out videoconference sessions, with domain-general audio features proving most critical. This work demonstrates that multimodal audio-video signals can effectively predict high-level subjective conversational outcomes. In addition, this is a contribution to research on videoconferencing user experience by showing that multimodal machine learning can be used to identify rare moments of negative user experience for further study or mitigation.	 | 视频会议现在是专业和非正式场合中常用的交流方式，但却往往缺乏面对面交流的流畅性和愉悦感。本研究利用多模态机器学习来预测视频会议中的负面体验时刻。我们从RoomReader语料库中抽取了数千个短片段，提取了音频嵌入、面部表情动作和身体运动特征，用于训练识别对话流畅度低、愉悦度低以及对话事件分类（应和、打断或停顿）的模型。我们的最佳模型在保留的视频会议会话上实现了高达0.87的ROC-AUC值，通用领域音频特征最为关键。这项工作表明，多模态的音频-视频信号能够有效预测高层次的主观对话结果。此外，这项工作为视频会议用户体验的研究做出了贡献，展示了多模态机器学习可以用于识别罕见的负面用户体验时刻，以供进一步的研究或干预。
2501.03176	 | Scalable Forward-Forward Algorithm	 | Andrii Krutsylo	 | 本文提出了一种无需反向传播的可扩展前向-前向（FF）算法，适用于大规模网络，并将其应用于现代卷积架构如MobileNetV3和ResNet18，实验结果显示该方法在性能上与标准反向传播相当，甚至在某些情况下表现更优。	 | We propose a scalable Forward-Forward (FF) algorithm that eliminates the need for backpropagation by training each layer separately. Unlike backpropagation, FF avoids backward gradients and can be more modular and memory efficient, making it appealing for large networks. We extend FF to modern convolutional architectures, such as MobileNetV3 and ResNet18, by introducing a new way to compute losses for convolutional layers. Experiments show that our method achieves performance comparable to standard backpropagation. Furthermore, when we divide the network into blocks, such as the residual blocks in ResNet, and apply backpropagation only within each block, but not across blocks, our hybrid design tends to outperform backpropagation baselines while maintaining a similar training speed. Finally, we present experiments on small datasets and transfer learning that confirm the adaptability of our method.	 | 我们提出了一种可扩展的前向-前向（FF）算法，通过单独训练每一层来消除对反向传播的需求。与反向传播不同，FF算法避免了反向梯度，因此更加模块化且内存效率更高，使其适用于大规模网络。我们将FF算法扩展到了现代的卷积架构，如MobileNetV3和ResNet18，通过引入一种新的计算卷积层损失的方法。实验表明，我们的方法在性能上与标准的反向传播相当。此外，当我们把网络分为块，例如ResNet中的残差块，并在每个块内仅使用反向传播而不在块之间使用时，我们的混合设计往往在保持类似训练速度的同时优于反向传播的基础模型。最后，我们在小数据集和迁移学习上的实验验证了我们方法的适应性。
2501.03162	 | Deep-Relative-Trust-Based Diffusion for Decentralized Deep Learning	 | Muyun Li,Aaron Fainman,Stefan Vlaski	 | 该研究提出了一种基于深度相对信任（DRT）的新去中心化学习算法，旨在通过促进神经网络输出的一致性来提高泛化能力，并通过收敛性分析和图像分类任务中的实验结果验证了其有效性。	 | Decentralized learning strategies allow a collection of agents to learn efficiently from local data sets without the need for central aggregation or orchestration. Current decentralized learning paradigms typically rely on an averaging mechanism to encourage agreement in the parameter space. We argue that in the context of deep neural networks, which are often over-parameterized, encouraging consensus of the neural network outputs, as opposed to their parameters can be more appropriate. This motivates the development of a new decentralized learning algorithm, termed DRT diffusion, based on deep relative trust (DRT), a recently introduced similarity measure for neural networks. We provide convergence analysis for the proposed strategy, and numerically establish its benefit to generalization, especially with sparse topologies, in an image classification task.	 | 去中心化的学习策略允许多个代理能够从本地数据集高效学习，而无需中央聚合或协调。当前的去中心化学习范式通常依赖于平均机制来促进参数空间中的共识。我们认为，在深度神经网络的背景下，这些网络往往具有过度参数化的特点，鼓励神经网络输出的一致性，而不是参数的一致性可能是更合适的。这促使我们基于深度相对信任（DRT）开发了一种新的去中心化学习算法，DRT 是最近引入的用于神经网络的相似度量。我们为所提出的方法提供了收敛性分析，并通过图像分类任务中的稀疏拓扑结构，数值地证明了其在泛化方面的好处。
2501.03152	 | The Scaling Law for LoRA Base on Mutual Information Upper Bound	 | Jing Zhang,Hui Gao,Peng Zhang,Shuzhen Sun,Chang Yang,Yuexian Hou	 | LoRA微调方法的性能、参数和数据复杂性之间的规律一直是研究焦点，为此研究提出了一种基于互信息上界（MIUB）的内部指标，该指标在Llama3-8B和Phi3-3B模型上验证了其对LoRA微调扩展规律的更准确和稳定对齐，优于传统的外部评估指标如交叉熵和困惑度。	 | LoRA (Low-Rank Adaptation) is a widely used model fine-tuning method. In fine-tuning, the law among model performance, model parameters, and data complexity has been a focal issue in the field. Existing methods often leverage external metrics (such as cross-entropy or perplexity) to evaluate model performance. In the fine-tuning process for large models, two types of knowledge are typically involved: the frozen, general knowledge acquired by the model during pre-training and the new knowledge learned through the LoRA module from the current data. Generally, the less LoRA's learned knowledge relies on the large model, the more it captures the specific knowledge of new data, thereby enhancing its adaptability to new tasks. However, external metrics do not readily capture the dependency relationship between these two types of knowledge. Therefore, we designed an internal metric based on the Mutual Information Upper Bound (MIUB) theory to investigate the scaling law of large-model LoRA fine-tuning. In our experiments, we validated this approach on benchmark datasets, using the Llama3-8B and Phi3-3B models. The results show that the proposed MIUB metric aligns more accurately and stably with the scaling law of LoRA fine-tuning compared to cross-entropy and perplexity.	 | LoRA（低秩适应）是一种广泛使用的模型微调方法。在微调过程中，模型性能、模型参数和数据复杂性之间的规律一直是该领域的焦点问题。现有的方法通常依赖外部指标（例如交叉熵或困惑度）来评估模型性能。在大型模型的微调过程中，通常涉及两种知识：模型在预训练期间获取的冻结的通用知识，以及通过LoRA模块从当前数据中学到的新知识。通常而言，LoRA学到的知识对大型模型的依赖程度越低，它就越能够捕获新数据的具体知识，从而增强其对新任务的适应性。然而，外部指标并不能轻易捕捉这两种知识之间的依赖关系。因此，我们基于互信息上界（MIUB）理论设计了一个内部指标，以研究大型模型LoRA微调的扩展规律。在我们的实验中，我们使用Llama3-8B和Phi3-3B模型在基准数据集上验证了这一方法。结果表明，提出的MIUB指标与LoRA微调的扩展规律更准确且更稳定地对齐，优于交叉熵和困惑度。
2501.03132	 | Communication Bounds for the Distributed Experts Problem	 | Zhihao Jia,Qi Pang,Trung Tran,David Woodruff,Zhihao Zhang,Wenting Zheng	 | 该研究探讨了分布式环境下专家问题的成本聚合，并提出了高效的通信协议，即使面对强适应性对手也能实现接近最优的后悔率；此外，该研究还给出了通信效率的下界，证明了所提协议接近最优，并在实际测试中取得了显著的成本节省。	 | In this work, we study the experts problem in the distributed setting where an expert's cost needs to be aggregated across multiple servers. Our study considers various communication models such as the message-passing model and the broadcast model, along with multiple aggregation functions, such as summing and taking the $\ell_p$ norm of an expert's cost across servers. We propose the first communication-efficient protocols that achieve near-optimal regret in these settings, even against a strong adversary who can choose the inputs adaptively. Additionally, we give a conditional lower bound showing that the communication of our protocols is nearly optimal. Finally, we implement our protocols and demonstrate empirical savings on the HPO-B benchmarks.	 | 在本工作中，我们研究了分布式设置下的专家问题，其中专家的成本需要在多个服务器之间进行聚合。我们的研究考虑了多种通信模型，如消息传递模型和广播模型，并使用了多种聚合函数，例如在服务器之间对专家成本进行求和和计算$\ell_p$范数。我们提出了在这些设置中实现接近最优后悔率的第一批通信高效协议，即使在强对手（可以适应性选择输入）的情况下也是如此。此外，我们给出了一种条件下的下界，表明我们协议的通信量几乎是最优的。最后，我们实现了这些协议，并在HPO-B基准测试中展示了实际成本的节省。
2501.03130	 | Learning DAGs and Root Causes from Time-Series Data	 | Panagiotis Misiakos,Markus Püschel	 | DAG-TFRC是一种用于从少量根原因的时间序列中学习有向无环图的新方法，它基于结构向量自回归模型，并在合成和实际金融数据上优于现有方法，能够扩展到数千个节点并发现主要的股票动作作为根原因。	 | We introduce DAG-TFRC, a novel method for learning directed acyclic graphs (DAGs) from time series with few root causes. By this, we mean that the data are generated by a small number of events at certain, unknown nodes and time points under a structural vector autoregression model. For such data, we (i) learn the DAGs representing both the instantaneous and time-lagged dependencies between nodes, and (ii) discover the location and time of the root causes. For synthetic data with few root causes, DAG-TFRC shows superior performance in accuracy and runtime over prior work, scaling up to thousands of nodes. Experiments on simulated and real-world financial data demonstrate the viability of our sparse root cause assumption. On S&P 500 data, DAG-TFRC successfully clusters stocks by sectors and discovers major stock movements as root causes.	 | 我们引入了DAG-TFRC，这是一种从具有少量根原因的时间序列中学习有向无环图（DAG）的新方法。这意味着数据是由特定但未知节点和时间点上的少量事件生成，并且基于结构向量自回归模型（Structural Vector Autoregression, SVAR）生成。对于这种数据，我们（i）学习表示节点间即时依赖和时间滞后依赖的DAG；（ii）发现根原因的位置和时间。对于具有少量根原因的合成数据，DAG-TFRC在准确性和运行时间方面均优于现有方法，并可扩展到数千个节点。在模拟和实际金融数据上的实验证明了我们稀疏根原因假设的可行性。在S&P 500数据上，DAG-TFRC成功地按行业对股票进行了聚类，并发现了主要的股票动作为根原因。
2501.03119	 | From Models to Network Topologies: A Topology Inference Attack in Decentralized Federated Learning	 | Chao Feng,Yuanzhe Gao,Alberto Huertas Celdran,Gerome Bovet,Burkhard Stiller	 | 该研究探讨了仅通过分析模型行为来推断去中心化联邦学习（DFL）系统中覆盖拓扑的可能性，并提出了有效的拓扑推断攻击方法，实验表明这种攻击可以准确识别DFL系统的拓扑结构，揭示了DFL系统中敏感信息泄漏的风险。	 | Federated Learning (FL) is widely recognized as a privacy-preserving machine learning paradigm due to its model-sharing mechanism that avoids direct data exchange. However, model training inevitably leaves exploitable traces that can be used to infer sensitive information. In Decentralized FL (DFL), the overlay topology significantly influences its models' convergence, robustness, and security. This study explores the feasibility of inferring the overlay topology of DFL systems based solely on model behavior, introducing a novel Topology Inference Attack. A taxonomy of topology inference attacks is proposed, categorizing them by the attacker's capabilities and knowledge. Practical attack strategies are developed for different scenarios, and quantitative experiments are conducted to identify key factors influencing the attack effectiveness. Experimental results demonstrate that analyzing only the public models of individual nodes can accurately infer the DFL topology, underscoring the risk of sensitive information leakage in DFL systems. This finding offers valuable insights for improving privacy preservation in decentralized learning environments.	 | 联邦学习（FL）因其通过模型共享机制避免直接数据交换而被广泛认为是一种隐私保护的机器学习范式。然而，模型训练不可避免地会留下可被利用的痕迹，从而可以推断出敏感信息。在去中心化联邦学习（DFL）中，覆盖拓扑对模型的收敛性、鲁棒性和安全性有显著影响。本研究探讨了仅基于模型行为推断DFL系统覆盖拓扑的可行性，提出了新颖的拓扑推断攻击。提出了拓扑推断攻击的分类，按攻击者的能力和知识进行分类。针对不同场景开发了实用的攻击策略，并通过定量实验识别影响攻击效果的关键因素。实验结果表明，仅分析各个节点的公共模型即可准确推断出DFL拓扑，强调了DFL系统中敏感信息泄漏的风险。这一发现为提高去中心化学习环境中的隐私保护提供了宝贵的见解。
2501.03113	 | Balancing Efficiency and Expressiveness: Subgraph GNNs with Walk-Based Centrality	 | Joshua Southern,Yam Eitan,Guy Bar-Shalom,Michael Bronstein,Haggai Maron,Fabrizio Frasca	 | 本文提出了一种结合子图GNNs和结构编码优势的方法HyMN，通过中心性度量有效选择子图并减轻计算负担，同时在合成和真实世界任务中表现出色，显著减少运行时间。	 | We propose an expressive and efficient approach that combines the strengths of two prominent extensions of Graph Neural Networks (GNNs): Subgraph GNNs and Structural Encodings (SEs). Our approach leverages walk-based centrality measures, both as a powerful form of SE and also as a subgraph selection strategy for Subgraph GNNs. By drawing a connection to perturbation analysis, we highlight the effectiveness of centrality-based sampling, and show it significantly reduces the computational burden associated with Subgraph GNNs. Further, we combine our efficient Subgraph GNN with SEs derived from the calculated centrality and demonstrate this hybrid approach, dubbed HyMN, gains in discriminative power. HyMN effectively addresses the expressiveness limitations of Message Passing Neural Networks (MPNNs) while mitigating the computational costs of Subgraph GNNs. Through a series of experiments on synthetic and real-world tasks, we show it outperforms other subgraph sampling approaches while being competitive with full-bag Subgraph GNNs and other state-of-the-art approaches with a notably reduced runtime.	 | 我们提出了一种兼具表达性和高效性的方法，该方法结合了图神经网络（GNNs）两种重要的扩展：子图GNNs和结构编码（SEs）的优势。我们的方法利用基于行走的中心性度量，既作为一种强大的SE形式，也作为子图GNNs的子图选择策略。通过将其与扰动分析联系起来，我们突出了基于中心性采样的有效性，并展示了它显著减轻了子图GNNs的计算负担。此外，我们将高效的子图GNN与计算出的中心性衍生的SEs结合起来，证明这种混合方法（称为HyMN）在区分能力上有所提升。HyMN有力建立在消息传递神经网络（MPNNs）的表达性限制之上，并减轻了子图GNNs的计算成本。通过一系列针对合成和真实世界的任务的实验，我们展示了它在采样子图方法中表现出色，并且在与全包子图GNNs和最新其他方法竞争时，具有显著减少的运行时间。
2501.03078	 | Qinco2: Vector Compression and Search with Improved Implicit Neural Codebooks	 | Théophane Vallaeys,Matthew Muckley,Jakob Verbeek,Matthijs Douze	 | QINCo2是一种改进的矢量量化技术，通过使用神经网络和优化的训练过程，提高了向量压缩和大规模最近邻搜索的性能，在多种数据集上表现优异，分别在16字节向量压缩和8字节编码中实现了34%和24%的性能提升。	 | Vector quantization is a fundamental technique for compression and large-scale nearest neighbor search. For high-accuracy operating points, multi-codebook quantization associates data vectors with one element from each of multiple codebooks. An example is residual quantization (RQ), which iteratively quantizes the residual error of previous steps. Dependencies between the different parts of the code are, however, ignored in RQ, which leads to suboptimal rate-distortion performance. QINCo recently addressed this inefficiency by using a neural network to determine the quantization codebook in RQ based on the vector reconstruction from previous steps. In this paper we introduce QINCo2 which extends and improves QINCo with (i) improved vector encoding using codeword pre-selection and beam-search, (ii) a fast approximate decoder leveraging codeword pairs to establish accurate short-lists for search, and (iii) an optimized training procedure and network architecture. We conduct experiments on four datasets to evaluate QINCo2 for vector compression and billion-scale nearest neighbor search. We obtain outstanding results in both settings, improving the state-of-the-art reconstruction MSE by 34% for 16-byte vector compression on BigANN, and search accuracy by 24% with 8-byte encodings on Deep1M.	 | 矢量量化是一种基本的技术，用于压缩和大规模最近邻搜索。在高精度操作点上，多码本量化将数据向量与多个码本中的一个元素关联起来。例如，残差量化（RQ）通过迭代量化前一步的残差误差。然而，在RQ中忽略了不同部分之间的依赖性，导致次优的速率-失真性能。QINCo最近通过使用神经网络基于前一步的向量重构来确定RQ中的量化码本，从而解决了这一低效率问题。在本文中，我们引入了QINCo2，它在QINCo的基础上进行了扩展和改进，包括（i）改进的向量编码，使用码字预选和束搜索；（ii）快速近似解码器，利用码字对建立搜索的准确候选列表；（iii）优化的训练过程和网络架构。我们在四个数据集上进行了实验，以评估QINCo2在向量压缩和十亿规模最近邻搜索中的表现。在两种场景下，我们均取得了出色的结果：对于BigANN上的16字节向量压缩，重建均方误差提高了34%；对于Deep1M上的8字节编码，搜索准确性提高了24%。
2501.03058	 | Survival Analysis Revisited: Understanding and Unifying Poisson, Exponential, and Cox Models in Fall Risk Analysis	 | Tianhua Chen	 | 本文探讨了生存分析的基础和应用，并通过跌倒风险评估案例研究展示了生存模型之间的关系，尤其是证明了泊松回归是Cox模型的一种特例，强调了生存分析在医疗保健领域的实用价值和可解释性。	 | This paper explores foundational and applied aspects of survival analysis, using fall risk assessment as a case study. It revisits key time-related probability distributions and statistical methods, including logistic regression, Poisson regression, Exponential regression, and the Cox Proportional Hazards model, offering a unified perspective on their relationships within the survival analysis framework. A contribution of this work is the step-by-step derivation and clarification of the relationships among these models, particularly demonstrating that Poisson regression in the survival context is a specific case of the Cox model. These insights address gaps in understanding and reinforce the simplicity and interpretability of survival models. The paper also emphasizes the practical utility of survival analysis by connecting theoretical insights with real-world applications. In the context of fall detection, it demonstrates how these models can simultaneously predict fall risk, analyze contributing factors, and estimate time-to-event outcomes within a single streamlined framework. In contrast, advanced deep learning methods often require complex post-hoc interpretation and separate training for different tasks particularly when working with structured numerical data. This highlights the enduring relevance of classical statistical frameworks and makes survival models especially valuable in healthcare settings, where explainability and robustness are critical. By unifying foundational concepts and offering a cohesive perspective on time-to-event analysis, this work serves as an accessible resource for understanding survival models and applying them effectively to diverse analytical challenges.	 | 本文探讨了生存分析的基础和应用方面，并以跌倒风险评估为例进行案例研究。它重新审视了关键的时间相关概率分布和统计方法，包括逻辑回归、泊松回归、指数回归和Cox比例风险模型，提供了一个统一的视角来理解这些模型之间的关系，尤其是在生存分析框架中的关系。这项工作的贡献之一是对这些模型之间的关系进行了逐步推导和阐明，特别是证明了在生存分析背景下，泊松回归是Cox模型的一种特例。这些见解弥补了理解上的空白，强化了生存模型的简单性和可解释性。文章还强调了生存分析的实用价值，通过将理论洞察与实际应用联系起来。在跌倒检测的背景下，它展示了如何通过单一简洁的框架同时预测跌倒风险、分析影响因素并估计事件发生时间。相比之下，先进的深度学习方法通常需要复杂的后处理解释，并且在处理结构化数值数据时需要分别对不同任务进行训练。这突显了经典统计框架的持久相关性，并使生存模型在医疗保健领域尤为重要，因为在这些领域中可解释性和稳健性是关键。通过统一基础概念并从整体视角阐述时间事件分析，本文为理解生存模型及其在各种分析挑战中的有效应用提供了易懂的资源。
2501.03040	 | ChronoSense: Exploring Temporal Understanding in Large Language Models with Time Intervals of Events	 | Duygu Sezen Islakoglu,Jan-Christoph Kalo	 | ChronoSense 是一个新的基准测试，旨在评估大型语言模型（LLMs）在时间理解方面的能力，特别是艾伦区间关系和时间算术，结果显示模型在处理这些关系时存在显著差异，强调了提升LLMs时间理解能力的重要性。	 | Large Language Models (LLMs) have achieved remarkable success in various NLP tasks, yet they still face significant challenges in reasoning and arithmetic. Temporal reasoning, a critical component of natural language understanding, has raised increasing research attention. However, comprehensive testing of Allen's interval relations (e.g., before, after, during) -- a fundamental framework for temporal relationships -- remains underexplored. To fill this gap, we present ChronoSense, a new benchmark for evaluating LLMs' temporal understanding. It includes 16 tasks, focusing on identifying the Allen relation between two temporal events and temporal arithmetic, using both abstract events and real-world data from Wikidata. We assess the performance of seven recent LLMs using this benchmark and the results indicate that models handle Allen relations, even symmetrical ones, quite differently. Moreover, the findings suggest that the models may rely on memorization to answer time-related questions. Overall, the models' low performance highlights the need for improved temporal understanding in LLMs and ChronoSense offers a robust framework for future research in this area. Our dataset and the source code are available at https://github.com/duyguislakoglu/chronosense.	 | 大型语言模型（LLMs）在各种自然语言处理（NLP）任务中取得了显著的成功，但在推理和算术方面仍然面临重大挑战。时间推理是自然语言理解的一个关键组成部分，近年来引起了越来越多的研究关注。然而，艾伦区间关系（如before、after、during等）——时间关系的基本框架——的全面测试仍然相对缺乏。为填补这一空白，我们提出了ChronoSense，这是一个新的基准测试，用于评估LLMs的时间理解能力。它包括16项任务，重点是识别两个时间事件之间的艾伦关系以及时间算术，使用的是抽象事件和来自Wikidata的真实世界数据。我们使用这个基准测试评估了七个最近的LLMs的表现，结果显示，模型在处理艾伦关系（甚至对称性关系）方面表现出很大的差异。此外，研究结果表明，模型可能依赖记忆来回答与时间相关的问题。总体而言，模型的较低表现突显了提高LLMs时间理解能力的必要性，而ChronoSense则提供了一个坚实的框架，以促进该领域未来的研究。我们的数据集和源代码可在https://github.com/duyguislakoglu/chronosense 获取。
2501.03018	 | Probably Correct Optimal Stable Matching for Two-Sided Markets Under Uncertainty	 | Andreas Athanasopoulos,Anne-Marie George,Christos Dimitrakakis	 | 在偏好未知时，本文研究了集中式稳定婚姻模型的学习问题，提出了几种贝塔算法来快速识别左端最优的稳定匹配，并通过理论分析和合成数据实验验证了方法的有效性。	 | We consider a learning problem for the stable marriage model under unknown preferences for the left side of the market. We focus on the centralized case, where at each time step, an online platform matches the agents, and obtains a noisy evaluation reflecting their preferences. Our aim is to quickly identify the stable matching that is left-side optimal, rendering this a pure exploration problem with bandit feedback. We specifically aim to find Probably Correct Optimal Stable Matchings and present several bandit algorithms to do so. Our findings provide a foundational understanding of how to efficiently gather and utilize preference information to identify the optimal stable matching in two-sided markets under uncertainty. An experimental analysis on synthetic data complements theoretical results on sample complexities for the proposed methods.	 | 我们考虑在市场左端的偏好未知时，稳定婚姻模型的学习问题。我们专注于集中式情况，在每一时间步，一个在线平台匹配代理，并获得反映其偏好的噪声评价。我们的目标是迅速识别出左端最优的稳定匹配，这使得这是一个纯探索问题，带有贝塔反馈。我们特别旨在找到几乎正确的最优稳定匹配，并提出几种贝塔算法来实现这一目标。我们的发现为在不确定性条件下高效收集和利用偏好信息，以识别两方市场中的最优稳定匹配提供了一个基础性的理解。在合成数据上的实验分析补充了所提方法的样本复杂性的理论结果。
2501.03017	 | Convexity in ReLU Neural Networks: beyond ICNNs?	 | Anne Gagneux,Mathurin Massias,Emmanuel Soubies,Rémi Gribonval	 | 该研究探讨了输入凸性神经网络（ICNNs）在表示凸函数方面的能力，并提供了ReLU神经网络为凸函数的必要且充分条件，发现单隐藏层的ReLU网络可以完全由ICNN表示，但这一性质在更多层中不再成立。此外，研究还提出了一种数值程序来精确检查具有大量仿射区域的ReLU神经网络的凸性。	 | Convex functions and their gradients play a critical role in mathematical imaging, from proximal optimization to Optimal Transport. The successes of deep learning has led many to use learning-based methods, where fixed functions or operators are replaced by learned neural networks. Regardless of their empirical superiority, establishing rigorous guarantees for these methods often requires to impose structural constraints on neural architectures, in particular convexity. The most popular way to do so is to use so-called Input Convex Neural Networks (ICNNs). In order to explore the expressivity of ICNNs, we provide necessary and sufficient conditions for a ReLU neural network to be convex. Such characterizations are based on product of weights and activations, and write nicely for any architecture in the path-lifting framework. As particular applications, we study our characterizations in depth for 1 and 2-hidden-layer neural networks: we show that every convex function implemented by a 1-hidden-layer ReLU network can be also expressed by an ICNN with the same architecture; however this property no longer holds with more layers. Finally, we provide a numerical procedure that allows an exact check of convexity for ReLU neural networks with a large number of affine regions.	 | 凸函数及其梯度在数学成像中发挥着关键作用，从近端优化到最优传输。深度学习的成功促使许多人使用基于学习的方法，其中固定函数或算子被学习的神经网络所取代。尽管这些方法在实验上具有显著优势，但要为这些方法建立严格的保证通常需要对神经架构施加结构约束，特别是凸性约束。最常见的方法是使用所谓的输入凸性神经网络（ICNNs）。  为了探索ICNNs的表达能力，我们提供了ReLU神经网络为凸函数的必要且充分条件。这些表征基于权重和激活值的乘积，并且在路径提升框架中以简洁的形式呈现。作为特定应用，我们深入研究了这种表征在单隐藏层和双隐藏层神经网络中的情况：我们证明，由单隐藏层ReLU网络实现的每个凸函数也可以用具有相同架构的ICNN表示；然而，这一性质在更多的层中不再成立。最后，我们提供了一种数值程序，可以对具有大量仿射区域的ReLU神经网络的凸性进行精确检查。
2501.02975	 | Fuzzy Granule Density-Based Outlier Detection with Multi-Scale Granular Balls	 | Can Gao,Xiaofeng Tan,Jie Zhou,Weiping Ding,Witold Pedrycz	 | 本文提出了一种基于模糊粗糙集的多尺度异常检测方法，该方法通过结合相对模糊粒度密度和多尺度粒度球计算来识别不同类型的异常，并使用三元决策确定的样本训练加权支持向量机，实验结果表明该方法在多个数据集上显著优于现有方法。	 | Outlier detection refers to the identification of anomalous samples that deviate significantly from the distribution of normal data and has been extensively studied and used in a variety of practical tasks. However, most unsupervised outlier detection methods are carefully designed to detect specified outliers, while real-world data may be entangled with different types of outliers. In this study, we propose a fuzzy rough sets-based multi-scale outlier detection method to identify various types of outliers. Specifically, a novel fuzzy rough sets-based method that integrates relative fuzzy granule density is first introduced to improve the capability of detecting local outliers. Then, a multi-scale view generation method based on granular-ball computing is proposed to collaboratively identify group outliers at different levels of granularity. Moreover, reliable outliers and inliers determined by the three-way decision are used to train a weighted support vector machine to further improve the performance of outlier detection. The proposed method innovatively transforms unsupervised outlier detection into a semi-supervised classification problem and for the first time explores the fuzzy rough sets-based outlier detection from the perspective of multi-scale granular balls, allowing for high adaptability to different types of outliers. Extensive experiments carried out on both artificial and UCI datasets demonstrate that the proposed outlier detection method significantly outperforms the state-of-the-art methods, improving the results by at least 8.48% in terms of the Area Under the ROC Curve (AUROC) index. { The source codes are released at \url{https://github.com/Xiaofeng-Tan/MGBOD}. }	 | 异常检测是指识别与正常数据分布显著偏离的异常样本，并且已经在各种实际任务中得到了广泛的研究和应用。然而，大多数无监督异常检测方法都是针对特定类型的异常设计的，而现实世界的数据可能包含了不同类型的异常。在本研究中，我们提出了一种基于模糊粗糙集的多尺度异常检测方法，以识别各种类型的异常。具体来说，首先引入了一种结合相对模糊粒度密度的新型模糊粗糙集方法，以提高检测局部异常的能力。然后，提出了基于粒度球计算的多尺度视图生成方法，以在不同粒度级别上协作识别群组异常。此外，基于三元决策确定的可靠异常和正常样本用于训练加权支持向量机，以进一步提高异常检测性能。所提出的方法创新地将无监督异常检测转换为半监督分类问题，并从多尺度粒度球的角度首次探讨了基于模糊粗糙集的异常检测方法，具有较高的适应不同类型的异常的能力。在人工数据集和UCI数据集上进行的广泛实验表明，所提出的异常检测方法显著优于最先进的方法，在接收操作特征曲线下面积（AUROC）指标上提高了至少8.48%。{源代码在\url{https://github.com/Xiaofeng-Tan/MGBOD}发布。}
2501.02969	 | LOHA: Direct Graph Spectral Contrastive Learning Between Low-pass and High-pass Views	 | Ziyun Zou,Yinghui Jiang,Lian Shen,Juan Liu,Xiangrong Liu	 | 本文提出了一种自监督对比框架LOHA，通过整合低通和高通滤波器的视角，有效提升了光谱图神经网络在不同同质性水平图上的性能，尤其是在次要模型上平均提升了2.8%，甚至在某些数据集上超越了全监督模型。	 | Spectral Graph Neural Networks effectively handle graphs with different homophily levels, with low-pass filter mining feature smoothness and high-pass filter capturing differences. When these distinct filters could naturally form two opposite views for self-supervised learning, the commonalities between the counterparts for the same node remain unexplored, leading to suboptimal performance. In this paper, a simple yet effective self-supervised contrastive framework, LOHA, is proposed to address this gap. LOHA optimally leverages low-pass and high-pass views by embracing "harmony in diversity". Rather than solely maximizing the difference between these distinct views, which may lead to feature separation, LOHA harmonizes the diversity by treating the propagation of graph signals from both views as a composite feature. Specifically, a novel high-dimensional feature named spectral signal trend is proposed to serve as the basis for the composite feature, which remains relatively unaffected by changing filters and focuses solely on original feature differences. LOHA achieves an average performance improvement of 2.8% over runner-up models on 9 real-world datasets with varying homophily levels. Notably, LOHA even surpasses fully-supervised models on several datasets, which underscores the potential of LOHA in advancing the efficacy of spectral GNNs for diverse graph structures.	 | 光谱图神经网络有效地处理不同同质性水平的图，低通滤波器挖掘特征平滑性，高通滤波器捕捉差异。当这些不同的滤波器可以自然形成两种对立的视角时，相同节点的对应部分之间的共性尚未被探索，导致性能欠佳。本文提出了一种简单而有效的自监督对比框架LOHA，以弥补这一缺口。LOHA通过“多样性中的和谐”最优地利用低通和高通视角。LOHA不单独最大化这两种不同视角之间的差异，而是通过将两种视角的图信号传播视为复合特征来和谐地利用多样性。具体而言，提出了一种新的高维特征——光谱信号趋势，作为复合特征的基础，这种特征在滤波器变化时相对稳定，专注于原始特征差异。在9个不同同质性水平的真实世界数据集上，LOHA在次要模型上实现了2.8%的平均性能提升。值得注意的是，LOHA甚至在某些数据集上超过了全监督模型，这突显了LOHA在提高光谱GNNs处理多样化图结构的有效性方面的潜力。
2501.02949	 | MSA-CNN: A Lightweight Multi-Scale CNN with Attention for Sleep Stage Classification	 | Stephan Goerttler,Yucheng Wang,Emadeldeen Eldele,Min Wu,Fei He	 | 本文提出了一种轻量级的多尺度和注意力卷积神经网络（MSA-CNN），通过减少模型复杂性来实现自动睡眠阶段分类，并在三个公开数据集上优于九种最先进的基准模型，证明了其在准确性和效率上的优势。	 | Recent advancements in machine learning-based signal analysis, coupled with open data initiatives, have fuelled efforts in automatic sleep stage classification. Despite the proliferation of classification models, few have prioritised reducing model complexity, which is a crucial factor for practical applications. In this work, we introduce Multi-Scale and Attention Convolutional Neural Network (MSA-CNN), a lightweight architecture featuring as few as ~10,000 parameters. MSA-CNN leverages a novel multi-scale module employing complementary pooling to eliminate redundant filter parameters and dense convolutions. Model complexity is further reduced by separating temporal and spatial feature extraction and using cost-effective global spatial convolutions. This separation of tasks not only reduces model complexity but also mirrors the approach used by human experts in sleep stage scoring. We evaluated both small and large configurations of MSA-CNN against nine state-of-the-art baseline models across three public datasets, treating univariate and multivariate models separately. Our evaluation, based on repeated cross-validation and re-evaluation of all baseline models, demonstrated that the large MSA-CNN outperformed all baseline models on all three datasets in terms of accuracy and Cohen's kappa, despite its significantly reduced parameter count. Lastly, we explored various model variants and conducted an in-depth analysis of the key modules and techniques, providing deeper insights into the underlying mechanisms. The code for our models, baselines, and evaluation procedures is available at https://github.com/sgoerttler/MSA-CNN.	 | 近年来，基于机器学习的信号分析技术的进步，以及开放数据倡议的推动，促进了自动睡眠阶段分类工作的进展。尽管分类模型层出不穷，但很少有模型将减少模型复杂性作为优先事项，这是实际应用中的关键因素。在本文中，我们提出了多尺度和注意力卷积神经网络（MSA-CNN），这是一种轻量级架构，参数量少至约10,000个。MSA-CNN 利用一种新颖的多尺度模块，采用互补池化来消除冗余的滤波器参数和密集卷积。通过分离时间和空间特征提取，并使用经济有效的全局空间卷积，进一步减少了模型复杂性。这种任务分离不仅减少了模型复杂性，还与人类专家在睡眠阶段评分中所用的方法相呼应。我们在三个公开数据集上分别评估了MSA-CNN的小型和大型配置，与九种最先进的基准模型进行了比较。基于重复交叉验证和所有基准模型的重新评估，我们的评估证明，大型MSA-CNN在三个数据集上的准确性和康氏κ系数方面均优于所有基准模型，尽管其参数数量显著较少。最后，我们探讨了各种模型变体，并对关键模块和技术进行了深入分析，提供了对潜在机制的更深入理解。我们的模型、基准和评估程序的代码可在 https://github.com/sgoerttler/MSA-CNN 获取。
2501.02945	 | The Tabular Foundation Model TabPFN Outperforms Specialized Time Series Forecasting Models Based on Simple Features	 | Shi Bin Hoo,Samuel Müller,David Salinas,Frank Hutter	 | 本文介绍了一种名为TabPFN-TS的方法，该方法结合了TabPFN模型和简单的特征工程，应用于时间序列预测，表现出色且参数量少，仅需少量预训练数据，避免了大规模训练数据集的需求和基准数据集污染的风险。	 | Foundation models have become popular in forecasting due to their ability to make accurate predictions, even with minimal fine-tuning on specific datasets. In this paper, we demonstrate how the newly released regression variant of TabPFN, a general tabular foundation model, can be applied to time series forecasting. We propose a straightforward approach, TabPFN-TS, which pairs TabPFN with simple feature engineering to achieve strong forecasting performance. Despite its simplicity and with only 11M parameters, TabPFN-TS outperforms Chronos-Mini, a model of similar size, and matches or even slightly outperforms Chronos-Large, which has 65-fold more parameters. A key strength of our method lies in its reliance solely on artificial data during pre-training, avoiding the need for large training datasets and eliminating the risk of benchmark contamination.	 | 基础模型因其能够通过最少的特定数据集微调实现准确预测而成为预测的热门选择。本文展示了最近发布的TabPFN的回归变体——一个通用的表格基础模型——如何应用于时间序列预测。我们提出了一种简单的方法，TabPFN-TS，它将TabPFN与简单的特征工程相结合，以实现强大的预测性能。尽管它的结构简单且仅有11M参数，但TabPFN-TS在与之相似大小的Chronos-Mini模型上表现出色，并在参数量多65倍的Chronos-Large模型上达到了相当或略高的性能。我们方法的一个关键优势在于，它完全依赖于预训练期间使用的人工数据，从而避免了大规模训练数据集的需求，并消除了基准数据集污染的风险。
2501.02931	 | Self-Attention as a Parametric Endofunctor: A Categorical Framework for Transformer Architectures	 | Charles O'Neill	 | 该研究通过范畴代数的形式化理解自注意力机制，证明其线性部分可以视为参数范畴中的内函子，并且多个自注意力层的堆叠可以构建自由幺半群，从而统一了注意力机制的几何、代数和解释性分析。	 | Self-attention mechanisms have revolutionised deep learning architectures, but their mathematical foundations remain incompletely understood. We establish that these mechanisms can be formalised through categorical algebra, presenting a framework that focuses on the linear components of self-attention. We prove that the query, key, and value maps in self-attention naturally form a parametric endofunctor in the 2-category $\mathbf{Para}(\mathbf{Vect})$ of parametric morphisms. We show that stacking multiple self-attention layers corresponds to constructing the free monad on this endofunctor. For positional encodings, we demonstrate that strictly additive position embeddings constitute monoid actions on the embedding space, while standard sinusoidal encodings, though not additive, possess a universal property among faithful position-preserving functors. We establish that the linear portions of self-attention exhibit natural equivariance properties with respect to permutations of input tokens. Finally, we prove that the ``circuits'' identified in mechanistic interpretability correspond precisely to compositions of parametric morphisms in our framework. This categorical perspective unifies geometric, algebraic, and interpretability-based approaches to transformer analysis, while making explicit the mathematical structures underlying attention mechanisms. Our treatment focuses exclusively on linear maps, setting aside nonlinearities like softmax and layer normalisation, which require more sophisticated categorical structures. Our results extend recent work on categorical foundations for deep learning while providing insights into the algebraic structure of attention mechanisms.	 | 自注意力机制已经革新了深度学习架构，但其数学基础仍然不清楚。我们证明这些机制可以通过范畴代数的形式化进行理解，提出了一种框架，专注于自注意力的线性部分。我们证明自注意力中的查询、键和值映射自然形成了参数范畴 \(\mathbf{Para}(\mathbf{Vect})\) 中的一个参数内函子。我们展示了堆叠多个自注意力层对应于在这个内函子上构造自由幺半群。对于位置编码，我们证明严格加性的位置嵌入构成了嵌入空间上的半群作用，而标准的正弦位置编码虽然不是加性的，但在忠实的位置保持函子中具有普遍性质。我们证明了自注意力的线性部分表现出对输入标记排列的自然不变性性质。最后，我们证明了在机械解释中识别的“电路”恰好对应于我们框架中参数化态射的组合。这种范畴论视角统一了自注意力机制的几何、代数和解释性分析方法，同时明确指出了这些机制背后的数学结构。我们的处理仅专注于线性映射，而省略了如softmax和层规范化等非线性操作，这些操作需要更复杂的范畴论结构。我们的结果扩展了近年来关于深度学习范畴论基础的工作，并为理解注意力机制的代数结构提供了新的见解。
2501.02926	 | Offline-to-online hyperparameter transfer for stochastic bandits	 | Dravyansh Sharma,Arun Sai Suggala	 | 该研究探讨了在未知任务分布的情况下，如何利用多个bandit问题的离线数据来学习最优超参数，以解决探索与利用之间的权衡问题，并提供了跨任务和跨任务内样本复杂性的理论边界。实验结果表明，从离线数据中转移超参数在在线学习中是重要且有效的。	 | Classic algorithms for stochastic bandits typically use hyperparameters that govern their critical properties such as the trade-off between exploration and exploitation. Tuning these hyperparameters is a problem of great practical significance. However, this is a challenging problem and in certain cases is information theoretically impossible. To address this challenge, we consider a practically relevant transfer learning setting where one has access to offline data collected from several bandit problems (tasks) coming from an unknown distribution over the tasks. Our aim is to use this offline data to set the hyperparameters for a new task drawn from the unknown distribution. We provide bounds on the inter-task (number of tasks) and intra-task (number of arm pulls for each task) sample complexity for learning near-optimal hyperparameters on unseen tasks drawn from the distribution. Our results apply to several classic algorithms, including tuning the exploration parameters in UCB and LinUCB and the noise parameter in GP-UCB. Our experiments indicate the significance and effectiveness of the transfer of hyperparameters from offline problems in online learning with stochastic bandit feedback.	 | 经典的随机多臂 bandit 算法通常使用超参数来控制其关键属性，如探索与利用之间的权衡。调整这些超参数是具有重大实际意义的问题。然而，这是一项具有挑战性的工作，在某些情况下，从信息论的角度来看是不可能的。为了应对这一挑战，我们考虑了一个实际相关的学习迁移设置，其中可以访问来自未知任务分布的多个 bandit 问题（任务）的离线数据。我们的目标是利用这些离线数据为新任务设置超参数，该新任务是从未知分布中抽取的。我们提供了在从分布中抽取的未见过的任务上学习近最优超参数的跨任务（任务数量）和跨任务内（每个任务的臂拉取次数）样本复杂性的边界。我们的结果适用于几种经典的算法，包括调整 UCB 和 LinUCB 的探索参数以及 GP-UCB 的噪声参数。我们的实验表明，在具有随机多臂反馈的在线学习中，从离线问题转移超参数的重要性及其有效性。
2501.02905	 | Skillful High-Resolution Ensemble Precipitation Forecasting with an Integrated Deep Learning Framework	 | Shuangshuang He,Hongli Liang,Yuanting Zhang,Xingyuan Yuan	 | 该研究提出了一种基于物理的深度学习框架，用于高分辨率（0.05° × 0.05°）的集合降水预报，通过整合确定性和概率性模型，显著提高了空间分辨率和预报精度，并在实际降水事件中表现出色。	 | High-resolution precipitation forecasts are crucial for providing accurate weather prediction and supporting effective responses to extreme weather events. Traditional numerical models struggle with stochastic subgrid-scale processes, while recent deep learning models often produce blurry results. To address these challenges, we propose a physics-inspired deep learning framework for high-resolution (0.05\textdegree{} $\times$ 0.05\textdegree{}) ensemble precipitation forecasting. Trained on ERA5 and CMPA high-resolution precipitation datasets, the framework integrates deterministic and probabilistic components. The deterministic model, based on a 3D SwinTransformer, captures average precipitation at mesoscale resolution and incorporates strategies to enhance performance, particularly for moderate to heavy rainfall. The probabilistic model employs conditional diffusion in latent space to account for uncertainties in residual precipitation at convective scales. During inference, ensemble members are generated by repeatedly sampling latent variables, enabling the model to represent precipitation uncertainty. Our model significantly enhances spatial resolution and forecast accuracy. Rank histogram shows that the ensemble system is reliable and unbiased. In a case study of heavy precipitation in southern China, the model outputs align more closely with observed precipitation distributions than ERA5, demonstrating superior capability in capturing extreme precipitation events. Additionally, 5-day real-time forecasts show good performance in terms of CSI scores.	 | 高分辨率降水预报对于提供准确的天气预测和有效应对极端天气事件至关重要。传统的数值模型难以处理子网格尺度上的随机过程，而最近的深度学习模型往往产生模糊的结果。为了解决这些问题，我们提出了一种基于物理的深度学习框架，用于高分辨率（0.05° × 0.05°）集合降水预报。该框架基于ERA5和CMPA高分辨率降水数据集进行训练，整合了确定性和概率性组件。确定性模型基于三维SwinTransformer，能够捕捉中尺度分辨率下的平均降水，并采用策略来提高性能，特别是对于中等至重度降水。概率性模型利用潜在空间中的条件扩散来考虑对流尺度上残差降水的不确定性。在推理过程中，通过多次采样潜在变量生成集合成员，从而使模型能够表示降水不确定性。我们的模型显著提高了空间分辨率和预报精度。秩直方图表明，集合系统可靠且无偏。在中国南部一次强降水事件的研究案例中，模型输出与观测到的降水分布更为吻合，表现出在捕捉极端降水事件方面的优越能力。此外，5天的实时预报在CSI评分方面表现出良好的性能。
2501.02880	 | Conditional Mutual Information Based Diffusion Posterior Sampling for Solving Inverse Problems	 | Shayan Mohajer Hamidi,En-Hui Yang	 | 该研究针对逆问题提出了一个信息论方法，通过最大化条件互信息来改进扩散模型在图像修复等任务中的性能，从而能够更有效地保留测量值中的信息，并在定性和定量上提升任务效果。	 | Inverse problems are prevalent across various disciplines in science and engineering. In the field of computer vision, tasks such as inpainting, deblurring, and super-resolution are commonly formulated as inverse problems. Recently, diffusion models (DMs) have emerged as a promising approach for addressing noisy linear inverse problems, offering effective solutions without requiring additional task-specific training. Specifically, with the prior provided by DMs, one can sample from the posterior by finding the likelihood. Since the likelihood is intractable, it is often approximated in the literature. However, this approximation compromises the quality of the generated images. To overcome this limitation and improve the effectiveness of DMs in solving inverse problems, we propose an information-theoretic approach. Specifically, we maximize the conditional mutual information $\mathrm{I}(\boldsymbol{x}_0; \boldsymbol{y} | \boldsymbol{x}_t)$, where $\boldsymbol{x}_0$ represents the reconstructed signal, $\boldsymbol{y}$ is the measurement, and $\boldsymbol{x}_t$ is the intermediate signal at stage $t$. This ensures that the intermediate signals $\boldsymbol{x}_t$ are generated in a way that the final reconstructed signal $\boldsymbol{x}_0$ retains as much information as possible about the measurement $\boldsymbol{y}$. We demonstrate that this method can be seamlessly integrated with recent approaches and, once incorporated, enhances their performance both qualitatively and quantitatively.	 | 逆问题在科学和工程的各个领域中普遍存在。在计算机视觉领域，图像修复、去模糊和超分辨率等任务通常被形式化为逆问题。近年来，扩散模型（DMs）作为一种解决噪声线性逆问题的有效方法引起了人们的关注，它们能够在不需要额外的任务特定训练的情况下提供有效的解决方案。具体来说，借助DMs提供的先验知识，可以通过寻找似然性来从后验中采样。由于似然性难以计算，通常在文献中对其进行近似。然而，这种近似会降低生成图像的质量。为了克服这一局限性并提高DMs在解决逆问题中的有效性，我们提出了一种信息论方法。具体而言，我们最大化条件互信息 $\mathrm{I}(\boldsymbol{x}_0; \boldsymbol{y} | \boldsymbol{x}_t)$，其中 $\boldsymbol{x}_0$ 代表重构信号，$\boldsymbol{y}$ 是测量值，$\boldsymbol{x}_t$ 是第 $t$ 阶段的中间信号。这种方法确保中间信号 $\boldsymbol{x}_t$ 的生成方式能够让最终重构信号 $\boldsymbol{x}_0$ 保留尽可能多关于测量值 $\boldsymbol{y}$ 的信息。我们证明了这种方法可以无缝地与最近的方法集成，并且一旦集成，能够从定性和定量两个方面提升其性能。
2501.02860	 | Seeing the Whole in the Parts in Self-Supervised Representation Learning	 | Arthur Aubret,Céline Teulière,Jochen Triesch	 | 本文提出了一种新的自我监督学习方法CO-SSL，通过在池化前对局部表示进行对齐来建模空间共现，该方法在多个数据集上优于现有方法，并且对噪声和对抗攻击更为稳健；研究表明，CO-SSL学习出的高度冗余局部表示是其稳健性的关键。	 | Recent successes in self-supervised learning (SSL) model spatial co-occurrences of visual features either by masking portions of an image or by aggressively cropping it. Here, we propose a new way to model spatial co-occurrences by aligning local representations (before pooling) with a global image representation. We present CO-SSL, a family of instance discrimination methods and show that it outperforms previous methods on several datasets, including ImageNet-1K where it achieves 71.5% of Top-1 accuracy with 100 pre-training epochs. CO-SSL is also more robust to noise corruption, internal corruption, small adversarial attacks, and large training crop sizes. Our analysis further indicates that CO-SSL learns highly redundant local representations, which offers an explanation for its robustness. Overall, our work suggests that aligning local and global representations may be a powerful principle of unsupervised category learning.	 | 近年来，自我监督学习（SSL）模型通过遮蔽图像的一部分或对其进行激烈裁剪来建模视觉特征的空间共现。在这里，我们提出了一种新的方法，通过在池化之前对局部表示进行对齐来建模空间共现。我们引入了CO-SSL这一实例鉴别方法家族，并表明它在多个数据集上优于先前的方法，例如在ImageNet-1K数据集上，使用100个预训练周期可实现71.5%的Top-1精度。CO-SSL对噪声干扰、内部干扰、小规模对抗攻击以及大训练裁剪尺寸也更为稳健。我们的分析进一步表明，CO-SSL学习出高度冗余的局部表示，这为其实现的稳健性提供了解释。总体而言，我们的工作表明，对齐局部和全局表示可能是无监督类别学习的一个强大原则。
2501.02825	 | Randomly Sampled Language Reasoning Problems Reveal Limits of LLMs	 | Kavi Gupta,Kate Sanders,Armando Solar-Lezama	 | 研究发现，即使在极为简单的由确定性有限自动机（DFAs）定义的语言任务中，大型语言模型的表现也逊色于未参数化的n-gram模型，这表明这些模型在处理与训练数据显著不同的语言时存在困难。	 | Can LLMs pick up language structure from examples? Evidence in prior work seems to indicate yes, as pretrained models repeatedly demonstrate the ability to adapt to new language structures and vocabularies. However, this line of research typically considers languages that are present within common pretraining datasets, or otherwise share notable similarities with these seen languages. In contrast, in this work we attempt to measure models' language understanding capacity while circumventing the risk of dataset recall. We parameterize large families of language tasks recognized by deterministic finite automata (DFAs), and can thus sample novel language reasoning problems to fairly evaulate LLMs regardless of training data. We find that, even in the strikingly simple setting of 3-state DFAs, LLMs underperform unparameterized ngram models on both language recognition and synthesis tasks. These results suggest that LLMs struggle to match the ability of basic language models in recognizing and reasoning over languages that are sufficiently distinct from the ones they see at training time, underscoring the distinction between learning individual languages and possessing a general theory of language.	 | 大型语言模型能否从示例中学会语言结构？前期研究中的证据似乎表明可以，因为预训练模型反复展示出适应新语言结构和词汇表的能力。然而，这类研究通常考虑的是预训练数据集中已有的语言，或者至少与这些已知语言有显著相似之处的语言。相比之下，在本项研究中，我们试图在避免依赖于数据集回忆的情况下衡量模型的语言理解能力。我们参数化了一系列由确定性有限自动机（DFAs）识别的语言任务，从而能够采样出全新的语言推理问题，无论模型的训练数据如何，都能公平地评估LLM的表现。我们发现，即使是在仅有3个状态的DFAs这一极其简单的环境中，LLM在语言识别和合成任务上的表现也逊色于未参数化的n-gram模型。这些结果表明，LLM在识别和处理与训练时所见语言足够不同的语言时存在困难，这强调了学习个别语言与掌握语言一般理论之间的区别。
2501.02808	 | DarkFarseer: Inductive Spatio-temporal Kriging via Hidden Style Enhancement and Sparsity-Noise Mitigation	 | Zhuoxuan Liang,Wei Li,Dalin Zhang,Yidan Chen,Zhihong Wang,Xiangping Zheng,Moustafa Youssef	 | DarkFarseer 是一种改进的感应时空克里金（ISK）框架，通过引入邻域隐藏样式增强模块、虚拟组件对比学习和基于相似性的图去噪策略，提升了虚拟传感器的时空特征提取和噪声连接处理能力，从而显著优于现有方法。	 | With the rapid growth of the Internet of Things and Cyber-Physical Systems, widespread sensor deployment has become essential. However, the high costs of building sensor networks limit their scale and coverage, making fine-grained deployment challenging. Inductive Spatio-Temporal Kriging (ISK) addresses this issue by introducing virtual sensors. Based on graph neural networks (GNNs) extracting the relationships between physical and virtual sensors, ISK can infer the measurements of virtual sensors from physical sensors. However, current ISK methods rely on conventional message-passing mechanisms and network architectures, without effectively extracting spatio-temporal features of physical sensors and focusing on representing virtual sensors. Additionally, existing graph construction methods face issues of sparse and noisy connections, destroying ISK performance. To address these issues, we propose DarkFarseer, a novel ISK framework with three key components. First, we propose the Neighbor Hidden Style Enhancement module with a style transfer strategy to enhance the representation of virtual nodes in a temporal-then-spatial manner to better extract the spatial relationships between physical and virtual nodes. Second, we propose Virtual-Component Contrastive Learning, which aims to enrich the node representation by establishing the association between the patterns of virtual nodes and the regional patterns within graph components. Lastly, we design a Similarity-Based Graph Denoising Strategy, which reduces the connectivity strength of noisy connections around virtual nodes and their neighbors based on their temporal information and regional spatial patterns. Extensive experiments demonstrate that DarkFarseer significantly outperforms existing ISK methods.	 | 随着物联网和网络物理系统的快速发展，广泛部署传感器变得至关重要。然而，构建传感器网络的高成本限制了其规模和覆盖范围，使得精细部署变得困难。感应时空克里金（Inductive Spatio-Temporal Kriging, ISK）通过引入虚拟传感器解决了这一问题。基于图神经网络（GNNs）提取物理和虚拟传感器之间的关系，ISK可以从物理传感器中推断出虚拟传感器的测量值。然而，现有的ISK方法依赖于传统的消息传递机制和网络架构，未能有效地提取物理传感器的时空特征，也没有专注于表示虚拟传感器。此外，现有的图构建方法面临着稀疏和噪声连接的问题，破坏了ISK的性能。为了解决这些问题，我们提出了DarkFarseer，这是一种具有三个关键组件的新颖的ISK框架。首先，我们提出了邻域隐藏样式增强模块（Neighbor Hidden Style Enhancement module），采用风格迁移策略，以时间然后空间的方式增强虚拟节点的表示，以便更好地提取物理和虚拟节点之间的空间关系。其次，我们提出了虚拟组件对比学习（Virtual-Component Contrastive Learning），旨在通过建立虚拟节点模式与图组件内区域模式之间的关联来丰富节点表示。最后，我们设计了一种基于相似性的图去噪策略（Similarity-Based Graph Denoising Strategy），根据时间信息和区域空间模式，减少虚拟节点及其邻居周围的噪声连接强度。广泛实验表明，DarkFarseer 显著优于现有的ISK方法。
2501.02781	 | From Dense to Sparse: Event Response for Enhanced Residential Load Forecasting	 | Xin Cao,Qinghua Tao,Yingjie Zhou,Lu Zhang,Le Zhang,Dongjin Song,Dapeng Oliver Wu,Ce Zhu	 | 本文提出了一种事件响应知识引导的方法（ERKG）用于住宅负荷预测，该方法通过估算不同电器的用电事件并挖掘事件相关的稀疏知识来提高预测准确性。实验结果表明，ERKG能够显著提升预测性能，相较于最先进的预测模型，其均方误差降低了超过8%。	 | Residential load forecasting (RLF) is crucial for resource scheduling in power systems. Most existing methods utilize all given load records (dense data) to indiscriminately extract the dependencies between historical and future time series. However, there exist important regular patterns residing in the event-related associations among different appliances (sparse knowledge), which have yet been ignored.In this paper, we propose an Event-Response Knowledge Guided approach (ERKG) for RLF by incorporating the estimation of electricity usage events for different appliances, mining event-related sparse knowledge from the load series. With ERKG, the event-response estimation enables portraying the electricity consumption behaviors of residents, revealing regular variations in appliance operational states.To be specific, ERKG consists of knowledge extraction and guidance: i) a forecasting model is designed for the electricity usage events by estimating appliance operational states, aiming to extract the event-related sparse knowledge; ii) a novel knowledge-guided mechanism is established by fusing such state estimates of the appliance events into the RLF model, which can give particular focuses on the patterns of users' electricity consumption behaviors.Notably, ERKG can flexibly serve as a plug-in module to boost the capability of existing forecasting models by leveraging event response. In numerical experiments, extensive comparisons and ablation studies have verified the effectiveness of our ERKG, e.g., over 8% MAE can be reduced on the tested state-of-the-art forecasting models. The source code will be available at https://github.com/ergoucao/ERKG.	 | 住宅负荷预测（RLF）对于电力系统的资源调度至关重要。现有的大多数方法利用所有给定的负荷记录（密集数据）来无差别地提取历史时间和未来时间序列之间的依赖关系。然而，不同电器之间事件相关的关联中存在重要的规律性模式（稀疏知识），这些模式尚未被忽略。本文提出了一个事件响应知识引导的方法（ERKG）来解决住宅负荷预测问题，通过估算不同电器的用电事件以及从负荷序列中挖掘事件相关的稀疏知识。通过ERKG，事件响应估计可以展示居民的用电行为，揭示电器操作状态中的规律性变化。  具体来说，ERKG 包括知识提取和引导：i) 设计一个预测模型来估算电器用电事件，目的是提取事件相关的稀疏知识；ii) 建立一种新颖的知识引导机制，通过将电器事件状态估计融合到负荷预测模型中，可以特别关注用户的用电行为模式。值得注意的是，ERKG 可灵活作为插件模块增强现有预测模型的能力，利用事件响应。数值实验中，广泛的比较和消融研究验证了ERKG的有效性，例如，在测试的最先进的预测模型上，MAE可以降低超过8%。源代码将可在https://github.com/ergoucao/ERKG提供。
2501.02774	 | Learn A Flexible Exploration Model for Parameterized Action Markov Decision Processes	 | Zijian Wang,Bin Wang,Mingwen Shao,Hongbo Dou,Boxiang Tao	 | FLEXplore 是一种基于模型的强化学习算法，通过学习参数化动作条件下的动力学模型和改进的 Model Predictive Path Integral 控制，来提高代理的学习效率和探索性能；实验结果表明，FLEXplore 在多个基准测试中表现出色，具有出色的性能。	 | Hybrid action models are widely considered an effective approach to reinforcement learning (RL) modeling. The current mainstream method is to train agents under Parameterized Action Markov Decision Processes (PAMDPs), which performs well in specific environments. Unfortunately, these models either exhibit drastic low learning efficiency in complex PAMDPs or lose crucial information in the conversion between raw space and latent space. To enhance the learning efficiency and asymptotic performance of the agent, we propose a model-based RL (MBRL) algorithm, FLEXplore. FLEXplore learns a parameterized-action-conditioned dynamics model and employs a modified Model Predictive Path Integral control. Unlike conventional MBRL algorithms, we carefully design the dynamics loss function and reward smoothing process to learn a loose yet flexible model. Additionally, we use the variational lower bound to maximize the mutual information between the state and the hybrid action, enhancing the exploration effectiveness of the agent. We theoretically demonstrate that FLEXplore can reduce the regret of the rollout trajectory through the Wasserstein Metric under given Lipschitz conditions. Our empirical results on several standard benchmarks show that FLEXplore has outstanding learning efficiency and asymptotic performance compared to other baselines.	 | 混合动作模型被认为是 reinforcement learning (RL) 模型中一种有效的方法。当前主流的方法是在参数化动作马尔可夫决策过程 (PAMDPs) 下训练代理，这种方法在特定环境中表现良好。不幸的是，这些模型要么在复杂的 PAMDPs 中表现出极低的学习效率，要么在从原始空间到潜在空间的转换中丢失了关键信息。为了提高代理的学习效率和渐近性能，我们提出了一种基于模型的 RL (MBRL) 算法——FLEXplore。FLEXplore 学习一个参数化动作条件下的动力学模型，并采用改进的 Model Predictive Path Integral 控制。与传统的 MBRL 算法不同，我们精心设计了动力学损失函数和奖励平滑过程，以学习一个宽松但灵活的模型。此外，我们使用变分下界来最大化状态与混合动作之间的互信息，从而增强代理的探索效果。我们从理论上证明，在给定的 Lipschitz 条件下，FLEXplore 可以通过 Wasserstein 距度减少展开轨迹的遗憾。我们在几个标准基准上的实验结果表明，FLEXplore 相比其他基线具有出色的學習效率和渐近性能。
2501.02767	 | Enhancing Trustworthiness of Graph Neural Networks with Rank-Based Conformal Training	 | Ting Wang,Zhixin Zhou,Rui Luo	 | 本文提出了一种基于排名的元学习框架（RCP-GNN），在图神经网络的训练过程中为节点分类任务提供可靠的不确定性估计，从而提高模型的可信度。实验结果表明，该方法能够在满足预定义覆盖目标的同时，显著减少无效性。	 | Graph Neural Networks (GNNs) has been widely used in a variety of fields because of their great potential in representing graph-structured data. However, lacking of rigorous uncertainty estimations limits their application in high-stakes. Conformal Prediction (CP) can produce statistically guaranteed uncertainty estimates by using the classifier's probability estimates to obtain prediction sets, which contains the true class with a user-specified probability. In this paper, we propose a Rank-based CP during training framework to GNNs (RCP-GNN) for reliable uncertainty estimates to enhance the trustworthiness of GNNs in the node classification scenario. By exploiting rank information of the classifier's outcome, prediction sets with desired coverage rate can be efficiently constructed. The strategy of CP during training with differentiable rank-based conformity loss function is further explored to adapt prediction sets according to network topology information. In this way, the composition of prediction sets can be guided by the goal of jointly reducing inefficiency and probability estimation errors. Extensive experiments on several real-world datasets show that our model achieves any pre-defined target marginal coverage while significantly reducing the inefficiency compared with state-of-the-art methods.	 | 图神经网络（GNNs）因其在表示图结构数据方面巨大的潜力而在多个领域得到了广泛应用。然而，缺乏严格的不确定性估计限制了它们在高风险情境下的应用。一致预测（Conformal Prediction，CP）可以通过利用分类器的概率估计来生成包含真实类别的预测集，并且这些预测集以用户指定的概率包含真相。在本文中，我们提出了一种在训练期间基于排名的CP框架（RCP-GNN），用于在节点分类场景中为GNNs提供可靠的不确定性估计，从而提高GNNs的可信度。通过利用分类器结果的排名信息，可以高效地构建具有所需覆盖率的预测集。我们进一步探索了基于可微秩一致度损失函数的训练期间CP策略，以便根据网络拓扑信息调整预测集。这样，预测集的构成可以同时减少无效性和概率估计误差。在几个真实世界数据集上的广泛实验表明，我们的模型可以实现任何预定义的目标边际覆盖，同时显著减少了与现有最佳方法相比的无效性。
2501.02735	 | Sequence Complementor: Complementing Transformers For Time Series Forecasting with Learnable Sequences	 | Xiwen Chen,Peijie Qiu,Wenhui Zhu,Huayu Li,Hao Wang,Aristeidis Sotiras,Yalin Wang,Abolfazl Razi	 | 自引入以来，变压器模型通过捕捉时间序列中的全局依赖关系，逐渐取代了传统的RNN和MLP模型，而后续研究主要集中在改进令牌化和自注意力模块以适应时间序列的特殊挑战。本文提出了一种新的注意力机制——序列补全机制，并通过一种多样化的损失函数增强其实用性，实验结果表明其在长期和短期预测中显著优于最新方法。	 | Since its introduction, the transformer has shifted the development trajectory away from traditional models (e.g., RNN, MLP) in time series forecasting, which is attributed to its ability to capture global dependencies within temporal tokens. Follow-up studies have largely involved altering the tokenization and self-attention modules to better adapt Transformers for addressing special challenges like non-stationarity, channel-wise dependency, and variable correlation in time series. However, we found that the expressive capability of sequence representation is a key factor influencing Transformer performance in time forecasting after investigating several representative methods, where there is an almost linear relationship between sequence representation entropy and mean square error, with more diverse representations performing better. In this paper, we propose a novel attention mechanism with Sequence Complementors and prove feasible from an information theory perspective, where these learnable sequences are able to provide complementary information beyond current input to feed attention. We further enhance the Sequence Complementors via a diversification loss that is theoretically covered. The empirical evaluation of both long-term and short-term forecasting has confirmed its superiority over the recent state-of-the-art methods.	 | 自引入以来，变压器模型已经将时间序列预测的发展轨迹从传统的模型（例如RNN、MLP）转移开来，这归因于它能够捕捉时间序列中全局依赖关系的能力。后续的研究主要集中在对令牌化和自注意力模块进行修改，以便更好地适应时间序列中非平稳性、通道间依赖性和变量相关性等特殊挑战。然而，我们在研究了几种代表性方法之后发现，在时间预测中，序列表示的表达能力是影响变压器性能的关键因素之一，其中序列表示的熵与均方误差之间几乎呈线性关系，表示更为多样的序列表现更好。在本文中，我们提出了一种新的注意力机制——序列补全机制，并从信息论的角度证明了其可行性，其中这些可学习的序列能够提供超出当前输入的补充信息以供注意力使用。我们进一步通过一种多样化的损失来增强序列补全机制，该损失从理论上得到了覆盖。对长期和短期预测的实证评估表明，其性能明显优于最近的先进方法。
2501.02732	 | AFed: Algorithmic Fair Federated Learning	 | Huiqiang Chen,Tianqing Zhu,Wanlei Zhou,Wei Zhao	 | 联邦学习（FL）因其能够保护客户端隐私而受到关注，但同时也带来了公平性挑战。为解决这一问题，本文提出AFed框架，通过学习全局数据分布来增强客户端数据，从而促进群体公平性，实验证明其在多个基准方法上取得了显著改进。	 | Federated Learning (FL) has gained significant attention as it facilitates collaborative machine learning among multiple clients without centralizing their data on a server. FL ensures the privacy of participating clients by locally storing their data, which creates new challenges in fairness. Traditional debiasing methods assume centralized access to sensitive information, rendering them impractical for the FL setting. Additionally, FL is more susceptible to fairness issues than centralized machine learning due to the diverse client data sources that may be associated with group information. Therefore, training a fair model in FL without access to client local data is important and challenging. This paper presents AFed, a straightforward yet effective framework for promoting group fairness in FL. The core idea is to circumvent restricted data access by learning the global data distribution. This paper proposes two approaches: AFed-G, which uses a conditional generator trained on the server side, and AFed-GAN, which improves upon AFed-G by training a conditional GAN on the client side. We augment the client data with the generated samples to help remove bias. Our theoretical analysis justifies the proposed methods, and empirical results on multiple real-world datasets demonstrate a substantial improvement in AFed over several baselines.	 | 联邦学习（Federated Learning，FL）因其能够允许多个客户端在不集中数据到服务器的情况下进行协作机器学习而受到了广泛关注。FL通过本地存储客户端数据来确保参与客户端的隐私，这为公平性带来了新的挑战。传统的去偏方法假设可以集中访问敏感信息，这在FL环境中是不切实际的。此外，与集中式机器学习相比，FL更容易受到公平性问题的影响，因为多样化的客户端数据源可能会与群体信息相关联。因此，在不访问客户端本地数据的情况下训练一个公平的模型是重要且具有挑战性的。本文提出了AFed框架，这是一种简单而有效的促进FL中群体公平性的方法。核心理念是通过学习全局数据分布来绕过受限的数据访问。本文提出了两种方法：AFed-G，它在服务端训练一个条件生成器；AFed-GAN，它通过在客户端训练一个条件生成对抗网络（GAN）来改进AFed-G。我们通过添加生成样本来增强客户端数据，以帮助消除偏见。理论分析证明了所提出方法的有效性，而多个现实世界数据集上的实证结果表明，AFed在多个基准方法上取得了显著的改进。
2501.02728	 | OpenGU: A Comprehensive Benchmark for Graph Unlearning	 | Bowen Fan,Yuming Ai,Xunkai Li,Zhilin Guo,Rong-Hua Li,Guoren Wang	 | 图去学习（Graph Unlearning，GU）作为一种新兴技术，旨在高效地从图神经网络（GNNs）中移除敏感信息，而无需重新训练模型，从而保护隐私并确保合规性。为了提供一个公平的比较基准，并支持多样化的下游任务和去学习请求，我们提出了OpenGU，这是首个集成多种GU算法和多领域数据集的基准框架，有助于推动该领域的研究进展。	 | Graph Machine Learning is essential for understanding and analyzing relational data. However, privacy-sensitive applications demand the ability to efficiently remove sensitive information from trained graph neural networks (GNNs), avoiding the unnecessary time and space overhead caused by retraining models from scratch. To address this issue, Graph Unlearning (GU) has emerged as a critical solution, with the potential to support dynamic graph updates in data management systems and enable scalable unlearning in distributed data systems while ensuring privacy compliance. Unlike machine unlearning in computer vision or other fields, GU faces unique difficulties due to the non-Euclidean nature of graph data and the recursive message-passing mechanism of GNNs. Additionally, the diversity of downstream tasks and the complexity of unlearning requests further amplify these challenges. Despite the proliferation of diverse GU strategies, the absence of a benchmark providing fair comparisons for GU, and the limited flexibility in combining downstream tasks and unlearning requests, have yielded inconsistencies in evaluations, hindering the development of this domain. To fill this gap, we present OpenGU, the first GU benchmark, where 16 SOTA GU algorithms and 37 multi-domain datasets are integrated, enabling various downstream tasks with 13 GNN backbones when responding to flexible unlearning requests. Based on this unified benchmark framework, we are able to provide a comprehensive and fair evaluation for GU. Through extensive experimentation, we have drawn $8$ crucial conclusions about existing GU methods, while also gaining valuable insights into their limitations, shedding light on potential avenues for future research.	 | 图机器学习对于理解和分析关系数据至关重要。然而，涉及隐私的应用程序要求能够高效地从训练好的图神经网络（GNNs）中移除敏感信息，避免从头重新训练模型带来的不必要的时间和空间开销。为了解决这个问题，图去学习（Graph Unlearning，GU）作为一种关键解决方案逐渐兴起，它有可能支持数据管理系统中的动态图更新，并在分布式数据系统中实现可扩展的去学习，同时确保合规性。与计算机视觉或其他领域的机器去学习不同，GU 面临独特的挑战，因为图数据的非欧几里得性质以及 GNN 的递归消息传递机制。此外，下游任务的多样性以及去学习请求的复杂性进一步加剧了这些挑战。尽管存在多种多样化的 GU 策略，但由于缺乏提供公平比较的基准、以及在结合下游任务和去学习请求上的有限灵活性，导致评估结果不一致，阻碍了该领域的进展。为了填补这一空白，我们提出了 OpenGU，这是首个 GU 基准，它集成了 16 种最先进的 GU 算法和 37 个多领域数据集，能够以 13 种不同的 GNN 后端支持灵活的去学习请求下的各种下游任务。基于这一统一的基准框架，我们能够提供全面且公平的 GU 评估。通过广泛的实验，我们得出了关于现有 GU 方法的 8 条关键结论，同时也对这些方法的局限性有了宝贵的认识，为未来的研究指明了潜在的方向。
2501.02721	 | Learning Stochastic Nonlinear Dynamics with Embedded Latent Transfer Operators	 | Naichang Ke,Ryogo Tanaka,Yoshinobu Kawahara	 | 该研究提出了一种基于再生核希尔伯特空间中随机演变的算子表示的隐马尔可夫模型，并开发了一种谱方法来学习这种表示，同时探讨了其在序列状态估计和动态特征模态分解中的应用。通过合成数据和真实世界数据的实验，验证了该方法的有效性和性能。	 | We consider an operator-based latent Markov representation of a stochastic nonlinear dynamical system, where the stochastic evolution of the latent state embedded in a reproducing kernel Hilbert space is described with the corresponding transfer operator, and develop a spectral method to learn this representation based on the theory of stochastic realization. The embedding may be learned simultaneously using reproducing kernels, for example, constructed with feed-forward neural networks. We also address the generalization of sequential state-estimation (Kalman filtering) in stochastic nonlinear systems, and of operator-based eigen-mode decomposition of dynamics, for the representation. Several examples with synthetic and real-world data are shown to illustrate the empirical characteristics of our methods, and to investigate the performance of our model in sequential state-estimation and mode decomposition.	 | 我们考虑了一个基于算子的隐马尔可夫表示的随机非线性动力系统，其中隐状态嵌入在再生核希尔伯特空间中的随机演变用相应的传输算子来描述，并基于随机实现理论开发了一种谱方法来学习这种表示。嵌入可以通过使用再生核来同时学习，例如，通过使用前馈神经网络构建的再生核。我们还探讨了随机非线性系统的序列状态估计（卡尔曼滤波）和基于算子的动态特征模态分解在表示中的泛化。通过合成数据和真实世界数据的多个示例，展示了我们的方法的实证特征，并研究了我们的模型在序列状态估计和模态分解中的性能。
2501.02709	 | Horizon Generalization in Reinforcement Learning	 | Vivek Myers,Catherine Ji,Benjamin Eysenbach	 | 该研究从泛化角度探讨目标条件下的强化学习，旨在学习能够在不同视野范围内泛化的目标导向策略，这种泛化与规划的不变性密切相关，通过策略在接近目标时的表现来预测其在远处目标的表现。理论分析和实验结果支持了这种视野泛化的可能性，并为将机器学习中的不变性技术应用于强化学习提供了新的视角。	 | We study goal-conditioned RL through the lens of generalization, but not in the traditional sense of random augmentations and domain randomization. Rather, we aim to learn goal-directed policies that generalize with respect to the horizon: after training to reach nearby goals (which are easy to learn), these policies should succeed in reaching distant goals (which are quite challenging to learn). In the same way that invariance is closely linked with generalization is other areas of machine learning (e.g., normalization layers make a network invariant to scale, and therefore generalize to inputs of varying scales), we show that this notion of horizon generalization is closely linked with invariance to planning: a policy navigating towards a goal will select the same actions as if it were navigating to a waypoint en route to that goal. Thus, such a policy trained to reach nearby goals should succeed at reaching arbitrarily-distant goals. Our theoretical analysis proves that both horizon generalization and planning invariance are possible, under some assumptions. We present new experimental results and recall findings from prior work in support of our theoretical results. Taken together, our results open the door to studying how techniques for invariance and generalization developed in other areas of machine learning might be adapted to achieve this alluring property.	 | 我们从泛化的角度研究目标条件下的强化学习，但这并不是传统意义上的通过随机增强和领域随机化来泛化。相反，我们的目标是学习能够针对视野进行泛化的目标导向策略：在训练过程中，这些策略能够在接近的目标（相对容易学习）上表现出色，而在远处的目标（相对难以学习）上也应能够成功。就像在机器学习的其他领域中不变性与泛化密切相关（例如，归一化层使网络对尺度不变，从而能够处理不同尺度的输入）一样，我们证明了这种视野泛化与规划的不变性密切相关：一个朝向目标导航的策略会选择与它朝向通向目标的航点导航相同的动作。因此，训练这些策略以达到接近目标的策略应在达到任意远的目标时取得成功。我们的理论分析证明，在某些假设下，这两种类型的泛化和规划不变性都是可能的。我们提出了新的实验结果，并回顾了先前工作的发现来支持我们的理论结果。综合起来，我们的结果为研究如何将其他机器学习领域中发展起来的不变性和泛化技术适应于实现这一诱人特性的可能性打开了大门。
2501.02705	 | Knowledge Distillation with Adapted Weight	 | Sirong Wu,Xi Luo,Junjie Liu,Yuhui Deng	 | 该研究提出了一个名为KD-AIF的知识蒸馏框架，该框架通过自适应影响权重优化了学生模型的学习效率和泛化能力，并在多个基准上优于现有方法。KD-AIF不仅提高了模型的可解释性和透明度，还在半监督学习中表现出色。	 | Although large models have shown a strong capacity to solve large-scale problems in many areas including natural language and computer vision, their voluminous parameters are hard to deploy in a real-time system due to computational and energy constraints. Addressing this, knowledge distillation through Teacher-Student architecture offers a sustainable pathway to compress the knowledge of large models into more manageable sizes without significantly compromising performance. To enhance the robustness and interpretability of this framework, it is critical to understand how individual training data impact model performance, which is an area that remains underexplored. We propose the \textbf{Knowledge Distillation with Adaptive Influence Weight (KD-AIF)} framework which leverages influence functions from robust statistics to assign weights to training data, grounded in the four key SAFE principles: Sustainability, Accuracy, Fairness, and Explainability. This novel approach not only optimizes distillation but also increases transparency by revealing the significance of different data. The exploration of various update mechanisms within the KD-AIF framework further elucidates its potential to significantly improve learning efficiency and generalization in student models, marking a step toward more explainable and deployable Large Models. KD-AIF is effective in knowledge distillation while also showing exceptional performance in semi-supervised learning with outperforms existing baselines and methods in multiple benchmarks (CIFAR-100, CIFAR-10-4k, SVHN-1k, and GLUE).	 | 尽管大型模型在自然语言处理和计算机视觉等多个领域展现出了强大的解决大规模问题的能力，但由于计算和能源限制，它们庞大的参数使得在实时系统中部署变得困难。针对这一问题，通过师-生架构的知识蒸馏提供了压缩大型模型知识为更易于管理的规模的一种可持续途径，同时不显著牺牲性能。为了增强这一框架的稳健性和可解释性，理解单个训练数据如何影响模型性能至关重要，这是一个尚未得到充分探索的领域。我们提出了**带有自适应影响权重的知识蒸馏框架（KD-AIF）**，该框架利用稳健统计中的影响函数为训练数据分配权重，并基于可持续性、准确性、公平性和可解释性四个关键原则（SAFE）进行构建。这一新颖的方法不仅优化了知识蒸馏，还通过揭示不同数据的重要性增加了透明度。在KD-AIF框架中探索各种更新机制进一步阐明了其提高学生模型学习效率和泛化能力的巨大潜力，朝着更具可解释性和可部署性的大型模型迈出了重要一步。KD-AIF在知识蒸馏中有效，并在半监督学习中表现出色，在多个基准（CIFAR-100、CIFAR-10-4k、SVHN-1k 和 GLUE）上优于现有基准和方法。
2501.02704	 | Persistence of Backdoor-based Watermarks for Neural Networks: A Comprehensive Evaluation	 | Anh Tu Ngo,Chuan Song Heng,Nandish Chattopadhyay,Anupam Chattopadhyay	 | 近年来，虽然深度神经网络因其卓越性能受到广泛关注，但在微调场景下现有的后门水印方案仍然存在稳健性不确定性。本文提出了一种基于数据的新方法，在微调后恢复水印而不暴露触发集，实验表明在模型参数未发生剧烈变化时，水印可以恢复到100%的触发准确率。	 | Deep Neural Networks (DNNs) have gained considerable traction in recent years due to the unparalleled results they gathered. However, the cost behind training such sophisticated models is resource intensive, resulting in many to consider DNNs to be intellectual property (IP) to model owners. In this era of cloud computing, high-performance DNNs are often deployed all over the internet so that people can access them publicly. As such, DNN watermarking schemes, especially backdoor-based watermarks, have been actively developed in recent years to preserve proprietary rights. Nonetheless, there lies much uncertainty on the robustness of existing backdoor watermark schemes, towards both adversarial attacks and unintended means such as fine-tuning neural network models. One reason for this is that no complete guarantee of robustness can be assured in the context of backdoor-based watermark. In this paper, we extensively evaluate the persistence of recent backdoor-based watermarks within neural networks in the scenario of fine-tuning, we propose/develop a novel data-driven idea to restore watermark after fine-tuning without exposing the trigger set. Our empirical results show that by solely introducing training data after fine-tuning, the watermark can be restored if model parameters do not shift dramatically during fine-tuning. Depending on the types of trigger samples used, trigger accuracy can be reinstated to up to 100%. Our study further explores how the restoration process works using loss landscape visualization, as well as the idea of introducing training data in fine-tuning stage to alleviate watermark vanishing.	 | 近年来，深度神经网络（DNNs）因其无与伦比的结果而获得了广泛关注。然而，训练如此复杂的模型所需的资源成本高昂，导致许多人认为DNNs属于模型所有者的知识产权。在云计算盛行的时代，高性能DNNs常常部署在网络的各个角落，以便公众可以访问它们。因此，近年来积极开发了一种特别的DNN水印方案——后门水印，以保护知识产权。然而，现有后门水印方案在对抗攻击和意外手段（如微调神经网络模型）方面的稳健性存在很大不确定性。其中一个原因是，在后门水印的背景下，无法保证绝对的稳健性。在本文中，我们广泛评估了近年来后门水印在微调场景下的持久性。我们提出并开发了一种基于数据的新颖想法，在微调后恢复水印而不暴露触发集。我们的实验证据表明，仅通过在微调后引入训练数据，如果模型参数在微调过程中没有发生剧烈变化，水印是可以恢复的。根据使用的触发样本类型，触发准确率可以恢复到100%。进一步的研究通过损失景观可视化和在微调阶段引入训练数据来探讨恢复过程的工作原理，并提出了一种减轻水印消失的方法。
2501.02673	 | Exploring a Datasets Statistical Effect Size Impact on Model Performance, and Data Sample-Size Sufficiency	 | Arya Hatamian,Lionel Levine,Haniyeh Ehsani Oskouie,Majid Sarrafzadeh	 | 本文通过两项实验研究了基本描述性统计措施是否可以作为判断数据集在训练机器学习模型时充足性的指标，结果表明效果大小与模型性能及学习速率之间的关系并不能有效预测模型性能或适当样本大小，仍需进一步研究来更好地评估数据充足性。	 | Having a sufficient quantity of quality data is a critical enabler of training effective machine learning models. Being able to effectively determine the adequacy of a dataset prior to training and evaluating a model's performance would be an essential tool for anyone engaged in experimental design or data collection. However, despite the need for it, the ability to prospectively assess data sufficiency remains an elusive capability. We report here on two experiments undertaken in an attempt to better ascertain whether or not basic descriptive statistical measures can be indicative of how effective a dataset will be at training a resulting model. Leveraging the effect size of our features, this work first explores whether or not a correlation exists between effect size, and resulting model performance (theorizing that the magnitude of the distinction between classes could correlate to a classifier's resulting success). We then explore whether or not the magnitude of the effect size will impact the rate of convergence of our learning rate, (theorizing again that a greater effect size may indicate that the model will converge more rapidly, and with a smaller sample size needed). Our results appear to indicate that this is not an effective heuristic for determining adequate sample size or projecting model performance, and therefore that additional work is still needed to better prospectively assess adequacy of data.	 | 拥有足够的高质量数据是训练有效机器学习模型的关键因素。能够在训练和评估模型性能之前有效确定数据集的充足性，将会是对实验设计或数据收集人员而言的一项重要工具。然而，尽管需要这种能力，前瞻性地评估数据充足性仍是一项难以实现的能力。在这篇论文中，我们报告了两项实验的结果，旨在更清楚地了解基本描述性统计措施是否可以成为判断数据集在训练结果模型方面的有效性的指标。  利用我们特征的效果大小，这项工作首先探讨了效果大小与结果模型性能之间是否存在相关性（假设类间差异的大小能够与分类器的成功程度相关）。然后，我们探讨了效果大小的幅度是否会影响学习率的收敛速度（再次假设，较大的效果大小可能意味着模型将更快收敛，并且需要较少的样本量）。我们的结果似乎表明，这并不是一个有效的判断适当样本大小或预测模型性能的启发式方法，因此仍然需要更多的工作来更好地前瞻性地评估数据的充足性。
2501.02662	 | Incentive-Compatible Federated Learning with Stackelberg Game Modeling	 | Simin Javaherian,Bryce Turney,Li Chen,Nian-Feng Tzeng	 | FLamma是一种基于自适应伽马级数Stackelberg博弈的联邦学习框架，旨在解决异构环境中客户端资源差异导致的性能削弱问题，通过动态调整衰减因子来促进公平性而不牺牲整体模型性能或收敛速度。	 | Federated Learning (FL) has gained prominence as a decentralized machine learning paradigm, allowing clients to collaboratively train a global model while preserving data privacy. Despite its potential, FL faces significant challenges in heterogeneous environments, where varying client resources and capabilities can undermine overall system performance. Existing approaches primarily focus on maximizing global model accuracy, often at the expense of unfairness among clients and suboptimal system efficiency, particularly in non-IID (non-Independent and Identically Distributed) settings. In this paper, we introduce FLamma, a novel Federated Learning framework based on adaptive gamma-based Stackelberg game, designed to address the aforementioned limitations and promote fairness. Our approach allows the server to act as the leader, dynamically adjusting a decay factor while clients, acting as followers, optimally select their number of local epochs to maximize their utility. Over time, the server incrementally balances client influence, initially rewarding higher-contributing clients and gradually leveling their impact, driving the system toward a Stackelberg Equilibrium. Extensive simulations on both IID and non-IID datasets show that our method significantly improves fairness in accuracy distribution without compromising overall model performance or convergence speed, outperforming traditional FL baselines.	 | 联邦学习（Federated Learning，FL）作为一种分散式的机器学习范式，已经逐渐引起人们的关注。它允许客户端协同训练一个全局模型，同时保护数据隐私。尽管FL具有巨大的潜力，但在异构环境中，客户端资源和能力的差异可能会削弱整个系统的性能。现有的方法主要集中在最大化全局模型的准确性，这往往是以客户端之间的不公平和系统效率低下为代价的，尤其是在非IID（非独立同分布）设置中。本文中，我们提出了FLamma，一种基于自适应伽马级数Stackelberg博弈的新颖联邦学习框架，旨在解决上述问题并促进公平性。我们的方法允许服务器作为领导者，动态调整衰减因子，而客户端作为跟随者，根据其局部轮次数量最大化自身的效用。随着时间的推移，服务器逐步平衡客户端的影响，初始时奖励贡献较高的客户端，并逐渐平衡其影响，推动系统向Stackelberg均衡过渡。在IID和非IID数据集上的广泛仿真实验表明，我们的方法在不牺牲整体模型性能或收敛速度的情况下，显著提高了准确性分布的公平性，优于传统的FL基线方法。
2501.02652	 | A New Interpretation of the Certainty-Equivalence Approach for PAC Reinforcement Learning with a Generative Model	 | Shivaram Kalyanakrishnan,Sheel Shah,Santhosh Kumar Guguloth	 | 本文揭示了强化学习中著名的确定等效性方法（CEM）可以被视为轨迹树方法（TTM）的一种应用，并利用这一观点证明了CEM在样本复杂度上的改进上界，特别是在非平稳和平稳马尔可夫决策过程（MDP）中，同时提供了有限时间horizon MDP的样本复杂度下界，证明了CEM在小误差概率区间内的最小最大最优性。	 | Reinforcement learning (RL) enables an agent interacting with an unknown MDP $M$ to optimise its behaviour by observing transitions sampled from $M$. A natural entity that emerges in the agent's reasoning is $\widehat{M}$, the maximum likelihood estimate of $M$ based on the observed transitions. The well-known \textit{certainty-equivalence} method (CEM) dictates that the agent update its behaviour to $\widehatπ$, which is an optimal policy for $\widehat{M}$. Not only is CEM intuitive, it has been shown to enjoy minimax-optimal sample complexity in some regions of the parameter space for PAC RL with a generative model~\citep{Agarwal2020GenModel}.   A seemingly unrelated algorithm is the ``trajectory tree method'' (TTM)~\citep{Kearns+MN:1999}, originally developed for efficient decision-time planning in large POMDPs. This paper presents a theoretical investigation that stems from the surprising finding that CEM may indeed be viewed as an application of TTM. The qualitative benefits of this view are (1) new and simple proofs of sample complexity upper bounds for CEM, in fact under a (2) weaker assumption on the rewards than is prevalent in the current literature. Our analysis applies to both non-stationary and stationary MDPs. Quantitatively, we obtain (3) improvements in the sample-complexity upper bounds for CEM both for non-stationary and stationary MDPs, in the regime that the ``mistake probability'' $δ$ is small. Additionally, we show (4) a lower bound on the sample complexity for finite-horizon MDPs, which establishes the minimax-optimality of our upper bound for non-stationary MDPs in the small-$δ$ regime.	 | 强化学习（RL）使智能体能够在与一个未知的马尔可夫决策过程（MDP）$M$交互时，通过观察从$M$中采样的转换来优化其行为。在智能体的推理中自然地出现了一个实体$\widehat{M}$，它是基于观测到的转换的最大似然估计。著名的“确定等效性”方法（CEM）规定，智能体应将其行为更新为$\widehat{\pi}$，这是$\widehat{M}$的最优策略。不仅CEM直观易懂，它还在某些区域的参数空间中被证明在带有生成模型的PAC RL中具有最小最大最优样本复杂度~\citep{Agarwal2020GenModel}。一个看似无关的算法是“轨迹树方法”（TTM）~\citep{Kearns+MN:1999}，最初是为了在大型部分可观测马尔可夫决策过程（POMDP）中高效地进行决策时的规划而开发的。本文提出了一项理论调查，其基础是出乎意料的发现，即CEM可以被视为TTM的应用。这一观点的质的益处包括：（1）为CEM提供了新的简单证明，证明其样本复杂度上界，实际上是在比现有文献中更弱的奖励假设下。（2）我们的分析适用于非平稳和平稳MDP。定量上，我们得到了CEM在非平稳和平稳MDP中的样本复杂度上界改进，在错误概率$δ$较小的区间内。此外，我们展示了（3）有限时间horizon MDP的样本复杂度下界，这确立了在小$δ$区间内，我们上界对于非平稳MDP的最小最大最优性。
2501.02648	 | Representation Learning of Lab Values via Masked AutoEncoder	 | David Restrepo,Chenwei Wu,Yueran Jia,Jaden K. Sun,Jack Gallifant,Catherine G. Bielick,Yugang Jia,Leo A. Celi	 | 该研究提出了一种名为Lab-MAE的新颖变压器基掩码自编码框架，用于插补电子健康记录中的缺失实验室值，并在MIMIC-IV数据集上显著优于现有方法，特别是在公平性方面表现出色。研究表明，Lab-MAE能够有效处理复杂的时序依赖性和上下文依赖性，并且其环境影响较低。	 | Accurate imputation of missing laboratory values in electronic health records (EHRs) is critical to enable robust clinical predictions and reduce biases in AI systems in healthcare. Existing methods, such as variational autoencoders (VAEs) and decision tree-based approaches such as XGBoost, struggle to model the complex temporal and contextual dependencies in EHR data, mainly in underrepresented groups. In this work, we propose Lab-MAE, a novel transformer-based masked autoencoder framework that leverages self-supervised learning for the imputation of continuous sequential lab values. Lab-MAE introduces a structured encoding scheme that jointly models laboratory test values and their corresponding timestamps, enabling explicit capturing temporal dependencies. Empirical evaluation on the MIMIC-IV dataset demonstrates that Lab-MAE significantly outperforms the state-of-the-art baselines such as XGBoost across multiple metrics, including root mean square error (RMSE), R-squared (R2), and Wasserstein distance (WD). Notably, Lab-MAE achieves equitable performance across demographic groups of patients, advancing fairness in clinical predictions. We further investigate the role of follow-up laboratory values as potential shortcut features, revealing Lab-MAE's robustness in scenarios where such data is unavailable. The findings suggest that our transformer-based architecture, adapted to the characteristics of the EHR data, offers a foundation model for more accurate and fair clinical imputation models. In addition, we measure and compare the carbon footprint of Lab-MAE with the baseline XGBoost model, highlighting its environmental requirements.	 | 在电子健康记录（EHRs）中准确插补缺失的实验室值对于实现稳健的临床预测和减少AI系统中的偏差至关重要。现有方法，如变分自编码器（VAEs）和决策树方法（如XGBoost），难以建模EHR数据中的复杂时间依赖性和上下文依赖性，尤其是在代表性不足的群体中。在这项工作中，我们提出了一种名为Lab-MAE的新型变压器基掩码自编码框架，利用自监督学习来插补连续的序列实验室值。Lab-MAE引入了一种结构化的编码方案，联合建模实验室测试值及其相应的时间戳，使时间依赖性的明确捕获成为可能。在MIMIC-IV数据集上的实证评估表明，Lab-MAE在多个指标上，包括均方根误差（RMSE）、决定系数（R²）和Wasserstein距离（WD）上，显著优于当前最先进的基线方法，如XGBoost。值得注意的是，Lab-MAE在不同患者人群的统计群体中实现了公平的表现，推进了临床预测中的公平性。我们进一步探讨了随访实验室值作为潜在捷径特征的作用，揭示了Lab-MAE在缺乏此类数据的情况下仍具有鲁棒性。研究结果表明，我们的变压器架构适应EHR数据的特性，为更准确和公平的临床插补模型奠定了基础。此外，我们测量并比较了Lab-MAE和基线XGBoost模型的碳足迹，突显了其环境要求。
2501.02625	 | HALO: Hadamard-Assisted Lossless Optimization for Efficient Low-Precision LLM Training and Fine-Tuning	 | Saleh Ashkboos,Mahdi Nikdan,Soroush Tabesh,Roberto L. Castro,Torsten Hoefler,Dan Alistarh	 | HALO是一种新颖的量化感知训练方法，通过结合哈达玛旋转、FSDP集成和高性能内核支持，在Transformer模型中实现了准确且高效的低精度训练，能够在LLAMA家族模型上实现接近全精度的结果并获得最高1.31倍的端到端加速。	 | Quantized training of Large Language Models (LLMs) remains an open challenge, as maintaining accuracy while performing all matrix multiplications in low precision has proven difficult. This is particularly the case when fine-tuning pre-trained models, which often already have large weight and activation outlier values that render quantized optimization difficult. We present HALO, a novel quantization-aware training approach for Transformers that enables accurate and efficient low-precision training by combining 1) strategic placement of Hadamard rotations in both forward and backward passes, to mitigate outliers during the low-precision computation, 2) FSDP integration for low-precision communication, and 3) high-performance kernel support. Our approach ensures that all large matrix multiplications during the forward and backward passes are executed in lower precision. Applied to LLAMA-family models, HALO achieves near-full-precision-equivalent results during fine-tuning on various tasks, while delivering up to 1.31x end-to-end speedup for full fine-tuning on RTX 4090 GPUs. Our method supports both standard and parameter-efficient fine-tuning (PEFT) methods, both backed by efficient kernel implementations. Our results demonstrate the first practical approach to fully quantized LLM fine-tuning that maintains accuracy in FP8 precision, while delivering performance benefits.	 | 大规模语言模型（LLMs）的量化训练仍然是一个开放性的挑战，因为在低精度下执行所有矩阵乘法以保持准确性一直是一个难题。特别是在微调预训练模型时，这种情况尤为突出，因为这些模型往往已经有了很大的权重和激活异常值，使得量化优化变得困难。我们提出了HALO，这是一种新颖的量化感知训练方法，用于Transformer模型，通过结合以下三种方式实现准确且高效的低精度训练：1）在正向和反向传播过程中战略性地放置哈达玛旋转，以在低精度计算中缓解异常值，2）集成FSDP以实现低精度通信，3）支持高性能内核。我们的方法确保在正向和反向传播过程中所有的大矩阵乘法都在较低的精度下执行。将HALO应用于LLAMA家族模型时，该方法在各种任务上的微调过程中可以达到接近全精度的结果，同时在RTX 4090 GPU上实现最高可达1.31倍的端到端加速。我们的方法同时支持标准和参数高效微调（PEFT）方法，并且这两种方法都有高效的内核实现。我们的结果展示了第一个实用的全量化LLM微调方法，它可以保持在FP8精度下准确，并提供性能优势。
2501.02616	 | Multi-layer Radial Basis Function Networks for Out-of-distribution Detection	 | Amol Khanna,Chenyi Ling,Derek Everett,Edward Raff,Nathan Inkawhich	 | 本文提出了一种多层径向基函数网络（MLRBFN），将分类和离分布外（OOD）检测结合为一体，并通过引入“抑郁机制”确保其适用于OOD检测，展示了在常见OOD检测方法中的竞争力，为OOD检测提供了新的研究方向。	 | Existing methods for out-of-distribution (OOD) detection use various techniques to produce a score, separate from classification, that determines how ``OOD'' an input is. Our insight is that OOD detection can be simplified by using a neural network architecture which can effectively merge classification and OOD detection into a single step. Radial basis function networks (RBFNs) inherently link classification confidence and OOD detection; however, these networks have lost popularity due to the difficult of training them in a multi-layer fashion. In this work, we develop a multi-layer radial basis function network (MLRBFN) which can be easily trained. To ensure that these networks are also effective for OOD detection, we develop a novel depression mechanism. We apply MLRBFNs as standalone classifiers and as heads on top of pretrained feature extractors, and find that they are competitive with commonly used methods for OOD detection. Our MLRBFN architecture demonstrates a promising new direction for OOD detection methods.	 | 现有的用于离分布外(OOD)检测的方法使用各种技术产生一个与分类分离的分数，以确定输入的OOD程度。我们的观点是，通过使用一个能够有效将分类和OOD检测合并为一步的神经网络架构，可以简化OOD检测。径向基函数网络(RBFN)本质上将分类置信度和OOD检测联系在一起；然而，由于难以多层训练这些网络，它们已经失去了流行性。在本工作中，我们开发了一种易于训练的多层径向基函数网络（MLRBFN）。为了确保这些网络也适用于OOD检测，我们开发了一种新颖的“抑郁机制”。我们将MLRBFN作为独立分类器和预训练特征提取器的头部应用，并发现它们在常见的OOD检测方法中具有竞争力。我们的MLRBFN架构展示了OOD检测方法的一个有希望的新方向。
2501.02612	 | Chameleon2++: An Efficient Chameleon2 Clustering with Approximate Nearest Neighbors	 | Priyanshu Singh,Kapil Ahuja	 | Chameleon2 是一种改进的层次聚类算法，但在复杂度和透明度方面存在缺陷；通过改进 \(k\)-NN 搜索方法，提出的新版本 Chameleon2++ 将复杂度降低至 \(O(n\log{n})\) 并显著提高了效率和性能。	 | Clustering algorithms are fundamental tools in data analysis, with hierarchical methods being particularly valuable for their flexibility. Chameleon is a widely used hierarchical clustering algorithm that excels at identifying high-quality clusters of arbitrary shapes, sizes, and densities. Chameleon2 is the most recent variant that has demonstrated significant improvements, but suffers from critical failings and there are certain improvements that can be made.   The first failure we address is that the complexity of Chameleon2 is claimed to be $O(n^2)$, while we demonstrate that it is actually $O(n^2\log{n})$, with $n$ being the number of data points. Furthermore, we suggest improvements to Chameleon2 that ensure that the complexity remains $O(n^2)$ with minimal to no loss of performance. The second failing of Chameleon2 is that it lacks transparency and it does not provide the fine-tuned algorithm parameters used to obtain the claimed results. We meticulously provide all such parameter values to enhance replicability.   The improvement which we make in Chameleon2 is that we replace the exact $k$-NN search with an approximate $k$-NN search. This further reduces the algorithmic complexity down to $O(n\log{n})$ without any performance loss. Here, we primarily configure three approximate nearest neighbor search algorithms (Annoy, FLANN and NMSLIB) to align with the overarching Chameleon2 clustering framework. Experimental evaluations on standard benchmark datasets demonstrate that the proposed Chameleon2++ algorithm is more efficient, robust, and computationally optimal.	 | 聚类算法是数据分析中的基本工具，层次聚类方法因其灵活性而尤其有价值。Chameleon 是一种广泛使用的层次聚类算法，能够高效地识别任意形状、大小和密度的高质量聚类。Chameleon2 是 Chameleon 的最新变体，展示了显著的改进，但存在一些严重的问题，并且还有改进的空间。我们首先解决的一个问题是，Chameleon2 的复杂度被宣称是 \(O(n^2)\)，而我们证明其实际复杂度为 \(O(n^2\log{n})\)，其中 \(n\) 是数据点的数量。此外，我们提出了一些建议以确保复杂度保持在 \(O(n^2)\)，同时几乎不会损失性能。Chameleon2 的第二个问题是缺乏透明度，且未提供用于获得宣称结果的精确算法参数。我们详细提供了所有这些参数值以增强可再现性。   我们对 Chameleon2 进行的改进是将精确的 \(k\)-NN 搜索替换为近似的 \(k\)-NN 搜索，这进一步将算法复杂度降低至 \(O(n\log{n})\)，同时没有任何性能损失。在此过程中，我们主要配置了三种近似最近邻搜索算法（Annoy、FLANN 和 NMSLIB），以与 Chameleon2 的整体聚类框架相一致。在标准基准数据集上的实验评估表明，提出的 Chameleon2++ 算法在效率、稳健性和计算优化方面更具优势。
2501.02573	 | LeetDecoding: A PyTorch Library for Exponentially Decaying Causal Linear Attention with CUDA Implementations	 | Jiaping Wang,Simiao Zhang,Qiao-Chu He,Yifan Chen	 | LeetDecoding是一个Python包，提供了指数衰减因果线性注意力的基本操作的大量计算实现，旨在简化大语言模型中这种新颖注意力机制的研究和集成。该包填补了当前在理解和高效计算此操作方面的空白，并通过易于使用的接口提升了对机器学习和数据科学社区的支持。	 | The machine learning and data science community has made significant while dispersive progress in accelerating transformer-based large language models (LLMs), and one promising approach is to replace the original causal attention in a generative pre-trained transformer (GPT) with \emph{exponentially decaying causal linear attention}. In this paper, we present LeetDecoding, which is the first Python package that provides a large set of computation routines for this fundamental operator. The launch of LeetDecoding was motivated by the current lack of (1) clear understanding of the complexity regarding this operator, (2) a comprehensive collection of existing computation methods (usually spread in seemingly unrelated fields), and (3) CUDA implementations for fast inference on GPU. LeetDecoding's design is easy to integrate with existing linear-attention LLMs, and allows for researchers to benchmark and evaluate new computation methods for exponentially decaying causal linear attention. The usage of LeetDecoding does not require any knowledge of GPU programming and the underlying complexity analysis, intentionally making LeetDecoding accessible to LLM practitioners. The source code of LeetDecoding is provided at \href{https://github.com/Computational-Machine-Intelligence/LeetDecoding}{this GitHub repository}, and users can simply install LeetDecoding by the command \texttt{pip install leet-decoding}.	 | 机器学习和数据科学社区在加速基于变换器的大语言模型（LLMs）方面取得了显著但分散的进展，一种有前途的方法是将生成预训练变换器（GPT）中的原始因果注意力替换为\emph{指数衰减因果线性注意力}。在本文中，我们介绍了LeetDecoding，这是首个提供该基本操作大量计算程序的Python包。LeetDecoding的推出主要是由于当前缺乏以下几点：（1）对该操作复杂性的清晰理解，（2）现有计算方法的全面集合（通常分散在看似无关的领域），以及（3）针对GPU快速推理的CUDA实现。LeetDecoding的设计易于与现有的线性注意力LLMs集成，并允许研究人员基准测试和评估新的指数衰减因果线性注意力计算方法。使用LeetDecoding不需要任何GPU编程和底层复杂性分析的知识，旨在使LeetDecoding对LLM从业者更加易用。LeetDecoding的源代码可在\href{https://github.com/Computational-Machine-Intelligence/LeetDecoding}{GitHub仓库}中找到，用户只需通过命令\texttt{pip install leet-decoding}即可安装LeetDecoding。
2501.02565	 | Efficient Graph Condensation via Gaussian Process	 | Lin Wang,Qing Li	 | 本文提出了一种新的图凝缩方法——高斯过程图凝缩（GCGP），该方法通过高斯过程估计凝缩后图的预测后验分布，从而有效减少大型图数据的规模并保持性能，同时解决了传统方法依赖双层优化带来的训练效率低下的问题。	 | Graph condensation reduces the size of large graphs while preserving performance, addressing the scalability challenges of Graph Neural Networks caused by computational inefficiencies on large datasets. Existing methods often rely on bi-level optimization, requiring extensive GNN training and limiting their scalability. To address these issues, this paper proposes Graph Condensation via Gaussian Process (GCGP), a novel and computationally efficient approach to graph condensation. GCGP utilizes a Gaussian Process (GP), with the condensed graph serving as observations, to estimate the posterior distribution of predictions. This approach eliminates the need for the iterative and resource-intensive training typically required by GNNs. To enhance the capability of the GCGP in capturing dependencies between function values, we derive a specialized covariance function that incorporates structural information. This covariance function broadens the receptive field of input nodes by local neighborhood aggregation, thereby facilitating the representation of intricate dependencies within the nodes. To address the challenge of optimizing binary structural information in condensed graphs, Concrete random variables are utilized to approximate the binary adjacency matrix in a continuous counterpart. This relaxation process allows the adjacency matrix to be represented in a differentiable form, enabling the application of gradient-based optimization techniques to discrete graph structures. Experimental results show that the proposed GCGP method efficiently condenses large-scale graph data while preserving predictive performance, addressing the scalability and efficiency challenges. The implementation of our method is publicly available at https://github.com/WANGLin0126/GCGP.	 | 图凝缩可以减少大型图的规模同时保持性能，从而解决由于在大数据集上计算效率低下而导致的图神经网络的可扩展性挑战。现有方法往往依赖于双层优化，这需要大量的GNN训练并限制了它们的可扩展性。为了解决这些问题，本文提出了通过高斯过程（GCGP）进行图凝缩的新颖且计算高效的图凝缩方法。GCGP 使用高斯过程（GP），其中凝缩后的图作为观测值，来估计预测的后验分布。这种方法消除了GNN通常所需的迭代和资源密集型训练的需求。为了增强GCGP在捕获函数值之间的依赖性方面的能力，我们推导出一种特殊的协方差函数，该函数包含了结构信息。这种协方差函数通过局部邻域聚合扩展了输入节点的感受野，从而有助于节点内部复杂依赖性的表示。为了应对凝缩图中二元结构信息优化的挑战，我们利用连续对偶中的Concrete随机变量来近似二元邻接矩阵。这种松弛过程允许邻接矩阵以可微形式表示，从而允许对离散图结构应用基于梯度的优化技术。实验结果表明，提出的GCGP方法能够高效地凝缩大规模的图数据，同时保持预测性能，解决了可扩展性和效率方面的挑战。我们的方法的实现可以在 https://github.com/WANGLin0126/GCGP 公开获得。
2501.02548	 | AMM: Adaptive Modularized Reinforcement Model for Multi-city Traffic Signal Control	 | Zherui Huang,Yicheng Liu,Chumeng Liang,Guanjie Zheng	 | 本文提出了一种自适应模块化模型（AMM），通过模块化交通信号控制问题和网络模型，并利用元学习聚合多城市经验，以减少交互次数和训练成本，提高在新环境中的性能。实验结果表明，AMM在不同城市中均能实现优异的性能，并优于现有方法。	 | Traffic signal control (TSC) is an important and widely studied direction. Recently, reinforcement learning (RL) methods have been used to solve TSC problems and achieve superior performance over conventional TSC methods. However, applying RL methods to the real world is challenging due to the huge cost of experiments in real-world traffic environments. One possible solution is TSC domain adaptation, which adapts trained models to target environments and reduces the number of interactions and the training cost. However, existing TSC domain adaptation methods still face two major issues: the lack of consideration for differences across cities and the low utilization of multi-city data.   To solve aforementioned issues, we propose an approach named Adaptive Modularized Model (AMM). By modularizing TSC problems and network models, we overcome the challenge of possible changes in environmental observations. We also aggregate multi-city experience through meta-learning. We conduct extensive experiments on different cities and show that AMM can achieve excellent performance with limited interactions in target environments and outperform existing methods. We also demonstrate the feasibility and generalizability of our method.	 | 交通信号控制（TSC）是一个重要且广泛研究的方向。近年来，强化学习（RL）方法被用于解决TSC问题，并在性能上超过了传统的TSC方法。然而，将RL方法应用于现实世界是一个挑战，因为在真实的交通环境中进行实验的成本巨大。一种可能的解决方案是TSC领域适应，它能够将训练好的模型适应目标环境，并减少交互次数和训练成本。然而，现有的TSC领域适应方法仍然面临两个主要问题：忽视了不同城市之间的差异以及对多城市数据的低利用。  为了解决上述问题，我们提出了一种名为自适应模块化模型（AMM）的方法。通过模块化TSC问题和网络模型，我们克服了环境观察可能发生变化的挑战。我们还利用元学习聚合了多城市的经验。我们在不同的城市进行了广泛的实验，并展示了AMM能够在目标环境中以有限的交互次数实现出色性能，并且优于现有的方法。我们还证明了我们方法的可行性和通用性。
2501.02535	 | A completely uniform transformer for parity	 | Alexander Kozachinskiy,Tomasz Steifer	 | 该研究构建了一个三层的常维变换器，用于识别偶数语言，其参数矩阵和位置编码不依赖于输入长度，改进了Chiang和Cholak依赖于输入长度的位置编码但仅包含两层的方法。	 | We construct a 3-layer constant-dimension transformer, recognizing the parity language, where neither parameter matrices nor the positional encoding depend on the input length. This improves upon a construction of Chiang and Cholak who use a positional encoding, depending on the input length (but their construction has 2 layers).	 | 我们构建了一个三层的常维变换器，该变换器可以识别偶数语言，其中的参数矩阵和位置编码都不依赖于输入长度。这改进了 Chiang 和 Cholak 的构建方法，他们在其中使用的位置编码依赖于输入长度（但他们的构造仅包含两层）。
2501.02508	 | PTEENet: Post-Trained Early-Exit Neural Networks Augmentation for Inference Cost Optimization	 | Assaf Lahiany,Yehudit Aperstein	 | 本文提出了一种通过引入“捷径”跳过昂贵的前向传递计算来减少深度神经网络推理成本的方法，同时保持合理准确度，并通过调整分支的置信度阈值来实时控制模型的速度与准确性的权衡。	 | For many practical applications, a high computational cost of inference over deep network architectures might be unacceptable. A small degradation in the overall inference accuracy might be a reasonable price to pay for a significant reduction in the required computational resources. In this work, we describe a method for introducing "shortcuts" into the DNN feedforward inference process by skipping costly feedforward computations whenever possible. The proposed method is based on the previously described BranchyNet (Teerapittayanon et al., 2016) and the EEnet (Demir, 2019) architectures that jointly train the main network and early exit branches. We extend those methods by attaching branches to pre-trained models and, thus, eliminating the need to alter the original weights of the network. We also suggest a new branch architecture based on convolutional building blocks to allow enough training capacity when applied on large DNNs. The proposed architecture includes confidence heads that are used for predicting the confidence level in the corresponding early exits. By defining adjusted thresholds on these confidence extensions, we can control in real-time the amount of data exiting from each branch and the overall tradeoff between speed and accuracy of our model. In our experiments, we evaluate our method using image datasets (SVHN and CIFAR10) and several DNN architectures (ResNet, DenseNet, VGG) with varied depth. Our results demonstrate that the proposed method enables us to reduce the average inference computational cost and further controlling the tradeoff between the model accuracy and the computation cost.	 | 在许多实际应用中，深度网络架构的推理计算成本可能不可接受。整体推理准确度略有降低可能是一个合理的代价，以换取显著减少所需的计算资源。在这项工作中，我们介绍了一种方法，通过在可能的情况下跳过昂贵的前向传递计算，在DNN的前向推理过程中引入“捷径”。该方法基于先前描述的BranchyNet（Teerapittayanon等人，2016年）和EEnet（Demir，2019年）架构，这些架构联合训练主网络和早期退出分支。我们通过将分支连接到预训练模型上，从而消除修改网络原始权重的需要。我们还提出了一种新的分支架构，基于卷积模块，以便在应用于大型DNN时有足够的训练容量。所提出的架构包含置信度头，用于预测相应早期退出的置信水平。通过在这些置信度扩展上定义调整后的阈值，可以在实时控制从每个分支退出的数据量以及模型速度与准确性的整体权衡。在我们的实验中，我们使用图像数据集（SVHN和CIFAR10）和不同的DNN架构（ResNet、DenseNet、VGG）进行了评估，这些架构具有不同的深度。我们的结果表明，所提出的方法使我们能够减少平均推理计算成本，并进一步控制模型准确性和计算成本之间的权衡。
2501.02481	 | The Meta-Representation Hypothesis	 | Zhengpeng Xie,Jiahang Cao,Qiang Zhang,Jianxiong Zhang,Changwei Wang,Renjing Xu	 | 该研究展示了如何通过学习元表示来提高强化学习代理的泛化能力，并且认为代理之间的深度互学习有助于达到这种元表示，从而改善其在面对任务变化时的表现。	 | Humans rely on high-level meta-representations to engage in abstract reasoning. In complex cognitive tasks, these meta-representations help individuals abstract general rules from experience. However, constructing such meta-representations from high-dimensional observations remains a longstanding challenge for reinforcement learning agents. For instance, a well-trained agent often fails to generalize to even minor variations of the same task, such as changes in background color, while humans can easily handle. In this paper, we build a bridge between meta-representation and generalization, showing that generalization performance benefits from meta-representation learning. We also hypothesize that deep mutual learning (DML) among agents can help them converge to meta-representations. Empirical results provide support for our theory and hypothesis. Overall, this work provides a new perspective on the generalization of deep reinforcement learning.	 | 人类依赖高层次的元表示来进行抽象推理。在复杂的认知任务中，这些元表示帮助个体从经验中提炼出通用规则。然而，从高维观察中构建这样的元表示仍然是强化学习代理的一项长期挑战。例如，一个训练有素的代理经常无法泛化到同一任务的微小变化，如背景颜色的变化，而人类可以轻易地处理。在本文中，我们建立了一个从元表示到泛化的桥梁，表明元表示学习可以提高泛化性能。我们还假设代理之间的深度互学习（DML）可以帮助它们收敛到元表示。实验证据支持了我们的理论和假设。总体而言，这项工作为深度强化学习的泛化提供了一个新的视角。
2501.02477	 | A Deep Positive-Negative Prototype Approach to Integrated Prototypical Discriminative Learning	 | Ramin Zarei-Sabzevar,Ahad Harati	 | 本文提出了一种深度正负原型（DPNP）模型，将原型学习与区分性方法结合，通过统一类原型与权重向量建立结构化的潜在空间，从而实现类内紧凑性和类间距离的最大化，提升分类准确性和可解释性。	 | This paper proposes a novel Deep Positive-Negative Prototype (DPNP) model that combines prototype-based learning (PbL) with discriminative methods to improve class compactness and separability in deep neural networks. While PbL traditionally emphasizes interpretability by classifying samples based on their similarity to representative prototypes, it struggles with creating optimal decision boundaries in complex scenarios. Conversely, discriminative methods effectively separate classes but often lack intuitive interpretability. Toward exploiting advantages of these two approaches, the suggested DPNP model bridges between them by unifying class prototypes with weight vectors, thereby establishing a structured latent space that enables accurate classification using interpretable prototypes alongside a properly learned feature representation. Based on this central idea of unified prototype-weight representation, Deep Positive Prototype (DPP) is formed in the latent space as a representative for each class using off-the-shelf deep networks as feature extractors. Then, rival neighboring class DPPs are treated as implicit negative prototypes with repulsive force in DPNP, which push away DPPs from each other. This helps to enhance inter-class separation without the need for any extra parameters. Hence, through a novel loss function that integrates cross-entropy, prototype alignment, and separation terms, DPNP achieves well-organized feature space geometry, maximizing intra-class compactness and inter-class margins. We show that DPNP can organize prototypes in nearly regular positions within feature space, such that it is possible to achieve competitive classification accuracy even in much lower-dimensional feature spaces. Experimental results on several datasets demonstrate that DPNP outperforms state-of-the-art models, while using smaller networks.	 | 本文提出了一种新颖的深度正负原型（DPNP）模型，该模型将原型学习（PbL）与区分性方法相结合，以提高深度神经网络中的类别紧凑性和可分性。传统上，PbL 强调可解释性，通过将样本分类为代表性原型的相似性来进行分类，但在复杂场景中创建最优决策边界的方面存在困难。相反，区分性方法虽然能有效地区分类别，但往往缺乏直观的解释性。为了利用这两种方法的优势，建议的 DPNP 模型通过将类原型与权重向量统一起来，建立了结构化的潜在空间，在该空间中使用可解释的原型和适当学习的特征表示进行准确分类。基于这种统一的原型-权重表示的中心思想，在潜在空间中使用现成的深度网络作为特征提取器形成每个类别的深度正原型（DPP）。然后，在 DPNP 中，竞争相邻类别的 DPP 被视为具有排斥力的隐式负原型，推动 DPP 互相远离。这有助于增强类别间分离，而无需额外的参数。因此，通过集成了交叉熵、原型对齐和分离项的新损失函数，DPNP 实现了组织良好的特征空间几何结构，最大化了类内紧凑性和类间距离。我们展示了 DPNP 可以在特征空间内将原型组织成几乎规则的位置，使得即使在低维特征空间中也能实现具有竞争力的分类准确性。在多个数据集上的实验结果表明，DPNP 在使用较小的网络时优于现有最先进的模型。
2501.02438	 | Efficient Deployment of Large Language Models on Resource-constrained Devices	 | Zhiwei Yao,Yang Xu,Hongli Xu,Yunming Liao,Zuan Xie	 | FedSpine是一种结合参数高效微调和结构化剪枝的联邦学习框架，旨在提高在资源受限设备上部署大型语言模型的效率和准确性，实验结果表明其在微调速度和最终准确率上均优于其他基准方法。	 | Deploying Large Language Models (LLMs) on resource-constrained (or weak) devices presents significant challenges due to limited resources and heterogeneous data distribution. To address the data concern, it is necessary to fine-tune LLMs using on-device private data for various downstream tasks. While Federated Learning (FL) offers a promising privacy-preserving solution, existing fine-tuning methods retain the original LLM size, leaving issues of high inference latency and excessive memory demands unresolved. Hence, we design FedSpine, an FL framework that combines Parameter- Efficient Fine-Tuning (PEFT) with structured pruning for efficient deployment of LLMs on resource-constrained devices. Specifically, FedSpine introduces an iterative process to prune and tune the parameters of LLMs. To mitigate the impact of device heterogeneity, an online Multi-Armed Bandit (MAB) algorithm is employed to adaptively determine different pruning ratios and LoRA ranks for heterogeneous devices without any prior knowledge of their computing and communication capabilities. As a result, FedSpine maintains higher inference accuracy while improving fine-tuning efficiency. Experimental results conducted on a physical platform with 80 devices demonstrate that FedSpine can speed up fine-tuning by 1.4$\times$-6.9$\times$ and improve final accuracy by 0.4%-4.5% under the same sparsity level compared to other baselines.	 | 在资源受限（或较弱）设备上部署大型语言模型（LLMs）面临着显著挑战，由于资源有限和数据分布的异质性。为了应对数据方面的担忧，有必要使用设备上的私人数据对LLMs进行微调以适应各种下游任务。尽管联邦学习（FL）提供了一种有前景的隐私保护解决方案，但现有的微调方法保留了原始LLM的大小，未能解决高推理延迟和过高的内存需求问题。因此，我们设计了FedSpine，这是一种结合参数高效微调（PEFT）和结构化剪枝的FL框架，以便在资源受限设备上高效部署LLMs。具体而言，FedSpine引入了一个迭代过程来剪枝和调整LLMs的参数。为应对设备异质性的影响，我们使用在线多臂bandit（MAB）算法来适应地确定不同设备的剪枝比率和LoRA秩，而无需先了解它们的计算和通信能力。因此，FedSpine在保持更高的推理准确性的同时，提高了微调的效率。在物理平台上使用80个设备进行的实验结果表明，与其它基准方法相比，在相同的稀疏度水平下，FedSpine可以在微调速度上提高1.4倍至6.9倍，并且最终准确率提高0.4%至4.5%。
2501.02436	 | An Analysis Framework for Understanding Deep Neural Networks Based on Network Dynamics	 | Yuchen Lin,Yong Zhang,Sihan Feng,Hong Zhao	 | 本文提出了一种基于神经元动态性的简单分析框架，揭示了深度神经网络通过优化神经元比例来最大化信息提取的方式，并通过训练样本的吸引盆地来表征泛化能力，从而统一解释了“平坦极小值效应”、“grokking”和“双下降现象”等DNN基本行为。	 | Advancing artificial intelligence demands a deeper understanding of the mechanisms underlying deep learning. Here, we propose a straightforward analysis framework based on the dynamics of learning models. Neurons are categorized into two modes based on whether their transformation functions preserve order. This categorization reveals how deep neural networks (DNNs) maximize information extraction by rationally allocating the proportion of neurons in different modes across deep layers. We further introduce the attraction basins of the training samples in both the sample vector space and the weight vector space to characterize the generalization ability of DNNs. This framework allows us to identify optimal depth and width configurations, providing a unified explanation for fundamental DNN behaviors such as the "flat minima effect," "grokking," and double descent phenomena. Our analysis extends to networks with depths up to 100 layers.	 | 推进人工智能的发展需要更深入地理解深度学习背后的机理。在这里，我们提出了一种基于学习模型动态性的简单分析框架。我们将神经元分为两种模式，根据它们的变换函数是否保持顺序来分类。这种分类揭示了深度神经网络（DNNs）通过合理分配不同模式下神经元的比例来最大化信息提取的方式。此外，我们引入了训练样本在样本向量空间和权重向量空间中的吸引盆地来表征DNNs的泛化能力。这种框架使我们能够识别最优的深度和宽度配置，为我们提供了对DNN基本行为如“平坦极小值效应”、“grokking”和“双下降现象”的统一解释。我们的分析扩展到了最多100层的网络。
2501.02423	 | Scaling Laws for Floating Point Quantization Training	 | Xingwu Sun,Shuaipeng Li,Ruobing Xie,Weidong Han,Kan Wu,Zhen Yang,Yixing Li,An Wang,Shuai Li,Jinbao Xue,Yu Cheng,Yangyu Tao,Zhanhui Kang,Chengzhong Xu,Di Wang,Jie Jiang	 | 本文探讨了浮点量化对大规模语言模型性能的影响，并提出了一种精确的统一缩放定律，发现指数位比尾数位对模型性能的贡献略多，最佳指数-尾数位比例可作为未来硬件设计的参考，同时指出过多训练数据可能反而降低模型性能，且最优量化精度随计算能力增加而增加，但在广泛范围内最佳成本效益精度在4到8位之间。	 | Low-precision training is considered an effective strategy for reducing both training and downstream inference costs. Previous scaling laws for precision mainly focus on integer quantization, which pay less attention to the constituents in floating-point quantization and thus cannot well fit the LLM losses in this scenario. In contrast, while floating-point quantization training is more commonly implemented in production, the research on it has been relatively superficial. In this paper, we thoroughly explore the effects of floating-point quantization targets, exponent bits, mantissa bits, and the calculation granularity of the scaling factor in floating-point quantization training performance of LLM models. While presenting an accurate floating-point quantization unified scaling law, we also provide valuable suggestions for the community: (1) Exponent bits contribute slightly more to the model performance than mantissa bits. We provide the optimal exponent-mantissa bit ratio for different bit numbers, which is available for future reference by hardware manufacturers; (2) We discover the formation of the critical data size in low-precision LLM training. Too much training data exceeding the critical data size will inversely bring in degradation of LLM performance; (3) The optimal floating-point quantization precision is directly proportional to the computational power, but within a wide computational power range, we estimate that the best cost-performance precision lies between 4-8 bits.	 | 低精度训练被认为是一种有效策略，可以降低训练和下游推理的成本。之前的精度缩放定律主要集中在整数量化上，较少关注浮点量化中的组成部分，因此在当前场景下无法很好地拟合大规模语言模型（LLM）的损失。相比之下，虽然生产中更常见的是实施浮点量化训练，但这方面的研究相对浅显。在本文中，我们深入探讨了浮点量化目标、指数位、尾数位以及缩放因子计算粒度对大规模语言模型浮点量化训练性能的影响。我们不仅提供了一个精确的浮点量化统一缩放定律，还为社区提供了宝贵建议：（1）指数位比尾数位对模型性能的贡献略多。我们提供了不同位数的最佳指数-尾数位比例，这可供未来硬件制造商参考；（2）我们发现了低精度LLM训练中的关键数据大小形成机制。过多超过关键数据大小的训练数据会逆向导致LLM性能下降；（3）最优的浮点量化精度与计算能力成正比，但在广泛的计算能力范围内，我们估计最佳的成本效益精度在4到8位之间。
2501.02409	 | Interpretable Neural ODEs for Gene Regulatory Network Discovery under Perturbations	 | Zaikang Lin,Sei Chang,Aaron Zweig,Elham Azizi,David A. Knowles	 | PerturbODE 是一种结合神经偏微分方程的新框架，用于建模细胞状态轨迹并从参数中推导出因果基因调控网络，适用于大规模发现基因之间的调节相互作用。该方法在模拟和真实数据集上展示了在轨迹预测和基因调控网络推断方面的有效性。	 | Modern high-throughput biological datasets with thousands of perturbations provide the opportunity for large-scale discovery of causal graphs that represent the regulatory interactions between genes. Numerous methods have been proposed to infer a directed acyclic graph (DAG) corresponding to the underlying gene regulatory network (GRN) that captures causal gene relationships. However, existing models have restrictive assumptions (e.g. linearity, acyclicity), limited scalability, and/or fail to address the dynamic nature of biological processes such as cellular differentiation. We propose PerturbODE, a novel framework that incorporates biologically informative neural ordinary differential equations (neural ODEs) to model cell state trajectories under perturbations and derive the causal GRN from the neural ODE's parameters. We demonstrate PerturbODE's efficacy in trajectory prediction and GRN inference across simulated and real over-expression datasets.	 | 现代高通量生物数据集包含数千种扰动，为大规模发现描述基因之间调节相互作用的因果图提供了机会。已提出了许多方法来推断与潜在基因调控网络（GRN）对应的有向无环图（DAG），以捕获因果基因关系。然而，现有模型具有限制性假设（例如线性、无环性），扩展性有限，或者未能解决细胞分化等生物过程的动态性质。我们提出了一种名为PerturbODE的新框架，该框架结合了具有生物信息的神经偏微分方程（神经ODEs），用于在扰动下建模细胞状态轨迹，并从神经ODE的参数中推导出因果GRN。我们在模拟数据集和真实的过表达数据集中展示了PerturbODE在轨迹预测和GRN推断方面的有效性。
2501.02393	 | Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers	 | Markus J. Buehler	 | 该研究提出了一种将图神经网络与Transformer结合的方法，通过引入图意识同构注意力（Graph-Aware Isomorphic Attention）和稀疏GIN-注意力，增强了Transformer在复杂关系建模中的能力，特别是在生物信息学和材料科学等领域展示了更好的泛化和适应性。	 | We present an approach to modifying Transformer architectures by integrating graph-aware relational reasoning into the attention mechanism, merging concepts from graph neural networks and language modeling. Building on the inherent connection between attention and graph theory, we reformulate the Transformer's attention mechanism as a graph operation and propose Graph-Aware Isomorphic Attention. This method leverages advanced graph modeling strategies, including Graph Isomorphism Networks (GIN) and Principal Neighborhood Aggregation (PNA), to enrich the representation of relational structures. Our approach captures complex dependencies and generalizes across tasks, as evidenced by a reduced generalization gap and improved learning performance. Additionally, we expand the concept of graph-aware attention to introduce Sparse GIN-Attention, a fine-tuning approach that employs sparse GINs. By interpreting attention matrices as sparse adjacency graphs, this technique enhances the adaptability of pre-trained foundational models with minimal computational overhead, endowing them with graph-aware capabilities. Sparse GIN-Attention fine-tuning achieves improved training dynamics and better generalization compared to alternative methods like low-rank adaption (LoRA). We discuss latent graph-like structures within traditional attention mechanisms, offering a new lens through which Transformers can be understood. By evolving Transformers as hierarchical GIN models for relational reasoning. This perspective suggests profound implications for foundational model development, enabling the design of architectures that dynamically adapt to both local and global dependencies. Applications in bioinformatics, materials science, language modeling, and beyond could benefit from this synthesis of relational and sequential data modeling, setting the stage for interpretable and generalizable modeling strategies.	 | 我们提出了一种通过将图意识关系推理集成到注意力机制中来修改Transformer架构的方法，结合了图神经网络和语言建模的概念。基于注意力机制与图论之间的内在联系，我们将Transformer的注意力机制重新表述为图操作，并提出了图意识同构注意力（Graph-Aware Isomorphic Attention）。该方法利用先进的图建模策略，包括图同构网络（GIN）和主节点聚合（PNA），来丰富关系结构的表现形式。我们的方法捕捉到复杂的依赖关系，并在任务之间进行泛化，这从减少泛化差距和改进学习性能得到了体现。此外，我们扩展了图意识注意力的概念，引入了稀疏GIN-注意力，这是一种采用稀疏GIN进行微调的方法。通过将注意力矩阵解释为稀疏邻接图，该技术能够以最小的计算开销增强预训练基础模型的适应性，并赋予它们图意识的能力。稀疏GIN-注意力微调相比低秩适应（LoRA）等其他方法，在训练动态和泛化方面取得了更好的效果。我们讨论了传统注意力机制中的潜在图样结构，提供了理解Transformer的一种新视角。通过将Transformer演化为层次化GIN模型，以实现关系推理，这种视角提出了基础模型开发的深刻影响，使设计能够动态适应局部和全局依赖关系的架构成为可能。这种关系和序列数据建模的结合在生物信息学、材料科学、语言建模等领域中，可以为可解释和泛化建模策略的开发奠定基础。
2501.02379	 | Tensor-GaLore: Memory-Efficient Training via Gradient Tensor Decomposition	 | Robert Joseph George,David Pitt,Jiawei Zhao,Jean Kossaifi,Cheng Luo,Yuandong Tian,Anima Anandkumar	 | Tensor-GaLore 是一种用于高效训练具有高阶张量权重的神经网络的新方法，特别是在处理高分辨率科学计算问题时，能显著减少内存使用量并提高性能，特别是在Fourier神经运算符和偏微分方程求解任务中，内存使用量最多可降低75%。	 | We present Tensor-GaLore, a novel method for efficient training of neural networks with higher-order tensor weights. Many models, particularly those used in scientific computing, employ tensor-parameterized layers to capture complex, multidimensional relationships. When scaling these methods to high-resolution problems makes memory usage grow intractably, and matrix based optimization methods lead to suboptimal performance and compression. We propose to work directly in the high-order space of the complex tensor parameter space using a tensor factorization of the gradients during optimization. We showcase its effectiveness on Fourier Neural Operators (FNOs), a class of models crucial for solving partial differential equations (PDE) and prove the theory of it. Across various PDE tasks like the Navier Stokes and Darcy Flow equations, Tensor-GaLore achieves substantial memory savings, reducing optimizer memory usage by up to 75%. These substantial memory savings across AI for science demonstrate Tensor-GaLore's potential.	 | 我们提出了Tensor-GaLore，这是一种用于高效训练具有高阶张量权重的神经网络的新方法。许多模型，尤其是那些用于科学计算的模型，使用张量参数化的层来捕捉复杂的多维关系。当将这些方法扩展到高分辨率问题时，会导致内存使用量变得难以管理，基于矩阵的优化方法会导致性能不佳和压缩效果不够理想。我们建议在优化过程中直接在复杂的张量参数空间的高阶空间中工作，使用梯度的张量分解。我们展示了其在Fourier神经运算符（FNOs）中的有效性，这是一种对于求解偏微分方程（PDE）至关重要的模型，并证明了相关理论。在Navier-Stokes方程和Darcy流动方程等各种PDE任务中，Tensor-GaLore能够实现显著的内存节省，将优化器的内存使用量降低高达75%。这些显著的内存节省表明了Tensor-GaLore在科学计算中应用的潜力。
2501.02378	 | A ghost mechanism: An analytical model of abrupt learning	 | Fatih Dinc,Ege Cirakman,Yiqi Jiang,Mert Yuksekgonul,Mark J. Schnitzer,Hidenori Tanaka	 | 该研究通过一个基于延迟激活任务的简约动态系统，揭示了一维系统中可以表现出断学习的现象，并通过“鬼点”而非分叉的方式证明了其机制。研究还发现，在递归神经网络中，通过降低模型输出的置信度或增加冗余参数可以稳定学习动态，从而避免断学习。	 | \emph{Abrupt learning} is commonly observed in neural networks, where long plateaus in network performance are followed by rapid convergence to a desirable solution. Yet, despite its common occurrence, the complex interplay of task, network architecture, and learning rule has made it difficult to understand the underlying mechanisms. Here, we introduce a minimal dynamical system trained on a delayed-activation task and demonstrate analytically how even a one-dimensional system can exhibit abrupt learning through ghost points rather than bifurcations. Through our toy model, we show that the emergence of a ghost point destabilizes learning dynamics. We identify a critical learning rate that prevents learning through two distinct loss landscape features: a no-learning zone and an oscillatory minimum. Testing these predictions in recurrent neural networks (RNNs), we confirm that ghost points precede abrupt learning and accompany the destabilization of learning. We demonstrate two complementary remedies: lowering the model output confidence prevents the network from getting stuck in no-learning zones, while increasing trainable ranks beyond task requirements (\textit{i.e.}, adding sloppy parameters) provides more stable learning trajectories. Our model reveals a bifurcation-free mechanism for abrupt learning and illustrates the importance of both deliberate uncertainty and redundancy in stabilizing learning dynamics.	 | 断学习在神经网络中是一种常见的现象，其中网络性能会经历长时间的平台期，随后迅速收敛到一个理想的解决方案。尽管这种情况很常见，但任务、网络架构和学习规则之间的复杂相互作用使其难以理解其背后的机制。在此，我们引入了一个基于延迟激活任务训练的最小动态系统，并通过鬼点而非分叉的方式，证明了一维系统也可以表现出断学习。通过我们的玩具模型，我们展示了鬼点如何使学习动态变得不稳定。我们确定了一个关键的学习速率，它通过两个不同的损失景观特征防止了学习：一个无学习区域和一个振荡最小值。在测试这些预测时，在递归神经网络（RNNs）中我们发现鬼点在断学习之前出现，并伴随着学习动态的不稳定。我们展示了两种互补的解决方案：降低模型输出的置信度可以防止网络陷入无学习区域，而增加超过任务要求的可训练秩（即增加冗余参数）则提供了更稳定的学习轨迹。我们的模型揭示了一种没有分叉机制的断学习，并说明了在稳定学习动态中刻意的不确定性与冗余的重要性。
2501.02373	 | BADTV: Unveiling Backdoor Threats in Third-Party Task Vectors	 | Chia-Yi Hsu,Yu-Lin Tsai,Yu Zhe,Yan-Lun Chen,Chih-Hsun Lin,Chia-Mu Yu,Yang Zhang,Chun-Ying Huang,Jun Sakuma	 | 研究发现，利用任务向量（TVs）进行模块化更新的大规模预训练模型存在安全漏洞，尤其是BadTV这种后门攻击，能够在任务学习、遗忘和类比操作下有效破坏模型完整性，现有防御措施无法有效检测或缓解这种攻击。	 | Task arithmetic in large-scale pre-trained models enables flexible adaptation to diverse downstream tasks without extensive re-training. By leveraging task vectors (TVs), users can perform modular updates to pre-trained models through simple arithmetic operations like addition and subtraction. However, this flexibility introduces new security vulnerabilities. In this paper, we identify and evaluate the susceptibility of TVs to backdoor attacks, demonstrating how malicious actors can exploit TVs to compromise model integrity. By developing composite backdoors and eliminating redudant clean tasks, we introduce BadTV, a novel backdoor attack specifically designed to remain effective under task learning, forgetting, and analogies operations. Our extensive experiments reveal that BadTV achieves near-perfect attack success rates across various scenarios, significantly impacting the security of models using task arithmetic. We also explore existing defenses, showing that current methods fail to detect or mitigate BadTV. Our findings highlight the need for robust defense mechanisms to secure TVs in real-world applications, especially as TV services become more popular in machine-learning ecosystems.	 | 大规模预训练模型中的任务算术能够通过简单的算术运算（如加减法）灵活适应多种下游任务，而无需进行大量重新训练。通过利用任务向量（TVs），用户可以对预训练模型进行模块化更新。然而，这种灵活性也带来了新的安全漏洞。在本文中，我们识别并评估了TVs对后门攻击的敏感性，展示了恶意行为者如何利用TVs来破坏模型的完整性。通过构建复合后门并消除冗余的干净任务，我们提出了BadTV，这是一种专门设计的后门攻击，能够在任务学习、遗忘和类比操作下保持有效性。我们的广泛实验表明，BadTV在各种场景下几乎都能实现完美的攻击成功率，严重影响使用任务算术的模型的安全性。我们还探讨了现有的防御措施，指出当前的方法无法检测或缓解BadTV。我们的研究结果强调了在实际应用中需要开发 robust 的防御机制来保护TVs，特别是随着TV服务在机器学习生态系统中越来越流行。
2501.02369	 | Predicting two-dimensional spatiotemporal chaotic patterns with optimized high-dimensional hybrid reservoir computing	 | Tamon Nakano Sebastian Baur Christoph Räth	 | 研究将知识驱动模型与水库计算结合，提出了三种混合方法（全混合、输入混合和输出混合），并通过Barkley模型的模拟数据验证了在高维时空模式预测中的性能，发现输出混合方法在小模型误差和小RC尺寸时表现最佳，而局部状态假设有助于应对高维情况下的维数灾难。	 | As an alternative approach for predicting complex dynamical systems where physics-based models are no longer reliable, reservoir computing (RC) has gained popularity. The hybrid approach is considered an interesting option for improving the prediction performance of RC. The idea is to combine a knowledge-based model (KBM) to support the fully data-driven RC prediction. There are three types of hybridization for RC, namely full hybrid (FH), input hybrid (IH) and output hybrid (OH), where it was shown that the latter one is superior in terms of the accuracy and the robustness for the prediction of low-dimensional chaotic systems. Here, we extend the formalism to the prediction of spatiotemporal patterns in two dimensions. To overcome the curse of dimensionality for this very high-dimensional case we employ the local states ansatz, where only a few locally adjacent time series are utilized for the RC-based prediction. Using simulation data from the Barkley model describing chaotic electrical wave propagation in cardiac tissue, we outline the formalism of high-dimensional hybrid RC and assess the performance of the different hybridization schemes. We find that all three methods (FH, IH and OH) perform better than reservoir only, where improvements are small when the model is very inaccurate. For small model errors and small reservoirs FH and OH perform nearly equally well and better than IH. Given the smaller CPU needs for OH and especially the better interpretability of it, OH is to be favored. For large reservoirs the performance of OH drops below that of FH and IH. Generally, it maybe advisable to test the three setups for a given application and select the best suited one that optimizes between the counteracting factors of prediction performance and CPU needs.	 | 作为一种在物理模型不再可靠的复杂动力系统预测中替代的方法，水库计算（Reservoir Computing，RC）逐渐受到关注。混合方法被认为是提高RC预测性能的一种有趣选择。其基本思想是结合知识驱动模型（Knowledge-Based Model，KBM）以支持完全基于数据驱动的RC预测。RC的混合方法有三种类型，即全混合（Full Hybrid，FH）、输入混合（Input Hybrid，IH）和输出混合（Output Hybrid，OH），研究表明，对于低维混沌系统的预测，输出混合方法在准确性和鲁棒性方面表现更优。在此基础上，我们将其扩展到二维时空模式的预测。为了解决这个非常高维情况下的维数灾难问题，我们采用了局部状态假设，即仅使用少数局部相邻的时间序列来进行基于RC的预测。使用描述心脏组织中的混沌电波传播的Barkley模型的模拟数据，我们阐述了高维混合RC的形式，并评估了不同混合方案的性能。我们发现，所有三种方法（FH、IH和OH）都比仅使用RC更好，但当模型非常不准确时，改进幅度较小。对于小模型误差和小尺寸的RC，FH和OH的表现几乎一样好，并优于IH。考虑到OH所需的CPU资源更少且具有更好的可解释性，OH应被优先考虑。对于大尺寸的RC，OH的表现低于FH和IH。总的来说，在特定应用中，测试三种设置并选择在预测性能和CPU需求之间的最佳平衡方法可能是明智的。
2501.02364	 | Understanding How Nonlinear Layers Create Linearly Separable Features for Low-Dimensional Data	 | Alec S. Xu,Can Yaras,Peng Wang,Qing Qu	 | 本文通过引入低维子空间的并集表示方法，证明了单一层的非线性变换可以将低维数据转化为线性可分的集合，并且这种转换在使用随机权重和二次激活时以高概率发生，从而在理论上填补了深层网络线性可分性研究中的空白。实验结果支持了这些理论发现，展示了非线性网络在实际应用场景中的线性分离特性，为模型的可解释性和泛化能力提供了新的见解。	 | Deep neural networks have attained remarkable success across diverse classification tasks. Recent empirical studies have shown that deep networks learn features that are linearly separable across classes. However, these findings often lack rigorous justifications, even under relatively simple settings. In this work, we address this gap by examining the linear separation capabilities of shallow nonlinear networks. Specifically, inspired by the low intrinsic dimensionality of image data, we model inputs as a union of low-dimensional subspaces (UoS) and demonstrate that a single nonlinear layer can transform such data into linearly separable sets. Theoretically, we show that this transformation occurs with high probability when using random weights and quadratic activations. Notably, we prove this can be achieved when the network width scales polynomially with the intrinsic dimension of the data rather than the ambient dimension. Experimental results corroborate these theoretical findings and demonstrate that similar linear separation properties hold in practical scenarios beyond our analytical scope. This work bridges the gap between empirical observations and theoretical understanding of the separation capacity of nonlinear networks, offering deeper insights into model interpretability and generalization.	 | 深度神经网络在各种分类任务中取得了显著的成功。最近的经验研究表明，深层网络能够学习到不同类别间线性可分的特征。然而，这些发现通常缺乏严格的证明，即使是在相对简单的环境中也是如此。在本文中，我们通过研究浅层非线性网络的线性分离能力，填补了这一空白。具体而言，受到图像数据固有低维性的启发，我们将输入表示为低维子空间的并集（UoS），并证明单一层的非线性变换可以将这种数据转化为线性可分的集合。理论上，我们证明了在使用随机权重和二次激活时，这种转换以高概率发生。值得注意的是，我们证明了这种结果可以在网络宽度随着数据的固有维度而不是环境维度的多项式增长时实现。实验结果支持了这些理论发现，并展示了在超出我们分析范围的实际场景中也保持类似的线性分离特性。本文在经验观察和对非线性网络分离能力的理论理解之间架起了桥梁，为模型的可解释性和泛化能力提供了更深入的见解。
2501.02362	 | Easing Optimization Paths: a Circuit Perspective	 | Ambroise Odonnat,Wassim Bouaziz,Vivien Cabannes	 | 梯度下降是训练大型人工智能系统的主要方法，对此方法进行更深入的理解有助于降低计算成本并引导系统避免有害行为；为此，研究提出了一种基于电路视角的机制可解释性方法，并设计了一种高效学习课程。	 | Gradient descent is the method of choice for training large artificial intelligence systems. As these systems become larger, a better understanding of the mechanisms behind gradient training would allow us to alleviate compute costs and help steer these systems away from harmful behaviors. To that end, we suggest utilizing the circuit perspective brought forward by mechanistic interpretability. After laying out our intuition, we illustrate how it enables us to design a curriculum for efficient learning in a controlled setting. The code is available at \url{https://github.com/facebookresearch/pal}.	 | 梯度下降是训练大型人工智能系统的首选方法。随着这些系统的规模增大，对梯度训练机制的更深入理解将有助于降低计算成本，并引导这些系统避免有害行为。为此，我们建议利用机制可解释性提出的电路视角。在阐述了我们的直觉后，我们展示了如何利用这一视角设计一种在受控环境中高效学习的课程。代码可在 \url{https://github.com/facebookresearch/pal} 获取。
2501.02356	 | When is the Computation of a Feature Attribution Method Tractable?	 | P. Barceló,R. Cominetti,M. Morgado	 | 本文研究了超越SHAP的特征归因方法的计算复杂性，发现某些权力指数的计算等价于期望值评估，并引入了伯努利权力指数以简化计算；此外，还分析了交互权力指数的计算复杂性。	 | Feature attribution methods have become essential for explaining machine learning models. Many popular approaches, such as SHAP and Banzhaf values, are grounded in power indices from cooperative game theory, which measure the contribution of features to model predictions. This work studies the computational complexity of power indices beyond SHAP, addressing the conditions under which they can be computed efficiently. We identify a simple condition on power indices that ensures that computation is polynomially equivalent to evaluating expected values, extending known results for SHAP. We also introduce Bernoulli power indices, showing that their computation can be simplified to a constant number of expected value evaluations. Furthermore, we explore interaction power indices that quantify the importance of feature subsets, proving that their computation complexity mirrors that of individual features.	 | 特征归因方法已成为解释机器学习模型的必要工具。许多流行的近似方法，如SHAP和Banachef值，都是基于合作博弈论中的权力指数，这些指数衡量了特征对模型预测的贡献。本工作研究了超越SHAP的权力指数的计算复杂性，探讨了它们在什么条件下可以高效计算。我们确定了一个简单的条件，该条件确保了权力指数的计算等效于评估期望值，从而扩展了关于SHAP已知的结果。我们还引入了伯努利权力指数，证明了它们的计算可以通过常数数量的期望值评估来简化。此外，我们探讨了交互权力指数，这些指数量化了特征子集的重要性，并证明了它们的计算复杂性与单个特征相呼应。
2501.02353	 | Reweighting Improves Conditional Risk Bounds	 | Yikai Zhang,Jiahe Lin,Fengpei Li,Songzhu Zheng,Anant Raj,Anderson Schneider,Yuriy Nevmyvaka	 | 本文研究了在加权经验风险最小化方案中引入数据相关权重函数的效果，表明在某些特定区域（如分类中的大Margin区域和回归中的低方差区域），加权ERM估计量相比标准ERM估计量具有更优的表现，并通过合成数据实验得到了支持。	 | In this work, we study the weighted empirical risk minimization (weighted ERM) schema, in which an additional data-dependent weight function is incorporated when the empirical risk function is being minimized. We show that under a general ``balanceable" Bernstein condition, one can design a weighted ERM estimator to achieve superior performance in certain sub-regions over the one obtained from standard ERM, and the superiority manifests itself through a data-dependent constant term in the error bound. These sub-regions correspond to large-margin ones in classification settings and low-variance ones in heteroscedastic regression settings, respectively. Our findings are supported by evidence from synthetic data experiments.	 | 在本文中，我们研究了加权经验风险最小化（加权ERM）方案，在该方案中，当最小化经验风险函数时，引入了一个数据相关的权重函数。我们表明，在一般的“可平衡”的伯努利条件下，可以通过设计一个加权ERM估计量，在某些子区域上实现相对于标准ERM估计量的优越性能，这种优越性体现在误差界中的数据相关的常数项。这些子区域在分类设置中对应于大 Margin 的区域，在异方差回归设置中对应于低方差的区域。我们的发现得到了合成数据实验的证据支持。
2501.02342	 | Optimizing Small Language Models for In-Vehicle Function-Calling	 | Yahya Sowti Khiabani,Farris Atif,Chieh Hsu,Sven Stahlmann,Tobias Michels,Sebastian Kramer,Benedikt Heidrich,M. Saquib Sarfraz,Julian Merten,Faezeh Tafazzoli	 | 该研究提出了一种将小型语言模型部署在车辆边缘设备上的方法，通过模型压缩和优化，保持了高性能的同时提升了用户体验。研究结果表明，尽管模型大幅压缩，但仍能实现实时、高效的复杂任务处理，为车辆控制系统的转型奠定了基础。	 | We propose a holistic approach for deploying Small Language Models (SLMs) as function-calling agents within vehicles as edge devices, offering a more flexible and robust alternative to traditional rule-based systems. By leveraging SLMs, we simplify vehicle control mechanisms and enhance the user experience. Given the in-vehicle hardware constraints, we apply state-of-the-art model compression techniques, including structured pruning, healing, and quantization, ensuring that the model fits within the resource limitations while maintaining acceptable performance. Our work focuses on optimizing a representative SLM, Microsoft's Phi-3 mini, and outlines best practices for enabling embedded models, including compression, task-specific fine-tuning, and vehicle integration. We demonstrate that, despite significant reduction in model size which removes up to 2 billion parameters from the original model, our approach preserves the model's ability to handle complex in-vehicle tasks accurately and efficiently. Furthermore, by executing the model in a lightweight runtime environment, we achieve a generation speed of 11 tokens per second, making real-time, on-device inference feasible without hardware acceleration. Our results demonstrate the potential of SLMs to transform vehicle control systems, enabling more intuitive interactions between users and their vehicles for an enhanced driving experience.	 | 我们提出了一种综合方法，将小型语言模型（SLMs）部署为车辆中的边缘设备上的功能调用代理，提供了一种比传统基于规则系统更为灵活和稳健的替代方案。通过利用SLMs，我们简化了车辆控制机制并提升了用户体验。鉴于车内硬件的限制，我们应用了最先进的模型压缩技术，包括结构化剪枝、愈合和量化，确保模型在资源限制内运行的同时保持可接受的性能。我们的工作集中在优化一个代表性的SLM——微软的Phi-3 mini，并概述了启用嵌入式模型的最佳实践，包括压缩、任务特定微调和车辆集成。我们证明，尽管模型大小显著减少，从原始模型中移除了多达20亿个参数，但我们的方法仍能保持模型处理复杂车内任务的能力，且准确高效。此外，通过在轻量级运行时环境中执行模型，我们实现了每秒生成11个标记的速度，使实时、在设备上的推理成为可能，无需硬件加速。我们的结果展示了SLMs有潜力转型车辆控制系统，使用户与车辆之间的交互更加直观，从而提升驾驶体验。
2501.02330	 | SR-Reward: Taking The Path More Traveled	 | Seyed Mahdi B. Azad,Zahra Padar,Gabriel Kalweit,Joschka Boedecker	 | 本文提出了一种名为SR-Reward的新方法，该方法通过将奖励函数与学习者策略分离，利用后继表示编码预期未来状态访问，从而实现更稳定和高效的训练过程，并在D4RL基准上展示了与传统离线RL算法的竞争力。	 | In this paper, we propose a novel method for learning reward functions directly from offline demonstrations. Unlike traditional inverse reinforcement learning (IRL), our approach decouples the reward function from the learner's policy, eliminating the adversarial interaction typically required between the two. This results in a more stable and efficient training process. Our reward function, called \textit{SR-Reward}, leverages successor representation (SR) to encode a state based on expected future states' visitation under the demonstration policy and transition dynamics. By utilizing the Bellman equation, SR-Reward can be learned concurrently with most reinforcement learning (RL) algorithms without altering the existing training pipeline. We also introduce a negative sampling strategy to mitigate overestimation errors by reducing rewards for out-of-distribution data, thereby enhancing robustness. This strategy inherently introduces a conservative bias into RL algorithms that employ the learned reward. We evaluate our method on the D4RL benchmark, achieving competitive results compared to offline RL algorithms with access to true rewards and imitation learning (IL) techniques like behavioral cloning. Moreover, our ablation studies on data size and quality reveal the advantages and limitations of SR-Reward as a proxy for true rewards.	 | 在本文中，我们提出了一种从离线示范直接学习奖励函数的新方法。与传统的逆强化学习（Inverse Reinforcement Learning, IRL）不同，我们的方法将奖励函数与学习者的策略分离，从而消除了两者之间通常所需的对抗性互动。这导致了一个更加稳定和高效的训练过程。我们的奖励函数称为SR-Reward，它利用后继表示（Successor Representation, SR）来编码基于演示策略下预期未来状态访问的状态。通过使用贝尔曼方程，SR-Reward 可以与大多数强化学习（Reinforcement Learning, RL）算法并行学习，而不改变现有的训练管道。我们还引入了一种负采样策略，通过减少离分布数据的奖励来缓解过估计错误，从而增强鲁棒性。这一策略在使用学习到的奖励的强化学习算法中引入了一种保守的偏差。我们在 D4RL 基准上评估了我们的方法，结果表明与可以访问真实奖励和模仿学习（Imitation Learning, IL）技术如行为克隆的离线 RL 算法相比，我们的方法具有竞争力。此外，我们在数据大小和质量上的消融研究揭示了 SR-Reward 作为真实奖励代理的优势和局限性。
2501.02313	 | DiffGraph: Heterogeneous Graph Diffusion Model	 | Zongwei Li,Lianghao Xia,Hua Hua,Shijie Zhang,Shuangyang Wang,Chao Huang	 | 本文提出了一个名为DiffGraph的新型异构图扩散模型，该模型通过引入创新的多视角去噪策略和复杂的隐层异构图扩散机制，解决了传统图神经网络在处理噪声数据和异构关系转换方面的局限性，显著提升了链接预测和节点分类任务的性能。	 | Recent advances in Graph Neural Networks (GNNs) have revolutionized graph-structured data modeling, yet traditional GNNs struggle with complex heterogeneous structures prevalent in real-world scenarios. Despite progress in handling heterogeneous interactions, two fundamental challenges persist: noisy data significantly compromising embedding quality and learning performance, and existing methods' inability to capture intricate semantic transitions among heterogeneous relations, which impacts downstream predictions. To address these fundamental issues, we present the Heterogeneous Graph Diffusion Model (DiffGraph), a pioneering framework that introduces an innovative cross-view denoising strategy. This advanced approach transforms auxiliary heterogeneous data into target semantic spaces, enabling precise distillation of task-relevant information. At its core, DiffGraph features a sophisticated latent heterogeneous graph diffusion mechanism, implementing a novel forward and backward diffusion process for superior noise management. This methodology achieves simultaneous heterogeneous graph denoising and cross-type transition, while significantly simplifying graph generation through its latent-space diffusion capabilities. Through rigorous experimental validation on both public and industrial datasets, we demonstrate that DiffGraph consistently surpasses existing methods in link prediction and node classification tasks, establishing new benchmarks for robustness and efficiency in heterogeneous graph processing. The model implementation is publicly available at: https://github.com/HKUDS/DiffGraph.	 | 最近在图神经网络（GNNs）方面的进展已经彻底改变了图结构数据建模，但传统的GNNs在处理真实世界场景中普遍存在的复杂异构结构方面仍然存在困难。尽管在处理异构交互方面取得了进展，但仍然存在两个基本挑战：噪声数据严重损害了嵌入质量和学习表现，以及现有方法无法捕捉异构关系间的复杂语义转换，这影响了下游预测。为了应对这些基本问题，我们提出了异构图扩散模型（DiffGraph），这是一个开创性的框架，引入了一种创新的多视角去噪策略。这一先进方法将辅助异构数据转化为目标语义空间，从而精确提炼出与任务相关的信息。DiffGraph的核心在于一个复杂的隐层异构图扩散机制，该机制实现了新颖的正向和反向扩散过程，以实现更好的噪声管理。这种方法同时实现了异构图去噪和跨类型转换，通过隐空间扩散能力大大简化了图的生成。通过在公共数据集和工业数据集上进行严格的实验验证，我们证明DiffGraph在链接预测和节点分类任务中始终超越了现有方法，为异构图处理的稳健性和效率设立了新的基准。模型实现已在以下网址公开提供：https://github.com/HKUDS/DiffGraph。
2501.02241	 | Interpretable Load Forecasting via Representation Learning of Geo-distributed Meteorological Factors	 | Yangze Zhou,Guoxin Lin,Gonghao Zhang,Yi Wang	 | 本文提出了一种基于表示学习的框架，以考虑气象因素的空间关系并提取分布式气象因素，同时使用图模型中的Shapley值揭示不同地点气象因素对负荷预测的影响，并通过加速方法降低计算复杂性，从而提高日前负荷预测的准确性。	 | Meteorological factors (MF) are crucial in day-ahead load forecasting as they significantly influence the electricity consumption behaviors of consumers. Numerous studies have incorporated MF into the load forecasting model to achieve higher accuracy. Selecting MF from one representative location or the averaged MF as the inputs of the forecasting model is a common practice. However, the difference in MF collected in various locations within a region may be significant, which poses a challenge in selecting the appropriate MF from numerous locations. A representation learning framework is proposed to extract geo-distributed MF while considering their spatial relationships. In addition, this paper employs the Shapley value in the graph-based model to reveal connections between MF collected in different locations and loads. To reduce the computational complexity of calculating the Shapley value, an acceleration method is adopted based on Monte Carlo sampling and weighted linear regression. Experiments on two real-world datasets demonstrate that the proposed method improves the day-ahead forecasting accuracy, especially in extreme scenarios such as the "accumulation temperature effect" in summer and "sudden temperature change" in winter. We also find a significant correlation between the importance of MF in different locations and the corresponding area's GDP and mainstay industry.	 | 气象因素（MF）在日前负荷预测中至关重要，因为它们显著影响消费者的用电行为。许多研究已经将MF纳入负荷预测模型以实现更高的准确性。从一个代表性地点或平均MF中选择MF作为预测模型的输入是一种常见做法。然而，同一区域内不同地点采集的MF之间可能存在显著差异，这给选择合适的MF带来了挑战。本文提出了一种表示学习框架，以考虑MF的空间关系，从而提取分布式的MF。此外，本文在图模型中使用Shapley值来揭示不同地点采集的MF与负荷之间的联系。为了降低计算Shapley值的计算复杂性，本文采用基于蒙特卡洛采样和加权线性回归的加速方法。实验结果表明，所提出的方法在极端情况下，如夏季的“累计温度效应”和冬季的“突然温度变化”等情景中，提高了日前的预测准确性。我们还发现，不同地点MF的重要性与其对应的地区GDP和支柱产业之间存在显著相关性。
2501.02219	 | Diffusion Model-Based Data Synthesis Aided Federated Semi-Supervised Learning	 | Zhongwei Wang,Tong Wu,Zhiyong Chen,Liang Qian,Yin Xu,Meixia Tao	 | DDSA-FSSL提出了一种通过扩散模型生成合成数据来解决联邦半监督学习中标签数据稀缺和非独立非同分布问题的新方法，显著提高了模型性能，如在CIFAR-10数据集上使用10%标签数据时准确率从38.46%提升到52.14%。	 | Federated semi-supervised learning (FSSL) is primarily challenged by two factors: the scarcity of labeled data across clients and the non-independent and identically distribution (non-IID) nature of data among clients. In this paper, we propose a novel approach, diffusion model-based data synthesis aided FSSL (DDSA-FSSL), which utilizes a diffusion model (DM) to generate synthetic data, bridging the gap between heterogeneous local data distributions and the global data distribution. In DDSA-FSSL, clients address the challenge of the scarcity of labeled data by employing a federated learning-trained classifier to perform pseudo labeling for unlabeled data. The DM is then collaboratively trained using both labeled and precision-optimized pseudo-labeled data, enabling clients to generate synthetic samples for classes that are absent in their labeled datasets. This process allows clients to generate more comprehensive synthetic datasets aligned with the global distribution. Extensive experiments conducted on multiple datasets and varying non-IID distributions demonstrate the effectiveness of DDSA-FSSL, e.g., it improves accuracy from 38.46% to 52.14% on CIFAR-10 datasets with 10% labeled data.	 | 联邦半监督学习（FSSL）主要受到两个因素的挑战：客户端标签数据的稀缺性以及客户端数据的非独立且非同分布（non-IID）特性。本文提出了一种新的方法——基于扩散模型的数据合成辅助FSSL（DDSA-FSSL），该方法通过利用扩散模型（DM）生成合成数据，弥合了异构本地数据分布与全局数据分布之间的差距。在DDSA-FSSL中，客户端通过使用联邦学习训练的分类器对未标记数据进行伪标签标注来应对标签数据稀缺的挑战。然后，DM使用标签数据和精度优化后的伪标记数据进行协作训练，使客户端能够生成未出现在其标签数据集中的类别的合成样本。这一过程允许客户端生成更符合全局分布的综合合成数据集。在多个数据集和不同非IID分布上进行的大量实验表明，DDSA-FSSL的有效性，例如，在CIFAR-10数据集上使用10%标签数据时，准确率从38.46%提高到52.14%。
2501.02205	 | Digital Twin Calibration with Model-Based Reinforcement Learning	 | Hua Zheng,Wei Xie,Ilya O. Ryzhov,Keilung Choy	 | 本文提出了一种“行为-模拟器”方法，结合数字双胞胎校准和基于模型的强化学习，以更有效地控制具有复杂非线性动力学的随机系统，尤其适用于生物制药制造等过程动力学复杂且数据有限的行业。该方法能够同时校准模型并搜索最优控制策略，通过平衡探索和利用来改进策略性能，在数值实验中表现出色并优于现有方法。	 | This paper presents a novel methodological framework, called the Actor-Simulator, that incorporates the calibration of digital twins into model-based reinforcement learning for more effective control of stochastic systems with complex nonlinear dynamics. Traditional model-based control often relies on restrictive structural assumptions (such as linear state transitions) and fails to account for parameter uncertainty in the model. These issues become particularly critical in industries such as biopharmaceutical manufacturing, where process dynamics are complex and not fully known, and only a limited amount of data is available. Our approach jointly calibrates the digital twin and searches for an optimal control policy, thus accounting for and reducing model error. We balance exploration and exploitation by using policy performance as a guide for data collection. This dual-component approach provably converges to the optimal policy, and outperforms existing methods in extensive numerical experiments based on the biopharmaceutical manufacturing domain.	 | 本文提出了一种名为“行为-模拟器”的新方法论框架，该框架将数字双胞胎的校准与基于模型的强化学习结合在一起，以更有效地控制具有复杂非线性动力学的随机系统。传统的基于模型的控制通常依赖于严格的结构假设（如线性状态转换），并且未能考虑模型中的参数不确定性。这些问题在生物制药制造等行业尤其关键，因为在这些行业中，过程动力学非常复杂且不完全已知，而且可用的数据量有限。我们的方法同时校准数字双胞胎并搜索最优控制策略，从而考虑并减少了模型误差。我们通过将策略性能作为数据收集的指南来平衡探索和利用。这一双组件方法已被证明能够收敛到最优策略，并在基于生物制药制造领域的广泛数值实验中表现出色，优于现有方法。
2501.02198	 | Fresh-CL: Feature Realignment through Experts on Hypersphere in Continual Learning	 | Zhongyi Zhou,Yaxin Peng,Pin Yi,Minjie Zhu,Chaomin Shen	 | 本研究提出了一种名为Fresh-CL的方法，通过在超球体上利用固定单纯形等角紧框架（ETF）分类器进行特征重定位，以改善持续学习中的特征分离。通过动态扩展ETF并适应不同子空间，该方法在多个数据集上提升了模型性能，特别是在细粒度数据集上，准确率提高了2%。	 | Continual Learning enables models to learn and adapt to new tasks while retaining prior knowledge.Introducing new tasks, however, can naturally lead to feature entanglement across tasks, limiting the model's capability to distinguish between new domain data.In this work, we propose a method called Feature Realignment through Experts on hyperSpHere in Continual Learning (Fresh-CL). By leveraging predefined and fixed simplex equiangular tight frame (ETF) classifiers on a hypersphere, our model improves feature separation both intra and inter tasks.However, the projection to a simplex ETF shifts with new tasks, disrupting structured feature representation of previous tasks and degrading performance. Therefore, we propose a dynamic extension of ETF through mixture of experts, enabling adaptive projections onto diverse subspaces to enhance feature representation.Experiments on 11 datasets demonstrate a 2\% improvement in accuracy compared to the strongest baseline, particularly in fine-grained datasets, confirming the efficacy of combining ETF and MoE to improve feature distinction in continual learning scenarios.	 | 持续学习使模型能够学习和适应新任务同时保留先前的知识。然而，引入新任务会自然导致任务间的特征纠缠，限制了模型区分新领域数据的能力。在本研究中，我们提出了一种名为持续学习中基于超球体专家的特征重定位（Fresh-CL）的方法。通过利用预定义且固定的单纯形等角紧框架（ETF）分类器在超球体上，我们的模型在任务内和任务间的特征分离方面均得到了改进。然而，投影到单纯形ETF在新任务中会发生变化，破坏了之前任务的结构化特征表示，从而降低性能。因此，我们提出了通过混合专家的ETF动态扩展方法，使投影适应到不同的子空间，从而增强特征表示。在11个数据集上的实验表明，与最强基线相比，Fresh-CL在准确性上提高了2%，特别是在细粒度数据集上，这证实了结合ETF和混合专家以提高持续学习场景中特征区分效果的有效性。
2501.02191	 | On LLM-Enhanced Mixed-Type Data Imputation with High-Order Message Passing	 | Jianwei Wang,Kai Wang,Ying Zhang,Wenjie Zhang,Xiwei Xu,Xuemin Lin	 | 本文提出了UnIMP，一种利用大型语言模型和高阶消息传递的统一数据插补框架，旨在填补混合型数据集（包含数值、分类和文本数据）中的缺失值，并通过高效的双向高阶消息传递网络和适配器Xfusion实现对复杂数据模式的学习。实验结果表明，UnIMP在10个真实世界数据集上优于现有技术。	 | Missing data imputation, which aims to impute the missing values in the raw datasets to achieve the completeness of datasets, is crucial for modern data-driven models like large language models (LLMs) and has attracted increasing interest over the past decades. Despite its importance, existing solutions for missing data imputation either 1) only support numerical and categorical data or 2) show an unsatisfactory performance due to their design prioritizing text data and the lack of key properties for tabular data imputation. In this paper, we propose UnIMP, a Unified IMPutation framework that leverages LLM and high-order message passing to enhance the imputation of mixed-type data including numerical, categorical, and text data. Specifically, we first introduce a cell-oriented hypergraph to model the table. We then propose BiHMP, an efficient Bidirectional High-order Message-Passing network to aggregate global-local information and high-order relationships on the constructed hypergraph while capturing the inter-column heterogeneity and intra-column homogeneity. To effectively and efficiently align the capacity of the LLM with the information aggregated by BiHMP, we introduce Xfusion, which, together with BiHMP, acts as adapters for the LLM. We follow a pre-training and fine-tuning pipeline to train UnIMP, integrating two optimizations: chunking technique, which divides tables into smaller chunks to enhance efficiency; and progressive masking technique, which gradually adapts the model to learn more complex data patterns. Both theoretical proofs and empirical experiments on 10 real world datasets highlight the superiority of UnIMP over existing techniques.	 | 缺失数据插补是旨在填补原始数据集中的缺失值以实现数据集的完整性，对于现代数据驱动模型（如大型语言模型LLMs）至关重要，并在过去几十年中吸引了越来越多的关注。尽管其重要性无可忽视，但现有的缺失数据插补解决方案要么仅支持数值和分类数据，要么因设计侧重于文本数据而缺乏表数据插补的关键属性，插补效果不尽如人意。本文中，我们提出了UnIMP，一种统一的数据插补框架，利用LLM和高阶消息传递来增强混合型数据（包括数值、分类和文本数据）的插补能力。具体而言，我们首先引入了一个单元为中心的超图来建模表格。然后，我们提出了BiHMP，一种高效的双向高阶消息传递网络，用于在构建的超图上聚合全局-局部信息和高阶关系，同时捕捉列间异质性和列内同质性。为了有效地并高效地使LLM的容量与BiHMP聚合的信息对齐，我们引入了Xfusion，它与BiHMP一起作为LLM的适配器。我们遵循预训练和微调的流程来训练UnIMP，整合了两项优化：切块技术，该技术将表格划分为更小的块以提高效率；以及渐进式掩码技术，该技术逐步适应模型学习更复杂的数据模式。理论证明和在10个真实世界数据集上的实验证明了UnIMP相对于现有技术的优越性。
2501.02182	 | AdaMixup: A Dynamic Defense Framework for Membership Inference Attack Mitigation	 | Ying Chen,Jiajing Chen,Yijie Weng,ChiaHua Chang,Dezhi Yu,Guanbiao Lin	 | AdaMixup 是一种新颖的防御机制，通过在训练过程中动态调整自适应混合策略来增强模型对会员推理攻击的鲁棒性，从而有效降低隐私泄露风险并保持高性能。实验结果表明，AdaMixup 在多个数据集上显著减轻了会员推理攻击的风险，并在防御效率和模型准确性之间取得了良好平衡。	 | Membership inference attacks have emerged as a significant privacy concern in the training of deep learning models, where attackers can infer whether a data point was part of the training set based on the model's outputs. To address this challenge, we propose a novel defense mechanism, AdaMixup. AdaMixup employs adaptive mixup techniques to enhance the model's robustness against membership inference attacks by dynamically adjusting the mixup strategy during training. This method not only improves the model's privacy protection but also maintains high performance. Experimental results across multiple datasets demonstrate that AdaMixup significantly reduces the risk of membership inference attacks while achieving a favorable trade-off between defensive efficiency and model accuracy. This research provides an effective solution for data privacy protection and lays the groundwork for future advancements in mixup training methods.	 | 会员推理攻击已成为深度学习模型训练中一个重要的隐私问题，攻击者可以通过模型的输出来推断数据点是否属于训练集。为应对这一挑战，我们提出了一种名为AdaMixup的新颖防御机制。AdaMixup采用自适应混合技术，在训练过程中动态调整混合策略，以增强模型对会员推理攻击的鲁棒性。该方法不仅提高了模型的隐私保护能力，还保持了高性能。在多个数据集上的实验结果表明，AdaMixup显著降低了会员推理攻击的风险，并在防御效率和模型准确性之间取得了良好的权衡。这项研究为数据隐私保护提供了有效解决方案，并为未来的混合训练方法奠定了基础。
2501.02156	 | The Race to Efficiency: A New Perspective on AI Scaling Laws	 | Chien-Ping Lu	 | 随着大型AI模型的扩展，训练成本增加且进步变得困难，因此需要平衡不断改进的硬件和算法与GPU集群的规模。通过引入相对损失方程，该研究提出了一个兼顾时间和效率的框架，表明持续的效率提升是实现可实现的近指数级进步的关键。	 | As large-scale AI models expand, training becomes costlier and sustaining progress grows harder. Classical scaling laws (e.g., Kaplan et al. (2020), Hoffmann et al. (2022)) predict training loss from a static compute budget yet neglect time and efficiency, prompting the question: how can we balance ballooning GPU fleets with rapidly improving hardware and algorithms? We introduce the relative-loss equation, a time- and efficiency-aware framework that extends classical AI scaling laws. Our model shows that, without ongoing efficiency gains, advanced performance could demand millennia of training or unrealistically large GPU fleets. However, near-exponential progress remains achievable if the "efficiency-doubling rate" parallels Moore's Law. By formalizing this race to efficiency, we offer a quantitative roadmap for balancing front-loaded GPU investments with incremental improvements across the AI stack. Empirical trends suggest that sustained efficiency gains can push AI scaling well into the coming decade, providing a new perspective on the diminishing returns inherent in classical scaling.	 | 随着大型AI模型的扩展，训练成本变得更高，持续取得进步变得更加困难。经典的比例法则（例如，Kaplan等人（2020年），Hoffmann等人（2022年））预测固定计算预算下的训练损失，但却忽略了时间和效率问题，由此提出一个问题：我们如何在不断改进的硬件和算法面前平衡不断膨胀的GPU集群？我们引入了相对损失方程，这是一种兼顾时间和效率的框架，扩展了经典的人工智能缩放定律。我们的模型表明，如果没有持续的效率提升，先进的性能可能会需要数万年的训练时间，或需要无法实现规模的GPU集群。然而，如果“效率翻倍率”与摩尔定律相匹配的话，近指数级的进步仍是可实现的。通过正式化这场效率竞赛，我们提供了一个定量的路线图，用于平衡对GPU的前期投资与AI堆栈中逐步改进的比例。实证趋势表明，持续的效率提升可以将AI的扩展推进到未来的十年，提供了一个新的视角，审视经典缩放中的固有递减回报。
2501.02111	 | How Your Location Relates to Health: Variable Importance and Interpretable Machine Learning for Environmental and Sociodemographic Data	 | Ishaan Maitra,Raymond Lin,Eric Chen,Jon Donnelly,Sanja Šćepanović,Cynthia Rudin	 | 该研究利用MEDSAT数据集分析了多种环境和社会经济因素对健康结果的影响，通过广义加性模型和多尺度地理加权回归开发了一种可解释的机器学习框架，发现NO2是多种健康问题如哮喘、高血压和焦虑的全球预测因子，同时揭示了空气污染和太阳辐射的局部变化。	 | Health outcomes depend on complex environmental and sociodemographic factors whose effects change over location and time. Only recently has fine-grained spatial and temporal data become available to study these effects, namely the MEDSAT dataset of English health, environmental, and sociodemographic information. Leveraging this new resource, we use a variety of variable importance techniques to robustly identify the most informative predictors across multiple health outcomes. We then develop an interpretable machine learning framework based on Generalized Additive Models (GAMs) and Multiscale Geographically Weighted Regression (MGWR) to analyze both local and global spatial dependencies of each variable on various health outcomes. Our findings identify NO2 as a global predictor for asthma, hypertension, and anxiety, alongside other outcome-specific predictors related to occupation, marriage, and vegetation. Regional analyses reveal local variations with air pollution and solar radiation, with notable shifts during COVID. This comprehensive approach provides actionable insights for addressing health disparities, and advocates for the integration of interpretable machine learning in public health.	 | 健康结果取决于复杂的环境和社会经济因素，这些因素的效果会因地理位置和时间而变化。直到最近，才有了足够细致的空间和时间数据来研究这些影响，例如英国健康、环境和社会经济信息的MEDSAT数据集。利用这一新资源，我们采用了多种变量重要性技术，以稳健的方式识别出对多种健康结果最具信息性的预测因子。然后，我们开发了一种基于广义加性模型（GAMs）和多尺度地理加权回归（MGWR）的可解释机器学习框架，以分析每个变量在各种健康结果上的局部和全局空间依赖性。我们的研究发现NO2是哮喘、高血压和焦虑等健康结果的全球预测因子，同时还包括与职业、婚姻和植被相关的其他特定结果预测因子。区域分析揭示了空气污染和太阳辐射的局部变化，特别是在COVID期间有显著变化。这一全面的方法提供了应对健康不平等的可操作见解，并倡导在公共卫生中整合可解释的机器学习。
2501.02107	 | Online Detection of Water Contamination Under Concept Drift	 | Jin Li,Kleanthis Malialis,Stelios G. Vrachimis,Marios M. Polycarpou	 | 该研究提出了一种名为双阈值异常和漂移检测（AD&DD）的方法，结合了双阈值漂移检测机制和LSTM-VAE，用于实时检测水分配网络中的污染，并有效识别传感器偏移引起的概念漂移，性能优于其他方法。	 | Water Distribution Networks (WDNs) are vital infrastructures, and contamination poses serious public health risks. Harmful substances can interact with disinfectants like chlorine, making chlorine monitoring essential for detecting contaminants. However, chlorine sensors often become unreliable and require frequent calibration. This study introduces the Dual-Threshold Anomaly and Drift Detection (AD&DD) method, an unsupervised approach combining a dual-threshold drift detection mechanism with an LSTM-based Variational Autoencoder(LSTM-VAE) for real-time contamination detection. Tested on two realistic WDNs, AD&DD effectively identifies anomalies with sensor offsets as concept drift, and outperforms other methods. A proposed decentralized architecture enables accurate contamination detection and localization by deploying AD&DD on selected nodes.	 | 水分配网络（WDNs）是重要的基础设施，污染会严重威胁公共卫生。有害物质可以与消毒剂如氯发生交互作用，因此氯监测对于检测污染物至关重要。然而，氯传感器经常变得不可靠且需要频繁校准。本研究引入了一种称为双阈值异常和漂移检测（AD&DD）的方法，该方法结合了双阈值漂移检测机制和基于LSTM的变分自编码器（LSTM-VAE），用于实时检测污染。该方法在两个真实的WDNs上进行了测试，有效识别出由于传感器偏移引起的概念漂移，并且在性能上优于其他方法。提出的去中心化架构通过在选定节点部署AD&DD，能够实现准确的污染检测和定位。
2501.02089	 | On the Statistical Complexity for Offline and Low-Adaptive Reinforcement Learning with Structures	 | Ming Yin,Mengdi Wang,Yu-Xiang Wang	 | 本文回顾了离线和低适应性设置下强化学习的统计基础的最新进展，重点讨论了离线策略评估和离线下策略学习的关键问题及其算法思想，并指出了离线RL的局限性及低适应性探索的重要性。	 | This article reviews the recent advances on the statistical foundation of reinforcement learning (RL) in the offline and low-adaptive settings. We will start by arguing why offline RL is the appropriate model for almost any real-life ML problems, even if they have nothing to do with the recent AI breakthroughs that use RL. Then we will zoom into two fundamental problems of offline RL: offline policy evaluation (OPE) and offline policy learning (OPL). It may be surprising to people that tight bounds for these problems were not known even for tabular and linear cases until recently. We delineate the differences between worst-case minimax bounds and instance-dependent bounds. We also cover key algorithmic ideas and proof techniques behind near-optimal instance-dependent methods in OPE and OPL. Finally, we discuss the limitations of offline RL and review a burgeoning problem of \emph{low-adaptive exploration} which addresses these limitations by providing a sweet middle ground between offline and online RL.	 | 本文回顾了在离线和低适应性设置下强化学习（RL）的统计基础的最新进展。我们将从以下几个方面展开讨论：首先，我们论证为什么离线RL是几乎所有实际生活中机器学习问题的合适模型，即使这些问题与最近使用RL的AI突破无关。然后，我们将聚焦于离线RL的两个基本问题：离线策略评估（OPE）和离线下策略学习（OPL）。对于这些问题是紧的界限，即使是对于表格形式和线性情况，直到最近也没有被人们所熟知。我们将区分最坏情况的对数最小最大界限和实例依赖的界限。我们还将覆盖OPE和OPL中接近最优实例依赖方法的关键算法思想和证明技术。最后，我们将讨论离线RL的局限性，并回顾一个 burgeoning 的问题，即低适应性探索，该问题通过提供离线和在线RL之间的甜蜜中间地带来解决这些局限性。
2501.02087	 | Beyond CVaR: Leveraging Static Spectral Risk Measures for Enhanced Decision-Making in Distributional Reinforcement Learning	 | Mehrdad Moghimi,Hyejin Ku	 | 本文提出了一种新型的分布强化学习算法，该算法能够优化更广泛的静态光谱风险度量类，并提供了理论保证和对学习策略的清晰解释，实验结果表明该方法在多种场景下优于现有模型。	 | In domains such as finance, healthcare, and robotics, managing worst-case scenarios is critical, as failure to do so can lead to catastrophic outcomes. Distributional Reinforcement Learning (DRL) provides a natural framework to incorporate risk sensitivity into decision-making processes. However, existing approaches face two key limitations: (1) the use of fixed risk measures at each decision step often results in overly conservative policies, and (2) the interpretation and theoretical properties of the learned policies remain unclear. While optimizing a static risk measure addresses these issues, its use in the DRL framework has been limited to the simple static CVaR risk measure. In this paper, we present a novel DRL algorithm with convergence guarantees that optimizes for a broader class of static Spectral Risk Measures (SRM). Additionally, we provide a clear interpretation of the learned policy by leveraging the distribution of returns in DRL and the decomposition of static coherent risk measures. Extensive experiments demonstrate that our model learns policies aligned with the SRM objective, and outperforms existing risk-neutral and risk-sensitive DRL models in various settings.	 | 在金融、医疗和机器人等领埴中，管理最坏情况场景至关重要，因为未能这样做可能导致灾难性后果。分布强化学习（DRL）提供了一种自然框架，可以在决策过程中纳入风险敏感性。然而，现有的方法面临两个关键限制：（1）在每个决策步骤中使用固定的风险度量往往会导致过于保守的策略；（2）对学习到的策略的解释及其理论性质仍然不清楚。尽管优化静态风险度量可以解决这些问题，但在DRL框架中的应用仅限于简单的静态CVaR风险度量。在本文中，我们提出了一种具有收敛保证的新型DRL算法，该算法优化了更广泛静态光谱风险度量（SRM）类。此外，我们通过利用DRL中的收益率分布和静态一致风险度量的分解，对学习到的策略进行了清晰的解释。广泛实验表明，我们的模型会学习符合SRM目标的策略，并在多种设置中优于现有的风险中性和风险敏感的DRL模型。
2501.02069	 | Counterfactual Explanation for Auto-Encoder Based Time-Series Anomaly Detection	 | Abhishek Srinivasan,Varun Singapuri Ravi,Juan Carlos Andresen,Anders Holst	 | 本文提出了一种结合特征选择器和基于梯度的反事实解释方法来提高基于自动编码器的异常检测模型的可解释性和可信度。实验结果表明，该方法能够提供有价值的模型决策过程见解，提升异常检测模型的可靠性和透明度。	 | The complexity of modern electro-mechanical systems require the development of sophisticated diagnostic methods like anomaly detection capable of detecting deviations. Conventional anomaly detection approaches like signal processing and statistical modelling often struggle to effectively handle the intricacies of complex systems, particularly when dealing with multi-variate signals. In contrast, neural network-based anomaly detection methods, especially Auto-Encoders, have emerged as a compelling alternative, demonstrating remarkable performance. However, Auto-Encoders exhibit inherent opaqueness in their decision-making processes, hindering their practical implementation at scale. Addressing this opacity is essential for enhancing the interpretability and trustworthiness of anomaly detection models. In this work, we address this challenge by employing a feature selector to select features and counterfactual explanations to give a context to the model output. We tested this approach on the SKAB benchmark dataset and an industrial time-series dataset. The gradient based counterfactual explanation approach was evaluated via validity, sparsity and distance measures. Our experimental findings illustrate that our proposed counterfactual approach can offer meaningful and valuable insights into the model decision-making process, by explaining fewer signals compared to conventional approaches. These insights enhance the trustworthiness and interpretability of anomaly detection models.	 | 现代电气机械系统的复杂性要求发展出高明的诊断方法，如能够检测异常的异常检测技术。传统的异常检测方法，如信号处理和统计建模，往往难以有效地处理复杂系统的复杂性，尤其是在处理多变量信号时更是如此。相比之下，基于神经网络的异常检测方法，尤其是自动编码器，已经显示出强大的潜力，表现出卓越的性能。然而，自动编码器在其决策过程中固有的不透明性，阻碍了其在大规模应用中的实际实施。解决这一不透明性对于提高异常检测模型的可解释性和可信度至关重要。在本项工作中，我们通过使用特征选择器来选择特征，并利用反事实解释来为模型输出提供上下文，以应对这一挑战。我们在这项工作中使用了SKAB基准数据集和一个工业时间序列数据集进行了测试。基于梯度的反事实解释方法通过有效性、稀疏性和距离度量进行了评估。我们的实验结果表明，我们提出的方法可以提供关于模型决策过程的有意义且有价值的见解，通过解释更少的信号与传统方法相比。这些见解提高了异常检测模型的可信赖度和可解释性。
2501.02059	 | Active Learning Enables Extrapolation in Molecular Generative Models	 | Evan R. Antoniuk,Peggy Li,Nathan Keilbart,Stephen Weitzner,Bhavya Kailkhura,Anna M. Hiszpanski	 | 此研究通过创建一个基于主动学习的闭环分子生成管道，解决了生成模型在泛化新化学空间方面的能力不足问题，该方法能够生成性能优于训练数据的新分子，并显著提高了生成稳定分子的比例。	 | Although generative models hold promise for discovering molecules with optimized desired properties, they often fail to suggest synthesizable molecules that improve upon the known molecules seen in training. We find that a key limitation is not in the molecule generation process itself, but in the poor generalization capabilities of molecular property predictors. We tackle this challenge by creating an active-learning, closed-loop molecule generation pipeline, whereby molecular generative models are iteratively refined on feedback from quantum chemical simulations to improve generalization to new chemical space. Compared against other generative model approaches, only our active learning approach generates molecules with properties that extrapolate beyond the training data (reaching up to 0.44 standard deviations beyond the training data range) and out-of-distribution molecule classification accuracy is improved by 79%. By conditioning molecular generation on thermodynamic stability data from the active-learning loop, the proportion of stable molecules generated is 3.5x higher than the next-best model.	 | 尽管生成模型在发现具有优化所需特性的分子方面具有潜力，但它们往往无法建议可合成且性能优于训练中已知分子的新分子。我们发现，主要限制不是在分子生成过程本身，而是在分子特性预测器的差劲泛化能力。我们通过创建一个基于主动学习的闭环分子生成管道来解决这一挑战，其中分子生成模型通过从量子化学模拟中获得的反馈迭代改进，以提高对新化学空间的泛化能力。与其他生成模型方法相比，只有我们的主动学习方法能够生成特性能外推到训练数据之外的分子（超出训练数据范围0.44个标准差），并且离分布外分子分类准确性提高了79%。通过在主动学习环中将分子生成与热力学稳定性数据进行条件约束，生成的稳定分子的比例比下一个最佳模型高3.5倍。
2501.02042	 | Towards Robust and Accurate Stability Estimation of Local Surrogate Models in Text-based Explainable AI	 | Christopher Burger,Charles Walter,Thai Le,Lingwei Chen	 | 该研究探讨了自然语言处理中可解释人工智能方法在对抗性攻击下的脆弱性，尤其是在使用XAI作为决策基础时的担忧，并提出了一种新的加权方案来提高XAI方法对对抗性攻击的鲁棒性。	 | Recent work has investigated the concept of adversarial attacks on explainable AI (XAI) in the NLP domain with a focus on examining the vulnerability of local surrogate methods such as Lime to adversarial perturbations or small changes on the input of a machine learning (ML) model. In such attacks, the generated explanation is manipulated while the meaning and structure of the original input remain similar under the ML model. Such attacks are especially alarming when XAI is used as a basis for decision making (e.g., prescribing drugs based on AI medical predictors) or for legal action (e.g., legal dispute involving AI software). Although weaknesses across many XAI methods have been shown to exist, the reasons behind why remain little explored. Central to this XAI manipulation is the similarity measure used to calculate how one explanation differs from another. A poor choice of similarity measure can lead to erroneous conclusions about the stability or adversarial robustness of an XAI method. Therefore, this work investigates a variety of similarity measures designed for text-based ranked lists referenced in related work to determine their comparative suitability for use. We find that many measures are overly sensitive, resulting in erroneous estimates of stability. We then propose a weighting scheme for text-based data that incorporates the synonymity between the features within an explanation, providing more accurate estimates of the actual weakness of XAI methods to adversarial examples.	 | 近年来，有研究探讨了自然语言处理（NLP）领域可解释人工智能（XAI）中的对抗性攻击概念，重点关注局部替代方法（如Lime）对对抗性扰动或输入的小变化的脆弱性。在这种攻击中，生成的解释被操纵，但原始输入在机器学习（ML）模型下的意义和结构保持相似。当XAI被用作决策基础（例如，基于AI医学预测开药）或法律行动（例如，涉及AI软件的法律纠纷）时，这种攻击尤其令人担忧。虽然已证明许多XAI方法存在弱点，但其背后的原因仍很少被探索。XAI操纵的核心在于用于计算一种解释与另一种解释之间差异的相似度度量。不恰当的相似度度量选择可能导致关于XAI方法稳定性和对抗性鲁棒性的错误结论。因此，本研究调查了一系列用于文本排序列表的相似度度量，参考相关研究中的设计，以确定它们在实际应用中的比较适用性。我们发现许多度量过于敏感，导致对稳定性的错误估计。然后，我们提出了一个适用于文本数据的加权方案，该方案考虑了解释中特征之间的同义性，提供更准确的XAI方法对对抗性示例实际脆弱性的估计。
2501.02038	 | Architecture for Trajectory-Based Fishing Ship Classification with AIS Data	 | David Sánchez Pedroche,Daniel Amigo,Jesús García,Jose M. Molina	 | 本文提出了一种用于管理和检测实际海况下动力数据的数据准备流程，解决了噪声、不一致性和类别不均衡等问题，并通过时空特征提取实现了渔业船只与非渔业船只的二元分类。实验结果表明，即使使用最少信息，该流程仍能有效解决问题。	 | This paper proposes a data preparation process for managing real-world kinematic data and detecting fishing vessels. The solution is a binary classification that classifies ship trajectories into either fishing or non-fishing ships. The data used are characterized by the typical problems found in classic data mining applications using real-world data, such as noise and inconsistencies. The two classes are also clearly unbalanced in the data, a problem which is addressed using algorithms that resample the instances. For classification, a series of features are extracted from spatiotemporal data that represent the trajectories of the ships, available from sequences of Automatic Identification System (AIS) reports. These features are proposed for the modelling of ship behavior but, because they do not contain context-related information, the classification can be applied in other scenarios. Experimentation shows that the proposed data preparation process is useful for the presented classification problem. In addition, positive results are obtained using minimal information.	 | 本文提出了一种数据准备流程，用于管理和检测实际海况下的动力数据，并识别渔业船只。该解决方案是一个二元分类问题，将船只轨迹分为渔业船只和非渔业船只两类。所使用的数据具有经典数据挖掘应用中常见的典型问题，如噪声和不一致，例如在实际数据中常见的问题。此外，数据中的两类也存在明显的不均衡问题，通过重采样实例的算法解决了这一问题。为了进行分类，从自动识别系统（AIS）报告序列中的船只轨迹数据中提取了一系列时空特征，这些特征用于建模船只行为，但由于这些特征不包含上下文相关信息，因此分类可以应用于其他场景。实验表明，提出的数据准备流程对所提出的分类问题是有用的。此外，使用最少的信息也取得了积极的结果。
2501.02036	 | Deep Clustering via Community Detection	 | Tianyu Cheng,Qun Chen	 | 本文提出了一种新颖的深度聚类方法，通过社区检测初始化聚类并逐步扩展，从而提高聚类性能，尤其在缺乏准确标注数据的情况下表现出色。实验结果表明该方法能够显著提升当前的最佳聚类性能，并且通过社区检测可以获得更高质量的伪标签，进一步增强聚类效果。	 | Deep clustering is an essential task in modern artificial intelligence, aiming to partition a set of data samples into a given number of homogeneous groups (i.e., clusters). Even though many Deep Neural Network (DNN) backbones and clustering strategies have been proposed for the task, achieving increasingly improved performance, deep clustering remains very challenging due to the lack of accurately labeled samples. In this paper, we propose a novel approach of deep clustering via community detection. It initializes clustering by detecting many communities, and then gradually expands clusters by community merging. Compared with the existing clustering strategies, community detection factors in the new perspective of cluster network analysis. As a result, it has the inherent benefit of high pseudo-label purity, which is critical to the performance of self-supervision. We have validated the efficacy of the proposed approach on benchmark image datasets. Our extensive experiments have shown that it can effectively improve the SOTA performance. Our ablation study also demonstrates that the new network perspective can effectively improve community pseudo-label purity, resulting in improved clustering performance.	 | 深度聚类是现代人工智能中的一个重要任务，旨在将一组数据样本划分为给定数量的同质组（即聚类）。尽管已经提出了许多深度神经网络（DNN）骨干网络和聚类策略来完成这一任务，取得了越来越好的性能，但深度聚类仍然因缺乏准确标注的数据样本而极具挑战性。在本文中，我们提出了一种通过社区检测来进行深度聚类的新型方法。该方法通过检测许多社区来初始化聚类，然后通过社区合并逐步扩展聚类。与现有的聚类策略相比，社区检测从聚类网络分析的新视角出发，因此具有较高的伪标签纯度的固有优势，这对自监督的表现至关重要。我们已经在基准图像数据集上验证了所提出方法的有效性。我们的大量实验表明，它能够有效地提高当前最佳性能。此外，我们的消融研究也表明，新的网络视角可以有效提高社区伪标签的纯度，从而提高聚类性能。
2501.02029	 | Spot Risks Before Speaking! Unraveling Safety Attention Heads in Large Vision-Language Models	 | Ziwei Zheng,Junyao Zhao,Le Yang,Lijun He,Fan Li	 | 该研究发现，大型视觉-语言模型在生成第一个词元期间的内部激活可以通过“安全头”有效识别恶意提示，这些安全头能够抵御各种攻击，且不会影响模型功能。通过利用这些安全头，研究者构建了一个简单的恶意提示检测器，能够在不增加额外推理开销的情况下，显著提升模型安全性。	 | With the integration of an additional modality, large vision-language models (LVLMs) exhibit greater vulnerability to safety risks (e.g., jailbreaking) compared to their language-only predecessors. Although recent studies have devoted considerable effort to the post-hoc alignment of LVLMs, the inner safety mechanisms remain largely unexplored. In this paper, we discover that internal activations of LVLMs during the first token generation can effectively identify malicious prompts across different attacks. This inherent safety perception is governed by sparse attention heads, which we term ``safety heads." Further analysis reveals that these heads act as specialized shields against malicious prompts; ablating them leads to higher attack success rates, while the model's utility remains unaffected. By locating these safety heads and concatenating their activations, we construct a straightforward but powerful malicious prompt detector that integrates seamlessly into the generation process with minimal extra inference overhead. Despite its simple structure of a logistic regression model, the detector surprisingly exhibits strong zero-shot generalization capabilities. Experiments across various prompt-based attacks confirm the effectiveness of leveraging safety heads to protect LVLMs. Code is available at \url{https://github.com/Ziwei-Zheng/SAHs}.	 | 随着额外模态的集成，大型视觉-语言模型（LVLMs）相较于仅语言版本在安全性风险（例如：脱笼攻击）方面表现出更大的脆弱性。虽然最近的研究已经投入了大量努力来对LVLMs进行事后对齐，但其内部的安全机制仍然很大程度上未被探索。在本文中，我们发现：LVLMs在生成第一个词元期间的内部激活可以有效地识别不同攻击下的恶意提示。这种内在的安全感知由稀疏的注意力头控制，我们称这些头为“安全头”（Safety Heads）。进一步的分析表明，这些头能够专门抵御恶意提示；删除它们会导致更高的攻击成功率，而模型的功能不受影响。通过定位这些安全头并连接它们的激活，我们构建了一个简单而强大的恶意提示检测器，该检测器可以无缝地集成到生成过程中，且额外的推理开销较小。尽管检测器的基本结构仅是一个逻辑回归模型，但它却表现出惊人的零样本泛化能力。在各种基于提示的攻击实验中，利用安全头保护LVLMs的有效性得到了验证。代码可在<https://github.com/Ziwei-Zheng/SAHs> 获取。
2501.02025	 | RealDiffFusionNet: Neural Controlled Differential Equation Informed Multi-Head Attention Fusion Networks for Disease Progression Modeling Using Real-World Data	 | Aashish Cheruvu,Nathaniel Rigoni	 | 本文提出了一种名为RealDiffFusionNet的新方法，结合了神经控制微分方程和多头注意力机制，用于对齐多模态上下文数据，并在两个不同数据集上展示了优于基线模型的性能。该方法在预测疾病进展方面表现出色，特别是在使用多模态数据时。	 | This paper presents a novel deep learning-based approach named RealDiffFusionNet incorporating Neural Controlled Differential Equations (Neural CDE) - time series models that are robust in handling irregularly sampled data - and multi-head attention to align relevant multimodal context (image data, time invariant data, etc.) at each time point. Long short-term memory (LSTM) models were also used as a baseline. Two different datasets were used: a data from the Open-Source Imaging Consortium (OSIC) containing structured time series data of demographics and lung function with a baseline CT scan of the lungs and the second from the Alzheimer's Disease Neuroimaging Initiative (ADNI) containing a series of MRI scans along with demographics, physical examinations, and cognitive assessment data. An ablation study was performed to understand the role of CDEs, multimodal data, attention fusion, and interpolation strategies on model performance. When the baseline models were evaluated, the use of multimodal data resulted in an improvement in Neural CDE performance, with a lower test RMSE. Additionally, the performance of multimodal Neural CDE was also superior to multimodal LSTM. In the attention-based architectures, fusion through concatenation and rectilinear interpolation were found to improve model performance. The performance of the proposed RealDiffFusionNet was found to be superior (0.2570) to all models. For the ADNI dataset, between the Neural-CDE and LSTM models trained only on the structured data, the test RMSE were comparable (0.471 for LSTM vs. 0.4581 Neural-CDE). Furthermore, the addition of image features from patients' MRI series resulted in an improvement in performance, with a lower test RMSE (0.4372 with multimodal vs 0.4581 with structured data). RealDiffFusionNet has shown promise in utilizing CDEs and multimodal data to accurately predict disease progression.	 | 本文提出了一种名为RealDiffFusionNet的新型基于深度学习的方法，该方法结合了神经控制微分方程（Neural CDE）——一种在处理不规则采样数据方面表现稳健的时间序列模型——和多头注意力机制，以在每个时间点对相关多模态上下文（图像数据、时间不变数据等）进行对齐。还使用了长短期记忆（LSTM）模型作为基线。使用了两个不同的数据集：一个是来自开源成像联盟（OSIC）的数据，包含人口统计学和肺功能的结构化时间序列数据以及肺部的基本CT扫描图像；另一个是来自阿尔茨海默病神经影像学倡议（ADNI）的数据，包含一系列MRI扫描以及人口统计学、体格检查和认知评估数据。进行了消融研究，以了解控制微分方程（CDEs）、多模态数据、注意力融合以及插值策略在模型性能中的作用。当评估基线模型时，使用多模态数据导致了神经CDE性能的改进，并且测试RMSE较低。此外，多模态神经CDE的性能也优于多模态LSTM。在基于注意力的架构中，通过连接和矩形插值的融合被发现能够提高模型性能。所提出的RealDiffFusionNet的性能被发现优于所有模型（0.2570）。对于ADNI数据集，仅使用结构化数据训练的神经CDE模型和LSTM模型的测试RMSE相当（LSTM为0.471，神经CDE为0.4581）。此外，加入患者MRI系列中的图像特征也提高了性能，测试RMSE较低（使用多模态数据为0.4372，仅使用结构化数据为0.4581）。RealDiffFusionNet展示了使用CDEs和多模态数据准确预测疾病进展的潜力。
2501.02021	 | Weakly Supervised Learning on Large Graphs	 | Aditya Prakash	 | 该研究提出了一种弱监督图分类框架，利用滑动窗口和BFS基子图提取技术，并通过图注意网络识别具有分类信息的子图，实现对图整体标签的分类，从而减少对详细子图标注的需求。	 | Graph classification plays a pivotal role in various domains, including pathology, where images can be represented as graphs.In this domain, images can be represented as graphs, where nodes might represent individual nuclei, and edges capture the spatial or functional relationships between them. Often, the overall label of the graph, such as a cancer type or disease state, is determined by patterns within smaller, localized regions of the image. This work introduces a weakly-supervised graph classification framework leveraging two subgraph extraction techniques: (1) Sliding-window approach (2) BFS-based approach. Subgraphs are processed using a Graph Attention Network (GAT), which employs attention mechanisms to identify the most informative subgraphs for classification. Weak supervision is achieved by propagating graph-level labels to subgraphs, eliminating the need for detailed subgraph annotations.	 | 图分类在多个领域中发挥着关键作用，包括病理学，其中图像可以表示为图。在这一领域中，图像可以表示为图，其中节点可能代表单个核，而边则捕捉它们之间的空间或功能关系。通常，图的整体标签（如癌症类型或疾病状态）是由图像中较小区域内的模式决定的。本研究介绍了一种基于两种子图提取技术的弱监督图分类框架：（1）滑动窗口方法；（2）BFS基方法。子图通过图注意网络（GAT）进行处理，该网络采用注意机制来识别最具有分类信息的子图。通过将图级别的标签传播到子图中实现弱监督，从而避免了对详细子图标注的需求。
2501.02019	 | Benchmarking Constraint-Based Bayesian Structure Learning Algorithms: Role of Network Topology	 | Radha Nagarajan,Marco Scutari	 | 该研究探讨了不同网络拓扑结构对约束式贝叶斯结构学习算法敏感性的影响，发现从子线性到超线性拓扑结构的敏感性估计存在统计学显著下降，强调了在网络拓扑设计中考虑拓扑结构的重要性。	 | Modeling the associations between real world entities from their multivariate cross-sectional profiles can provide cues into the concerted working of these entities as a system. Several techniques have been proposed for deciphering these associations including constraint-based Bayesian structure learning (BSL) algorithms that model them as directed acyclic graphs. Benchmarking these algorithms have typically focused on assessing the variation in performance measures such as sensitivity as a function of the dimensionality represented by the number of nodes in the DAG, and sample size. The present study elucidates the importance of network topology in benchmarking exercises. More specifically, it investigates variations in sensitivity across distinct network topologies while constraining the nodes, edges, and sample-size to be identical, eliminating these as potential confounders. Sensitivity of three popular constraint-based BSL algorithms (Peter-Clarke, Grow-Shrink, Incremental Association Markov Blanket) in learning the network structure from multivariate cross-sectional profiles sampled from network models with sub-linear, linear, and super-linear DAG topologies generated using preferential attachment is investigated. Results across linear and nonlinear models revealed statistically significant $(α=0.05)$ decrease in sensitivity estimates from sub-linear to super-linear topology constitutively across the three algorithms. These results are demonstrated on networks with nodes $(N_{nods}=48,64)$, noise strengths $(σ=3,6)$ and sample size $(N = 2^{10})$. The findings elucidate the importance of accommodating the network topology in constraint-based BSL benchmarking exercises.	 | 从多个变量的横截面轮廓来建模现实世界实体之间的关联，可以提供这些实体作为一个系统共同作用的线索。已提出多种技术来解析这些关联，包括将它们建模为有向无环图（DAG）的约束式贝叶斯结构学习（BSL）算法。这些算法通常通过评估由DAG中节点数量表示的维度变化以及样本大小对性能指标如敏感性的变化来进行基准测试。本研究阐明了网络拓扑在基准测试中的重要性。具体而言，它调查了在节点、边和样本量都相同的条件下，不同网络拓扑结构下的敏感性变化，从而排除这些潜在混杂因素。研究考察了三种流行的约束式BSL算法（彼得-克拉克、Grow-Shrink、增量关联马尔可夫边界）从使用偏好连接生成的子线性、线性和超线性DAG拓扑结构的网络模型中学习网络结构时的敏感性。在不同的线性和非线性模型中进行的结果表明，在三种算法中，从子线性到超线性拓扑结构的敏感性估计存在统计学显著性（α=0.05）的下降。这些结果在具有节点数（N_{nods}=48,64）、噪声强度（σ=3,6）和样本量（N = 2^{10}）的网络上进行了演示。研究结果阐明了在约束式BSL基准测试中考虑网络拓扑的重要性。
2501.02016	 | ST-HCSS: Deep Spatio-Temporal Hypergraph Convolutional Neural Network for Soft Sensing	 | Hwa Hui Tew,Fan Ding,Gaoxuan Li,Junn Yong Loo,Chee-Ming Ting,Ze Yang Ding,Chee Pin Tan	 | 本文提出了一种深度时空超图卷积神经网络（ST-HCSS），能够在缺乏先验结构知识的情况下构建高阶图来模型传感器节点之间的复杂交互，从而更准确地描述传感器数据中的非线性动力学，并且实验结果验证了其在软传感器技术中的优越性。	 | Higher-order sensor networks are more accurate in characterizing the nonlinear dynamics of sensory time-series data in modern industrial settings by allowing multi-node connections beyond simple pairwise graph edges. In light of this, we propose a deep spatio-temporal hypergraph convolutional neural network for soft sensing (ST-HCSS). In particular, our proposed framework is able to construct and leverage a higher-order graph (hypergraph) to model the complex multi-interactions between sensor nodes in the absence of prior structural knowledge. To capture rich spatio-temporal relationships underlying sensor data, our proposed ST-HCSS incorporates stacked gated temporal and hypergraph convolution layers to effectively aggregate and update hypergraph information across time and nodes. Our results validate the superiority of ST-HCSS compared to existing state-of-the-art soft sensors, and demonstrates that the learned hypergraph feature representations aligns well with the sensor data correlations. The code is available at https://github.com/htew0001/ST-HCSS.git	 | 在现代工业环境中，高阶传感器网络通过允许多节点连接（超越简单的两节点连线）能够更准确地描述感官时间序列数据中的非线性动力学。基于这一点，我们提出了一种深度时空超图卷积神经网络（ST-HCSS）用于软传感器技术。特别是，我们提出的方法能够在缺乏先验结构知识的情况下构建并利用高阶图（超图）来模型传感器节点之间的复杂多重交互。为了捕捉传感器数据下的丰富时空关系，我们提出的ST-HCSS结合了堆叠的门控时间卷积和超图卷积层，以有效地在时间和节点之间聚合和更新超图信息。我们的实验结果验证了ST-HCSS在与现有最先进的软传感器相比的优势，并表明学习到的超图特征表示很好地与传感器数据之间的相关性对齐。代码可在 https://github.com/htew0001/ST-HCSS.git 获取。
2501.02015	 | KANS: Knowledge Discovery Graph Attention Network for Soft Sensing in Multivariate Industrial Processes	 | Hwa Hui Tew,Gaoxuan Li,Fan Ding,Xuewen Luo,Junn Yong Loo,Chee-Ming Ting,Ze Yang Ding,Chee Pin Tan	 | 该研究提出了一种名为KANS的知识发现图注意网络框架，用于工业过程中的软感知，能够自动发现多变量间的内在关联和复杂关系，优于现有方法并显著提高软感知准确性。	 | Soft sensing of hard-to-measure variables is often crucial in industrial processes. Current practices rely heavily on conventional modeling techniques that show success in improving accuracy. However, they overlook the non-linear nature, dynamics characteristics, and non-Euclidean dependencies between complex process variables. To tackle these challenges, we present a framework known as a Knowledge discovery graph Attention Network for effective Soft sensing (KANS). Unlike the existing deep learning soft sensor models, KANS can discover the intrinsic correlations and irregular relationships between the multivariate industrial processes without a predefined topology. First, an unsupervised graph structure learning method is introduced, incorporating the cosine similarity between different sensor embedding to capture the correlations between sensors. Next, we present a graph attention-based representation learning that can compute the multivariate data parallelly to enhance the model in learning complex sensor nodes and edges. To fully explore KANS, knowledge discovery analysis has also been conducted to demonstrate the interpretability of the model. Experimental results demonstrate that KANS significantly outperforms all the baselines and state-of-the-art methods in soft sensing performance. Furthermore, the analysis shows that KANS can find sensors closely related to different process variables without domain knowledge, significantly improving soft sensing accuracy.	 | 工业过程中的难以测量变量的软感知通常至关重要。目前的实践主要依赖于传统的建模技术，这些技术在提高准确度方面显示出成功，但它们忽视了复杂过程变量之间的非线性特性、动力学特征以及非欧几里得依赖关系。为了解决这些挑战，我们提出了一种名为“知识发现图注意网络有效软感知”（KANS）的框架。与现有的深度学习软传感器模型不同，KANS可以在没有预定义拓扑的情况下发现多变量工业过程之间的内在关联和不规则关系。首先，我们引入了一种无监督的图结构学习方法，结合不同传感器嵌入之间的余弦相似度来捕捉传感器之间的关联。接着，我们提出了一种基于图注意机制的表示学习方法，可以并行处理多变量数据，以增强模型学习复杂传感器节点和边的能力。为了充分探索KANS，还进行了知识发现分析，以展示该模型的可解释性。实验结果表明，KANS在软感知性能上显著优于所有基线和最先进的方法。此外，分析显示，KANS可以在没有领域知识的情况下找到与不同工艺变量紧密相关的传感器，从而显著提高软感知的准确性。
2501.02014	 | Machine Learning-Based Differential Diagnosis of Parkinson's Disease Using Kinematic Feature Extraction and Selection	 | Masahiro Matsumoto,Abu Saleh Musa Miah,Nobuyoshi Asai,Jungpil Shin	 | 该研究提出了一种基于机器学习的系统，用于通过运动特征区分帕金森病（PD）、进行性核上性麻痹（PSP）和多系统萎缩（MSA）患者与健康对照组，分类准确率达到66.67%，每位患者的分类准确率为88.89%，尤其是对MSA和健康对照组的区分表现突出。	 | Parkinson's disease (PD), the second most common neurodegenerative disorder, is characterized by dopaminergic neuron loss and the accumulation of abnormal synuclein. PD presents both motor and non-motor symptoms that progressively impair daily functioning. The severity of these symptoms is typically assessed using the MDS-UPDRS rating scale, which is subjective and dependent on the physician's experience. Additionally, PD shares symptoms with other neurodegenerative diseases, such as progressive supranuclear palsy (PSP) and multiple system atrophy (MSA), complicating accurate diagnosis. To address these diagnostic challenges, we propose a machine learning-based system for differential diagnosis of PD, PSP, MSA, and healthy controls (HC). This system utilizes a kinematic feature-based hierarchical feature extraction and selection approach. Initially, 18 kinematic features are extracted, including two newly proposed features: Thumb-to-index vector velocity and acceleration, which provide insights into motor control patterns. In addition, 41 statistical features were extracted here from each kinematic feature, including some new approaches such as Average Absolute Change, Rhythm, Amplitude, Frequency, Standard Deviation of Frequency, and Slope. Feature selection is performed using One-way ANOVA to rank features, followed by Sequential Forward Floating Selection (SFFS) to identify the most relevant ones, aiming to reduce the computational complexity. The final feature set is used for classification, achieving a classification accuracy of 66.67% for each dataset and 88.89% for each patient, with particularly high performance for the MSA and HC groups using the SVM algorithm. This system shows potential as a rapid and accurate diagnostic tool in clinical practice, though further data collection and refinement are needed to enhance its reliability.	 | 帕金森病（PD），是仅次于阿尔茨海默病的第二大常见的神经退行性疾病，其特征包括多巴胺能神经元的丧失和异常α突触核蛋白的积累。PD 患者表现出运动和非运动症状，这些症状逐渐损害日常生活能力。这些症状的严重程度通常使用 MDS-UPDRS 评分量表进行主观评估，且依赖于医生的经验。此外，PD 的症状与其他神经退行性疾病，如进行性核上性麻痹（PSP）和多系统萎缩（MSA）相似，这使得准确的诊断变得复杂。为解决这些诊断挑战，我们提出了一种基于机器学习的系统，用于区分PD、PSP、MSA和健康对照组（HC）。该系统利用基于运动特征的层次特征提取和选择方法。首先提取了18个运动特征，包括两个新提出的特征：拇指到食指向量的速度和加速度，这些特征提供了有关运动控制模式的见解。此外，从每个运动特征中还提取了41个统计特征，包括平均绝对变化、节律、振幅、频率、频率的标准差和斜率等一些新方法。特征选择使用单因素ANOVA进行排序，随后使用逐步向前选择（SFFS）来识别最相关的特征，以降低计算复杂度。最终的特征集用于分类，每个数据集的分类准确率为66.67%，每位患者的分类准确率为88.89%，特别是在使用SVM算法时，MSA和HC组的表现尤为突出。该系统显示出作为临床实践中快速且准确的诊断工具的潜力，但为了提高其可靠性，还需要更多的数据收集和改进。
2501.02012	 | Information Subtraction: Learning Representations for Conditional Entropy	 | Keng Hou Leong,Yuxuan Xiu,Wai Kin(Victor)Chan	 | 本文提出了一种信息减法框架，用于生成同时保留所需信息并消除不需要信息的表示，特别适用于连续变量，以解释变量间的独特效应，并在公平学习和领域泛化方面表现出色。	 | The representations of conditional entropy and conditional mutual information are significant in explaining the unique effects among variables. While previous studies based on conditional contrastive sampling have effectively removed information regarding discrete sensitive variables, they have not yet extended their scope to continuous cases. This paper introduces Information Subtraction, a framework designed to generate representations that preserve desired information while eliminating the undesired. We implement a generative-based architecture that outputs these representations by simultaneously maximizing an information term and minimizing another. With its flexibility in disentangling information, we can iteratively apply Information Subtraction to represent arbitrary information components between continuous variables, thereby explaining the various relationships that exist between them. Our results highlight the representations' ability to provide semantic features of conditional entropy. By subtracting sensitive and domain-specific information, our framework demonstrates effective performance in fair learning and domain generalization. The code for this paper is available at https://github.com/jh-liang/Information-Subtraction	 | 条件熵和条件互信息的表示对于解释变量间的独特效应至关重要。尽管基于条件对比采样的先前研究已有效地去除了关于离散敏感变量的信息，但它们尚未将范围扩展到连续情况。本文提出了信息减法（Information Subtraction）框架，该框架旨在生成同时保留所需信息并消除不需要信息的表示。我们实现了一个生成型架构，该架构通过同时最大化一个信息项并最小化另一个项来输出这些表示。凭借其在分解信息方面的灵活性，我们可以不断应用信息减法来表示连续变量间任意的信息组件，从而解释它们之间存在的各种关系。我们的结果突显了这些表示具有提供条件熵语义特征的能力。通过去除敏感和领域特定信息，我们的框架在公平学习和领域泛化方面表现出有效性能。本文的代码可以在https://github.com/jh-liang/Information-Subtraction 获取。
2501.02010	 | Explainable Neural Networks with Guarantees: A Sparse Estimation Approach	 | Antoine Ledent,Peng Liu	 | 本文提出了一种名为SparXnet的新方法，这是一种能同时平衡预测能力和解释能力的可解释神经网络。通过稀疏且联合学习的特征线性组合，SparXnet能够在保持预测性能的同时选择重要特征，并且具有理论上的良好泛化边界。	 | Balancing predictive power and interpretability has long been a challenging research area, particularly in powerful yet complex models like neural networks, where nonlinearity obstructs direct interpretation. This paper introduces a novel approach to constructing an explainable neural network that harmonizes predictiveness and explainability. Our model, termed SparXnet, is designed as a linear combination of a sparse set of jointly learned features, each derived from a different trainable function applied to a single 1-dimensional input feature. Leveraging the ability to learn arbitrarily complex relationships, our neural network architecture enables automatic selection of a sparse set of important features, with the final prediction being a linear combination of rescaled versions of these features. We demonstrate the ability to select significant features while maintaining comparable predictive performance and direct interpretability through extensive experiments on synthetic and real-world datasets. We also provide theoretical analysis on the generalization bounds of our framework, which is favorably linear in the number of selected features and only logarithmic in the number of input features. We further lift any dependence of sample complexity on the number of parameters or the architectural details under very mild conditions. Our research paves the way for further research on sparse and explainable neural networks with guarantee.	 | 长期以来，平衡预测能力和可解释性一直是一个具有挑战性的研究领域，特别是在像神经网络这样强大但复杂的模型中，非线性性阻碍了直接解释。本文介绍了一种构建可解释神经网络的新方法，该方法能同时平衡预测能力和解释能力。我们的模型称为SparXnet，它设计为一组稀疏且联合学习的特征的线性组合，每个特征均源自应用于单一1维输入特征的不同可训练函数。利用学习任意复杂关系的能力，我们的神经网络架构能够自动选择一组重要的稀疏特征，并将最终预测表示为这些特征的重新缩放版本的线性组合。我们通过在合成和真实数据集上的广泛实验，展示了能够在保持类似预测性能的同时选择重要特征，并保持直接可解释性。我们还对框架的泛化边界进行了理论分析，这些分析表明泛化边界与选定特征的数量呈线性关系，与输入特征的数量呈对数关系。在极其温和的条件下，我们进一步消除了样本复杂度对参数数量或架构细节的依赖。我们的研究为进一步研究具有保证的稀疏和可解释神经网络铺平了道路。
2501.02007	 | TART: Token-based Architecture Transformer for Neural Network Performance Prediction	 | Yannis Y. He	 | 本文提出了一种基于令牌的架构Transformer（TART），能够在无需训练候选网络的情况下预测神经网络的性能，并在DeepNets-1M数据集上取得最先进的性能，展示了Transformer在自动神经网络设计中的潜力。	 | In the realm of neural architecture design, achieving high performance is largely reliant on the manual expertise of researchers. Despite the emergence of Neural Architecture Search (NAS) as a promising technique for automating this process, current NAS methods still require human input to expand the search space and cannot generate new architectures. This paper explores the potential of Transformers in comprehending neural architectures and their performance, with the objective of establishing the foundation for utilizing Transformers to generate novel networks. We propose the Token-based Architecture Transformer (TART), which predicts neural network performance without the need to train candidate networks. TART attains state-of-the-art performance on the DeepNets-1M dataset for performance prediction tasks without edge information, indicating the potential of Transformers to aid in discovering novel and high-performing neural architectures.	 | 在神经网络架构设计领域，实现高性能主要依赖于研究人员的手动专业知识。尽管神经架构搜索（Neural Architecture Search, NAS）作为一种有望自动完成这一过程的技术已经出现，但当前的NAS方法仍然需要人工输入来扩展搜索空间，并且无法生成新的架构。本文探讨了Transformer在理解神经网络及其性能方面的潜力，旨在为利用Transformer生成新型网络奠定基础。我们提出了基于令牌的架构Transformer（TART），该模型能够在无需训练候选网络的情况下预测神经网络的性能。在没有边缘信息的情况下，TART在DeepNets-1M数据集上的性能预测任务中达到了最先进的性能，这表明Transformer有可能帮助发现新颖且高性能的神经网络架构。
2501.02006	 | Multi-Task Semantic Communication With Graph Attention-Based Feature Correlation Extraction	 | Xi Yu,Tiejun Lv,Weicai Li,Wei Ni,Dusit Niyato,Ekram Hossain	 | 本文提出了一种新的图注意力跨块（GAI）模块，用于多任务语义通信系统的编码器，通过嵌入中间输出来丰富特征，从而提高了特征之间的相关性捕获能力，并在CityScapes 2Task和NYU V2 3Task数据集上显著优于现有模型。	 | Multi-task semantic communication can serve multiple learning tasks using a shared encoder model. Existing models have overlooked the intricate relationships between features extracted during an encoding process of tasks. This paper presents a new graph attention inter-block (GAI) module to the encoder/transmitter of a multi-task semantic communication system, which enriches the features for multiple tasks by embedding the intermediate outputs of encoding in the features, compared to the existing techniques. The key idea is that we interpret the outputs of the intermediate feature extraction blocks of the encoder as the nodes of a graph to capture the correlations of the intermediate features. Another important aspect is that we refine the node representation using a graph attention mechanism to extract the correlations and a multi-layer perceptron network to associate the node representations with different tasks. Consequently, the intermediate features are weighted and embedded into the features transmitted for executing multiple tasks at the receiver. Experiments demonstrate that the proposed model surpasses the most competitive and publicly available models by 11.4% on the CityScapes 2Task dataset and outperforms the established state-of-the-art by 3.97% on the NYU V2 3Task dataset, respectively, when the bandwidth ratio of the communication channel (i.e., compression level for transmission over the channel) is as constrained as 1 12 .	 | 多任务语义通信可以使用一个共享编码器模型服务于多个学习任务。现有的模型忽视了在任务编码过程中提取的特征之间的复杂关系。本文向多任务语义通信系统的编码/发送器引入了一种新的图注意力跨块（GAI）模块，通过在特征中嵌入编码过程中的中间输出来丰富特征，相比于现有技术。关键思想是将编码器的中间特征提取块的输出解释为图的节点，以捕捉中间特征之间的相关性。另一个重要方面是，我们使用图注意力机制对节点表示进行细化，以提取相关性，并使用多层感知机网络将节点表示与不同任务关联起来。因此，中间特征被加权并嵌入到传输给接收器以执行多个任务的特征中。实验结果表明，当通信信道的带宽比（即通过信道传输时的压缩水平）受到严格限制为1/12时，所提出的模型在CityScapes 2Task数据集上优于最具竞争力的公开模型11.4%，在NYU V2 3Task数据集上分别超过现有最先进的技术水平3.97%。
2501.02004	 | General Information Metrics for Improving AI Model Training Efficiency	 | Jianfeng Xu,Congcong Liu,Xiaoying Tan,Xiaojie Zhu,Anpeng Wu,Huan Wan,Weijun Kong,Chun Li,Hu Xu,Kun Kuang,Fei Wu	 | 本文提出了一种通用信息度量评估（GIME）方法，利用客观信息理论中的多种信息度量优化训练数据集选择，从而在减少训练时间和成本的同时保持模型性能；在多个领域的应用表明，GIME 可以显著降低模型训练费用，展现出在支持高效和可持续AI开发方面的潜力。	 | To address the growing size of AI model training data and the lack of a universal data selection methodology-factors that significantly drive up training costs -- this paper presents the General Information Metrics Evaluation (GIME) method. GIME leverages general information metrics from Objective Information Theory (OIT), including volume, delay, scope, granularity, variety, duration, sampling rate, aggregation, coverage, distortion, and mismatch to optimize dataset selection for training purposes. Comprehensive experiments conducted across diverse domains, such as CTR Prediction, Civil Case Prediction, and Weather Forecasting, demonstrate that GIME effectively preserves model performance while substantially reducing both training time and costs. Additionally, applying GIME within the Judicial AI Program led to a remarkable 39.56% reduction in total model training expenses, underscoring its potential to support efficient and sustainable AI development.	 | 为应对AI模型训练数据规模日益增长以及缺乏通用数据选择方法——这些因素显著推高了训练成本——本文提出了通用信息度量评估（GIME）方法。GIME 利用客观信息理论（OIT）中的通用信息度量，包括体积、延迟、范围、粒度、多样性、持续时间、采样率、聚合、覆盖率、失真和不匹配，来优化训练数据集的选择。通过跨不同领域（如点击率预测、民事案件预测和天气预报）的全面实验表明，GIME 能够在显著减少训练时间和成本的同时，有效保持模型性能。此外，在司法AI项目中应用GIME，实现了总模型训练费用减少了39.56%，这表明其在支持高效和可持续AI开发方面的巨大潜力。
2501.02002	 | HMM-LSTM Fusion Model for Economic Forecasting	 | Guhan Sivakumar	 | 本文研究了将隐马尔可夫模型（HMM）推导出的隐藏状态和均值作为额外特征融入长短期记忆（LSTM）神经网络的方法，以提高经济预测（特别是CPI通胀率）的准确性和可解释性。结果表明，这种方法在捕捉复杂时间模式和减轻经济波动影响方面效果显著。	 | This paper explores the application of Hidden Markov Models (HMM) and Long Short-Term Memory (LSTM) neural networks for economic forecasting, focusing on predicting CPI inflation rates. The study explores a new approach that integrates HMM-derived hidden states and means as additional features for LSTM modeling, aiming to enhance the interpretability and predictive performance of the models. The research begins with data collection and preprocessing, followed by the implementation of the HMM to identify hidden states representing distinct economic conditions. Subsequently, LSTM models are trained using the original and augmented data sets, allowing for comparative analysis and evaluation. The results demonstrate that incorporating HMM-derived data improves the predictive accuracy of LSTM models, particularly in capturing complex temporal patterns and mitigating the impact of volatile economic conditions. Additionally, the paper discusses the implementation of Integrated Gradients for model interpretability and provides insights into the economic dynamics reflected in the forecasting outcomes.	 | 本文探讨了使用隐马尔可夫模型（HMM）和长短期记忆（LSTM）神经网络进行经济预测的应用，重点关注预测CPI通胀率。研究探讨了一种新的方法，即将HMM推导出的隐藏状态和均值作为LSTM模型的附加特征，以提高模型的可解释性和预测性能。研究始于数据收集和预处理，然后使用HMM来识别代表不同经济条件的隐藏状态。接着，使用原始数据集和扩充数据集训练LSTM模型，以便进行比较分析和评估。结果表明，将HMM推导出的数据纳入模型可以提高LSTM模型的预测准确性，特别是在捕捉复杂的时间模式和减轻易变经济条件的影响方面。此外，本文还介绍了集成梯度方法来提高模型的可解释性，并提供了关于预测结果反映的经济动态的见解。
2501.02001	 | Communication Efficient Cooperative Edge AI via Event-Triggered Computation Offloading	 | You Zhou,Changsheng You,Kaibin Huang	 | 本文提出了一种基于通道自适应、事件触发的边缘推断框架，优先处理罕见事件，以提高自动驾驶、医疗保健和工业自动化等关键任务应用中的响应速度和准确性；该框架通过双阈值、多出口架构和优化的卸载策略，实现了更高的分类准确性和更少的通信开销。	 | Rare events, despite their infrequency, often carry critical information and require immediate attentions in mission-critical applications such as autonomous driving, healthcare, and industrial automation. The data-intensive nature of these tasks and their need for prompt responses, combined with designing edge AI (or edge inference), pose significant challenges in systems and techniques. Existing edge inference approaches often suffer from communication bottlenecks due to high-dimensional data transmission and fail to provide timely responses to rare events, limiting their effectiveness for mission-critical applications in the sixth-generation (6G) mobile networks. To overcome these challenges, we propose a channel-adaptive, event-triggered edge-inference framework that prioritizes efficient rare-event processing. Central to this framework is a dual-threshold, multi-exit architecture, which enables early local inference for rare events detected locally while offloading more complex rare events to edge servers for detailed classification. To further enhance the system's performance, we developed a channel-adaptive offloading policy paired with an online algorithm to dynamically determine the optimal confidence thresholds for controlling offloading decisions. The associated optimization problem is solved by reformulating the original non-convex function into an equivalent strongly convex one. Using deep neural network classifiers and real medical datasets, our experiments demonstrate that the proposed framework not only achieves superior rare-event classification accuracy, but also effectively reduces communication overhead, as opposed to existing edge-inference approaches.	 | 尽管罕见事件发生的频率较低，但在自动驾驶、医疗保健和工业自动化等关键任务应用中，它们往往携带关键信息并需要立即关注。这些任务的数据密集性和对即时响应的要求，以及对边缘AI（或边缘推断）的设计，给系统和技术带来了重大挑战。现有的边缘推断方法往往因高维数据传输导致通信瓶颈，无法及时响应罕见事件，限制了其在第六代（6G）移动网络中关键任务应用中的有效性。为了克服这些挑战，我们提出了一种基于通道自适应、事件触发的边缘推断框架，以优先处理罕见事件。该框架的核心是一种双阈值、多出口架构，能够本地快速推断检测到的罕见事件，并将更复杂的罕见事件卸载到边缘服务器进行详细分类。为了进一步提高系统的性能，我们开发了一种基于通道自适应的卸载策略，并结合一个在线算法，动态确定控制卸载决策的最佳置信度阈值。通过将原始非凸函数重新表述为等价的严格凸函数，解决了相关优化问题。使用深度神经网络分类器和真实医疗数据集进行的实验表明，所提出的框架不仅在罕见事件分类准确性方面表现优异，还能有效减少通信开销，优于现有的边缘推断方法。
2501.01990	 | Towards Sustainable Large Language Model Serving	 | Sophia Nguyen,Beihao Zhou,Yi Ding,Sihang Liu	 | 本文从碳排放角度研究了大型语言模型（LLM），通过分析不同GPU类型和能源组合下的性能、能耗和碳排放，为可持续LLM服务铺平了道路，并强调了同时考虑运行时和嵌入式碳排放的重要性。	 | In this work, we study LLMs from a carbon emission perspective, addressing both operational and embodied emissions, and paving the way for sustainable LLM serving. We characterize the performance and energy of LLaMA with 1B, 3B, and 7B parameters using two Nvidia GPU types, a latest-generation RTX6000 Ada and an older-generation T4. We analytically model operational carbon emissions based on energy consumption and carbon intensities from three grid regions -- each representing a different energy source mix, and embodied carbon emissions based on chip area and memory size. Our characterization and modeling provide us with an in-depth understanding of the performance, energy, and carbon emissions of LLM serving. Our findings highlight the potential for optimizing sustainable LLM serving systems by considering both operational and embodied carbon emissions simultaneously.	 | 在这项工作中，我们从碳排放的角度研究了大型语言模型（LLM），不仅考虑了运行时的碳排放，还考虑了嵌入式碳排放，为可持续LLM服务铺平了道路。我们使用两种Nvidia GPU类型（最新的RTX6000 Ada和较早的T4）对具有1B、3B和7B参数的LLaMA的性能和能量进行了表征。我们根据来自三个电网区域的能源消耗和碳强度（每个区域代表一种不同的能源组合）进行分析性建模，以表征运行时的碳排放；并根据芯片面积和内存大小来表征嵌入式碳排放。我们的表征和建模使我们深入了解了LLM服务的性能、能耗和碳排放。我们的研究结果突显了通过同时考虑运行时和嵌入式碳排放来优化可持续LLM服务系统的潜在可能性。
2501.01963	 | Statistical learning does not always entail knowledge	 | Daniel Andrés Díaz-Pachón,H. Renata Gallegos,Ola Hössjer,J. Sunil Rao	 | 本文研究了代理通过贝叶斯方法更新其对某一命题的信念，并探讨了主动信息概念下的最大熵分布特性，在特征信息不足时完全学习和知识获取的局限性，以及区分主要学习与次要学习的重要性。	 | In this paper, we study learning and knowledge acquisition (LKA) of an agent about a proposition that is either true or false. We use a Bayesian approach, where the agent receives data to update his beliefs about the proposition according to a posterior distribution. The LKA is formulated in terms of active information, with data representing external or exogenous information that modifies the agent's beliefs. It is assumed that data provide details about a number of features that are relevant to the proposition. We show that this leads to a Gibbs distribution posterior, which is in maximum entropy relative to the prior, conditioned on the side constraints that the data provide in terms of the features. We demonstrate that full learning is sometimes not possible and full knowledge acquisition is never possible when the number of extracted features is too small. We also distinguish between primary learning (receiving data about features of relevance for the proposition) and secondary learning (receiving data about the learning of another agent). We argue that this type of secondary learning does not represent true knowledge acquisition. Our results have implications for statistical learning algorithms, and we claim that such algorithms do not always generate true knowledge. The theory is illustrated with several examples.	 | 在本文中，我们研究了代理对某个命题（该命题要么为真，要么为假）的学习与知识获取（LKA）。我们采用了贝叶斯方法，其中代理通过接收数据来更新他对命题的信念，按照后验分布进行更新。LKA 用主动信息的概念来表述，数据代表外部或内生信息，这些信息会修改代理的信念。假设数据提供了关于命题相关特征的详细信息。我们展示了这种假设导致后验分布成为相对于先验分布的最大熵分布，并且在数据提供的特征约束条件下成立。我们证明了当提取的特征数量过少时，完全学习有时是不可行的，而完全的知识获取也是不可能的。我们还区分了主要学习（接收关于命题相关特征的数据）和次要学习（接收关于另一代理学习的数据）。我们主张这种次要学习并不真正代表知识的获取。我们的结果对统计学习算法具有重要影响，我们声称这类算法并不总是产生真正知识。本文通过几个例子来阐明这一理论。
2501.03228	 | LightGNN: Simple Graph Neural Network for Recommendation	 | Guoxuan Chen,Lianghao Xia,Chao Huang	 | LightGNN是一种轻量级且基于蒸馏的图形神经网络剪枝框架，旨在大幅减少模型复杂性同时保持推荐系统的性能，在多个公开数据集上表现出出色的计算效率和推荐准确度。通过引入高效的剪枝模块和资源友好型的层次知识蒸馏目标，LightGNN成功地将边的数量减少了80%，嵌入项减少了90%。	 | Graph neural networks (GNNs) have demonstrated superior performance in collaborative recommendation through their ability to conduct high-order representation smoothing, effectively capturing structural information within users' interaction patterns. However, existing GNN paradigms face significant challenges in scalability and robustness when handling large-scale, noisy, and real-world datasets. To address these challenges, we present LightGNN, a lightweight and distillation-based GNN pruning framework designed to substantially reduce model complexity while preserving essential collaboration modeling capabilities. Our LightGNN framework introduces a computationally efficient pruning module that adaptively identifies and removes redundant edges and embedding entries for model compression. The framework is guided by a resource-friendly hierarchical knowledge distillation objective, whose intermediate layer augments the observed graph to maintain performance, particularly in high-rate compression scenarios. Extensive experiments on public datasets demonstrate LightGNN's effectiveness, significantly improving both computational efficiency and recommendation accuracy. Notably, LightGNN achieves an 80% reduction in edge count and 90% reduction in embedding entries while maintaining performance comparable to more complex state-of-the-art baselines. The implementation of our LightGNN framework is available at the github repository: https://github.com/HKUDS/LightGNN.	 | 图形神经网络（GNNs）在协作推荐中展示了优越的性能，这主要得益于它们能够进行高阶表示平滑，有效捕捉用户交互模式中的结构信息。然而，现有的GNN范式在处理大规模、嘈杂且真实世界的数据集时面临着显著的可扩展性和鲁棒性挑战。为了解决这些挑战，我们提出了LightGNN，这是一种轻量级且基于蒸馏的GNN剪枝框架，旨在大幅减少模型复杂性，同时保留关键的协作建模能力。LightGNN框架引入了一个计算高效的剪枝模块，能够自适应地识别并移除冗余边和嵌入项，以实现模型压缩。该框架由资源友好型的层次知识蒸馏目标引导，中间层增强观察到的图以保持性能，特别是在高压缩率场景中。在多个公开数据集上的实验表明，LightGNN在提高计算效率和推荐准确度方面表现出色。值得注意的是，LightGNN在保持性能方面与更复杂的状态最先进基线相当，同时将边的数量减少了80%，嵌入项减少了90%。我们的LightGNN框架的实现可以在github仓库：https://github.com/HKUDS/LightGNN找到。
2501.03226	 | BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning	 | Beichen Zhang,Yuhong Liu,Xiaoyi Dong,Yuhang Zang,Pan Zhang,Haodong Duan,Yuhang Cao,Dahua Lin,Jiaqi Wang	 | 最新的大规模语言模型在解决复杂数学问题时，尽管表现良好，但受限于ICL示例中的粒度不匹配和负面噪音问题。为此，提出的BoostStep方法在步骤粒度上对检索和推理进行了对齐，提供高度相关的ICL示例，显著提升了模型在每个步骤上的推理质量，最终在多种数学基准测试中分别提高了GPT-4o和Qwen2.5-Math-72B的性能3.6%和2.0%，结合MCTS后性能进一步提升7.5%。	 | Cutting-edge large language models (LLMs) demonstrate promising performance in solving complex math problems with a divide-and-conquer pipeline and the assistance of in-context learning (ICL) examples. However, their potential for improvement is limited by two critical problems within their ICL examples: granularity-mismatch and the ensuing negative-effect noise problem. Specifically, the LLMs are capable of the dividing process yet mostly failed by inaccurate reasoning within a few conquer steps, while the ICL examples retrieved in question-grained sometimes lack relevant steps for a specific challenging reasoning step. Further, this disconnect may hinder the correct reasoning due to its irrelevance. To this end, we focus on improving the reasoning quality within each step and present BoostStep. BoostStep aligns the granularity between the retrieving and reasoning on step grained, and provides highly related ICL examples for each reasoning step with a novel `first-try' strategy. BoostStep provides more relevant examples than the coarse question-grained strategy, enhancing the model reasoning quality within each step steadily. BoostStep is a general and robust reasoning-enhancing method that not only improves standalone reasoning performance but also integrates seamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate generation and decision-making. Quantitatively, it improves GPT-4o and Qwen2.5-Math-72B by 3.6\% and 2.0\% respectively on various mathematical benchmarks, and 7.5\% gain combined with MCTS.	 | 最新的大规模语言模型（LLMs）在通过分而治之的管道解决复杂数学问题时，借助上下文学习（ICL）示例表现出令人鼓舞的性能。然而，它们的改进潜力受限于ICL示例中的两个关键问题：粒度不匹配和由此产生的负面噪音问题。具体来说，LLMs能够执行分割过程，但在几个征服步骤中的推理却经常出现不准确的情况，而问题粒度下检索到的ICL示例有时会缺乏特定复杂推理步骤的相关步骤，进一步导致这种不相关性可能妨碍正确推理。为了解决这些问题，我们专注于提高每个步骤的推理质量，并提出了BoostStep。BoostStep在步骤粒度上对检索和推理进行了对齐，并为每个推理步骤提供高度相关的ICL示例，采用一种新颖的“初次尝试”策略。BoostStep提供了比粗略的问题粒度策略更相关的示例，逐步增强模型在每个步骤上的推理质量。BoostStep是一种通用且稳健的推理增强方法，不仅提高了独立推理性能，还能无缝集成到蒙特卡洛树搜索方法（MCTS）中，以改进候选生成和决策。定量结果显示，BoostStep分别提高了GPT-4o和Qwen2.5-Math-72B在各种数学基准测试上的性能3.6%和2.0%，并结合MCTS后获得了7.5%的性能提升。
2501.03225	 | Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation	 | Yuhui Zhang,Yuchang Su,Yiming Liu,Xiaohan Wang,James Burgess,Elaine Sui,Chenyu Wang,Josiah Aklilu,Alejandro Lozano,Anjiang Wei,Ludwig Schmidt,Serena Yeung-Levy	 | 本文提出了一种名为AutoConverter的代理框架，能够将开放性视觉问答问题自动转换为多项选择题，从而实现对视觉语言模型的客观评估，并构建了VMCBench基准，包含9018个转换后的问题，用于全面评估33个最先进的视觉语言模型。	 | The rapid development of vision language models (VLMs) demands rigorous and reliable evaluation. However, current visual question answering (VQA) benchmarks often depend on open-ended questions, making accurate evaluation difficult due to the variability in natural language responses. To address this, we introduce AutoConverter, an agentic framework that automatically converts these open-ended questions into multiple-choice format, enabling objective evaluation while reducing the costly question creation process. Our experiments demonstrate that AutoConverter can generate correct and challenging multiple-choice questions, with VLMs demonstrating consistently similar or lower accuracy on these questions compared to human-created ones. Using AutoConverter, we construct VMCBench, a benchmark created by transforming 20 existing VQA datasets into a unified multiple-choice format, totaling 9,018 questions. We comprehensively evaluate 33 state-of-the-art VLMs on VMCBench, setting a new standard for scalable, consistent, and reproducible VLM evaluation.	 | 视觉语言模型（VLMs）的快速发展迫切需要严格的且可靠的评估方法。然而，当前的视觉问答（VQA）基准往往依赖于开放性问题，这使得准确评估变得困难，由于自然语言响应的多变性。为了解决这个问题，我们引入了AutoConverter这一代理框架，它可以自动将开放性问题转换为多项选择题格式，从而实现客观评估，同时减少生成问题的高昂成本。我们的实验表明，AutoConverter能够生成正确且具有挑战性的多项选择题，VLMs在这些问题上的准确性与人类创建的问题类似甚至更低。利用AutoConverter，我们构建了VMCBench，这是一个基准，由将20个现有VQA数据集统一转换为多项选择格式的9,018个问题组成。我们对33个最先进的VLMs在VMCBench上进行了全面评估，为大规模、一致且可重复的VLM评估设定了新的标准。
2501.03223	 | Rate-My-LoRA: Efficient and Adaptive Federated Model Tuning for Cardiac MRI Segmentation	 | Xiaoxiao He,Haizhou Shi,Ligong Han,Chaowei Tan,Bo Liu,Zihao Xu,Meng Ye,Leon Axel,Kang Li,Dimitris Metaxas	 | 本文提出了一种高效且自适应的联邦学习方法，利用低秩适应（LoRA）和自定义聚合技术来解决心脏分割中的带宽限制和数据异质性问题，从而在减少通信开销的同时提升了模型性能。	 | Cardiovascular disease (CVD) and cardiac dyssynchrony are major public health problems in the United States. Precise cardiac image segmentation is crucial for extracting quantitative measures that help categorize cardiac dyssynchrony. However, achieving high accuracy often depends on centralizing large datasets from different hospitals, which can be challenging due to privacy concerns. To solve this problem, Federated Learning (FL) is proposed to enable decentralized model training on such data without exchanging sensitive information. However, bandwidth limitations and data heterogeneity remain as significant challenges in conventional FL algorithms. In this paper, we propose a novel efficient and adaptive federate learning method for cardiac segmentation that improves model performance while reducing the bandwidth requirement. Our method leverages the low-rank adaptation (LoRA) to regularize model weight update and reduce communication overhead. We also propose a \mymethod{} aggregation technique to address data heterogeneity among clients. This technique adaptively penalizes the aggregated weights from different clients by comparing the validation accuracy in each client, allowing better generalization performance and fast local adaptation. In-client and cross-client evaluations on public cardiac MR datasets demonstrate the superiority of our method over other LoRA-based federate learning approaches.	 | 心血管疾病（CVD）和心脏异步是美国重要的公共卫生问题。精确的心脏图像分割对于提取有助于分类心脏异步的定量指标至关重要。然而，实现高精度往往依赖于集中来自不同医院的大规模数据集，这由于隐私问题而变得具有挑战性。为了解决这一问题，提出了联邦学习（FL）来在不交换敏感信息的情况下对这些数据进行分散模型训练。然而，带宽限制和数据异质性仍然是传统FL算法中的重大挑战。在本文中，我们提出了一种新颖的高效且自适应的联邦学习方法，用于心脏分割，该方法可以在减少带宽需求的同时提高模型性能。我们的方法利用低秩适应（LoRA）来正则化模型权重更新并减少通信开销。我们还提出了一种\mymethod{}聚合技术来解决客户端之间数据异质性的问题。该技术通过比较每个客户端的验证准确性，适当地惩罚来自不同客户端的聚合权重，从而实现更好的泛化性能和快速本地适应。在公共心脏MR数据集上的本地客户端和跨客户端评估表明，我们的方法优于其他基于LoRA的联邦学习方法。
2501.03187	 | Turn-based Multi-Agent Reinforcement Learning Model Checking	 | Dennis Gross	 | 本文提出了一种新颖的方法，用于验证轮转制多智能体强化学习代理在随机多玩家游戏中的行为是否符合复杂规范，这种方法通过将TMARL与模型检查技术集成，克服了现有验证方法在可扩展性上的局限性，并在多种实验环境中展示了其有效性和优越性。	 | In this paper, we propose a novel approach for verifying the compliance of turn-based multi-agent reinforcement learning (TMARL) agents with complex requirements in stochastic multiplayer games. Our method overcomes the limitations of existing verification approaches, which are inadequate for dealing with TMARL agents and not scalable to large games with multiple agents. Our approach relies on tight integration of TMARL and a verification technique referred to as model checking. We demonstrate the effectiveness and scalability of our technique through experiments in different types of environments. Our experiments show that our method is suited to verify TMARL agents and scales better than naive monolithic model checking.	 | 在这篇论文中，我们提出了一个新颖的方法，用于验证轮转制多智能体强化学习（TMARL）代理在随机多玩家游戏中是否符合复杂的规范要求。我们的方法克服了现有验证方法的局限性，这些方法对于处理TMARL代理和大规模多代理游戏的可扩展性都显得不足。我们的方法依赖于将TMARL与一种称为模型检查的验证技术紧密集成。我们通过在不同类型环境中的实验展示了我们技术的有效性和可扩展性。我们的实验表明，我们的方法适用于验证TMARL代理，并且在可扩展性方面优于简单的整体模型检查方法。
2501.03184	 | Noise-Robust Target-Speaker Voice Activity Detection Through Self-Supervised Pretraining	 | Holger Severin Bovbjerg(1),Jan Østergaard(1),Jesper Jensen(1 and 2),Zheng-Hua Tan(1) ((1) Aalborg University, (2) Oticon A/S)	 | 该研究提出了一种因果自监督学习框架——去噪自回归预测编码（DN-APC），以提高目标演讲者语音活动检测（TS-VAD）在噪声条件下的性能，并通过实验验证了其有效性，尤其是在未见过的噪声条件下，性能提升了约2%。	 | Target-Speaker Voice Activity Detection (TS-VAD) is the task of detecting the presence of speech from a known target-speaker in an audio frame. Recently, deep neural network-based models have shown good performance in this task. However, training these models requires extensive labelled data, which is costly and time-consuming to obtain, particularly if generalization to unseen environments is crucial. To mitigate this, we propose a causal, Self-Supervised Learning (SSL) pretraining framework, called Denoising Autoregressive Predictive Coding (DN-APC), to enhance TS-VAD performance in noisy conditions. We also explore various speaker conditioning methods and evaluate their performance under different noisy conditions. Our experiments show that DN-APC improves performance in noisy conditions, with a general improvement of approx. 2% in both seen and unseen noise. Additionally, we find that FiLM conditioning provides the best overall performance. Representation analysis via tSNE plots reveals robust initial representations of speech and non-speech from pretraining. This underscores the effectiveness of SSL pretraining in improving the robustness and performance of TS-VAD models in noisy environments.	 | 目标演讲者语音活动检测（TS-VAD）是指在音频帧中检测已知目标演讲者语音存在的任务。近年来，基于深度神经网络的模型在这项任务中表现出良好的性能。然而，训练这些模型需要大量的标注数据，这些数据获取成本高昂且耗时，特别是在需要对未见过的环境进行泛化时更为关键。为缓解这一问题，我们提出了一种因果自监督学习（SSL）预训练框架，称为去噪自回归预测编码（DN-APC），以提高TS-VAD在噪声条件下的性能。我们还探讨了各种演讲者条件方法，并在不同的噪声条件下评估其性能。我们的实验表明，DN-APC在噪声条件下提高了性能，尤其是在已见过和未见过的噪声条件下，性能分别提升了约2%。此外，我们发现FiLM条件方法提供了最佳的整体性能。通过tSNE图进行的表示分析揭示了预训练中对语音和非语音的鲁棒初始表示。这突显了SSL预训练在提高TS-VAD模型在噪声环境下的鲁棒性和性能方面的有效性。
2501.03172	 | GLiREL -- Generalist Model for Zero-Shot Relation Extraction	 | Jack Boylan,Chris Hokamp,Demian Gholipour Ghalandari	 | 该研究提出了一种高效的GLiREL架构，能够在单次前向传递中准确预测多个实体间的零样本关系标签，实验表明其在零样本关系分类任务上达到最佳性能；同时贡献了一种合成生成多样化关系标签数据集的方法。	 | We introduce GLiREL (Generalist Lightweight model for zero-shot Relation Extraction), an efficient architecture and training paradigm for zero-shot relation classification. Inspired by recent advancements in zero-shot named entity recognition, this work presents an approach to efficiently and accurately predict zero-shot relationship labels between multiple entities in a single forward pass. Experiments using the FewRel and WikiZSL benchmarks demonstrate that our approach achieves state-of-the-art results on the zero-shot relation classification task. In addition, we contribute a protocol for synthetically-generating datasets with diverse relation labels.	 | 我们介绍了一种用于零样本关系提取的高效架构和训练范式——GLiREL（通用轻量级模型）。该工作受到了近期零样本命名实体识别进展的启发，提出了一种在单次前向传递中高效且准确地预测多个实体之间零样本关系标签的方法。使用FewRel和WikiZSL基准进行的实验表明，我们的方法在零样本关系分类任务上达到了最先进的性能。此外，我们还贡献了一种用于合成生成具有多样化关系标签数据集的协议。
2501.03166	 | Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot In-Context Learning for SQL2Text	 | Ali Al-Lawati,Jason Lucas,Prasenjit Mitra	 | 本文研究了大规模语言模型在自然语言处理任务中的逆向过程，即从代码转换为自然语言（SQL2Text），并提出了一种基于迭代式实例级条件语言提示的方法，利用SQL查询的图形属性来增强逆向任务的数据集鲁棒性，实验结果显示这种方法在BLEU得分上显著优于随机选择。	 | Large Language Models (LLMs) have demonstrated remarkable performance in various NLP tasks, including semantic parsing, which trans lates natural language into formal code representations. However, the reverse process, translating code into natural language, termed semantic captioning, has received less attention. This task is becoming increasingly important as LLMs are integrated into platforms for code generation, security analysis, and educational purposes. In this paper, we focus on the captioning of SQL query (SQL2Text) to address the critical need for understanding and explaining SQL queries in an era where LLM-generated code poses potential security risks. We repurpose Text2SQL datasets for SQL2Text by introducing an iterative ICL prompt using GPT-4o to generate multiple additional utterances, which enhances the robustness of the datasets for the reverse task. We conduct our experiments using in-context learning (ICL) based on different sample selection methods, emphasizing smaller, more computationally efficient LLMs. Our findings demonstrate that leveraging the inherent graph properties of SQL for ICL sample selection significantly outperforms random selection by up to 39% on BLEU score and provides better results than alternative methods. Dataset and codes are published: \url{https://github.com/aliwister/ast-icl}.	 | 大规模语言模型（LLMs）在各种自然语言处理（NLP）任务中表现出色，包括语义解析，即将自然语言转换为规范的代码表示。然而，逆向过程，即从代码转换为自然语言，称为语义描述的过程，却较少受到关注。随着LLMs被集成到代码生成、安全分析和教育平台中，这一任务的重要性日益提升。在本文中，我们专注于SQL查询的描述（SQL2Text），以应对在LLM生成的代码可能带来潜在安全风险的时代背景下，理解并解释SQL查询的迫切需求。我们通过引入迭代式实例级条件语言（ICL）提示，使用GPT-4o为SQL2Text重新利用Text2SQL数据集，从而增强逆向任务的数据集鲁棒性。我们通过基于不同采样方法的上下文学习（ICL）进行实验，强调利用更小、更具计算效率的LLMs。我们的研究发现，利用SQL固有的图形属性进行ICL样本选择，相较于随机选择，在BLEU得分上高出39%，并且比其他方法提供了更好的结果。数据集和代码已发布：\url{https://github.com/aliwister/ast-icl}。
2501.03151	 | Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches	 | Alhassan Mumuni,Fuseini Mumuni	 | 多模态大规模语言模型（MLLMs）在多种领域展示了强大的处理能力，包括推理和情感理解，但其认知能力仍然表面化且脆弱，需解决实体化、符号接地、因果关系和记忆等基础问题以增强其泛化能力和类人认知属性。本文探讨了在大规模语言模型中实现这些概念的最新方法，以推动实现人工通用智能（AGI）。	 | Generative artificial intelligence (AI) systems based on large-scale pretrained foundation models (PFMs) such as vision-language models, large language models (LLMs), diffusion models and vision-language-action (VLA) models have demonstrated the ability to solve complex and truly non-trivial AI problems in a wide variety of domains and contexts. Multimodal large language models (MLLMs), in particular, learn from vast and diverse data sources, allowing rich and nuanced representations of the world and, thereby, providing extensive capabilities, including the ability to reason, engage in meaningful dialog; collaborate with humans and other agents to jointly solve complex problems; and understand social and emotional aspects of humans. Despite this impressive feat, the cognitive abilities of state-of-the-art LLMs trained on large-scale datasets are still superficial and brittle. Consequently, generic LLMs are severely limited in their generalist capabilities. A number of foundational problems -- embodiment, symbol grounding, causality and memory -- are required to be addressed for LLMs to attain human-level general intelligence. These concepts are more aligned with human cognition and provide LLMs with inherent human-like cognitive properties that support the realization of physically-plausible, semantically meaningful, flexible and more generalizable knowledge and intelligence. In this work, we discuss the aforementioned foundational issues and survey state-of-the art approaches for implementing these concepts in LLMs. Specifically, we discuss how the principles of embodiment, symbol grounding, causality and memory can be leveraged toward the attainment of artificial general intelligence (AGI) in an organic manner.	 | 基于大规模预训练基础模型（PFMs）的生成型人工智能（AI）系统，例如视觉-语言模型、大规模语言模型（LLMs）、扩散模型和视觉-语言-动作（VLA）模型，已经在多种领域和情境中展示了解决复杂且真正非平凡AI问题的能力。特别是多模态大规模语言模型（MLLMs）可以从广泛的多样化数据源中学习，从而获得对世界的丰富和细腻的表示，提供广泛的处理能力，包括推理、有意义对话的能力；与人类及其他代理合作解决复杂问题；以及理解人类的情感和社会方面。尽管取得了如此令人瞩目的成就，但基于大规模数据集训练的先进LLMs的认知能力仍然表面化且脆弱。因此，通用的LLMs在其泛化能力方面受到严重限制。为了使LLMs达到人类水平的通用智能，需要解决一些基础问题——实体化、符号接地、因果关系和记忆。这些概念与人类认知更为一致，赋予LLMs固有的类人认知属性，支持实现物理上合理、语义上有意义、灵活且更具泛化性的知识和智能。在本工作中，我们讨论了上述基础问题，并概述了在LLMs中实现这些概念的最新方法。具体而言，我们探讨了如何通过有机的方式利用实体化、符号接地、因果关系和记忆的原则，以实现人工通用智能（AGI）。
2501.03145	 | Geometry Restoration and Dewarping of Camera-Captured Document Images	 | Valery Istomin,Oleg Pereziabov,Ilya Afanasyev	 | 本研究提出了一种结合深度学习和计算机视觉技术的方法，用于恢复通过相机拍摄的纸张文件数字图像的拓扑结构，并通过实验证明了其在视觉效果和OCR识别准确性方面的优势，优于现有基准方法。	 | This research focuses on developing a method for restoring the topology of digital images of paper documents captured by a camera, using algorithms for detection, segmentation, geometry restoration, and dewarping. Our methodology employs deep learning (DL) for document outline detection, followed by computer vision (CV) to create a topological 2D grid using cubic polynomial interpolation and correct nonlinear distortions by remapping the image. Using classical CV methods makes the document topology restoration process more efficient and faster, as it requires significantly fewer computational resources and memory. We developed a new pipeline for automatic document dewarping and reconstruction, along with a framework and annotated dataset to demonstrate its efficiency. Our experiments confirm the promise of our methodology and its superiority over existing benchmarks (including mobile apps and popular DL solutions, such as RectiNet, DocGeoNet, and DocTr++) both visually and in terms of document readability via Optical Character Recognition (OCR) and geometry restoration metrics. This paves the way for creating high-quality digital copies of paper documents and enhancing the efficiency of OCR systems. Project page: https://github.com/HorizonParadox/DRCCBI	 | 本研究致力于开发一种方法，用于恢复通过相机拍摄的纸张文件数字图像的拓扑结构，采用检测、分割、几何恢复和去翘曲的算法。我们的方法利用深度学习（DL）进行文档轮廓检测，随后使用计算机视觉（CV）技术通过三次多项式插值创建一个拓扑2D网格，并通过重新映射图像来纠正非线性失真。使用传统的计算机视觉方法使得文档拓扑结构恢复过程更加高效和快速，因为它需要的计算资源和内存显著减少。我们开发了一种新的自动文档去翘曲和重建管道，并提供了一个框架和注释数据集来展示其效率。我们的实验证实了该方法的潜力及其在视觉效果和读取性（通过光学字符识别OCR和几何恢复指标）方面优于现有基准（包括移动应用程序和流行的DL解决方案，如RectiNet、DocGeoNet和DocTr++）。这为创建高质量的纸张文件数字副本和增强OCR系统的效率铺平了道路。项目页面：https://github.com/HorizonParadox/DRCCBI
2501.03142	 | Co-Activation Graph Analysis of Safety-Verified and Explainable Deep Reinforcement Learning Policies	 | Dennis Gross,Helge Spieker	 | 该研究结合了深度强化学习策略模型检查和共激活图分析，以解释和理解RL策略的安全决策过程，并展示了该方法在多种实验中的适用性。	 | Deep reinforcement learning (RL) policies can demonstrate unsafe behaviors and are challenging to interpret. To address these challenges, we combine RL policy model checking--a technique for determining whether RL policies exhibit unsafe behaviors--with co-activation graph analysis--a method that maps neural network inner workings by analyzing neuron activation patterns--to gain insight into the safe RL policy's sequential decision-making. This combination lets us interpret the RL policy's inner workings for safe decision-making. We demonstrate its applicability in various experiments.	 | 深度强化学习（RL）策略可能会表现出不安全的行为，并且难以解释。为了解决这些挑战，我们结合了RL策略模型检查——一种确定RL策略是否表现出不安全行为的技术——与共激活图分析——一种通过分析神经元激活模式来映射神经网络内部运作的方法——以获得对安全RL策略的顺序决策过程的理解。这种结合使我们能够解释RL策略的内部运作，以便进行安全决策。我们在各种实验中展示了其适用性。
2501.03124	 | PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models	 | Mingyang Song,Zhaochen Su,Xiaoye Qu,Jiawei Zhou,Yu Cheng	 | PRMBench 是一种专门设计的流程级基准测试，旨在评估过程级奖励模型在细粒度错误检测方面的性能，并发现当前模型存在的显著弱点。通过包含6,216个精心设计的问题和83,456个步骤级标签，PRMBench 从多个维度评估模型，揭示了当前过程级奖励模型面临的挑战。	 | Process-level Reward Models (PRMs) are crucial for complex reasoning and decision-making tasks, where each intermediate step plays an important role in the reasoning process. Since language models are prone to various types of errors during the reasoning process, PRMs are required to possess nuanced capabilities for detecting various implicit error types in real-world scenarios. However, current benchmarks primarily focus on step correctness, failing to evaluate PRMs' performance systematically. To address this gap, we introduce PRMBench, a process-level benchmark specifically designed to assess the fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216 carefully designed problems and 83,456 step-level labels, evaluating models across multiple dimensions, including simplicity, soundness, and sensitivity. In our experiments on 15 models, spanning both open-source PRMs and closed-source large language models prompted as critic models, we uncover significant weaknesses in current PRMs. These findings underscore the challenges inherent in process-level evaluation and highlight key directions for future research. We hope PRMBench can be a robust bench for advancing research on PRM evaluation and development.	 | 过程级奖励模型（PRMs）对于复杂的推理和决策任务至关重要，其中每个中间步骤在推理过程中都发挥着重要作用。由于语言模型在推理过程中容易出现各种类型的错误，PRMs 需要具备细致的能力来检测现实世界中的各种隐含错误类型。然而，当前的基准测试主要集中在步骤的准确性上，未能系统地评估 PRMs 的性能。为了解决这一缺口，我们引入了 PRMBench，这是一种专门设计的流程级基准测试，旨在评估 PRMs 的细粒度错误检测能力。PRMBench 包含了 6,216 个精心设计的问题和 83,456 个步骤级标签，从简单性、正确性和敏感性等多个维度评估模型。在对 15 模型的实验中，包括开源的 PRMs 和被提示为批评模型的闭源大语言模型，我们发现当前 PRMs 存在显著的弱点。这些发现突显了过程级评估中固有的挑战，并强调了未来研究的关键方向。我们期望 PRMBench 能够成为推进 PRM 评估和发展的强大基准。
2501.03112	 | LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases	 | Dylan Bouchard,Mohit Singh Chauhan,David Skarbrevik,Viren Bajaj,Zeya Ahmad	 | LangFair是一个开源Python包，旨在评估大型语言模型在特定应用场景下的偏见和公平性风险，并提供生成评估数据集和选择相关指标的工具。	 | Large Language Models (LLMs) have been observed to exhibit bias in numerous ways, potentially creating or worsening outcomes for specific groups identified by protected attributes such as sex, race, sexual orientation, or age. To help address this gap, we introduce LangFair, an open-source Python package that aims to equip LLM practitioners with the tools to evaluate bias and fairness risks relevant to their specific use cases. The package offers functionality to easily generate evaluation datasets, comprised of LLM responses to use-case-specific prompts, and subsequently calculate applicable metrics for the practitioner's use case. To guide in metric selection, LangFair offers an actionable decision framework.	 | 大型语言模型（LLMs）在诸多方面被观察到存在偏见，可能在由性别、种族、性取向或年龄等保护属性标识的具体群体中制造或加剧不利结果。为解决这一问题，我们引入了LangFair，这是一个开源的Python包，旨在为LLM的实践者提供评估其特定应用场景下偏见和公平性风险的工具。该包提供了生成评估数据集的功能，这些数据集由LLM对特定场景的提示作出的回应组成，并可进一步计算适用于实践者场景的具体指标。为了指导指标选择，LangFair提供了一个可操作的决策框架。
2501.03070	 | Slim multi-scale convolutional autoencoder-based reduced-order models for interpretable features of a complex dynamical system	 | Philipp Teutsch,Philipp Pfeffer,Mohammad Sharifi Ghazijahani,Christian Cierpka,Jörg Schumacher,Patrick Mäder	 | 本文提出了一种新的可解释卷积自编码器方法，适用于高维流体流动数据，该方法在保持重建质量的同时增强了模型的解释性，并显著减少了资源消耗，相比传统的POD方法在低模式数量下重建性能提升了显著比例。	 | In recent years, data-driven deep learning models have gained significant interest in the analysis of turbulent dynamical systems. Within the context of reduced-order models (ROMs), convolutional autoencoders (CAEs) pose a universally applicable alternative to conventional approaches. They can learn nonlinear transformations directly from data, without prior knowledge of the system. However, the features generated by such models lack interpretability. Thus, the resulting model is a black-box which effectively reduces the complexity of the system, but does not provide insights into the meaning of the latent features. To address this critical issue, we introduce a novel interpretable CAE approach for high-dimensional fluid flow data that maintains the reconstruction quality of conventional CAEs and allows for feature interpretation. Our method can be easily integrated into any existing CAE architecture with minor modifications of the training process. We compare our approach to Proper Orthogonal Decomposition (POD) and two existing methods for interpretable CAEs. We apply all methods to three different experimental turbulent Rayleigh-Bénard convection datasets with varying complexity. Our results show that the proposed method is lightweight, easy to train, and achieves relative reconstruction performance improvements of up to 6.4% over POD for 64 modes. The relative improvement increases to up to 229.8% as the number of modes decreases. Additionally, our method delivers interpretable features similar to those of POD and is significantly less resource-intensive than existing CAE approaches, using less than 2% of the parameters. These approaches either trade interpretability for reconstruction performance or only provide interpretability to a limited extend.	 | 近年来，基于数据驱动的深度学习模型在湍流动力系统分析中引起了广泛关注。在降阶模型（ROMs）的背景下，卷积自编码器（CAEs）提供了一种通用的替代方案，与传统方法相比，它们可以从数据中直接学习非线性变换，而无需事先了解系统。然而，这些模型生成的特征缺乏可解释性，因此所得模型本质上是黑盒的，虽然可以减少系统的复杂性，但无法揭示潜藏特征的意义。为解决这一关键问题，我们提出了一种新的可解释的卷积自编码器方法，该方法适用于高维流体流动数据，同时保持传统CAEs的重建质量，并允许特征解释。我们的方法可以轻松地整合到任何现有的CAE架构中，并在训练过程中进行少量修改。我们将我们的方法与恰当正交分解（POD）和两种现有的可解释自编码器方法进行了比较。我们应用所有方法到三个不同实验的湍流Rayleigh-Bénard对流数据集中，这些数据集具有不同的复杂性。实验结果显示，我们的方法轻量级且易于训练，与POD相比，在64个模式下，相对重建性能提高了6.4%。随着模式数量的减少，相对改进率可高达229.8%。此外，我们的方法提供了与POD类似的可解释特征，且资源消耗明显少于现有CAE方法，参数量仅占2%。现有的方法要么以牺牲可解释性来换取重建性能，要么只能在有限的范围内提供解释性。
2501.03059	 | Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation	 | Guy Yariv,Yuval Kirstain,Amit Zohar,Shelly Sheynin,Yaniv Taigman,Yossi Adi,Sagie Benaim,Adam Polyak	 | 该研究提出了一个两阶段的图像到视频生成框架，通过引入基于掩码的运动轨迹作为中间表示来生成多对象场景的逼真视频，显著提升了时间连贯性、运动逼真性和文本提示忠实度。此外，该研究还引入了一个新的基准来评估单对象和多对象的图像到视频生成任务。	 | We consider the task of Image-to-Video (I2V) generation, which involves transforming static images into realistic video sequences based on a textual description. While recent advancements produce photorealistic outputs, they frequently struggle to create videos with accurate and consistent object motion, especially in multi-object scenarios. To address these limitations, we propose a two-stage compositional framework that decomposes I2V generation into: (i) An explicit intermediate representation generation stage, followed by (ii) A video generation stage that is conditioned on this representation. Our key innovation is the introduction of a mask-based motion trajectory as an intermediate representation, that captures both semantic object information and motion, enabling an expressive but compact representation of motion and semantics. To incorporate the learned representation in the second stage, we utilize object-level attention objectives. Specifically, we consider a spatial, per-object, masked-cross attention objective, integrating object-specific prompts into corresponding latent space regions and a masked spatio-temporal self-attention objective, ensuring frame-to-frame consistency for each object. We evaluate our method on challenging benchmarks with multi-object and high-motion scenarios and empirically demonstrate that the proposed method achieves state-of-the-art results in temporal coherence, motion realism, and text-prompt faithfulness. Additionally, we introduce \benchmark, a new challenging benchmark for single-object and multi-object I2V generation, and demonstrate our method's superiority on this benchmark. Project page is available at https://guyyariv.github.io/TTM/.	 | 我们考虑将静态图像转换为基于文本描述的现实视频序列的图像到视频（I2V）生成任务。尽管最近的技术产生了高度逼真的输出，但在多对象场景中创建具有准确和一致对象运动的视频仍然是一个挑战。为了解决这些限制，我们提出了一种两阶段组合框架，将I2V生成分解为：（i）显式的中间表示生成阶段，随后是（ii）基于该表示进行的视频生成阶段。我们的关键创新在于引入了一种基于掩码的运动轨迹作为中间表示，它既捕捉了语义对象信息，又捕捉了运动，从而提供了一种既具有表现力又紧凑的运动和语义表示。为了在第二阶段中利用学习到的表示，我们采用了对象级注意力目标。具体来说，我们考虑了一个空间化、针对每个对象的掩码交叉注意力目标，将对象特定的提示融入相应的潜空间区域，并采用掩码空间-时间自注意力目标，以确保每个对象帧到帧的一致性。我们在具有多对象和高运动场景的挑战性基准上评估了我们的方法，并实证证明，所提出的方法在时间连贯性、运动逼真性和文本提示忠实度方面达到了最先进的结果。此外，我们引入了 \benchmark，一个新的挑战性基准，用于单对象和多对象的I2V生成，并在该基准上展示了我们方法的优越性。项目页面可在https://guyyariv.github.io/TTM/中找到。
2501.03041	 | Group Shapley with Robust Significance Testing and Its Application to Bond Recovery Rate Prediction	 | Jingyi Wang,Ying Chen,Paolo Giudici	 | 该研究提出了一种新的Group Shapley度量标准及其显著性检验方法，用于评估特征组在结构化预测变量中的重要性，并证明其在稀疏和偏斜数据下的性能优于Wald检验；应用该方法对全球债券回收率预测的数据集进行分析，发现市场相关变量组最具影响力，并且Group Shapley在分配特征重要性方面比个体Shapley值更为公平。	 | We propose Group Shapley, a metric that extends the classical individual-level Shapley value framework to evaluate the importance of feature groups, addressing the structured nature of predictors commonly found in business and economic data. More importantly, we develop a significance testing procedure based on a three-cumulant chi-square approximation and establish the asymptotic properties of the test statistics for Group Shapley values. Our approach can effectively handle challenging scenarios, including sparse or skewed distributions and small sample sizes, outperforming alternative tests such as the Wald test. Simulations confirm that the proposed test maintains robust empirical size and demonstrates enhanced power under diverse conditions. To illustrate the method's practical relevance in advancing Explainable AI, we apply our framework to bond recovery rate predictions using a global dataset (1996-2023) comprising 2,094 observations and 98 features, grouped into 16 subgroups and five broader categories: bond characteristics, firm fundamentals, industry-specific factors, market-related variables, and macroeconomic indicators. Our results identify the market-related variables group as the most influential. Furthermore, Lorenz curves and Gini indices reveal that Group Shapley assigns feature importance more equitably compared to individual Shapley values.	 | 我们提出了一种新的度量标准——Group Shapley，该度量标准将经典的个体层面Shapley值框架扩展到评估特征组的重要性，以应对商业和经济数据中常见的结构化预测变量。更重要的是，我们基于三累积量卡方近似开发了一种显著性检验程序，并建立了Group Shapley值检验统计量的渐近性质。我们的方法能够有效处理稀疏分布、偏斜分布和小样本大小等具有挑战性的场景，其性能优于Wald检验等替代测试方法。模拟实验证实，所提出的检验维持了稳健的实证尺寸，并在多种条件下展示了增强的检验效能。为了说明该方法在可解释人工智能领域的实际应用价值，我们运用该框架对全球数据集（1996-2023年，包含2094个观察值和98个特征，分为16个子组和五个更广泛的类别：债券特征、公司基础性因素、行业特定因素、市场相关变量和宏观经济指标）进行债券回收率预测。我们的结果表明，市场相关变量组是最具影响力的。此外，洛伦兹曲线和基尼系数显示，Group Shapley在分配特征重要性方面更为公平，优于个体Shapley值。
2501.03038	 | Piano Transcription by Hierarchical Language Modeling with Pretrained Roll-based Encoders	 | Dichucheng Li,Yongyi Zang,Qiuqiang Kong	 | 本文提出了一种结合预训练卷积编码器和语言模型解码器的混合方法，用于自动音乐转录，并采用分层预测策略以降低计算成本，结果在起始-终止-速度F1分数上优于传统的钢琴卷预测，展示了其作为音乐转录编码器性能增强插件的潜力。	 | Automatic Music Transcription (AMT), aiming to get musical notes from raw audio, typically uses frame-level systems with piano-roll outputs or language model (LM)-based systems with note-level predictions. However, frame-level systems require manual thresholding, while the LM-based systems struggle with long sequences. In this paper, we propose a hybrid method combining pre-trained roll-based encoders with an LM decoder to leverage the strengths of both methods. Besides, our approach employs a hierarchical prediction strategy, first predicting onset and pitch, then velocity, and finally offset. The hierarchical prediction strategy reduces computational costs by breaking down long sequences into different hierarchies. Evaluated on two benchmark roll-based encoders, our method outperforms traditional piano-roll outputs 0.01 and 0.022 in onset-offset-velocity F1 score, demonstrating its potential as a performance-enhancing plug-in for arbitrary roll-based music transcription encoder. We release the code of this work at https://github.com/yongyizang/AMT_train.	 | 自动音乐转录（Automatic Music Transcription, AMT）的目标是从原始音频中获取音乐符号，通常使用帧级系统输出钢琴卷或基于语言模型（LM）的系统进行音符级预测。然而，帧级系统需要手动阈值处理，而基于LM的系统在处理长序列时表现不佳。本文提出了一种结合预训练的卷基编码器和LM解码器的混合方法，以充分利用两种方法的优势。此外，我们的方法采用了一种分层预测策略，首先预测起始和音高，然后预测速度，最后预测终止。分层预测策略通过将长序列分解为不同层次来降低计算成本。在两个基准卷基编码器上进行评估，我们的方法在起始-终止-速度F1分数上分别优于传统的钢琴卷0.01和0.022，展示了其作为任意卷基音乐转录编码器性能增强插件的潜力。我们已将此工作的代码发布在https://github.com/yongyizang/AMT_train。
2501.03026	 | Putnam's Critical and Explanatory Tendencies Interpreted from a Machine Learning Perspective	 | Sheldon Z. Soudin	 | 本文旨在重构哈特曼关于理论选择的批判性和解释性倾向区分，并通过机器学习的视角论证这两种倾向的双向必要性，以探讨其如何搅动正常科学和非常科学中的理论选择问题。	 | Making sense of theory choice in normal and across extraordinary science is central to philosophy of science. The emergence of machine learning models has the potential to act as a wrench in the gears of current debates. In this paper, I will attempt to reconstruct the main movements that lead to and came out of Putnam's critical and explanatory tendency distinction, argue for the biconditional necessity of the tendencies, and conceptualize that wrench through a machine learning interpretation of my claim.	 | 理解正常科学和非常科学中的理论选择是哲学科学的核心问题。机器学习模型的出现有可能搅乱当前的争论。在本文中，我将尝试重构哈特曼批判性和解释性倾向区分的主要发展过程，论证这两种倾向的双向必要性，并通过机器学习视角来概念化这种搅动。
2501.03021	 | A Trust-Guided Approach to MR Image Reconstruction with Side Information	 | Arda Atalık,Sumit Chopra,Daniel K. Sodickson	 | 该研究提出了一种新型端到端深度学习框架TGVN，通过整合上下文侧信息有效解决了线性反问题中的歧义性，从而提高了多线圈、多对比度MRI图像的质量，特别是在高度欠采样条件下，显著提升了图像质量和采集速度。	 | Reducing MRI scan times can improve patient care and lower healthcare costs. Many acceleration methods are designed to reconstruct diagnostic-quality images from limited sets of acquired $\textit{k}$-space data. This task can be framed as a linear inverse problem (LIP), where, as a result of undersampling, the forward operator may become rank-deficient or exhibit small singular values. This results in ambiguities in reconstruction, in which multiple generally incorrect or non-diagnostic images can map to the same acquired data. To address such ambiguities, it is crucial to incorporate prior knowledge, for example in the form of regularization. Another form of prior knowledge less commonly used in medical imaging is contextual side information garnered from other sources than the current acquisition. Here, we propose the $\textbf{T}$rust-$\textbf{G}$uided $\textbf{V}$ariational $\textbf{N}$etwork $\textbf{(TGVN)}$, a novel end-to-end deep learning framework that effectively integrates side information into LIPs. TGVN eliminates undesirable solutions from the ambiguous space of the forward operator while remaining faithful to the acquired data. We demonstrate its effectiveness in multi-coil, multi-contrast MR image reconstruction, where incomplete or low-quality measurements from one contrast are used as side information to reconstruct high-quality images of another contrast from heavily under-sampled data. Our method is robust across different contrasts, anatomies, and field strengths. Compared to baselines that also utilize side information, TGVN achieves superior image quality at challenging under-sampling levels, drastically speeding up acquisition while minimizing hallucinations. Our approach is also versatile enough to incorporate many different types of side information (including previous scans or even text) into any LIP.	 | 减少MRI扫描时间可以提高患者护理质量和降低医疗成本。许多加速方法旨在从有限的k空间数据集中重构诊断质量的图像。这项任务可以被表述为线性反问题（LIP），由于欠采样，前向操作符可能会变得秩亏或表现出较小的奇异值。这会导致重构中的歧义性，在这种情况下，多个通常不正确或非诊断的图像可以映射到相同的数据。为了应对这些歧义性，有必要融入先验知识，例如以正则化形式。另一种在医学成像中较少使用的先验知识是来自除当前采集之外的其他来源的上下文侧信息。在此，我们提出了一种新型端到端深度学习框架——**T**rust-**G**uided **V**ariational **N**etwork（TGVN），它有效地将侧信息整合进了LIPs中。TGVN在保留所采集数据真实性的前提下，消除了前向操作符中的不利解，从而提高了图像的质量。我们展示了TGVN在多线圈、多对比度MRI图像重构中的有效性，其中利用一种对比度的不完整或低质量测量数据作为侧信息，重构另一种对比度的高质量图像，数据是高度欠采样的。我们的方法在不同的对比度、解剖结构和磁场强度下都表现稳定。与也利用侧信息的基线方法相比，TGVN在具有挑战性的欠采样水平下实现了更优的图像质量，大幅提高了采集速度并减少了幻象。我们的方法还足够灵活，可以将多种不同类型的侧信息（包括之前扫描或甚至文本）整合到任何LIP中。
2501.02994	 | NeuroPMD: Neural Fields for Density Estimation on Product Manifolds	 | William Consagra,Zhiling Gu,Zhengwu Zhang	 | 本文提出了一种新颖的深度神经网络方法，用于在乘积黎曼流形上进行密度估计，该方法通过惩罚最大似然框架训练网络直接参数化概率密度函数，并通过精心设计的网络架构有效解决了高维问题，实验结果表明其在仿真实验和脑结构连接数据应用中表现优于现有方法。	 | We propose a novel deep neural network methodology for density estimation on product Riemannian manifold domains. In our approach, the network directly parameterizes the unknown density function and is trained using a penalized maximum likelihood framework, with a penalty term formed using manifold differential operators. The network architecture and estimation algorithm are carefully designed to handle the challenges of high-dimensional product manifold domains, effectively mitigating the curse of dimensionality that limits traditional kernel and basis expansion estimators, as well as overcoming the convergence issues encountered by non-specialized neural network methods. Extensive simulations and a real-world application to brain structural connectivity data highlight the clear advantages of our method over the competing alternatives.	 | 我们提出了一种新颖的深度神经网络方法，用于在乘积黎曼流形域上进行密度估计。在我们的方法中，网络直接参数化未知的概率密度函数，并利用带有流形微分算子形成的惩罚项的惩罚最大似然框架进行训练。网络架构和估计算法被精心设计，以应对高维乘积流形域的挑战，有效地缓解了传统核估计和基扩展估计方法所面临的维数灾难问题，同时也克服了通用神经网络方法所遇到的收敛问题。广泛的仿真实验和对脑结构连接数据的实际应用表明，我们的方法在与竞争方法的对比中具有明显的优势。
2501.02989	 | Classifier Weighted Mixture models	 | Elouan Argouarc'h,François Desbouvries,Eric Barat,Eiji Kawasaki,Thomas Dautremer	 | 该研究提出了一种扩展的标准混合随机模型，使用分类器定义的功能权重替代常数混合权重，能够在变分估计中直接进行密度评估和明确采样，同时提高表达能力而不增加成分数量或增加成分的复杂性。	 | This paper proposes an extension of standard mixture stochastic models, by replacing the constant mixture weights with functional weights defined using a classifier. Classifier Weighted Mixtures enable straightforward density evaluation, explicit sampling, and enhanced expressivity in variational estimation problems, without increasing the number of components nor the complexity of the mixture components.	 | 本文提出了一种标准混合随机模型的扩展，将常数混合权重替换为通过分类器定义的功能权重。这种分类器加权混合模型在变分估计问题中能够直接进行密度评估、明确采样，并提高表达能力，同时无需增加成分数量或增加混合成分的复杂性。
2501.02966	 | Human Gaze Boosts Object-Centered Representation Learning	 | Timothy Schaumlöffel,Arthur Aubret,Gemma Roig,Jochen Triesch	 | 通过在模拟的自我中心视觉体验上聚焦中心视觉信息并利用人类凝视预测生成裁剪输入，研究发现自监督学习模型能够产生更好的以物体为中心的视觉表征，这表明中心视觉信息的重要性并代表了生物启发式视觉表征学习的进展。	 | Recent self-supervised learning (SSL) models trained on human-like egocentric visual inputs substantially underperform on image recognition tasks compared to humans. These models train on raw, uniform visual inputs collected from head-mounted cameras. This is different from humans, as the anatomical structure of the retina and visual cortex relatively amplifies the central visual information, i.e. around humans' gaze location. This selective amplification in humans likely aids in forming object-centered visual representations. Here, we investigate whether focusing on central visual information boosts egocentric visual object learning. We simulate 5-months of egocentric visual experience using the large-scale Ego4D dataset and generate gaze locations with a human gaze prediction model. To account for the importance of central vision in humans, we crop the visual area around the gaze location. Finally, we train a time-based SSL model on these modified inputs. Our experiments demonstrate that focusing on central vision leads to better object-centered representations. Our analysis shows that the SSL model leverages the temporal dynamics of the gaze movements to build stronger visual representations. Overall, our work marks a significant step toward bio-inspired learning of visual representations.	 | 最近在人类视角中心视输入上训练的自监督学习（SSL）模型在图像识别任务上的表现远逊于人类。这些模型是在从头戴式摄像机收集的原始且均匀的视觉输入上进行训练的。这与人类不同，因为视网膜和视觉皮层的解剖结构相对较大地放大了中心视觉信息，即人类视线位置附近的区域。人类这种选择性放大可能有助于形成以物体为中心的视觉表征。在此，我们研究了是否聚焦中心视觉信息能够提升以自我为中心的视觉物体学习。我们使用大规模Ego4D数据集模拟了5个月的以自我为中心的视觉体验，并利用人类凝视预测模型生成凝视位置。为了强调人类中心视觉的重要性，我们裁剪围绕凝视位置的视觉区域。最后，我们在这些修改后的输入上训练一个基于时间的SSL模型。我们的实验表明，聚焦中心视觉能够产生更好的以物体为中心的表征。我们的分析表明，SSL模型利用凝视运动的时间动态来构建更强的视觉表征。总体而言，我们的工作代表了视觉表征生物启发式学习的一个重要里程碑。
2501.02961	 | A Point Process Model for Optimizing Repeated Personalized Action Delivery to Users	 | Alexander Merkov,David Rohde	 | 本文提出了一种受在线广告用户-广告商交互启发的形式化表示，适用于一类重要的因果推理问题，并将其应用于时间标记点过程的扩展，建议使用神经点过程来解决某些特殊情况。	 | This paper provides a formalism for an important class of causal inference problems inspired by user-advertiser interaction in online advertiser. Then this formalism is specialized to an extension of temporal marked point processes and the neural point processes are suggested as practical solutions to some interesting special cases.	 | 本文提供了一种形式化表示，该形式化表示受在线广告商中用户-广告商交互的启发，适用于一类重要的因果推理问题。然后，这种形式化表示被专门应用于时间标记点过程的扩展，并建议神经点过程作为解决某些有趣特殊情况的实用解决方案。
2501.02950	 | Key-value memory in the brain	 | Samuel J. Gershman,Ila Fiete,Kazuki Irie	 | 传统记忆模型基于检索线索和存储模式的相似性，尽管简洁有效，但无法为存储和检索提供不同的表示。相比之下，键值记忆系统能够分别优化存储的保真度和检索的可区分性，并在现代机器学习和生物实现方面展现出潜力。	 | Classical models of memory in psychology and neuroscience rely on similarity-based retrieval of stored patterns, where similarity is a function of retrieval cues and the stored patterns. While parsimonious, these models do not allow distinct representations for storage and retrieval, despite their distinct computational demands. Key-value memory systems, in contrast, distinguish representations used for storage (values) and those used for retrieval (keys). This allows key-value memory systems to optimize simultaneously for fidelity in storage and discriminability in retrieval. We review the computational foundations of key-value memory, its role in modern machine learning systems, related ideas from psychology and neuroscience, applications to a number of empirical puzzles, and possible biological implementations.	 | 心理学和神经科学中的传统记忆模型依赖于基于检索线索和存储模式之间相似性的检索，相似性是这两个因素的函数。虽然这些模型简洁有效，但它们无法为存储和检索提供不同的表示，尽管它们的计算需求各不相同。相比之下，键值记忆系统能够区分用于存储的表示（值）和用于检索的表示（键）。这使键值记忆系统能够同时优化存储的保真度和检索的可区分性。我们回顾了键值记忆的计算基础，它在现代机器学习系统中的作用，与心理学和神经科学相关的思想，对多种经验问题的应用，以及可能的生物实现方式。
2501.02942	 | Improved Approximation Algorithms for Low-Rank Problems Using Semidefinite Optimization	 | Ryan Cory-Wright,Jean Pauphilet	 | 受Goemans-Williamson算法的启发，该研究提出了一种“先松弛后采样”的策略，应用于低秩优化问题，通过开发更紧致的半定规划松弛，获得接近最优的解，并通过数值实验验证了其在正交约束二次优化问题和矩阵补全问题上的有效性和可扩展性。	 | Inspired by the impact of the Goemans-Williamson algorithm on combinatorial optimization, we construct an analogous relax-then-sample strategy for low-rank optimization problems. First, for orthogonally constrained quadratic optimization problems, we derive a semidefinite relaxation and a randomized rounding scheme, which obtains provably near-optimal solutions, mimicking the blueprint from Goemans and Williamson for the Max-Cut problem. We then extend our approach to generic low-rank optimization problems by developing new semidefinite relaxations that are both tighter and more broadly applicable than those in prior works. Although our original proposal introduces large semidefinite matrices as decision variables, we show that most of the blocks in these matrices can be safely omitted without altering the optimal value, hence improving the scalability of our approach. Using several examples (including matrix completion, basis pursuit, and reduced-rank regression), we show how to reduce the size of our relaxation even further. Finally, we numerically illustrate the effectiveness and scalability of our relaxation and our sampling scheme on orthogonally constrained quadratic optimization and matrix completion problems.	 | 受Goemans-Williamson算法对组合优化领域影响的启发，我们构建了一种类似于“先松弛后采样”的策略，应用于低秩优化问题。首先，对于具有正交约束的二次优化问题，我们推导出一个半定规划松弛和一个随机化取舍方案，该方案能获得可证明接近最优的解，模仿了Goemans和Williamson为最大割问题设计的模板。随后，我们将这种方法扩展到通用的低秩优化问题，通过开发新的半定规划松弛，这些松弛不仅更紧致，而且比之前的工作更广泛适用。尽管最初的建议引入了大规模的半定矩阵作为决策变量，我们展示了这些矩阵中的大部分块可以安全地省略，而不改变最优值，从而提高了我们方法的可扩展性。通过几个示例（包括矩阵补全、基本追求和降秩回归），我们展示了如何进一步减少松弛的规模。最后，我们通过数值实验展示了我们对正交约束二次优化问题和矩阵补全问题的松弛和采样方案的有效性和可扩展性。
2501.02934	 | A Bayesian Approach for Discovering Time- Delayed Differential Equation from Data	 | Debangshu Chowdhury,Souvik Chakraborty	 | BayTiDe是一种基于贝叶斯方法的算法，可以从数据中发现时间延迟微分方程，尤其适用于处理大时间延迟的情况，并且准确度与输入数据的分辨率成正比；该方法通过结合贝叶斯推理和稀疏促进的不连续尖峰-门控先验，有效缩小搜索空间，提高了计算效率和鲁棒性。	 | Time-delayed differential equations (TDDEs) are widely used to model complex dynamic systems where future states depend on past states with a delay. However, inferring the underlying TDDEs from observed data remains a challenging problem due to the inherent nonlinearity, uncertainty, and noise in real-world systems. Conventional equation discovery methods often exhibit limitations when dealing with large time delays, relying on deterministic techniques or optimization-based approaches that may struggle with scalability and robustness. In this paper, we present BayTiDe - Bayesian Approach for Discovering Time-Delayed Differential Equations from Data, that is capable of identifying arbitrarily large values of time delay to an accuracy that is directly proportional to the resolution of the data input to it. BayTiDe leverages Bayesian inference combined with a sparsity-promoting discontinuous spike-and-slab prior to accurately identify time-delayed differential equations. The approach accommodates arbitrarily large time delays with accuracy proportional to the input data resolution, while efficiently narrowing the search space to achieve significant computational savings. We demonstrate the efficiency and robustness of BayTiDe through a range of numerical examples, validating its ability to recover delayed differential equations from noisy data.	 | 时间延迟微分方程（TDDEs）广泛用于建模未来的状态依赖于过去状态的复杂动态系统。然而，从观测数据中推断背后的TDDEs仍然是一个具有挑战性的问题，因为实际系统中存在固有的非线性、不确定性以及噪声。传统的方法在处理大时间延迟时常常存在局限性，这些方法依赖于确定性技术或基于优化的方法，可能会在可扩展性和鲁棒性方面遇到困难。本文中，我们提出了BayTiDe - 基于贝叶斯方法从数据中发现时间延迟微分方程，它能够识别任意大的时间延迟，准确度直接与输入数据的分辨率成正比。BayTiDe 利用贝叶斯推理结合稀疏促进的不连续尖峰-门控先验来准确识别时间延迟微分方程。该方法能够处理任意大的时间延迟，准确度与输入数据的分辨率成正比，同时高效地缩小搜索空间，从而实现显著的计算节省。我们通过一系列数值示例展示了BayTiDe的高效性和鲁棒性，验证了它从噪声数据中恢复延迟微分方程的能力。
2501.02932	 | Predicting band gap from chemical composition: A simple learned model for a material property with atypical statistics	 | Andrew Ma,Owen Dugan,Marin Soljačić	 | 本文介绍了一种仅基于晶体材料的化学组成来预测电子能隙的简单机器学习模型，该模型通过将能隙预测视为混合随机变量建模任务来设计，利用材料组成中每个元素的独特参数来提供化学可解释性。	 | In solid-state materials science, substantial efforts have been devoted to the calculation and modeling of the electronic band gap. While a wide range of ab initio methods and machine learning algorithms have been created that can predict this quantity, the development of new computational approaches for studying the band gap remains an active area of research. Here we introduce a simple machine learning model for predicting the band gap using only the chemical composition of the crystalline material. To motivate the form of the model, we first analyze the empirical distribution of the band gap, which sheds new light on its atypical statistics. Specifically, our analysis enables us to frame band gap prediction as a task of modeling a mixed random variable, and we design our model accordingly. Our model formulation incorporates thematic ideas from chemical heuristic models for other material properties in a manner that is suited towards the band gap modeling task. The model has exactly one parameter corresponding to each element, which is fit using data. To predict the band gap for a given material, the model computes a weighted average of the parameters associated with its constituent elements and then takes the maximum of this quantity and zero. The model provides heuristic chemical interpretability by intuitively capturing the associations between the band gap and individual chemical elements.	 | 在固体材料科学中，已经投入了大量的努力来计算和建模电子能隙。虽然已经开发了多种从头算方法和机器学习算法来预测这一量值，但对于研究能隙的新计算方法的发展仍然是活跃的研究领域。在这里，我们介绍了一种仅使用晶体材料的化学组成来预测能隙的简单机器学习模型。为了说明该模型的形式，我们首先分析了能隙的经验分布，这为我们提供了对其不寻常统计特征的新见解。具体来说，我们的分析使我们能够将能隙预测任务视为建模混合随机变量的任务，我们据此设计了该模型。该模型框架融合了其他材料性质的化学启发模型中的主题思想，使其适用于能隙建模任务。该模型包括对应每个元素的唯一参数，这些参数通过数据进行拟合。为了预测给定材料的能隙，模型计算其组成元素相关参数的加权平均值，然后取该值的最大值和零。该模型通过直观地捕获能隙与各个化学元素之间的关联提供了启发式的化学可解释性。
2501.02916	 | Spiking monocular event based 6D pose estimation for space application	 | Jonathan Courtois,Benoît Miramond,Alain Pegatoquet	 | 本文介绍了首个事件驱动数据集SEENIC，并展示了使用小型突触端到端网络（S2E2）实现的21厘米位置误差和14度旋转误差的结果，这是向全事件驱动处理的嵌入式航天器姿态估计迈出的重要一步。	 | With the growing interest in on On-orbit servicing (OOS) and Active Debris Removal (ADR) missions, spacecraft poses estimation algorithms are being developed using deep learning to improve the precision of this complex task and find the most efficient solution. With the advances of bio-inspired low-power solutions, such a spiking neural networks and event-based processing and cameras, and their recent work for space applications, we propose to investigate the feasibility of a fully event-based solution to improve event-based pose estimation for spacecraft. In this paper, we address the first event-based dataset SEENIC with real event frames captured by an event-based camera on a testbed. We show the methods and results of the first event-based solution for this use case, where our small spiking end-to-end network (S2E2) solution achieves interesting results over 21cm position error and 14degree rotation error, which is the first step towards fully event-based processing for embedded spacecraft pose estimation.	 | 随着对在轨服务（OOS）和主动碎片清除（ADR）任务的兴趣日益增长，正在使用深度学习开发航天器姿态估计算法，以提高该复杂任务的精度并找到最高效的解决方案。随着生物启发低功耗解决方案的发展，如突触神经网络和事件驱动处理和相机，以及它们在航天应用中的最新进展，我们提出研究全事件驱动解决方案的可能性，以改进航天器的姿态估计。在本文中，我们介绍了第一个事件驱动数据集SEENIC，该数据集包含由测试床上的事件驱动相机捕获的真实事件帧。我们展示了针对此用例的第一个事件驱动解决方案的方法和结果，其中我们的小型突触端到端网络（S2E2）解决方案实现了21厘米位置误差和14度旋转误差的有趣结果，这是向嵌入式航天器姿态估计的全事件驱动处理迈出的第一步。
2501.02902	 | Sim-to-Real Transfer for Mobile Robots with Reinforcement Learning: from NVIDIA Isaac Sim to Gazebo and Real ROS 2 Robots	 | Sahar Salimpour,Jorge Peña-Queralta,Diego Paez-Granados,Jukka Heikkonen,Tomi Westerlund	 | 本文介绍了使用NVIDIA Isaac Sim训练基于深度强化学习的控制器以实现局部规划和障碍物避免的应用，展示了这些策略在Isaac模拟器中的性能，并证明了其在实际机器人上的零样本迁移能力，同时为自定义机器人平台的快速部署提供了指导。	 | Unprecedented agility and dexterous manipulation have been demonstrated with controllers based on deep reinforcement learning (RL), with a significant impact on legged and humanoid robots. Modern tooling and simulation platforms, such as NVIDIA Isaac Sim, have been enabling such advances. This article focuses on demonstrating the applications of Isaac in local planning and obstacle avoidance as one of the most fundamental ways in which a mobile robot interacts with its environments. Although there is extensive research on proprioception-based RL policies, the article highlights less standardized and reproducible approaches to exteroception. At the same time, the article aims to provide a base framework for end-to-end local navigation policies and how a custom robot can be trained in such simulation environment. We benchmark end-to-end policies with the state-of-the-art Nav2, navigation stack in Robot Operating System (ROS). We also cover the sim-to-real transfer process by demonstrating zero-shot transferability of policies trained in the Isaac simulator to real-world robots. This is further evidenced by the tests with different simulated robots, which show the generalization of the learned policy. Finally, the benchmarks demonstrate comparable performance to Nav2, opening the door to quick deployment of state-of-the-art end-to-end local planners for custom robot platforms, but importantly furthering the possibilities by expanding the state and action spaces or task definitions for more complex missions. Overall, with this article we introduce the most important steps, and aspects to consider, in deploying RL policies for local path planning and obstacle avoidance with Isaac Sim training, Gazebo testing, and ROS 2 for real-time inference in real robots. The code is available at https://github.com/sahars93/RL-Navigation.	 | 基于深度强化学习（RL）的控制器展示了前所未有的敏捷性和灵巧操作，对腿部和类人机器人产生了重大影响。现代工具和模拟平台，如NVIDIA Isaac Sim，推动了这些进步。本文主要展示了Isaac在局部规划和障碍物避免方面的应用，这是移动机器人与环境交互的最基础方式之一。尽管关于本体感觉为基础的RL策略的研究非常广泛，本文强调了对外部感觉较少标准化和可重复性的方法。同时，本文旨在为端到端的局部导航策略提供一个基础框架，并展示如何在这样的模拟环境中训练自定义机器人。我们使用最新的Nav2导航堆栈作为基准，评估了Isaac模拟器中训练策略与Robot Operating System (ROS)中的性能。此外，我们还展示了训练策略在Isaac模拟器中的策略在实际机器人上的零样本迁移。通过使用不同模拟机器人的测试，进一步证明了学习到的策略的泛化能力。最后，基准测试表明与Nav2的性能相当，为自定义机器人平台的快速部署最先进的端到端局部路径规划器和障碍物避免策略打开了大门，同时通过扩展状态和动作空间或任务定义，进一步扩大了复杂任务的可能性。总体而言，本文介绍了使用Isaac Sim训练、Gazebo测试和ROS 2进行实时推理，部署RL策略进行局部路径规划和障碍物避免时最重要的步骤和注意事项。代码可在https://github.com/sahars93/RL-Navigation获得。
2501.02885	 | MDP3: A Training-free Approach for List-wise Frame Selection in Video-LLMs	 | Hui Sun,Shiyin Lu,Huanyu Wang,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Ming Li	 | 本文提出了一种基于马尔可夫决策定式点过程与动态规划的方法（MDP3），用于有效选择视频帧，以解决Video-LLMs在处理多帧时遇到的上下文长度限制和无关帧干扰问题。MDP3能够同时满足查询相关性、列表多样性和顺序性三个关键原则，实验结果证明其在多种场景下显著优于现有方法。	 | Video large language models (Video-LLMs) have made significant progress in understanding videos. However, processing multiple frames leads to lengthy visual token sequences, presenting challenges such as the limited context length cannot accommodate the entire video, and the inclusion of irrelevant frames hinders visual perception. Hence, effective frame selection is crucial. This paper emphasizes that frame selection should follow three key principles: query relevance, list-wise diversity, and sequentiality. Existing methods, such as uniform frame sampling and query-frame matching, do not capture all of these principles. Thus, we propose Markov decision determinantal point process with dynamic programming (MDP3) for frame selection, a training-free and model-agnostic method that can be seamlessly integrated into existing Video-LLMs. Our method first estimates frame similarities conditioned on the query using a conditional Gaussian kernel within the reproducing kernel Hilbert space~(RKHS). We then apply the determinantal point process~(DPP) to the similarity matrix to capture both query relevance and list-wise diversity. To incorporate sequentiality, we segment the video and apply DPP within each segment, conditioned on the preceding segment selection, modeled as a Markov decision process~(MDP) for allocating selection sizes across segments. Theoretically, MDP3 provides a \((1 - 1/e)\)-approximate solution to the NP-hard list-wise frame selection problem with pseudo-polynomial time complexity, demonstrating its efficiency. Empirically, MDP3 significantly outperforms existing methods, verifying its effectiveness and robustness.	 | 视频大型语言模型（Video-LLMs）在理解视频方面取得了显著进展。然而，处理多帧会导致视觉标记序列过长，从而引发诸如上下文长度有限无法容纳整段视频，以及包含无关帧妨碍视觉感知等问题。因此，有效的帧选择至关重要。本文强调帧选择应遵循三个关键原则：查询相关性、列表多样性以及顺序性。现有的方法，如均匀帧抽样和查询帧匹配，无法捕捉所有这些原则。因此，我们提出了一种基于马尔可夫决策定式点过程与动态规划的方法（MDP3），这是一种无需训练且模型无关的方法，可以无缝集成到现有的Video-LLMs中。我们的方法首先使用条件高斯核在再生核希尔伯特空间（RKHS）中估计给定查询条件下的帧相似性。然后，我们应用定式点过程（DPP）到相似性矩阵上，以捕捉查询相关性和列表多样性。为了引入顺序性，我们将视频分段，并在每个段内应用条件于前一段选择的定式点过程，将其建模为马尔可夫决策过程（MDP），以在各个段间分配选择量。理论上，MDP3为NP难的列表帧选择问题提供了\((1 - 1/e)\)近似解，具有伪多项式时间复杂度，证明了其高效性。实验上，MDP3显著优于现有方法，验证了其有效性和鲁棒性。
2501.02857	 | ParetoLens: A Visual Analytics Framework for Exploring Solution Sets of Multi-objective Evolutionary Algorithms	 | Yuxin Ma,Zherui Zhang,Ran Cheng,Yaochu Jin,Kay Chen Tan	 | ParetoLens 是一个用于增强多目标进化算法生成解集检查和探索的可视化分析框架，通过交互式可视化方法在决策空间和目标空间中对解的分布进行详细检查，有效缓解了高维度数据和静态可视化方法带来的问题。	 | In the domain of multi-objective optimization, evolutionary algorithms are distinguished by their capability to generate a diverse population of solutions that navigate the trade-offs inherent among competing objectives. This has catalyzed the ascension of evolutionary multi-objective optimization (EMO) as a prevalent approach. Despite the effectiveness of the EMO paradigm, the analysis of resultant solution sets presents considerable challenges. This is primarily attributed to the high-dimensional nature of the data and the constraints imposed by static visualization methods, which frequently culminate in visual clutter and impede interactive exploratory analysis. To address these challenges, this paper introduces ParetoLens, a visual analytics framework specifically tailored to enhance the inspection and exploration of solution sets derived from the multi-objective evolutionary algorithms. Utilizing a modularized, algorithm-agnostic design, ParetoLens enables a detailed inspection of solution distributions in both decision and objective spaces through a suite of interactive visual representations. This approach not only mitigates the issues associated with static visualizations but also supports a more nuanced and flexible analysis process. The usability of the framework is evaluated through case studies and expert interviews, demonstrating its potential to uncover complex patterns and facilitate a deeper understanding of multi-objective optimization solution sets. A demo website of ParetoLens is available at https://dva-lab.org/paretolens/.	 | 在多目标优化领域，进化算法因其能够生成多样化的解集，有效解决相互竞争目标之间的权衡问题而受到重视。这促使进化多目标优化（EMO）成为一种流行的解决方案。尽管EMO范式非常有效，但在分析产生的解集时仍面临重大挑战。主要原因在于数据的高维度以及静态可视化方法的限制，这些限制往往导致视觉混乱，阻碍了交互式探索分析。为解决这些问题，本文提出了一种名为ParetoLens的可视化分析框架，专门用于增强从多目标进化算法中获得的解集的检查和探索。ParetoLens采用模块化、算法无关的设计，通过一系列交互式可视化表示方法，能够在决策空间和目标空间中对解的分布进行详尽检查。这种方法不仅缓解了静态可视化带来的问题，还支持更细致和灵活的分析过程。论文通过案例研究和专家访谈评估了该框架的可用性，证明了它在发现复杂模式并促进对多目标优化解集的深入了解方面的潜力。ParetoLens的演示网站可访问：https://dva-lab.org/paretolens/。
2501.02844	 | Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text Classification	 | Yubo Wang,Haoyang Li,Fei Teng,Lei Chen	 | GORAG 是一个基于图的在线检索增强生成框架，用于动态少量标记的文本分类，通过构建自适应的信息图并使用定制的生成树动态检索相关上下文，从而提供更全面和准确的上下文信息，优于现有方法。	 | Text classification is a fundamental task in natural language processing, pivotal to various applications such as query optimization, data integration, and schema matching. While neural network-based models, such as CNN and BERT, have demonstrated remarkable performance in text classification, their effectiveness heavily relies on abundant labeled training data. This dependency makes these models less effective in dynamic few-shot text classification, where labeled data is scarce, and target labels frequently evolve based on application needs. Recently, large language models (LLMs) have shown promise due to their extensive pretraining and contextual understanding. Current approaches provide LLMs with text inputs, candidate labels, and additional side information (e.g., descriptions) to predict text labels. However, their effectiveness is hindered by the increased input size and the noise introduced through side information processing. To address these limitations, we propose a graph-based online retrieval-augmented generation framework, namely GORAG, for dynamic few-shot text classification. GORAG constructs and maintains an adaptive information graph by extracting side information across all target texts, rather than treating each input independently. It employs a weighted edge mechanism to prioritize the importance and reliability of extracted information and dynamically retrieves relevant context using a minimum-cost spanning tree tailored for each text input. Empirical evaluations demonstrate that GORAG outperforms existing approaches by providing more comprehensive and accurate contextual information.	 | 文本分类是自然语言处理中的一个基本任务，对于诸如查询优化、数据集成和模式匹配等各种应用至关重要。尽管基于神经网络的模型，如CNN和BERT，在文本分类任务中展现了卓越的性能，但它们的有效性高度依赖于大量的标记训练数据。这种依赖性使得这些模型在动态少量标记的文本分类任务中效果不佳，在这种任务中，标记数据稀缺，目标标签会根据应用需求频繁变化。最近，由于其广泛的预训练和上下文理解能力，大型语言模型（LLMs）已经显示出了潜力。当前的方法通过向LLMs提供文本输入、候选标签以及额外的辅助信息（如描述）来预测文本标签。然而，这些方法的有效性受到了输入尺寸增加以及侧信息处理引入的噪声的影响。为了克服这些局限性，我们提出了一种基于图的在线检索增强生成框架，即GORAG，用于动态少量标记的文本分类。GORAG通过提取所有目标文本中的侧信息来构建和维护一个自适应的信息图，而不是单独处理每个输入。它使用加权边机制来优先考虑提取信息的重要性与可靠性，并使用为每个文本输入量身定制的最小成本生成树来动态检索相关上下文。实证评估表明，GORAG通过提供更全面和准确的上下文信息，比现有方法表现更优。
2501.02843	 | RAHN: A Reputation Based Hourglass Network for Web Service QoS Prediction	 | Xia Chen,Yugen Du,Guoxing Tang,Yingwei Luo,Benchi Ma	 | 本文提出了一种基于声誉和深度学习的QoS预测网络RAHN，通过信誉计算模块、潜在特征提取模块和QoS预测hourglass网络来提高服务质量预测的准确性和效率，并在实际QoS数据集上验证了其优越性。	 | As the homogenization of Web services becomes more and more common, the difficulty of service recommendation is gradually increasing. How to predict Quality of Service (QoS) more efficiently and accurately becomes an important challenge for service recommendation. Considering the excellent role of reputation and deep learning (DL) techniques in the field of QoS prediction, we propose a reputation and DL based QoS prediction network, RAHN, which contains the Reputation Calculation Module (RCM), the Latent Feature Extraction Module (LFEM), and the QoS Prediction Hourglass Network (QPHN). RCM obtains the user reputation and the service reputation by using a clustering algorithm and a Logit model. LFEM extracts latent features from known information to form an initial latent feature vector. QPHN aggregates latent feature vectors with different scales by using Attention Mechanism, and can be stacked multiple times to obtain the final latent feature vector for prediction. We evaluate RAHN on a real QoS dataset. The experimental results show that the Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) of RAHN are smaller than the six baseline methods.	 | 随着Web服务的同质化越来越普遍，服务推荐的难度也在逐渐增加。如何更高效、更准确地预测服务质量（QoS）已成为服务推荐的重要挑战。考虑到声誉和深度学习（DL）技术在QoS预测领域的出色表现，我们提出了一种基于声誉和DL的QoS预测网络RAHN，该网络包含信誉计算模块（RCM）、潜在特征提取模块（LFEM）和QoS预测 hourglass 网络（QPHN）。RCM 通过使用聚类算法和Logit模型来获取用户信誉和服务信誉。LFEM 从已知信息中提取潜在特征并形成初始潜在特征向量。QPHN 通过注意机制聚合不同尺度的潜在特征向量，并可以多次堆叠以获得最终用于预测的潜在特征向量。我们使用一个实际的QoS数据集对RAHN进行了评估。实验结果表明，RAHN的平均绝对误差（MAE）和均方根误差（RMSE）比六种基线方法都要小。
2501.02842	 | Foundations of GenIR	 | Qingyao Ai,Jingtao Zhan,Yiqun Liu	 | 本章探讨了生成型人工智能模型对信息访问系统的影响，特别是它们如何通过信息生成和信息综合两种新范式提供高质量、类人的响应，从而为IA系统带来新的发展机遇。生成型AI不仅能根据用户需求即时生成定制内容，还能整合和重组现有信息，提高系统的效能和可靠性。	 | The chapter discusses the foundational impact of modern generative AI models on information access (IA) systems. In contrast to traditional AI, the large-scale training and superior data modeling of generative AI models enable them to produce high-quality, human-like responses, which brings brand new opportunities for the development of IA paradigms. In this chapter, we identify and introduce two of them in details, i.e., information generation and information synthesis. Information generation allows AI to create tailored content addressing user needs directly, enhancing user experience with immediate, relevant outputs. Information synthesis leverages the ability of generative AI to integrate and reorganize existing information, providing grounded responses and mitigating issues like model hallucination, which is particularly valuable in scenarios requiring precision and external knowledge. This chapter delves into the foundational aspects of generative models, including architecture, scaling, and training, and discusses their applications in multi-modal scenarios. Additionally, it examines the retrieval-augmented generation paradigm and other methods for corpus modeling and understanding, demonstrating how generative AI can enhance information access systems. It also summarizes potential challenges and fruitful directions for future studies.	 | 本章讨论了现代生成型人工智能模型对信息访问（IA）系统基础性的影响。与传统人工智能相比，生成型人工智能模型的大规模训练和卓越的数据建模能力使它们能够生成高质量、类人的响应，从而为IA范式的开发带来了全新机遇。在本章中，我们将详细探讨其中的两种新范式，即信息生成和信息综合。信息生成使AI能够直接根据用户需求生成定制内容，从而通过即时、相关的内容增强用户体验。信息综合则利用生成型人工智能整合和重组现有信息的能力，提供实际可靠的回答，并缓解如模型幻觉等问题，特别是在需要精确性和外部知识的场景中尤为有价值。本章深入探讨了生成模型的基础方面，包括架构、扩展和训练，并讨论了它们在多模态场景中的应用。此外，本章还研究了检索增强生成范式以及其他用于语料库建模和理解的方法，展示了生成型人工智能如何提高信息访问系统的效能。最后，本章总结了未来研究中可能面临的挑战和富有成果的方向。
2501.02840	 | Enhanced Rooftop Solar Panel Detection by Efficiently Aggregating Local Features	 | Kuldeep Kurte,Kedar Kulkarni	 | 本文提出了一种结合预训练卷积神经网络和局部聚合描述子（VLAD）的增强型方法，用于检测卫星图像中的屋顶太阳能光伏（PV）面板，并在三个城市的数据集上实现了高分类精度；此外，还提出了一种三阶段方法以高效扩展至新城市或区域。	 | In this paper, we present an enhanced Convolutional Neural Network (CNN)-based rooftop solar photovoltaic (PV) panel detection approach using satellite images. We propose to use pre-trained CNN-based model to extract the local convolutional features of rooftops. These local features are then combined using the Vectors of Locally Aggregated Descriptors (VLAD) technique to obtain rooftop-level global features, which are then used to train traditional Machine Learning (ML) models to identify rooftop images that do and do not contain PV panels. On the dataset used in this study, the proposed approach achieved rooftop-PV classification scores exceeding the predefined threshold of 0.9 across all three cities for each of the feature extractor networks evaluated. Moreover, we propose a 3-phase approach to enable efficient utilization of the previously trained models on a new city or region with limited labelled data. We illustrate the effectiveness of this 3-phase approach for multi-city rooftop-PV detection task.	 | 在这篇论文中，我们提出了一种增强型基于卷积神经网络（CNN）的卫星图像屋顶太阳能光伏（PV）面板检测方法。我们建议使用预训练的基于CNN的模型来提取屋顶的局部卷积特征。然后，使用局部聚合描述子向量（VLAD）技术结合这些局部特征以获得屋顶级别的全局特征，这些全局特征随后用于训练传统的机器学习（ML）模型以识别含有和不含有PV面板的屋顶图像。在本研究使用的数据集上，所提出的方法在评估的每个特征提取网络中，在三个城市中均实现了屋顶-PV分类得分超过预定义阈值0.9。此外，我们提出了一种三阶段方法，以在新城市或区域有限标注数据的情况下高效利用已训练的模型。我们展示了这种三阶段方法在多城市屋顶-PV检测任务中的有效性。
2501.02824	 | Proteomic Learning of Gamma-Aminobutyric Acid (GABA) Receptor-Mediated Anesthesia	 | Jian Jiang,Long Chen,Yueying Zhu,Yazhou Shi,Huahai Qiu,Bengong Zhang,Tianshou Zhou,Guo-Wei Wei	 | 该研究通过蛋白质组学方法和机器学习算法，基于γ-氨基丁酸（GABA）受体亚型，筛选出潜在的先导化合物，以开发副作用更少、效果更好的新型麻醉剂，同时优化现有麻醉剂的结构。	 | Anesthetics are crucial in surgical procedures and therapeutic interventions, but they come with side effects and varying levels of effectiveness, calling for novel anesthetic agents that offer more precise and controllable effects. Targeting Gamma-aminobutyric acid (GABA) receptors, the primary inhibitory receptors in the central nervous system, could enhance their inhibitory action, potentially reducing side effects while improving the potency of anesthetics. In this study, we introduce a proteomic learning of GABA receptor-mediated anesthesia based on 24 GABA receptor subtypes by considering over 4000 proteins in protein-protein interaction (PPI) networks and over 1.5 millions known binding compounds. We develop a corresponding drug-target interaction network to identify potential lead compounds for novel anesthetic design. To ensure robust proteomic learning predictions, we curated a dataset comprising 136 targets from a pool of 980 targets within the PPI networks. We employed three machine learning algorithms, integrating advanced natural language processing (NLP) models such as pretrained transformer and autoencoder embeddings. Through a comprehensive screening process, we evaluated the side effects and repurposing potential of over 180,000 drug candidates targeting the GABRA5 receptor. Additionally, we assessed the ADMET (absorption, distribution, metabolism, excretion, and toxicity) properties of these candidates to identify those with near-optimal characteristics. This approach also involved optimizing the structures of existing anesthetics. Our work presents an innovative strategy for the development of new anesthetic drugs, optimization of anesthetic use, and deeper understanding of potential anesthesia-related side effects.	 | 麻醉剂在手术程序和治疗干预中至关重要，但它们伴随有副作用和不同的有效性，因此需要新型麻醉剂以提供更加精确和可控的效果。针对γ-氨基丁酸（GABA）受体，这在中枢神经系统中是主要的抑制性受体，可能会增强其抑制作用，从而可能减少副作用并提高麻醉剂的效果。在这项研究中，我们基于24种GABA受体亚型，通过考虑超过4000种蛋白质在蛋白质-蛋白质相互作用（PPI）网络中的相互作用以及超过150万已知的结合化合物，开展了基于蛋白质组学的学习方法来研究GABA受体介导的麻醉作用。我们构建了一个相应的药物-靶点相互作用网络，以识别新型麻醉剂设计的潜在先导化合物。为了确保蛋白质组学学习预测的稳健性，我们从包含980种靶点的PPI网络中筛选出136个靶点进行数据集构建。我们采用了三种机器学习算法，并整合了先进的自然语言处理（NLP）模型，如预训练的变压器和自动编码器嵌入。通过全面的筛选过程，我们评估了超过18万个针对GABRA5受体的药物候选物的副作用和再利用潜力，并评估了它们的ADMET（吸收、分布、代谢、排泄和毒性）特性，以识别具有接近最优特性的候选物。此方法还涉及优化现有的麻醉剂结构。我们的工作提出了一种新的策略，用于开发新型麻醉药物、优化麻醉剂的使用以及更深入地理解潜在的麻醉相关副作用。
2501.02814	 | Analogue Forecast System for Daily Precipitation Prediction Using Autoencoder Feature Extraction: Application in Hong Kong	 | Yee Chun Tsoi,Yu Ting Kwok,Ming Chun Lam,Wai Kin Wong	 | 在香港天文台，通过运用深度学习技术中的自编码器并使用ERA5数据集，开发了一种增强的模拟预报系统（AFS），该系统在处理极端降雨情况方面表现优于现有系统。	 | In the Hong Kong Observatory, the Analogue Forecast System (AFS) for precipitation has been providing useful reference in predicting possible daily rainfall scenarios for the next 9 days, by identifying historical cases with similar weather patterns to the latest output from the deterministic model of the European Centre for Medium-Range Weather Forecasts (ECMWF). Recent advances in machine learning allow more sophisticated models to be trained using historical data and the patterns of high-impact weather events to be represented more effectively. As such, an enhanced AFS has been developed using the deep learning technique autoencoder. The datasets of the fifth generation of the ECMWF Reanalysis (ERA5) are utilised where more meteorological elements in higher horizontal, vertical and temporal resolutions are available as compared to the previous ECMWF reanalysis products used in the existing AFS. The enhanced AFS features four major steps in generating the daily rain class forecasts: (1) preprocessing of gridded ERA5 and ECMWF model forecast, (2) feature extraction by the pretrained autoencoder, (3) application of optimised feature weightings based on historical cases, and (4) calculation of the final rain class from a weighted ensemble of top analogues. The enhanced AFS demonstrates a consistent and superior performance over the existing AFS, especially in capturing heavy rain cases, during the verification period from 2019 to 2022. This paper presents the detailed formulation of the enhanced AFS and discusses its advantages and limitations in supporting precipitation forecasting in Hong Kong.	 | 在香港天文台，模拟预报系统（AFS）用于预测未来9天内可能出现的每日降雨情景，并通过识别与欧洲中期天气预报中心（ECMWF）确定性模型最新输出具有相似天气模式的历史案例来提供有用的参考。最近机器学习的进步使得可以使用历史数据训练更复杂的模型，并更有效地表示高影响力的天气事件的模式。因此，使用深度学习技术自编码器开发了增强的AFS。利用第五代欧洲中心再分析数据集（ERA5），与现有AFS中使用的以前的欧洲中心再分析产品相比，ERA5提供的气象要素在水平、垂直和时间分辨率上更高。增强的AFS生成每日降雨类预报的过程包含四个主要步骤：（1）网格化ERA5和ECMWF模型预报数据的预处理，（2）通过预训练的自编码器提取特征，（3）基于历史案例应用优化的特征权重，以及（4）通过加权集合的顶级类似案例计算最终降雨类。增强的AFS在2019年至2022年的验证期间表现一致且优于现有AFS，特别是在捕捉极端降雨情况方面。本文详细介绍了增强的AFS的开发，并讨论了其在香港支持降水预报中的优势和局限性。
2501.02796	 | GraphDART: Graph Distillation for Efficient Advanced Persistent Threat Detection	 | Saba Fathi Rabooki,Bowen Li,Falih Gozi Febrinanto,Ciyuan Peng,Elham Naghizade,Fengling Han,Feng Xia	 | 本文提出了一种名为GraphDART的模块化框架，旨在通过提炼复杂追溯图来提高计算物理社会系统的异常检测效率和性能，特别是在检测高级持续性威胁方面表现出色。GraphDART利用多种图提炼技术压缩大规模追溯图，同时保留关键信息，从而提升GNNs的学习效率和检测准确性。	 | Cyber-physical-social systems (CPSSs) have emerged in many applications over recent decades, requiring increased attention to security concerns. The rise of sophisticated threats like Advanced Persistent Threats (APTs) makes ensuring security in CPSSs particularly challenging. Provenance graph analysis has proven effective for tracing and detecting anomalies within systems, but the sheer size and complexity of these graphs hinder the efficiency of existing methods, especially those relying on graph neural networks (GNNs). To address these challenges, we present GraphDART, a modular framework designed to distill provenance graphs into compact yet informative representations, enabling scalable and effective anomaly detection. GraphDART can take advantage of diverse graph distillation techniques, including classic and modern graph distillation methods, to condense large provenance graphs while preserving essential structural and contextual information. This approach significantly reduces computational overhead, allowing GNNs to learn from distilled graphs efficiently and enhance detection performance. Extensive evaluations on benchmark datasets demonstrate the robustness of GraphDART in detecting malicious activities across cyber-physical-social systems. By optimizing computational efficiency, GraphDART provides a scalable and practical solution to safeguard interconnected environments against APTs.	 | 近年来，计算物理社会系统（CPSSs）在许多应用中崭露头角，对系统的安全问题关注度也在不断提高。复杂的威胁如高级持续性威胁（APTs）的崛起，让确保CPSSs的安全变得更加困难。追溯图分析（Provenance Graph Analysis）已被证明在追踪和检测系统中的异常方面非常有效，但这些图的庞大复杂性阻碍了现有方法的效率，尤其是依赖于图神经网络（GNNs）的方法。为了解决这些挑战，我们提出了一种模块化框架GraphDART，旨在将追溯图提炼成紧凑而信息丰富的表示，从而实现可扩展且有效的异常检测。GraphDART 可以利用多种图提炼技术，包括经典和现代的图提炼方法，来压缩大规模的追溯图，同时保留关键的结构和上下文信息。这种方法显著减少了计算开销，使GNNs能够高效地从提炼后的图中学习，并提高检测性能。在基准数据集上的广泛评估表明，GraphDART 在计算物理社会系统的恶意活动检测方面表现出很高的鲁棒性。通过优化计算效率，GraphDART 提供了一个可扩展且实用的解决方案，以保护互联环境免受APTs 的侵害。
2501.02793	 | Fairness Through Matching	 | Kunwoong Kim,Insung Kong,Jongjin Lee,Minwoo Chae,Sangchul Park,Yongdai Kim	 | 这项研究揭示了现有群体公平度量的一个隐含特性，并基于此发展了一种新的群体公平约束和算法，即公平性通过匹配（FTM），该算法使用匹配人口平等（MDP）度量来学习群体公平模型，从而成功地训练出具有期望属性的群体公平模型。	 | Group fairness requires that different protected groups, characterized by a given sensitive attribute, receive equal outcomes overall. Typically, the level of group fairness is measured by the statistical gap between predictions from different protected groups. In this study, we reveal an implicit property of existing group fairness measures, which provides an insight into how the group-fair models behave. Then, we develop a new group-fair constraint based on this implicit property to learn group-fair models. To do so, we first introduce a notable theoretical observation: every group-fair model has an implicitly corresponding transport map between the input spaces of each protected group. Based on this observation, we introduce a new group fairness measure termed Matched Demographic Parity (MDP), which quantifies the averaged gap between predictions of two individuals (from different protected groups) matched by a given transport map. Then, we prove that any transport map can be used in MDP to learn group-fair models, and develop a novel algorithm called Fairness Through Matching (FTM), which learns a group-fair model using MDP constraint with an user-specified transport map. We specifically propose two favorable types of transport maps for MDP, based on the optimal transport theory, and discuss their advantages. Experiments reveal that FTM successfully trains group-fair models with certain desirable properties by choosing the transport map accordingly.	 | 群体公平要求不同受保护群体（由给定的敏感属性界定）的整体结果相同。通常，群体公平度的水平通过不同受保护群体之间预测结果的统计差距来衡量。在这项研究中，我们揭示了现有群体公平度度量的一个隐含特性，这为我们理解群体公平模型的行为提供了新的见解。随后，我们基于这一隐含特性发展了一种新的群体公平约束，以学习群体公平模型。为此，我们首先提出一个重要的理论观察：每个群体公平模型都隐含地对应于每个受保护群体的输入空间之间的运输映射。基于这一观察，我们引入了一种新的群体公平性度量，称为匹配人口平等（Matched Demographic Parity，MDP），它量化了通过给定运输映射匹配的两个人中预测结果的平均差距。然后，我们证明了可以在MDP中使用任何运输映射来学习群体公平模型，并开发了一种名为公平性通过匹配（Fairness Through Matching，FTM）的新算法，该算法使用MDP约束（带用户指定的运输映射）来学习群体公平模型。我们基于最优运输理论，特别是提出了两种对MDP有利的运输映射类型，并讨论了它们的优点。实验表明，通过适当选择运输映射，FTM成功地训练出了具有某些期望属性的群体公平模型。
2501.02791	 | Orthogonal greedy algorithm for linear operator learning with shallow neural network	 | Ye Lin,Jiwei Jia,Young Ju Lee,Ran Zhang	 | 本文将正交贪婪算法（OGA）应用于线性算子学习任务，通过开发新的贪婪算法和点估计方法，提高了核函数估计的精度，并对算法的最优逼近速率进行了理论分析，证实了其在PDE和算子学习任务中的有效性和潜力。	 | Greedy algorithms, particularly the orthogonal greedy algorithm (OGA), have proven effective in training shallow neural networks for fitting functions and solving partial differential equations (PDEs). In this paper, we extend the application of OGA to the tasks of linear operator learning, which is equivalent to learning the kernel function through integral transforms. Firstly, a novel greedy algorithm is developed for kernel estimation rate in a new semi-inner product, which can be utilized to approximate the Green's function of linear PDEs from data. Secondly, we introduce the OGA for point-wise kernel estimation to further improve the approximation rate, achieving orders of accuracy improvement across various tasks and baseline models. In addition, we provide a theoretical analysis on the kernel estimation problem and the optimal approximation rates for both algorithms, establishing their efficacy and potential for future applications in PDEs and operator learning tasks.	 | 贪婪算法，特别是正交贪婪算法（OGA），已被证明在训练浅层神经网络以拟合函数和求解偏微分方程（PDEs）方面非常有效。在本文中，我们将OGA的应用扩展到线性算子学习任务，这相当于通过积分变换学习核函数。首先，我们开发了一种新的贪婪算法，用于在新的半内积中估计核函数的速率，该算法可用于从数据中近似线性PDE的格林函数。其次，我们引入了OGA进行点估计以进一步提高逼近速率，实现了各种任务和基线模型中的精度改进。此外，我们对核函数估计问题及其两种算法的最优逼近速率进行了理论分析，建立了它们在PDE和算子学习任务中的有效性及其未来应用的潜力。
2501.02785	 | Hybrid deep convolution model for lung cancer detection with transfer learning	 | Sugandha Saxena,S. N. Prasad,Ashwin M Polnaya,Shweta Agarwala	 | 该研究提出了一种结合迁移学习的混合深度卷积模型——最大灵敏度神经网络（MSNN），该模型在肺部CT图像的肺癌检测中取得了98%的准确率和97%的灵敏度，通过可视化灵敏度图提高了诊断的准确性和特异性。	 | Advances in healthcare research have significantly enhanced our understanding of disease mechanisms, diagnostic precision, and therapeutic options. Yet, lung cancer remains one of the leading causes of cancer-related mortality worldwide due to challenges in early and accurate diagnosis. While current lung cancer detection models show promise, there is considerable potential for further improving the accuracy for timely intervention. To address this challenge, we introduce a hybrid deep convolution model leveraging transfer learning, named the Maximum Sensitivity Neural Network (MSNN). MSNN is designed to improve the precision of lung cancer detection by refining sensitivity and specificity. This model has surpassed existing deep learning approaches through experimental validation, achieving an accuracy of 98% and a sensitivity of 97%. By overlaying sensitivity maps onto lung Computed Tomography (CT) scans, it enables the visualization of regions most indicative of malignant or benign classifications. This innovative method demonstrates exceptional performance in distinguishing lung cancer with minimal false positives, thereby enhancing the accuracy of medical diagnoses.	 | 医疗健康领域的研究进展显著提升了我们对疾病机制、诊断准确性和治疗方案的理解。然而，肺癌仍然是全球癌症相关死亡的主要原因之一，主要原因在于早期和准确诊断的挑战。尽管现有的肺癌检测模型展示了潜力，但进一步提高检测准确性的潜力仍然很大。为应对这一挑战，我们提出了一种结合迁移学习的混合深度卷积模型，命名为最大灵敏度神经网络（MSNN）。MSNN旨在通过细化灵敏度和特异性来提高肺癌检测的精确性。通过实验验证，该模型超越了现有的深度学习方法，达到了98%的准确率和97%的灵敏度。通过在肺部计算机断层扫描（CT）图像上叠加灵敏度图，它能够可视化最能指示恶性或良性分类的区域。这一创新方法在区分肺癌方面表现出色，几乎没有假阳性，从而提高了医学诊断的准确性。
2501.02761	 | Beyond $\mathcal{O}(\sqrt{T})$ Regret: Decoupling Learning and Decision-making in Online Linear Programming	 | Wenzhi Gao,Dongdong Ge,Chenyu Xue,Chunlin Sun,Yinyu Ye	 | 本文提出了一种通用框架，能够在特定误差边界条件下改进在线线性规划的一阶学习算法的遗憾度，首次在连续支持设置中实现了 $o(\sqrt{T})$ 的遗憾度，在有限支持设置中实现了 $\mathcal{O}(\log T)$ 的遗憾度，显著提升了在线线性规划的性能。	 | Online linear programming plays an important role in both revenue management and resource allocation, and recent research has focused on developing efficient first-order online learning algorithms. Despite the empirical success of first-order methods, they typically achieve a regret no better than $\mathcal{O} ( \sqrt{T} )$, which is suboptimal compared to the $\mathcal{O} (\log T)$ bound guaranteed by the state-of-the-art linear programming (LP)-based online algorithms. This paper establishes a general framework that improves upon the $\mathcal{O} ( \sqrt{T} )$ result when the LP dual problem exhibits certain error bound conditions. For the first time, we show that first-order learning algorithms achieve $o( \sqrt{T} )$ regret in the continuous support setting and $\mathcal{O} (\log T)$ regret in the finite support setting beyond the non-degeneracy assumption. Our results significantly improve the state-of-the-art regret results and provide new insights for sequential decision-making.	 | 在线线性规划在收益管理和资源分配中发挥着重要作用，最近的研究重点是开发高效的在线学习一阶算法。尽管一阶方法在实际应用中表现出色，但它们通常只能达到 $\mathcal{O}(\sqrt{T})$ 的遗憾度，这与最新线性规划（LP）基于的在线算法提供的 $\mathcal{O}(\log T)$ 边界相比是次优的。本论文建立了一个通用框架，在线性规划（LP）的对偶问题表现出特定误差边界条件时，可以改进 $\mathcal{O}(\sqrt{T})$ 的结果。首次证明了一阶学习算法在连续支持设置中实现了 $o(\sqrt{T})$ 的遗憾度，在有限支持设置中实现了 $\mathcal{O}(\log T)$ 的遗憾度，超越了非退化假设。我们的结果显著改进了最新的遗憾度结果，并为顺序决策提供了新的见解。
2501.02760	 | CHAT: Beyond Contrastive Graph Transformer for Link Prediction in Heterogeneous Networks	 | Shengming Zhang,Le Zhang,Jingbo Zhou,Hui Xiong	 |  Chatt提出了一种名为对比异质图变换器（CHAT）的方法，用于异质网络中的链接预测，通过选择性保留节点并利用连接感知变换器及专为异质网络设计的损失函数，提高了预测准确性。实验结果表明，CHAT在多个药物-靶标相互作用数据集上表现出色，优于现有方法。	 | Link prediction in heterogeneous networks is crucial for understanding the intricacies of network structures and forecasting their future developments. Traditional methodologies often face significant obstacles, including over-smoothing-wherein the excessive aggregation of node features leads to the loss of critical structural details-and a dependency on human-defined meta-paths, which necessitate extensive domain knowledge and can be inherently restrictive. These limitations hinder the effective prediction and analysis of complex heterogeneous networks. In response to these challenges, we propose the Contrastive Heterogeneous grAph Transformer (CHAT). CHAT introduces a novel sampling-based graph transformer technique that selectively retains nodes of interest, thereby obviating the need for predefined meta-paths. The method employs an innovative connection-aware transformer to encode node sequences and their interconnections with high fidelity, guided by a dual-faceted loss function specifically designed for heterogeneous network link prediction. Additionally, CHAT incorporates an ensemble link predictor that synthesizes multiple samplings to achieve enhanced prediction accuracy. We conducted comprehensive evaluations of CHAT using three distinct drug-target interaction (DTI) datasets. The empirical results underscore CHAT's superior performance, outperforming both general-task approaches and models specialized in DTI prediction. These findings substantiate the efficacy of CHAT in addressing the complex problem of link prediction in heterogeneous networks.	 | 异质网络中的链接预测对于理解网络结构的复杂性并预测其未来的演变至关重要。传统的方法常常面临显著的障碍，包括过度平滑——过多地聚合节点特征会导致关键的结构细节丢失——以及对人工定义元路径的依赖，这需要大量的领域知识并且本质上具有局限性。这些限制阻碍了对复杂异质网络的有效预测和分析。为应对这些挑战，我们提出了对比异质图变换器（Contrastive Heterogeneous GrAph Transformer，简称CHAT）。CHAT引入了一种新颖的基于采样的图变换器技术，该技术有选择地保留感兴趣的节点，从而避免了预定义元路径的需要。该方法利用一种创新的连接感知变换器，以高保真度编码节点序列及其相互连接关系，由专为异质网络链接预测设计的双重损失函数引导。此外，CHAT整合了一个集成链接预测器，通过合成多次采样以实现更高的预测准确性。我们使用三个不同的药物-靶标相互作用（DTI）数据集对CHAT进行了全面评估。实验证据表明，CHAT的性能优越，优于一般任务方法和专门针对DTI预测的模型。这些发现证实了CHAT在解决异质网络中复杂链接预测问题的有效性。
2501.02739	 | TARDiS : Text Augmentation for Refining Diversity and Separability	 | Kyungmin Kim,SangHun Im,GiBaeg Kim,Heung-Seon Oh	 | 本文提出了一种基于LLM的新颖文本增强方法TARDiS，通过引入两种生成过程和类适应方法来解决传统两阶段TA方法的挑战，并在多种少量样本文本分类任务中表现出色，优于现有方法。	 | Text augmentation (TA) is a critical technique for text classification, especially in few-shot settings. This paper introduces a novel LLM-based TA method, TARDiS, to address challenges inherent in the generation and alignment stages of two-stage TA methods. For the generation stage, we propose two generation processes, SEG and CEG, incorporating multiple class-specific prompts to enhance diversity and separability. For the alignment stage, we introduce a class adaptation (CA) method to ensure that generated examples align with their target classes through verification and modification. Experimental results demonstrate TARDiS's effectiveness, outperforming state-of-the-art LLM-based TA methods in various few-shot text classification tasks. An in-depth analysis confirms the detailed behaviors at each stage.	 | 文本增强（TA）是文本分类中的关键技术，尤其是在少量样本设置中尤为重要。本文介绍了一种基于LLM的新颖TA方法TARDiS，以应对两阶段TA方法在生成和对齐阶段固有的挑战。在生成阶段，我们提出了两种生成过程，SEG和CEG，通过引入多个类特定提示来增强多样性和区分性。在对齐阶段，我们引入了一种类适应（CA）方法，通过验证和修改确保生成的示例与目标类对齐。实验结果表明TARDiS的有效性，在多种少量样本文本分类任务中优于最先进的基于LLM的TA方法。深入分析进一步确认了每个阶段的详细行为。
2501.02737	 | Holistic Semantic Representation for Navigational Trajectory Generation	 | Ji Cao,Tongya Zheng,Qinghong Guo,Yu Wang,Junshu Dai,Shunyu Liu,Jie Yang,Jie Song,Mingli Song	 | 该研究提出了一个整体语义表示（HOSER）框架，用于生成导航轨迹，通过道路网络编码器、多粒度轨迹编码器和目的地导向导航器综合处理不同尺度的语义信息，显著提升了轨迹生成的质量，并在多个真实数据集上优于现有方法。	 | Trajectory generation has garnered significant attention from researchers in the field of spatio-temporal analysis, as it can generate substantial synthesized human mobility trajectories that enhance user privacy and alleviate data scarcity. However, existing trajectory generation methods often focus on improving trajectory generation quality from a singular perspective, lacking a comprehensive semantic understanding across various scales. Consequently, we are inspired to develop a HOlistic SEmantic Representation (HOSER) framework for navigational trajectory generation. Given an origin-and-destination (OD) pair and the starting time point of a latent trajectory, we first propose a Road Network Encoder to expand the receptive field of road- and zone-level semantics. Second, we design a Multi-Granularity Trajectory Encoder to integrate the spatio-temporal semantics of the generated trajectory at both the point and trajectory levels. Finally, we employ a Destination-Oriented Navigator to seamlessly integrate destination-oriented guidance. Extensive experiments on three real-world datasets demonstrate that HOSER outperforms state-of-the-art baselines by a significant margin. Moreover, the model's performance in few-shot learning and zero-shot learning scenarios further verifies the effectiveness of our holistic semantic representation.	 | 轨迹生成在时空分析领域受到了研究人员的广泛关注，因为它能够生成大量合成的人类移动轨迹，增强用户隐私并缓解数据稀缺问题。然而，现有的轨迹生成方法往往是从单一角度来提高轨迹生成质量，缺乏对不同尺度的综合语义理解。因此，我们受此启发，开发了一个整体语义表示（HOSER）框架，用于导航轨迹生成。给定一个起终点（OD）对和潜在轨迹的起始时间点，我们首先提出一种道路网络编码器以扩展道路和区域层次语义的接受域。其次，我们设计了一种多粒度轨迹编码器，以在点级和轨迹级整合生成轨迹的时空语义。最后，我们采用目的地导向导航器以无缝集成目的地导向引导。在三个真实世界数据集上的广泛实验表明，HOSER 在性能上显著超越了当前最先进的基线模型。此外，模型在少量样本学习和零样本学习场景中的表现进一步验证了我们整体语义表示的有效性。
2501.02715	 | Improved Data Encoding for Emerging Computing Paradigms: From Stochastic to Hyperdimensional Computing	 | Mehran Shoushtari Moghadam,Sercan Aygun,M.Hassan Najafi	 | 该研究提出了一种利用范德科普序列2的幂次基（VDC-2^n）作为随机数生成来源的先进编码策略，显著提高了随机计算（SC）和超维度计算（HDC）系统的准确性和效率，并降低了硬件复杂度，为资源受限环境下的高效AI实现提供了稳健框架。	 | Data encoding is a fundamental step in emerging computing paradigms, particularly in stochastic computing (SC) and hyperdimensional computing (HDC), where it plays a crucial role in determining the overall system performance and hardware cost efficiency. This study presents an advanced encoding strategy that leverages a hardware-friendly class of low-discrepancy (LD) sequences, specifically powers-of-2 bases of Van der Corput (VDC) sequences (VDC-2^n), as sources for random number generation. Our approach significantly enhances the accuracy and efficiency of SC and HDC systems by addressing challenges associated with randomness. By employing LD sequences, we improve correlation properties and reduce hardware complexity. Experimental results demonstrate significant improvements in accuracy and energy savings for SC and HDC systems. Our solution provides a robust framework for integrating SC and HDC in resource-constrained environments, paving the way for efficient and scalable AI implementations.	 | 数据编码是新兴计算范式中的一个基本步骤，特别是在随机计算（SC）和超维度计算（HDC）中，它在决定整个系统性能和硬件成本效率方面起着至关重要的作用。本研究提出了一种先进的编码策略，该策略利用了一类硬件友好的低不均匀性（LD）序列，特别是范德科普（Vandecorput）序列的2的幂次基（VDC-2^n），作为随机数生成的来源。我们的方法通过解决与随机性相关的一些挑战，显著提高了SC和HDC系统的准确性和效率。利用LD序列，我们改善了相关性属性并降低了硬件复杂度。实验证明，我们的方法在SC和HDC系统中显著提高了准确性和节能效果。我们的解决方案为在资源受限环境中集成SC和HDC提供了一个稳健的框架，铺平了高效和可扩展的AI实现的道路。
2501.02702	 | QuIM-RAG: Advancing Retrieval-Augmented Generation with Inverted Question Matching for Enhanced QA Performance	 | Binita Saha,Utsha Saha,Muhammad Zubair Malik	 | 本文提出了一种新颖的架构QuIM-RAG，通过将目标语料库转换为领域特定数据集并利用检索增强生成技术，改进了大型语言模型在问答任务中的表现，特别是在信息稀释和幻觉问题上优于传统RAG模型。	 | This work presents a novel architecture for building Retrieval-Augmented Generation (RAG) systems to improve Question Answering (QA) tasks from a target corpus. Large Language Models (LLMs) have revolutionized the analyzing and generation of human-like text. These models rely on pre-trained data and lack real-time updates unless integrated with live data tools. RAG enhances LLMs by integrating online resources and databases to generate contextually appropriate responses. However, traditional RAG still encounters challenges like information dilution and hallucinations when handling vast amounts of data. Our approach addresses these challenges by converting corpora into a domain-specific dataset and RAG architecture is constructed to generate responses from the target document. We introduce QuIM-RAG (Question-to-question Inverted Index Matching), a novel approach for the retrieval mechanism in our system. This strategy generates potential questions from document chunks and matches these with user queries to identify the most relevant text chunks for generating accurate answers. We have implemented our RAG system on top of the open-source Meta-LLaMA3-8B-instruct model by Meta Inc. that is available on Hugging Face. We constructed a custom corpus of 500+ pages from a high-traffic website accessed thousands of times daily for answering complex questions, along with manually prepared ground truth QA for evaluation. We compared our approach with traditional RAG models using BERT-Score and RAGAS, state-of-the-art metrics for evaluating LLM applications. Our evaluation demonstrates that our approach outperforms traditional RAG architectures on both metrics.	 | 本文提出了一种新颖的架构，用于构建检索增强生成（RAG）系统，以改善目标语料库中的问答（QA）任务。大型语言模型（LLMs）已经彻底改变了人类文本的分析和生成。这些模型依赖于预训练数据，除非与实时数据工具集成，否则无法进行实时更新。RAG 通过将在线资源和数据库集成到系统中，增强了LLMs，从而生成上下文相关响应。然而，传统的RAG 在处理大量数据时仍面临信息稀释和幻觉等挑战。我们的方法通过将语料库转换为领域特定数据集来解决这些挑战，并构建RAG架构以从目标文档生成响应。我们引入了QuIM-RAG（从问题到问题的倒排索引匹配），这是一种新颖的检索机制方法。该策略从文档片段生成潜在问题，并将这些问题与用户查询匹配，以识别生成准确答案的相关文本片段。我们基于Meta Inc.开源的Meta-LLaMA3-8B-instruct模型实现了一个RAG系统，该模型可在Hugging Face上获取。我们从访问量极大的网站构建了一个自定义语料库，包含500多页内容，并每天被访问数千次，以回答复杂问题，同时还包括手动准备的黄金标准QA用于评估。我们使用BERT-Score和RAGAS等最先进的评估指标，将我们的方法与传统的RAG模型进行了比较。我们的评估表明，我们的方法在两个指标上都优于传统的RAG架构。
2501.02687	 | Improving Quantum Machine Learning via Heat-Bath Algorithmic Cooling	 | Nayeli A. Rodríguez-Briones,Daniel K. Park	 | 本文提出了一种基于量子热力学的采样效率提升方法，将量子监督学习视为热力学冷却过程，并开发了一种量子制冷机协议，在训练和预测过程中通过熵压缩和热化步骤降低量子位熵，优化样本效率，从而为嘈杂的中间规模量子设备提供高效的量子机器学习算法。	 | This work introduces an approach rooted in quantum thermodynamics to enhance sampling efficiency in quantum machine learning (QML). We propose conceptualizing quantum supervised learning as a thermodynamic cooling process. Building on this concept, we develop a quantum refrigerator protocol that enhances sample efficiency during training and prediction without the need for Grover iterations or quantum phase estimation. Inspired by heat-bath algorithmic cooling protocols, our method alternates entropy compression and thermalization steps to decrease the entropy of qubits, increasing polarization towards the dominant bias. This technique minimizes the computational overhead associated with estimating classification scores and gradients, presenting a practical and efficient solution for QML algorithms compatible with noisy intermediate-scale quantum devices.	 | 本工作提出了一种基于量子热力学的方法，以提高量子机器学习（QML）中的采样效率。我们提议将量子监督学习视为一个热力学冷却过程。基于这一概念，我们开发了一种量子制冷机协议，该协议在训练和预测过程中可以提高样本效率，而无需进行Grover迭代或量子相位估计。受热浴算法冷却协议的启发，我们的方法通过交替进行熵压缩和热化步骤来降低量子位的熵，从而增加其向主导偏见极化的程度。该技术可以最小化估计分类分数和梯度所需的计算开销，为与嘈杂的中间规模量子设备兼容的QML算法提供了实用且高效的解决方案。
2501.02680	 | From thermodynamics to protein design: Diffusion models for biomolecule generation towards autonomous protein engineering	 | Wen-ran Li,Xavier F. Cadet,David Medina-Ortiz,Mehdi D. Davari,Ramanathan Sowdhamini,Cedric Damour,Yu Li,Alain Miranville,Frederic Cadet	 | 本文综述了扩散模型在蛋白质设计中的应用，特别是在使用去噪扩散概率模型（DDPM）和基于分数生成模型生成具有期望性质的蛋白质方面，并探讨了如何在蛋白质生成中保持E(3)群对称性以提高氨基酸框架的物理稳定性。	 | Protein design with desirable properties has been a significant challenge for many decades. Generative artificial intelligence is a promising approach and has achieved great success in various protein generation tasks. Notably, diffusion models stand out for their robust mathematical foundations and impressive generative capabilities, offering unique advantages in certain applications such as protein design. In this review, we first give the definition and characteristics of diffusion models and then focus on two strategies: Denoising Diffusion Probabilistic Models and Score-based Generative Models, where DDPM is the discrete form of SGM. Furthermore, we discuss their applications in protein design, peptide generation, drug discovery, and protein-ligand interaction. Finally, we outline the future perspectives of diffusion models to advance autonomous protein design and engineering. The E(3) group consists of all rotations, reflections, and translations in three-dimensions. The equivariance on the E(3) group can keep the physical stability of the frame of each amino acid as much as possible, and we reflect on how to keep the diffusion model E(3) equivariant for protein generation.	 | 蛋白质设计具有期望性质一直是一个长期的挑战。生成式人工智能是一种有前景的方法，并在各种蛋白质生成任务中取得了巨大成功。特别地，扩散模型因其坚实的数学基础和卓越的生成能力，在某些应用如蛋白质设计方面显示出独特的优势。在本文综述中，我们首先介绍了扩散模型的定义和特性，然后重点讨论了两种策略：去噪扩散概率模型（DDPM）和基于分数生成模型，其中DDPM是SGM的离散形式。接着，我们探讨了这些模型在蛋白质设计、肽生成、药物发现以及蛋白质-配体相互作用中的应用。最后，我们展望了扩散模型的未来前景，以推动自主蛋白质设计和工程的发展。E(3)群包括三维空间中的所有旋转、反射和平移。在E(3)群上的对称性可以尽可能保持每个氨基酸框架的物理稳定性，我们探讨了如何使扩散模型在蛋白质生成中保持E(3)对称性。
2501.02672	 | Re-examining Granger Causality from Causal Bayesian Networks Perspective	 | S. A. Adedayo	 | 该研究通过从Reichenbach的共同因果原则和因果贝叶斯网络的角度重新分析Granger因果性（GC），回应了对其作为因果关系识别工具的批评，并在理论和图形上证明了这种重新表述在特定假设下为GC提供了恰当的因果解释，同时在模拟中取得了满意的结果。	 | Characterizing cause-effect relationships in complex systems could be critical to understanding these systems. For many, Granger causality (GC) remains a computational tool of choice to identify causal relations in time series data. Like other causal discovery tools, GC has limitations and has been criticized as a non-causal framework. Here, we addressed one of the recurring criticisms of GC by endowing it with proper causal interpretation. This was achieved by analyzing GC from Reichenbach's Common Cause Principles (RCCPs) and causal Bayesian networks (CBNs) lenses. We showed theoretically and graphically that this reformulation endowed GC with a proper causal interpretation under certain assumptions and achieved satisfactory results on simulation.	 | 在复杂系统中剖析因果关系可能是理解这些系统的关键。对于许多人而言，Granger因果性（GC）仍然是识别时间序列数据中因果关系的首选计算工具。与其他因果发现工具一样，GC也存在局限性，并且因其非因果框架而受到批评。在这里，我们通过赋予GC恰当的因果解释来回应对其的一些反复出现的批评。这通过从Reichenbach的共同因果原则（RCCPs）和因果贝叶斯网络（CBNs）的角度来分析GC来实现。我们从理论上和图形上证明了这种重新表述在某些假设下赋予了GC恰当的因果解释，并在模拟中取得了令人满意的结果。
2501.02669	 | Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?	 | Simon Park,Abhishek Panigrahi,Yun Cheng,Dingli Yu,Anirudh Goyal,Sanjeev Arora	 | 本文提出一个合成框架来评估视觉语言模型在算法视觉推理任务中的多步推理能力，包括表格读取、网格导航和视觉类比三个任务，以研究VLMs的模态不平衡和脆弱性问题，并通过消融实验揭示了自回归训练和图像到文本转化对于改善从简单到困难任务泛化的重要性。	 | While Vision Language Models (VLMs) are impressive in tasks such as visual question answering (VQA) and image captioning, their ability to apply multi-step reasoning to images has lagged, giving rise to perceptions of modality imbalance or brittleness. Towards systematic study of such issues, we introduce a synthetic framework for assessing the ability of VLMs to perform algorithmic visual reasoning (AVR), comprising three tasks: Table Readout, Grid Navigation, and Visual Analogy. Each has two levels of difficulty, SIMPLE and HARD, and even the SIMPLE versions are difficult for frontier VLMs. We seek strategies for training on the SIMPLE version of the tasks that improve performance on the corresponding HARD task, i.e., S2H generalization. This synthetic framework, where each task also has a text-only version, allows a quantification of the modality imbalance, and how it is impacted by training strategy. Ablations highlight the importance of explicit image-to-text conversion in promoting S2H generalization when using auto-regressive training. We also report results of mechanistic study of this phenomenon, including a measure of gradient alignment that seems to identify training strategies that promote better S2H generalization.	 | 尽管视觉语言模型（VLMs）在视觉问答（VQA）和图像描述等任务中表现 impressive，但在进行多步推理方面的能力仍然滞后，这引发了对模态不平衡或脆弱性的感知。为了系统地研究这些问题，我们引入了一个合成框架来评估 VLMs 执行算法视觉推理（AVR）的能力，该框架包含三个任务：表格读取、网格导航和视觉类比。每个任务都有简单（SIMPLE）和困难（HARD）两个难度级别，即使是最简单的版本对前沿的 VLMs 也很具挑战性。我们寻求在任务的简单版本上进行训练并改善对应困难版本任务性能的策略，即从简单到困难的泛化（S2H泛化）。该合成框架中的每个任务还有文字版本，这使得可以量化模态不平衡，并了解训练策略对其的影响。消融实验强调了在使用自回归训练时，显式的图像到文本转化对促进从简单到困难的泛化（S2H泛化）的重要性。我们还报告了对该现象的机制性研究结果，包括一个梯度对齐度量，该度量似乎可以识别出能促进更好从简单到困难泛化（S2H泛化）的训练策略。
2501.02613	 | LWFNet: Coherent Doppler Wind Lidar-Based Network for Wind Field Retrieval	 | Ran Tao,Chong Wang,Hao Chen,Mingjiao Jia,Xiang Shang,Luoyuan Qu,Guoliang Shentu,Yanyu Lu,Yanfeng Huo,Lei Bai,Xianghui Xue,Xiankang Dou	 | 研究提出了一种名为LWFNet的神经网络，通过结合Transformer和柯尔莫哥洛夫-阿诺尔德网络，有效解决了相干多普勒风测仪在高海拔地区检测风场的难题，显著扩展了风场检测的范围并提高了精度。	 | Accurate detection of wind fields within the troposphere is essential for atmospheric dynamics research and plays a crucial role in extreme weather forecasting. Coherent Doppler wind lidar (CDWL) is widely regarded as the most suitable technique for high spatial and temporal resolution wind field detection. However, since coherent detection relies heavily on the concentration of aerosol particles, which cause Mie scattering, the received backscattering lidar signal exhibits significantly low intensity at high altitudes. As a result, conventional methods, such as spectral centroid estimation, often fail to produce credible and accurate wind retrieval results in these regions. To address this issue, we propose LWFNet, the first Lidar-based Wind Field (WF) retrieval neural Network, built upon Transformer and the Kolmogorov-Arnold network. Our model is trained solely on targets derived from the traditional wind retrieval algorithm and utilizes radiosonde measurements as the ground truth for test results evaluation. Experimental results demonstrate that LWFNet not only extends the maximum wind field detection range but also produces more accurate results, exhibiting a level of precision that surpasses the labeled targets. This phenomenon, which we refer to as super-accuracy, is explored by investigating the potential underlying factors that contribute to this intriguing occurrence. In addition, we compare the performance of LWFNet with other state-of-the-art (SOTA) models, highlighting its superior effectiveness and capability in high-resolution wind retrieval. LWFNet demonstrates remarkable performance in lidar-based wind field retrieval, setting a benchmark for future research and advancing the development of deep learning models in this domain.	 | 准确检测对流层内的风场对于大气动力学研究至关重要，并且在极端天气预报中起着关键作用。相干多普勒风测仪（CDWL）被认为是高空间和时间分辨率风场检测的最佳技术。然而，由于相干检测高度依赖于气溶胶粒子的浓度，这些粒子引起的米氏散射会导致高海拔接收到的后向散射激光信号强度显著降低。因此，传统的光谱中心估计方法往往无法在这些区域产生可靠的风场提取结果。为了解决这一问题，我们提出了一种名为LWFNet的激光雷达风场（WF）检索神经网络，该网络基于Transformer和柯尔莫哥洛夫-阿诺尔德网络构建。我们的模型仅基于传统的风场检索算法的目标进行训练，并利用探空仪测量作为测试结果的真实值进行评估。实验结果表明，LWFNet不仅扩展了风场检测的最大范围，而且产生了更准确的结果，其精度甚至超过了标注的目标，这一现象我们称之为超精度。我们通过对潜在因素的深入研究，探索了这种引人注目的现象背后的原因。此外，我们将LWFNet与当前最先进的（SOTA）模型进行了性能比较，突显了其在高分辨率风场检索中的优越效果和能力。LWFNet在基于激光雷达的风场检索中表现出色，为未来的研究建立了标准，并推动了该领域深度学习模型的发展。
2501.02599	 | Empowering Bengali Education with AI: Solving Bengali Math Word Problems through Transformer Models	 | Jalisha Jashim Era,Bidyarthi Paul,Tahmid Sattar Aothoi,Mirazur Rahman Zim,Faisal Muhammad Shah	 | 本文通过开发基于变换器模型的方法，解决了孟加拉语数学文字问题的转换任务，并通过“PatiGonit”数据集对mT5模型进行了优化，取得了97.30%的高准确性，显著推动了孟加拉语自然语言处理领域的发展。	 | Mathematical word problems (MWPs) involve the task of converting textual descriptions into mathematical equations. This poses a significant challenge in natural language processing, particularly for low-resource languages such as Bengali. This paper addresses this challenge by developing an innovative approach to solving Bengali MWPs using transformer-based models, including Basic Transformer, mT5, BanglaT5, and mBART50. To support this effort, the "PatiGonit" dataset was introduced, containing 10,000 Bengali math problems, and these models were fine-tuned to translate the word problems into equations accurately. The evaluation revealed that the mT5 model achieved the highest accuracy of 97.30%, demonstrating the effectiveness of transformer models in this domain. This research marks a significant step forward in Bengali natural language processing, offering valuable methodologies and resources for educational AI tools. By improving math education, it also supports the development of advanced problem-solving skills for Bengali-speaking students.	 | 数学文字问题（MWPs）涉及将文本描述转换为数学方程的任务。这在自然语言处理中构成了重大挑战，特别是对于低资源语言如孟加拉语而言。本文通过开发一种利用基于变换器模型的方法来解决孟加拉语MWPs，来应对这一挑战，包括基本变换器、mT5、BanglaT5和mBART50。为了支持这一努力，引入了“PatiGonit”数据集，该数据集包含10,000个孟加拉数学问题，并对这些模型进行了微调，以准确地将文字问题转换为方程。评估结果显示，mT5模型的准确性达到最高，为97.30%，展示了变换器模型在该领域中的有效性。这项研究标志着孟加拉语自然语言处理领域的重要进步，提供了改进教育人工智能工具的方法和资源。通过提高数学教育，还支持了孟加拉语学生的高级问题解决技能的发展。
2501.02598	 | GIT-CXR: End-to-End Transformer for Chest X-Ray Report Generation	 | Iustin Sîrbu,Iulia-Renata Sîrbu,Jasmina Bogojeska,Traian Rebedea	 | 该研究设计并评估了一种基于Transformer的端到端方法，用于自动生成胸部X射线的放射影像报告，显著提高了报告的准确性和临床准确性，同时减轻了医疗工作者的负担。实验结果在多个评估指标上达到了或超过了当前最先进的技术水平。	 | Medical imaging is crucial for diagnosing, monitoring, and treating medical conditions. The medical reports of radiology images are the primary medium through which medical professionals attest their findings, but their writing is time consuming and requires specialized clinical expertise. The automated generation of radiography reports has thus the potential to improve and standardize patient care and significantly reduce clinicians workload. Through our work, we have designed and evaluated an end-to-end transformer-based method to generate accurate and factually complete radiology reports for X-ray images. Additionally, we are the first to introduce curriculum learning for end-to-end transformers in medical imaging and demonstrate its impact in obtaining improved performance. The experiments have been conducted using the MIMIC-CXR-JPG database, the largest available chest X-ray dataset. The results obtained are comparable with the current state-of-the-art on the natural language generation (NLG) metrics BLEU and ROUGE-L, while setting new state-of-the-art results on F1 examples-averaged, F1-macro and F1-micro metrics for clinical accuracy and on the METEOR metric widely used for NLG.	 | 医学成像是诊断、监测和治疗医疗状况的重要工具。放射影像的医学报告是医疗专业人员对其发现进行验证的主要媒介，但撰写这些报告耗时且需要专门的临床专业知识。因此，自动生成放射影像报告具有提高和标准化患者护理以及显著减轻临床工作者工作负担的潜力。通过我们的工作，我们设计并评估了一种基于Transformer的端到端方法，以生成X射线图像的准确且事实完整的放射影像报告。此外，我们首次引入了在医学成像中端到端Transformer的课程学习，并展示了其在获得更好性能方面的影响力。实验在MIMIC-CXR-JPG数据库上进行，这是最大的可用胸部X射线数据集。所得结果在自然语言生成（NLG）指标BLEU和ROUGE-L上与当前最先进的技术相当，同时在临床准确性方面分别在F1平均值、F1宏平均值和F1微观平均值指标上设立了新的最先进的结果，并在广泛用于NLG的METEOR指标上取得了优异的表现。
2501.02593	 | Evolving Skeletons: Motion Dynamics in Action Recognition	 | Jushang Qiu,Lei Wang	 | 本文在NTU-60和NTU-120数据集上全面评估了ST-GCN和Hyperformer模型对传统骨架序列及Taylor变换后骨架的表现，揭示了Taylor变换在增强动作识别中运动动态表示方面的潜力及其面临的挑战。	 | Skeleton-based action recognition has gained significant attention for its ability to efficiently represent spatiotemporal information in a lightweight format. Most existing approaches use graph-based models to process skeleton sequences, where each pose is represented as a skeletal graph structured around human physical connectivity. Among these, the Spatiotemporal Graph Convolutional Network (ST-GCN) has become a widely used framework. Alternatively, hypergraph-based models, such as the Hyperformer, capture higher-order correlations, offering a more expressive representation of complex joint interactions. A recent advancement, termed Taylor Videos, introduces motion-enhanced skeleton sequences by embedding motion concepts, providing a fresh perspective on interpreting human actions in skeleton-based action recognition. In this paper, we conduct a comprehensive evaluation of both traditional skeleton sequences and Taylor-transformed skeletons using ST-GCN and Hyperformer models on the NTU-60 and NTU-120 datasets. We compare skeletal graph and hypergraph representations, analyzing static poses against motion-injected poses. Our findings highlight the strengths and limitations of Taylor-transformed skeletons, demonstrating their potential to enhance motion dynamics while exposing current challenges in fully using their benefits. This study underscores the need for innovative skeletal modelling techniques to effectively handle motion-rich data and advance the field of action recognition.	 | 基于骨架的动作识别因其能够以轻量级格式高效地表示时空信息而引起了广泛关注。现有的大多数方法使用基于图的模型来处理骨架序列，其中每个姿态以人体物理连接为基础构建一个骨骼图。在这些方法中，时空图卷积网络（ST-GCN）已成为广泛使用的框架之一。另一方面，基于超图的模型，如Hyperformer，能够捕获更高阶的关联，提供对复杂关节交互的更具表现力的表示。最近的一项进步称为Taylor Videos，通过嵌入运动概念引入了增强的骨架序列，为基于骨架的动作识别提供了新的视角。在这篇文章中，我们使用ST-GCN和Hyperformer模型在NTU-60和NTU-120数据集上全面评估了传统的骨架序列和Taylor变换后的骨架。我们比较了骨骼图和超图表示，分析了静态姿态与运动注入的姿态。我们的研究结果突显了Taylor变换后骨架的优缺点，展示了它们增强运动动态的潜力，同时也揭示了利用其全部优点所面临的当前挑战。这项研究强调了为了有效地处理丰富的运动数据并推动动作识别领域的发展，需要创新的骨骼建模技术的重要性。
2501.02584	 | Efficient Architectures for High Resolution Vision-Language Models	 | Miguel Carvalho,Bruno Martins	 | Pheye是一种新颖的视觉-语言模型架构，能够在处理高分辨率图像时减少训练参数量，同时保持甚至提升在精细图像理解和场景文本处理任务中的性能。	 | Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This work introduces Pheye, a novel architecture that efficiently processes high-resolution images while training fewer parameters than similarly sized VLMs. Notably, Pheye achieves a high efficiency while maintaining strong performance, particularly in tasks that demand fine-grained image understanding and/or the handling of scene-text.	 | 视觉-语言模型（VLMs）最近经历了一系列显著的进步。然而，准确识别高分辨率图像中的细微细节仍然面临挑战，这限制了其在多种任务中的性能。本文引入了Pheye，这是一种新颖的架构，能够在处理高分辨率图像的同时，训练的参数量比类似规模的VLMs更少。值得注意的是，Pheye在保持强大性能的同时，特别是在需要精细图像理解和/或处理场景文本的任务中，表现出高效率。
2501.02564	 | Balanced Multi-view Clustering	 | Zhenglai Li,Jun Wang,Chang Tang,Xinzhong Zhu,Wei Zhang,Xinwang Liu	 | 该研究分析了多视图聚类（MvC）中联合训练范式中的信息不平衡问题，并提出了一种新的平衡多视图聚类（BMvC）方法，通过引入视图特定对比正则化（VCR）来优化各视图的特征提取过程，从而更好地利用多视图信息进行聚类任务。实验结果表明，BMvC在多个基准数据集上优于现有方法。	 | Multi-view clustering (MvC) aims to integrate information from different views to enhance the capability of the model in capturing the underlying data structures. The widely used joint training paradigm in MvC is potentially not fully leverage the multi-view information, since the imbalanced and under-optimized view-specific features caused by the uniform learning objective for all views. For instance, particular views with more discriminative information could dominate the learning process in the joint training paradigm, leading to other views being under-optimized. To alleviate this issue, we first analyze the imbalanced phenomenon in the joint-training paradigm of multi-view clustering from the perspective of gradient descent for each view-specific feature extractor. Then, we propose a novel balanced multi-view clustering (BMvC) method, which introduces a view-specific contrastive regularization (VCR) to modulate the optimization of each view. Concretely, VCR preserves the sample similarities captured from the joint features and view-specific ones into the clustering distributions corresponding to view-specific features to enhance the learning process of view-specific feature extractors. Additionally, a theoretical analysis is provided to illustrate that VCR adaptively modulates the magnitudes of gradients for updating the parameters of view-specific feature extractors to achieve a balanced multi-view learning procedure. In such a manner, BMvC achieves a better trade-off between the exploitation of view-specific patterns and the exploration of view-invariance patterns to fully learn the multi-view information for the clustering task. Finally, a set of experiments are conducted to verify the superiority of the proposed method compared with state-of-the-art approaches both on eight benchmark MvC datasets and two spatially resolved transcriptomics datasets.	 | 多视图聚类（MvC）旨在通过整合不同视图的信息来增强模型捕捉底层数据结构的能力。广泛采用的联合训练范式在MvC中可能存在未能充分利用多视图信息的问题，因为统一的学习目标可能会导致各视图特定特征的不平衡和优化不足。例如，具有更多判别信息的特定视图可能在联合训练范式中主导学习过程，导致其他视图优化不足。为了解决这个问题，我们首先从每个视图特定特征提取器的梯度下降角度分析了多视图聚类联合训练范式中的不平衡现象。然后，我们提出了一种新颖的平衡多视图聚类（BMvC）方法，引入了视图特定对比正则化（VCR）来调节每个视图的优化过程。具体来说，VCR 保留了从联合特征和视图特定特征中捕获的样本相似性，并将其保留在相应的视图特定特征聚类分布中，以增强视图特定特征提取器的学习过程。此外，我们提供了理论分析来说明VCR如何根据视图特定特征提取器参数更新的梯度大小进行自适应调节，以实现平衡的多视图学习过程。通过这种方式，BMvC 在充分利用视图特定模式和探索视图不变模式之间取得了更好的平衡，从而充分利用多视图信息进行聚类任务。最后，我们在八个基准MvC数据集和两个空间解析转录组学数据集上进行了一系列实验，验证了与最先进的方法相比，所提出的方法在性能上的优越性。
2501.02547	 | Transformers Simulate MLE for Sequence Generation in Bayesian Networks	 | Yuan Cao,Yihan He,Dennis Wu,Hong-Yu Chen,Jianqing Fan,Han Liu	 | 本文探讨了Transformer在基于上下文最大似然估计的贝叶斯网络中生成序列的能力，并证明了存在一个简单的Transformer模型可以估计条件概率并自回归生成新样本；实验表明这种Transformer不仅在理论上可行，还能通过训练有效实现。	 | Transformers have achieved significant success in various fields, notably excelling in tasks involving sequential data like natural language processing. Despite these achievements, the theoretical understanding of transformers' capabilities remains limited. In this paper, we investigate the theoretical capabilities of transformers to autoregressively generate sequences in Bayesian networks based on in-context maximum likelihood estimation (MLE). Specifically, we consider a setting where a context is formed by a set of independent sequences generated according to a Bayesian network. We demonstrate that there exists a simple transformer model that can (i) estimate the conditional probabilities of the Bayesian network according to the context, and (ii) autoregressively generate a new sample according to the Bayesian network with estimated conditional probabilities. We further demonstrate in extensive experiments that such a transformer does not only exist in theory, but can also be effectively obtained through training. Our analysis highlights the potential of transformers to learn complex probabilistic models and contributes to a better understanding of large language models as a powerful class of sequence generators.	 |  transformer在各个领域取得了显著的成功，尤其是在涉及序列数据的任务，如自然语言处理方面表现出色。尽管取得了这些成就，但对transformer能力的理论理解仍有限。在本文中，我们探讨了transformer在基于上下文最大似然估计（MLE）的贝叶斯网络中自回归生成序列的能力。具体来说，我们考虑了一个上下文由根据贝叶斯网络生成的一组独立序列形成的设置。我们证明存在一个简单的transformer模型，该模型可以（i）根据上下文估计贝叶斯网络的条件概率，以及（ii）根据估计的条件概率自回归生成一个新的样本。我们进一步通过广泛的实验表明，这种transformer不仅在理论上存在，而且可以通过训练有效地获得。我们的分析突显了transformer学习复杂概率模型的潜力，并有助于更好地理解大规模语言模型作为强大序列生成器类的能力。
2501.02526	 | Unified Guidance for Geometry-Conditioned Molecular Generation	 | Sirine Ayadi,Leon Hetzel,Johanna Sommer,Fabian Theis,Stephan Günnemann	 | UniGuide是一种控制无条件扩散模型几何引导的框架，可在推理过程中灵活调整条件，无需额外训练，适用于多种药物设计应用，并在性能上与专门模型相当甚至更优。	 | Effectively designing molecular geometries is essential to advancing pharmaceutical innovations, a domain, which has experienced great attention through the success of generative models and, in particular, diffusion models. However, current molecular diffusion models are tailored towards a specific downstream task and lack adaptability. We introduce UniGuide, a framework for controlled geometric guidance of unconditional diffusion models that allows flexible conditioning during inference without the requirement of extra training or networks. We show how applications such as structure-based, fragment-based, and ligand-based drug design are formulated in the UniGuide framework and demonstrate on-par or superior performance compared to specialised models. Offering a more versatile approach, UniGuide has the potential to streamline the development of molecular generative models, allowing them to be readily used in diverse application scenarios.	 | 有效地设计分子几何结构是推动制药创新的关键，这一领域因生成模型的成功，特别是扩散模型的成功而受到了极大的关注。然而，当前的分子扩散模型大多针对特定下游任务，缺乏灵活性。我们提出了UniGuide框架，这是一种控制无条件扩散模型几何引导的框架，允许在推理过程中灵活调整条件，而无需额外的训练或网络。我们展示了结构基于、片段基于和配体基于的药物设计等应用如何在UniGuide框架中加以表述，并证明了UniGuide在性能上可以与专门模型相匹敌，甚至更优。提供了一种更具灵活性的方法，UniGuide有望简化分子生成模型的开发流程，使它们能够应用于多种不同的应用场景中。
2501.02497	 | Test-time Computing: from System-1 Thinking to System-2 Thinking	 | Yixin Ji,Juntao Li,Hai Ye,Kaixin Wu,Jia Xu,Linjian Mo,Min Zhang	 | 该研究展示了o1模型在复杂推理中通过测试时计算缩放能显著提升其推理能力，类似于System-2思维。综述追溯了测试时计算的概念从System-1模型到System-2模型的发展，并指出了未来研究的方向。	 | The remarkable performance of the o1 model in complex reasoning demonstrates that test-time computing scaling can further unlock the model's potential, enabling powerful System-2 thinking. However, there is still a lack of comprehensive surveys for test-time computing scaling. We trace the concept of test-time computing back to System-1 models. In System-1 models, test-time computing addresses distribution shifts and improves robustness and generalization through parameter updating, input modification, representation editing, and output calibration. In System-2 models, it enhances the model's reasoning ability to solve complex problems through repeated sampling, self-correction, and tree search. We organize this survey according to the trend of System-1 to System-2 thinking, highlighting the key role of test-time computing in the transition from System-1 models to weak System-2 models, and then to strong System-2 models. We also point out a few possible future directions.	 | o1模型在复杂推理中的出色表现表明，测试时计算缩放可以进一步激发模型的潜力，使其具备强大的System-2思考能力。然而，针对测试时计算缩放的全面综述仍然不足。我们追溯测试时计算的概念，将其回溯至System-1模型。在System-1模型中，测试时计算通过参数更新、输入修改、表示编辑和输出校准来应对分布偏移，提高模型的鲁棒性和泛化能力。在System-2模型中，测试时计算通过重复采样、自我纠正和树搜索来增强模型的推理能力，从而解决复杂问题。我们按照从System-1思维到System-2思维的发展趋势组织这次综述，强调测试时计算在从System-1模型过渡到弱System-2模型，再到强System-2模型中的关键作用。我们还指出了几个可能的未来研究方向。
2501.02493	 | Predicting Vulnerability to Malware Using Machine Learning Models: A Study on Microsoft Windows Machines	 | Marzieh Esnaashari,Nima Moradi	 | 本研究利用Microsoft Windows Defender的数据和机器学习技术，开发了一种先进的模型以准确预测恶意软件漏洞，旨在提升网络安全韧性并解决不断演变的恶意软件威胁。通过复杂ML算法和大规模实际数据集，该研究不仅推动了恶意软件检测技术的发展，还强调了特征分析的重要性，并提供了适应企业环境的主动防护方案。	 | In an era of escalating cyber threats, malware poses significant risks to individuals and organizations, potentially leading to data breaches, system failures, and substantial financial losses. This study addresses the urgent need for effective malware detection strategies by leveraging Machine Learning (ML) techniques on extensive datasets collected from Microsoft Windows Defender. Our research aims to develop an advanced ML model that accurately predicts malware vulnerabilities based on the specific conditions of individual machines. Moving beyond traditional signature-based detection methods, we incorporate historical data and innovative feature engineering to enhance detection capabilities. This study makes several contributions: first, it advances existing malware detection techniques by employing sophisticated ML algorithms; second, it utilizes a large-scale, real-world dataset to ensure the applicability of findings; third, it highlights the importance of feature analysis in identifying key indicators of malware infections; and fourth, it proposes models that can be adapted for enterprise environments, offering a proactive approach to safeguarding extensive networks against emerging threats. We aim to improve cybersecurity resilience, providing critical insights for practitioners in the field and addressing the evolving challenges posed by malware in a digital landscape. Finally, discussions on results, insights, and conclusions are presented.	 | 在日益严峻的网络威胁时代，恶意软件（malware）对个人和组织构成了重大风险，可能导致数据泄露、系统故障和巨额经济损失。本研究旨在通过利用Microsoft Windows Defender收集的大量数据，应对有效恶意软件检测策略的迫切需求，并借助机器学习（Machine Learning, ML）技术来实现这一目标。我们的研究旨在开发一种先进的ML模型，能够根据个体机器的特定条件准确预测恶意软件漏洞。我们超越了传统的基于签名的检测方法，结合历史数据和创新的特征工程来增强检测能力。本研究做出了几项重要贡献：首先，通过运用复杂的ML算法，推动了现有的恶意软件检测技术的发展；其次，使用大规模的实际数据集确保研究结果的适用性；第三，强调了特征分析在识别恶意软件感染关键指标中的重要性；第四，提出了可以适应企业环境的模型，提供了针对新兴威胁的主动防护方案。我们旨在提升网络安全韧性，为网络安全领域的从业者提供关键见解，并解决数字环境中不断演变的恶意软件挑战。最后，我们将讨论研究结果、见解和结论。
2501.02476	 | Noise-Tolerant Hybrid Prototypical Learning with Noisy Web Data	 | Chao Liang,Linchao Zhu,Zongxin Yang,Wei Chen,Yi Yang	 | 本文提出了一种名为SimNoiPro的方法，旨在从少量干净标注图像和大量噪声标注网络图像中学习无偏的类原型，通过最大化相似性损失来生成噪声容忍混合原型，从而改善分类器性能。该方法通过显式处理噪声图像的多样性，有效减少了优化偏差，优于现有技术。	 | We focus on the challenging problem of learning an unbiased classifier from a large number of potentially relevant but noisily labeled web images given only a few clean labeled images. This problem is particularly practical because it reduces the expensive annotation costs by utilizing freely accessible web images with noisy labels. Typically, prototypes are representative images or features used to classify or identify other images. However, in the few clean and many noisy scenarios, the class prototype can be severely biased due to the presence of irrelevant noisy images. The resulting prototypes are less compact and discriminative, as previous methods do not take into account the diverse range of images in the noisy web image collections. On the other hand, the relation modeling between noisy and clean images is not learned for the class prototype generation in an end-to-end manner, which results in a suboptimal class prototype. In this article, we introduce a similarity maximization loss named SimNoiPro. Our SimNoiPro first generates noise-tolerant hybrid prototypes composed of clean and noise-tolerant prototypes and then pulls them closer to each other. Our approach considers the diversity of noisy images by explicit division and overcomes the optimization discrepancy issue. This enables better relation modeling between clean and noisy images and helps extract judicious information from the noisy image set. The evaluation results on two extended few-shot classification benchmarks confirm that our SimNoiPro outperforms prior methods in measuring image relations and cleaning noisy data.	 | 我们专注于从大量可能相关但带有噪声标签的网络图像中学习一个无偏的分类器这一具有挑战性的问题，仅使用少量干净的标注图像。这个问题特别实用，因为它通过利用自由获取的带有噪声标签的网络图像，减少了昂贵的标注成本。通常，原型是用于分类或识别其他图像的代表性图像或特征。然而，在少量干净和大量噪声的情况下，类原型可能会因无关噪声图像的存在而严重偏斜。由此产生的原型不紧凑且区分性差，因为先前的方法没有考虑到网络图像集合中图像的多样范围。另一方面，噪声图像和干净图像之间的关系在端到端的方式下没有学习到，这导致了一个次优的类原型。在这篇文章中，我们引入了一种相似性最大化损失，称为SimNoiPro。我们的SimNoiPro首先生成由干净原型和噪声容忍原型组成的噪声容忍混合原型，然后将它们拉近彼此。我们的方法通过显式的划分考虑了噪声图像的多样性，克服了优化偏差的问题。这使得更好地建模了干净图像和噪声图像之间的关系，并有助于从噪声图像集中提取有价值的信息。在两个扩展的少量样本分类基准上的评估结果证实，我们的SimNoiPro在测量图像关系和清理噪声数据方面优于先前的方法。
2501.02473	 | IRIS: A Bayesian Approach for Image Reconstruction in Radio Interferometry with expressive Score-Based priors	 | Noé Dia,M. J. Yantovski-Barth,Alexandre Adam,Micah Bowles,Laurence Perreault-Levasseur,Yashar Hezaveh,Anna Scaife	 | 本文提出了一种名为IRIS的方法，通过结合基于光学星系图像训练的评分模型和uv空间中的高斯似然函数，从带噪声的ALMA数据中推断出原行星盘图像，这种方法在模拟中展示了相对于传统射电干涉测量成像算法的优势。	 | Inferring sky surface brightness distributions from noisy interferometric data in a principled statistical framework has been a key challenge in radio astronomy. In this work, we introduce Imaging for Radio Interferometry with Score-based models (IRIS). We use score-based models trained on optical images of galaxies as an expressive prior in combination with a Gaussian likelihood in the uv-space to infer images of protoplanetary disks from visibility data of the DSHARP survey conducted by ALMA. We demonstrate the advantages of this framework compared with traditional radio interferometry imaging algorithms, showing that it produces plausible posterior samples despite the use of a misspecified galaxy prior. Through coverage testing on simulations, we empirically evaluate the accuracy of this approach to generate calibrated posterior samples.	 | 从带有噪声的干涉测量数据中在原则性的统计框架下推断天空亮度分布一直是射电天文领域的关键挑战。在本文中，我们引入了一种名为射电干涉测量基于评分模型的成像方法（IRIS）。我们使用了基于光学星系图像训练的评分模型作为表达性的先验，并结合uv空间中的高斯似然函数，从ALMA进行的DSHARP调查的视在数据中推断原行星盘图像。我们展示了此框架相较于传统射电干涉测量成像算法的优势，证明尽管使用了错定标的星系先验，它仍能产生合理的后验样本。通过模拟中的覆盖测试，我们实证评估了此方法生成校准后验样本的准确性。
2501.02458	 | Neural Reflectance Fields for Radio-Frequency Ray Tracing	 | Haifeng Jia,Xinyi Chen,Yichen Wei,Yifei Sun,Yibo Pi	 | 本文提出了一种通过学习射频（RF）信号路径损耗来高效估计材料反射率的方法，将神经反射场从光学领域推广到RF领域，并通过一个可微的射线追踪框架优化神经反射场以匹配信号强度测量值，从而提高接收器功率预测的准确性。	 | Ray tracing is widely employed to model the propagation of radio-frequency (RF) signal in complex environment. The modelling performance greatly depends on how accurately the target scene can be depicted, including the scene geometry and surface material properties. The advances in computer vision and LiDAR make scene geometry estimation increasingly accurate, but there still lacks scalable and efficient approaches to estimate the material reflectivity in real-world environment. In this work, we tackle this problem by learning the material reflectivity efficiently from the path loss of the RF signal from the transmitters to receivers. Specifically, we want the learned material reflection coefficients to minimize the gap between the predicted and measured powers of the receivers. We achieve this by translating the neural reflectance field from optics to RF domain by modelling both the amplitude and phase of RF signals to account for the multipath effects. We further propose a differentiable RF ray tracing framework that optimizes the neural reflectance field to match the signal strength measurements. We simulate a complex real-world environment for experiments and our simulation results show that the neural reflectance field can successfully learn the reflection coefficients for all incident angles. As a result, our approach achieves better accuracy in predicting the powers of receivers with significantly less training data compared to existing approaches.	 | 射线追踪广泛用于模拟复杂环境中无线电频率（RF）信号的传播。模型性能很大程度上取决于目标场景的准确性，包括场景的几何形状和表面材料属性。计算视觉和LiDAR的进步使得场景几何形状估计越来越准确，但在现实世界环境中，仍然缺乏可扩展且高效的材料反射率估计方法。在这项工作中，我们通过学习从发射器到接收器的RF信号路径损耗来高效地估计材料反射率来解决这个问题。具体来说，我们希望学习到的材料反射系数能够最小化接收器预测功率与测量功率之间的差距。我们通过建模RF信号的幅度和相位，以考虑多路径效应，将神经反射场从光学领域转换到RF领域。我们进一步提出了一个可微的RF射线追踪框架，通过优化神经反射场以匹配信号强度测量值。我们为实验模拟了一个复杂的现实环境，并且仿真结果表明，神经反射场可以成功地学习到所有入射角度的反射系数。因此，与现有方法相比，我们的方法在显著减少训练数据的情况下，预测接收器功率的准确性更高。
2501.02447	 | MedSegDiffNCA: Diffusion Models With Neural Cellular Automata for Skin Lesion Segmentation	 | Avni Mittal,John Kalkhof,Anirban Mukhopadhyay,Arnav Bhavsar	 | 本文提出三种基于NCA的方法改进基于扩散的医学图像分割，包括Multi-MedSegDiffNCA、CBAM-MedSegDiffNCA和MultiCBAM-MedSegDiffNCA，并在病变分割任务中展示了MultiCBAM-MedSegDiffNCA在保持高分割精度的同时，参数量仅为Unet模型的1/60到1/110，适用于低资源环境。	 | Denoising Diffusion Models (DDMs) are widely used for high-quality image generation and medical image segmentation but often rely on Unet-based architectures, leading to high computational overhead, especially with high-resolution images. This work proposes three NCA-based improvements for diffusion-based medical image segmentation. First, Multi-MedSegDiffNCA uses a multilevel NCA framework to refine rough noise estimates generated by lower level NCA models. Second, CBAM-MedSegDiffNCA incorporates channel and spatial attention for improved segmentation. Third, MultiCBAM-MedSegDiffNCA combines these methods with a new RGB channel loss for semantic guidance. Evaluations on Lesion segmentation show that MultiCBAM-MedSegDiffNCA matches Unet-based model performance with dice score of 87.84% while using 60-110 times fewer parameters, offering a more efficient solution for low resource medical settings.	 | 去噪扩散模型（DDMs）广泛用于高质量图像生成和医学图像分割，但通常依赖于基于Unet的架构，导致高计算开销，尤其是在高分辨率图像中。本文提出三种基于NCA的方法，以改进基于扩散的医学图像分割。首先，Multi-MedSegDiffNCA使用多层次的NCA框架来细化由低层次NCA模型生成的粗糙噪声估计。其次，CBAM-MedSegDiffNCA结合通道和空间注意力以提高分割效果。第三，MultiCBAM-MedSegDiffNCA将这些方法与新的RGB通道损失结合，为语义指导提供支持。在病变分割上的评估显示，MultiCBAM-MedSegDiffNCA的Dice分数为87.84%时所使用的参数量仅为基于Unet模型的1/60到1/110，为低资源医学环境提供了更高效的解决方案。
2501.02441	 | A Statistical Hypothesis Testing Framework for Data Misappropriation Detection in Large Language Models	 | Yinpeng Cai,Lexin Li,Linjun Zhang	 | 本文关注大规模语言模型（LLMs）中数据不当利用的问题，提出了一种通过在受版权保护的训练数据中嵌入水印并将其检测形式化为假设检验问题的方法，从而有效识别LLMs是否包含由其他LLM生成的数据。	 | Large Language Models (LLMs) are rapidly gaining enormous popularity in recent years. However, the training of LLMs has raised significant privacy and legal concerns, particularly regarding the inclusion of copyrighted materials in their training data without proper attribution or licensing, which falls under the broader issue of data misappropriation. In this article, we focus on a specific problem of data misappropriation detection, namely, to determine whether a given LLM has incorporated data generated by another LLM. To address this issue, we propose embedding watermarks into the copyrighted training data and formulating the detection of data misappropriation as a hypothesis testing problem. We develop a general statistical testing framework, construct a pivotal statistic, determine the optimal rejection threshold, and explicitly control the type I and type II errors. Furthermore, we establish the asymptotic optimality properties of the proposed tests, and demonstrate its empirical effectiveness through intensive numerical experiments.	 | 近年来，大规模语言模型（LLMs）的 popularity 快速增长。然而，LLMs 的训练引发了重大隐私和法律问题，特别是在未经适当归属或许可的情况下将受版权保护的内容纳入训练数据，这属于更广泛的不当数据利用问题。在本文中，我们关注数据不当利用检测的特定问题，即确定给定的 LLM 是否已包含由另一个 LLM 生成的数据。为解决这一问题，我们提出将水印嵌入到受版权保护的训练数据中，并将数据不当利用的检测问题形式化为假设检验问题。我们发展了一般统计检验框架，构建了一个关键统计量，确定了最优的拒绝阈值，并显式地控制了一类错误和二类错误。此外，我们建立了所提检验的渐近最优性质，并通过密集的数值实验展示了其实用有效性。
2501.02414	 | Journey into Automation: Image-Derived Pavement Texture Extraction and Evaluation	 | Bingjie Lu(1),Han-Cheng Dan(1),Yichen Zhang(1),Zhetao Huang(1) ((1) Central South University)	 | 该研究开发了一种基于路面图像的自动化系统，用于提取纹理特征并评估均质纹理深度（MTD），通过三维图像处理技术改进和多变量预测模型建立，特别是梯度提升树模型表现出色（R2 = 0.9858），相比其他技术具有更高的预测精度和经济高效性。	 | Mean texture depth (MTD) is pivotal in assessing the skid resistance of asphalt pavements and ensuring road safety. This study focuses on developing an automated system for extracting texture features and evaluating MTD based on pavement images. The contributions of this work are threefold: firstly, it proposes an economical method to acquire three-dimensional (3D) pavement texture data; secondly, it enhances 3D image processing techniques and formulates features that represent various aspects of texture; thirdly, it establishes multivariate prediction models that link these features with MTD values. Validation results demonstrate that the Gradient Boosting Tree (GBT) model achieves remarkable prediction stability and accuracy (R2 = 0.9858), and field tests indicate the superiority of the proposed method over other techniques, with relative errors below 10%. This method offers a comprehensive end-to-end solution for pavement quality evaluation, from images input to MTD predictions output.	 | 均质纹理深度（MTD）在评估沥青路面的磨耗系数和保障道路安全方面起着关键作用。本研究旨在开发一种基于路面图像的自动化系统，用于提取纹理特征并评估MTD。本工作的贡献包括三个方面：首先，提出了一种经济高效的方法来获取三维（3D）路面纹理数据；其次，改进了3D图像处理技术，并制定了能够反映多种纹理特征的特征；最后，建立了多变量预测模型，将这些特征与MTD值联系起来。验证结果显示，梯度提升树（GBT）模型在预测稳定性和准确性方面表现出色（R2 = 0.9858），现场测试也显示了该方法优于其他技术的优势，相对误差低于10%。该方法从图像输入到MTD预测输出，提供了一个全面的一站式路面质量评估解决方案。
2501.02411	 | Transfer learning via Regularized Linear Discriminant Analysis	 | Hongzhe Zhang,Arnab Auddy,Hongzhe Lee	 | 本文提出了一种基于迁移学习框架下的正则化随机效应线性判别分析方法，通过结合目标和源模型的岭估计值来估计判别方向，从而提高分类性能，并通过随机矩阵理论分析了权重的渐近值及其对分类误差的影响。	 | Linear discriminant analysis is a widely used method for classification. However, the high dimensionality of predictors combined with small sample sizes often results in large classification errors. To address this challenge, it is crucial to leverage data from related source models to enhance the classification performance of a target model. We propose to address this problem in the framework of transfer learning.   In this paper, we present novel transfer learning methods via regularized random-effects linear discriminant analysis, where the discriminant direction is estimated as a weighted combination of ridge estimates obtained from both the target and source models. Multiple strategies for determining these weights are introduced and evaluated, including one that minimizes the estimation risk of the discriminant vector and another that minimizes the classification error. Utilizing results from random matrix theory, we explicitly derive the asymptotic values of these weights and the associated classification error rates in the high-dimensional setting, where $p/n \rightarrow \infty$, with $p$ representing the predictor dimension and $n$ the sample size. We also provide geometric interpretations of various weights and a guidance on which weights to choose. Extensive numerical studies, including simulations and analysis of proteomics-based 10-year cardiovascular disease risk classification, demonstrate the effectiveness of the proposed approach.	 | 线性判别分析是一种广泛用于分类的方法。然而，预测变量的高维度与样本量小的结合往往会导致较大的分类误差。为了解决这一挑战，利用相关源模型的数据来增强目标模型的分类性能至关重要。我们提出在迁移学习框架下解决这一问题。在本文中，我们通过正则化随机效应线性判别分析提出了新的迁移学习方法，其中判别方向被估计为来自目标和源模型的岭估计值的加权组合。我们引入并评估了多种确定这些权重的策略，包括一种能够最小化判别向量估计风险的方法和另一种能够最小化分类误差的方法。利用随机矩阵理论的结果，在高维设置 $p/n \rightarrow \infty$ （其中 $p$ 表示预测变量维度，$n$ 表示样本量）下，我们显式地推导出了这些权重及其相应的分类误差率的渐近值。我们还提供了各种权重的几何解释，并提供了选择权重的指导。广泛的数值研究，包括模拟和基于蛋白质组学的10年心血管疾病风险分类分析，都证明了所提出方法的有效性。
2501.02407	 | Anonymization by Design of Language Modeling	 | Antoine Boutet,Zakaria El Kazdam,Lucas Magnana,Helain Zimmermann	 | 本文提出了一种方法以保护自然语言处理模型中的隐私，通过使用掩码语言模型和因果语言模型避免记住直接和间接标识信息，从而在保持模型性能的同时提高医疗数据的隐私保护。	 | Rapid advances in Natural Language Processing (NLP) have revolutionized many fields, including healthcare. However, these advances raise significant privacy concerns, especially when models specialized on sensitive data can memorize and then expose and regurgitate confidential information. This paper presents a privacy-by-design language modeling approach to address the problem of language models anonymization, and thus promote their sharing. Specifically, we propose both a Masking Language Modeling (MLM) methodology to specialize a BERT-like language model, and a Causal Language Modeling (CLM) methodology to specialize a GPT-like model that avoids the model from memorizing direct and indirect identifying information present in the training data. We have comprehensively evaluated our approaches using medical datasets and compared them against different baselines. Our results indicate that by avoiding memorizing both direct and indirect identifiers during model specialization, our masking and causal language modeling schemes offer the best tradeoff for maintaining high privacy while retaining high utility.	 | 自然语言处理（NLP）的迅速发展已经彻底改变了许多领域，包括医疗保健。然而，这些进展也引发了重大的隐私问题，特别是当针对敏感数据的模型能够记住并泄露和重复机密信息时。本文提出了一种设计隐私的方法来解决语言模型匿名化的问题，从而促进模型的共享。具体而言，我们提出了一种掩码语言模型（MLM）方法，用于专门化类似于BERT的语言模型，以及一种因果语言模型（CLM）方法，避免模型记住训练数据中存在的直接和间接标识信息。我们使用医疗数据集全面评估了我们的方法，并与不同的基线进行了比较。结果显示，通过在模型专门化过程中避免记住直接和间接标识符，我们的掩码和因果语言模型方案提供了在保持高隐私性的同时保留高可用性的最佳权衡。
2501.02406	 | Who Wrote This? Zero-Shot Statistical Tests for LLM-Generated Text Detection using Finite Sample Concentration Inequalities	 | Tara Radvand,Mojtaba Abdolmaleki,Mohamed Mostagir,Ambuj Tewari	 | 本文提出了一种基于零样本统计测试的方法，以区分由不同语言模型（包括人类生成的文本）生成的文本，并证明了该测试的错误率会随着文本长度的增加呈指数级减少。通过这种方法，可以在高概率下确定有害的LLM生成文本的来源，从而有助于打击虚假信息。	 | Verifying the provenance of content is crucial to the function of many organizations, e.g., educational institutions, social media platforms, firms, etc. This problem is becoming increasingly difficult as text generated by Large Language Models (LLMs) becomes almost indistinguishable from human-generated content. In addition, many institutions utilize in-house LLMs and want to ensure that external, non-sanctioned LLMs do not produce content within the institution. In this paper, we answer the following question: Given a piece of text, can we identify whether it was produced by LLM $A$ or $B$ (where $B$ can be a human)? We model LLM-generated text as a sequential stochastic process with complete dependence on history and design zero-shot statistical tests to distinguish between (i) the text generated by two different sets of LLMs $A$ (in-house) and $B$ (non-sanctioned) and also (ii) LLM-generated and human-generated texts. We prove that the type I and type II errors for our tests decrease exponentially in the text length. In designing our tests, we derive concentration inequalities on the difference between log-perplexity and the average entropy of the string under $A$. Specifically, for a given string, we demonstrate that if the string is generated by $A$, the log-perplexity of the string under $A$ converges to the average entropy of the string under $A$, except with an exponentially small probability in string length. We also show that if $B$ generates the text, except with an exponentially small probability in string length, the log-perplexity of the string under $A$ converges to the average cross-entropy of $B$ and $A$. Lastly, we present preliminary experimental results to support our theoretical results. By enabling guaranteed (with high probability) finding of the origin of harmful LLM-generated text with arbitrary size, we can help fight misinformation.	 | 验证内容的来源对于许多组织的功能至关重要，例如教育机构、社交媒体平台、公司等。随着大型语言模型（LLMs）生成的文本变得几乎无法与人类生成的内容区分开来，这个问题变得越来越难以解决。此外，许多机构使用内部的LLMs，并希望确保外部未经批准的LLMs不会在机构内生成内容。在本文中，我们回答了以下问题：给定一段文本，我们能否确定它是由LLM \(A\) 还是 \(B\)（\(B\) 可以是人类）生成的？我们将LLM生成的文本建模为具有完全历史依赖性的序贯随机过程，并设计零样本统计测试来区分（i）由不同一组LLM \(A\)（内部）和 \(B\)（未经批准）生成的文本，以及（ii）LLM生成的文本和人类生成的文本。我们证明了我们测试的I型错误和II型错误随着文本长度的增加呈指数级减少。在设计我们的测试时，我们推导了LLM \(A\) 下字符串的对数困惑度与平均熵之间的差异的集中不等式。具体来说，对于给定的字符串，我们证明如果该字符串由 \(A\) 生成，则对数困惑度收敛于 \(A\) 下该字符串的平均熵，除以一个指数级于字符串长度的小概率。我们还表明，如果 \(B\) 生成文本，则除以一个指数级于字符串长度的小概率，对数困惑度收敛于 \(B\) 和 \(A\) 的平均交叉熵。最后，我们呈现了初步的实验结果来支持我们的理论结果。通过使我们能够以高概率找到任意大小的有害LLM生成文本的来源，我们可以帮助打击虚假信息。
2501.02352	 | GNSS/GPS Spoofing and Jamming Identification Using Machine Learning and Deep Learning	 | Ali Ghanbarzade,Hossein Soleimani	 | 随着全球导航卫星系统（GNSS）的应用日益广泛，其面临来自欺骗和干扰的恶意威胁，这些威胁可能导致严重后果。通过采用机器学习和深度学习技术，研究取得了显著成效，提高干扰检测准确性至约99%，并展示了在欺骗检测方面的巨大潜力。	 | The increasing reliance on Global Navigation Satellite Systems (GNSS), particularly the Global Positioning System (GPS), underscores the urgent need to safeguard these technologies against malicious threats such as spoofing and jamming. As the backbone for positioning, navigation, and timing (PNT) across various applications including transportation, telecommunications, and emergency services GNSS is vulnerable to deliberate interference that poses significant risks. Spoofing attacks, which involve transmitting counterfeit GNSS signals to mislead receivers into calculating incorrect positions, can result in serious consequences, from navigational errors in civilian aviation to security breaches in military operations. Furthermore, the lack of inherent security measures within GNSS systems makes them attractive targets for adversaries. While GNSS/GPS jamming and spoofing systems consist of numerous components, the ability to distinguish authentic signals from malicious ones is essential for maintaining system integrity. Recent advancements in machine learning and deep learning provide promising avenues for enhancing detection and mitigation strategies against these threats. This paper addresses both spoofing and jamming by tackling real-world challenges through machine learning, deep learning, and computer vision techniques. Through extensive experiments on two real-world datasets related to spoofing and jamming detection using advanced algorithms, we achieved state of the art results. In the GNSS/GPS jamming detection task, we attained approximately 99% accuracy, improving performance by around 5% compared to previous studies. Additionally, we addressed a challenging tasks related to spoofing detection, yielding results that underscore the potential of machine learning and deep learning in this domain.	 | 随着全球导航卫星系统（GNSS），特别是全球定位系统（GPS）的应用日益广泛，迫切需要保护这些技术免受诸如欺骗和干扰等恶意威胁。GNSS作为定位、导航和授时（PNT）的基础，在交通、电信和应急服务等各种应用中都至关重要，因此很容易受到故意干扰，这种干扰会带来重大风险。欺骗攻击通过传输伪造的GNSS信号误导接收器计算错误的位置，可能导致从民用航空导航错误到军事行动安全漏洞等严重后果。此外，GNSS系统缺乏内在的安全措施，使其成为对手的理想攻击目标。尽管GNSS/GPS欺骗和干扰系统包含许多组件，但从合法信号中区分出恶意信号的能力对于保持系统完整性至关重要。近年来，机器学习和深度学习的进步为增强对这些威胁的检测和缓解策略提供了有希望的途径。本文通过采用机器学习、深度学习和计算机视觉技术解决实际挑战，同时使用两个与欺骗和干扰检测相关的先进算法进行大量实验，取得了卓越的结果。在GNSS/GPS干扰检测任务中，我们达到了约99%的准确性，相比之前的研究提高了大约5%的性能。此外，我们还解决了欺骗检测中的一些具有挑战性的任务，结果表明机器学习和深度学习在这方面的潜力巨大。
2501.02333	 | On The Causal Network Of Face-selective Regions In Human Brain During Movie Watching	 | Ali Bavafa,Gholam-Ali Hossein-Zadeh	 | 该研究利用一种新颖的因果发现方法——基于M矩阵的无环图（DAGMA）——探索大脑面部选择性网络的因果结构，发现面部刺激对网络内的因果连接数量和强度有显著影响，并强调了亚皮质区域在因果完备性中的重要作用。	 | Understanding the causal interactions in simple brain tasks, such as face detection, remains a challenging and ambiguous process for researchers. In this study, we address this issue by employing a novel causal discovery method -- Directed Acyclic Graphs via M-matrices for Acyclicity (DAGMA) -- to investigate the causal structure of the brain's face-selective network and gain deeper insights into its mechanism. Using natural movie stimuli, we extract causal network of face-selective regions and analyze how frames containing faces influence this network. Our findings reveal that the presence of faces in the stimuli have causal effect both on the number and strength of causal connections within the network. Additionally, our results highlight the crucial role of subcortical regions in satisfying causal sufficiency, emphasizing its importance in causal studies of brain. This study provides a new perspective on understanding the causal architecture of the face-selective network of the brain, motivating further research on neural causality.	 | 理解简单大脑任务（如面部检测）中的因果相互作用仍然是研究人员面临的一个具有挑战性和模糊性的问题。本研究通过采用一种新颖的因果发现方法——基于M矩阵的无环图(Directed Acyclic Graphs via M-matrices for Acyclicity, DAGMA)——来探究大脑面部选择性网络的因果结构，以获得对其机制的更深入理解。使用自然电影刺激，我们提取面部选择性区域的因果网络，并分析包含面部的帧如何影响这一网络。我们的研究结果揭示了刺激中面部的存在对网络内的因果连接的数量和强度具有因果效应。此外，我们的结果强调了亚皮质区域在满足因果完备性方面的作用，突显了其在大脑因果研究中的重要性。本研究为理解大脑面部选择性网络的因果架构提供了新的视角，激励进一步研究神经因果关系。
2501.02311	 | Analysis of Fluorescence Telescope Data Using Machine Learning Methods	 | Mikhail Zotov,Pavel Zakharov(for the JEM-EUSO Collaboration)	 | 该研究利用地面小型望远镜EUSO-TA的模型数据，采用机器学习和神经网络方法识别广泛空气簇射轨迹并重建初级粒子的能量和方向，并探讨了该方法在其他荧光望远镜中的应用及改进途径。	 | Fluorescence telescopes are among the key instruments used for studying ultra-high energy cosmic rays in all modern experiments. We use model data for a small ground-based telescope EUSO-TA to try some methods of machine learning and neural networks for recognizing tracks of extensive air showers in its data and for reconstruction of energy and arrival directions of primary particles. We also comment on the opportunities to use this approach for other fluorescence telescopes and outline possible ways of improving the performance of the suggested methods.	 | 荧光望远镜是用于研究超高能宇宙射线的所有现代实验中关键仪器之一。我们使用地面小型望远镜EUSO-TA的模型数据，尝试使用机器学习和神经网络方法来识别其数据中广泛空气 shower 的轨迹，并重建初级粒子的能量和到达方向。我们还讨论了将此方法应用于其他荧光望远镜的可能性，并概述了改进建议方法性能的潜在途径。
2501.02300	 | Diabetic Retinopathy Detection Using CNN with Residual Block with DCGAN	 | Debjany Ghosh Aronno,Sumaiya Saeha	 | 该研究提出了一种使用残差块卷积神经网络和数据增强技术的自动化系统，用于早期检测和分类糖尿病性视网膜病变，以支持资源有限环境下的大规模筛查。通过生成多样化视网膜图像，该系统提高了模型的鲁棒性和泛化能力，能够准确分类从无糖尿病性视网膜病变到增生性糖尿病性视网膜病变的不同类别。	 | Diabetic Retinopathy (DR) is a major cause of blindness worldwide, caused by damage to the blood vessels in the retina due to diabetes. Early detection and classification of DR are crucial for timely intervention and preventing vision loss. This work proposes an automated system for DR detection using Convolutional Neural Networks (CNNs) with a residual block architecture, which enhances feature extraction and model performance. To further improve the model's robustness, we incorporate advanced data augmentation techniques, specifically leveraging a Deep Convolutional Generative Adversarial Network (DCGAN) for generating diverse retinal images. This approach increases the variability of training data, making the model more generalizable and capable of handling real-world variations in retinal images. The system is designed to classify retinal images into five distinct categories, from No DR to Proliferative DR, providing an efficient and scalable solution for early diagnosis and monitoring of DR progression. The proposed model aims to support healthcare professionals in large-scale DR screening, especially in resource-constrained settings.	 | 糖尿病性视网膜病变（Diabetic Retinopathy, DR）是全球导致失明的主要原因之一，由糖尿病引起的视网膜血管损伤所致。早期检测和分类DR对于及时干预和预防视力丧失至关重要。本研究提出了一种使用具有残差块架构的卷积神经网络（Convolutional Neural Networks, CNNs）的自动化系统，以增强特征提取和模型性能。为了进一步提高模型的鲁棒性，我们引入了先进的数据增强技术，并利用深度卷积生成对抗网络（Deep Convolutional Generative Adversarial Network, DCGAN）生成多样化的视网膜图像。这种方法增加了训练数据的多样性，使模型更具泛化能力，并能够处理真实世界中视网膜图像的各种差异。该系统设计用于将视网膜图像分类为五个不同的类别，从无糖尿病性视网膜病变（No DR）到增生性糖尿病性视网膜病变（Proliferative DR），提供了一种高效且可扩展的解决方案，用于早期诊断和监测DR的发展。所提出的模型旨在支持在资源有限的环境中进行大规模DR筛查的医疗专业人员。
2501.02298	 | Beyond Log-Concavity and Score Regularity: Improved Convergence Bounds for Score-Based Generative Models in W2-distance	 | Marta Gentiloni-Silveri,Antonio Ocello	 | 本文提出了一个新的框架来分析分数基于生成模型（SGMs）中的 $\mathcal{W}_2$ 距离收敛性，通过利用Ornstein-Uhlenbeck过程的正则化特性，放宽了原有的严格假设，证明了数据分布的弱对数凹性会演变成对数凹性，并展示了该框架在高斯混合模型上的有效性。	 | Score-based Generative Models (SGMs) aim to sample from a target distribution by learning score functions using samples perturbed by Gaussian noise. Existing convergence bounds for SGMs in the $\mathcal{W}_2$-distance rely on stringent assumptions about the data distribution. In this work, we present a novel framework for analyzing $\mathcal{W}_2$-convergence in SGMs, significantly relaxing traditional assumptions such as log-concavity and score regularity. Leveraging the regularization properties of the Ornstein-Uhlenbeck (OU) process, we show that weak log-concavity of the data distribution evolves into log-concavity over time. This transition is rigorously quantified through a PDE-based analysis of the Hamilton-Jacobi-Bellman equation governing the log-density of the forward process. Moreover, we establish that the drift of the time-reversed OU process alternates between contractive and non-contractive regimes, reflecting the dynamics of concavity. Our approach circumvents the need for stringent regularity conditions on the score function and its estimators, relying instead on milder, more practical assumptions. We demonstrate the wide applicability of this framework through explicit computations on Gaussian mixture models, illustrating its versatility and potential for broader classes of data distributions.	 | 基于分数的生成模型（Score-based Generative Models, SGMs）通过学习扰动高斯噪声的样本中的分数函数来从目标分布中进行采样。现有基于 $\mathcal{W}_2$ 距离的 SGMs 收敛性界依赖于对数据分布的严格假设。在本文中，我们提出了一个新的框架来分析 SGMs 中的 $\mathcal{W}_2$ 收敛性，显著地放宽了传统的假设，如对数凹性和分数正则性。利用Ornstein-Uhlenbeck (OU) 过程的正则化性质，我们证明了数据分布的弱对数凹性会随着时间的推移演变成对数凹性。这一转换通过Hamilton-Jacobi-Bellman 方程（该方程支配了前进过程的对数密度）的偏微分方程（PDE）分析得到了严格的量化。此外，我们证明了时间反转的 OU 过程的漂移交替处于收缩性和非收缩性之间，反映了凹性动力学。我们的方法避免了对分数函数及其估计器的严格正则性条件的需求，而是依赖于更温和、更实用的假设。我们通过在高斯混合模型上的显式计算展示了该框架的广泛应用性，这证明了其灵活性和适用于更广泛类别的数据分布的潜力。
2501.02270	 | Efficient Video-Based ALPR System Using YOLO and Visual Rhythm	 | Victor Nascimento Ribeiro,Nina S. T. Hirata	 | 该研究提出了一种自动车牌识别（ALPR）系统，能够从视频中为每辆车提取精确的一帧，并利用光学字符识别（OCR）模型识别车牌字符，实验表明该方法是可行的。	 | Automatic License Plate Recognition (ALPR) involves extracting vehicle license plate information from image or a video capture. These systems have gained popularity due to the wide availability of low-cost surveillance cameras and advances in Deep Learning. Typically, video-based ALPR systems rely on multiple frames to detect the vehicle and recognize the license plates. Therefore, we propose a system capable of extracting exactly one frame per vehicle and recognizing its license plate characters from this singular image using an Optical Character Recognition (OCR) model. Early experiments show that this methodology is viable.	 | 自动车牌识别（ALPR）涉及从图像或视频捕获中提取车辆车牌信息。由于低-cost监控摄像头的广泛可用性和深度学习的进步，这些系统已经变得非常流行。通常，基于视频的ALPR系统依赖于多帧来检测车辆并识别车牌。因此，我们提出了一种系统，该系统能够为每辆车提取精确的一帧，并利用光学字符识别（OCR）模型从这单一图像中识别其车牌字符。早期的实验表明，这种方法是可行的。
2501.02221	 | CORD: Generalizable Cooperation via Role Diversity	 | Kanefumi Matsuyama,Kefan Su,Jiangxing Wang,Deheng Ye,Zongqing Lu	 | CORD是一种分层的协作多智能体强化学习方法，通过最大化角色多样性来实现智能体间的有效协作，并能更好地泛化到未见过的合作者，从而在多种任务中展现出优于基线方法的性能，特别是在泛化测试中表现更佳。	 | Cooperative multi-agent reinforcement learning (MARL) aims to develop agents that can collaborate effectively. However, most cooperative MARL methods overfit training agents, making learned policies not generalize well to unseen collaborators, which is a critical issue for real-world deployment. Some methods attempt to address the generalization problem but require prior knowledge or predefined policies of new teammates, limiting real-world applications. To this end, we propose a hierarchical MARL approach to enable generalizable cooperation via role diversity, namely CORD. CORD's high-level controller assigns roles to low-level agents by maximizing the role entropy with constraints. We show this constrained objective can be decomposed into causal influence in role that enables reasonable role assignment, and role heterogeneity that yields coherent, non-redundant role clusters. Evaluated on a variety of cooperative multi-agent tasks, CORD achieves better performance than baselines, especially in generalization tests. Ablation studies further demonstrate the efficacy of the constrained objective in generalizable cooperation.	 | 协作多智能体强化学习（MARL）旨在开发能够有效协作的智能体。然而，大多数协作MARL方法会导致训练智能体过度拟合，使得学到的策略不能很好地泛化到未见过的合作者，这是在现实世界部署中的一个关键问题。一些方法尝试解决泛化问题，但需要关于新队友的先验知识或预定义策略，限制了其实用性。为解决这一问题，我们提出了一种分层MARL方法，通过角色多样性实现可泛化的协作，即CORD。CORD的高层控制器通过最大化角色熵来分配低层智能体的角色，并且受到约束。我们展示这种带约束的目标可以通过因果影响在角色中的分解来实现合理的角色分配，同时通过角色异质性产生一致且非冗余的角色簇。在多种协作多智能体任务上评估表明，CORD在性能上优于基线方法，特别是在泛化测试中表现更佳。进一步的消融研究还表明了带约束目标在可泛化协作中的有效性。
2501.02211	 | Examining the Robustness of Homogeneity Bias to Hyperparameter Adjustments in GPT-4	 | Messi H.J. Lee	 | 研究发现，大规模视觉-语言模型中广泛存在的同质性偏差在不同超参数设置下仍然存在，并且对种族和性别的影响表现出差异性，尽管超参数调整可以在一定程度上减少种族同质性偏差，但对性别偏差的影响有限，表明超参数调整不是解决同质性偏见的万能方法。	 | Vision-Language Models trained on massive collections of human-generated data often reproduce and amplify societal stereotypes. One critical form of stereotyping reproduced by these models is homogeneity bias-the tendency to represent certain groups as more homogeneous than others. We investigate how this bias responds to hyperparameter adjustments in GPT-4, specifically examining sampling temperature and top p which control the randomness of model outputs. By generating stories about individuals from different racial and gender groups and comparing their similarities using vector representations, we assess both bias robustness and its relationship with hyperparameter values. We find that (1) homogeneity bias persists across most hyperparameter configurations, with Black Americans and women being represented more homogeneously than White Americans and men, (2) the relationship between hyperparameters and group representations shows unexpected non-linear patterns, particularly at extreme values, and (3) hyperparameter adjustments affect racial and gender homogeneity bias differently-while increasing temperature or decreasing top p can reduce racial homogeneity bias, these changes show different effects on gender homogeneity bias. Our findings suggest that while hyperparameter tuning may mitigate certain biases to some extent, it cannot serve as a universal solution for addressing homogeneity bias across different social group dimensions.	 | 大规模人类生成数据训练的视觉-语言模型常常重现和放大社会刻板印象。这些模型中的一种关键形式的刻板印象再现是同质性偏差——即倾向于将某些群体表现为比其他群体更加同质。我们调查了这种偏差如何响应 GPT-4 的超参数调整，特别是研究了控制模型输出随机性的采样温度和 top p。通过生成不同种族和性别群体的个人故事，并使用向量表示来比较它们的相似性，我们评估了偏差的稳健性和其与超参数值之间的关系。我们发现：（1）同质性偏差在大多数超参数配置中依然存在，表现为黑人美国人和女性比白人美国人和男性更加同质；（2）超参数与群体表示之间的关系显示出意外的非线性模式，尤其是在极端值时；（3）超参数调整对种族和性别同质性偏差的影响不同——增加温度或减少 top p 可以减少种族同质性偏差，但这些变化对性别同质性偏差的影响不同。我们的研究结果表明，虽然超参数调整可以在一定程度上缓解某些偏见，但它不能作为解决不同社会群体维度上同质性偏见的通用解决方案。
2501.02208	 | Robust Multi-Dimensional Scaling via Accelerated Alternating Projections	 | Tong Deng,Tianming Wang	 | 本文提出了一种基于交替投影和切空间投影技术的鲁棒多维标度方法，该方法能在点对距离被离群值污染的情况下定位点的位置，并且在离群值稀疏时能够线性收敛到原始点。数值实验验证了该算法的有效性和性能。	 | We consider the robust multi-dimensional scaling (RMDS) problem in this paper. The goal is to localize point locations from pairwise distances that may be corrupted by outliers. Inspired by classic MDS theories, and nonconvex works for the robust principal component analysis (RPCA) problem, we propose an alternating projection based algorithm that is further accelerated by the tangent space projection technique. For the proposed algorithm, if the outliers are sparse enough, we can establish linear convergence of the reconstructed points to the original points after centering and rotation alignment. Numerical experiments verify the state-of-the-art performances of the proposed algorithm.	 | 本文考虑了鲁棒多维标度（RMDS）问题。目标是从可能被离群值污染的点对距离中定位点的位置。受经典多维标度（MDS）理论以及鲁棒主成分分析（RPCA）问题中非凸方法的启发，我们提出了一种基于交替投影的方法，并进一步通过切空间投影技术加速。对于所提出的算法，如果离群值足够稀疏，在中心化和旋转对齐后，重构点可以线性收敛到原始点。数值实验验证了所提出算法的前沿性能。
2501.02200	 | Learning Evolution via Optimization Knowledge Adaptation	 | Chao Wang,Licheng Jiao,Jiaxuan Zhao,Lingling Li,Fang Liu,Shuyuan Yang	 | 本文提出了一种优化知识适应进化模型（OKAEM），通过利用积累的知识动态调整进化操作参数，从而增强优化能力。实验表明OKAEM能够在多种场景下利用先验知识提升性能，并且即使在缺乏先验知识的情况下也能实现具有竞争力的性能。	 | Evolutionary algorithms (EAs) maintain populations through evolutionary operators to discover diverse solutions for complex tasks while gathering valuable knowledge, such as historical population data and fitness evaluations. However, traditional EAs face challenges in dynamically adapting to expanding knowledge bases, hindering the efficient exploitation of accumulated information and limiting adaptability to new situations. To address these issues, we introduce an Optimization Knowledge Adaptation Evolutionary Model (OKAEM), which features dynamic parameter adjustment using accumulated knowledge to enhance its optimization capabilities. OKAEM employs attention mechanisms to model the interactions among individuals, fitness landscapes, and genetic components separately, thereby parameterizing the evolutionary operators of selection, crossover, and mutation. These powerful learnable operators enable OKAEM to benefit from pre-learned extensive prior knowledge and self-tune with real-time evolutionary insights. Experimental results demonstrate that OKAEM: 1) exploits prior knowledge for significant performance gains across various knowledge transfer settings; 2) achieves competitive performance through self-tuning alone, even without prior knowledge; 3) outperforms state-of-the-art black-box baselines in a vision-language model tuning case; 4) can improve its optimization capabilities with growing knowledge; 5) is capable of emulating principles of natural selection and genetic recombination.	 | 进化算法（EAs）通过进化操作员维持种群，以发现复杂任务的多样解，同时收集有价值的知识，如历史种群数据和适应度评估。然而，传统的EAs在动态适应扩展的知识库方面面临挑战，这阻碍了对积累信息的有效利用，并限制了其在新情况下的适应性。为了解决这些问题，我们引入了一种优化知识适应进化模型（OKAEM），该模型通过利用积累的知识动态调整参数，从而增强其优化能力。OKAEM 使用注意力机制独立建模个体、适应度景观和遗传组件之间的交互，从而参数化选择、交叉和突变等进化操作。这些强大的可学习操作员使OKAEM能够从之前学习的广泛先验知识中受益，并根据实时进化洞察自我调整。实验结果显示：1）OKAEM 在各种知识转移设置中通过利用先验知识获得显著性能提升；2）即使没有先验知识，仅通过自我调整也能实现竞争力的性能；3）在视觉语言模型调优案例中，OKAEM 出色地超过了最先进的黑盒基线；4）随着知识的增长，OKAEM 可以提高其优化能力；5）OKAEM 能够模拟自然选择和遗传重组的原理。
2501.02197	 | Majorization-Minimization Dual Stagewise Algorithm for Generalized Lasso	 | Jianmin Chen,Kun Chen	 | 本文提出了一种最大值-最小值双重逐步算法（MM-DUST），用于高效追踪广义拉索问题的完整解路径，并且在非高斯和非线性模型中展示了其有效性和计算效率。	 | The generalized lasso is a natural generalization of the celebrated lasso approach to handle structural regularization problems. Many important methods and applications fall into this framework, including fused lasso, clustered lasso, and constrained lasso. To elevate its effectiveness in large-scale problems, extensive research has been conducted on the computational strategies of generalized lasso. However, to our knowledge, most studies are under the linear setup, with limited advances in non-Gaussian and non-linear models. We propose a majorization-minimization dual stagewise (MM-DUST) algorithm to efficiently trace out the full solution paths of the generalized lasso problem. The majorization technique is incorporated to handle different convex loss functions through their quadratic majorizers. Utilizing the connection between primal and dual problems and the idea of ``slow-brewing'' from stagewise learning, the minimization step is carried out in the dual space through a sequence of simple coordinate-wise updates on the dual coefficients with a small step size. Consequently, selecting an appropriate step size enables a trade-off between statistical accuracy and computational efficiency. We analyze the computational complexity of MM-DUST and establish the uniform convergence of the approximated solution paths. Extensive simulation studies and applications with regularized logistic regression and Cox model demonstrate the effectiveness of the proposed approach.	 | 广义拉索是拉索方法在处理结构性正则化问题时的自然扩展。许多重要的方法和应用都属于这一框架，包括融合拉索、聚类拉索和约束拉索等。为了在大规模问题中提高其有效性，已经进行了大量的计算策略研究来处理广义拉索问题。然而，据我们所知，大多数研究仍然局限于线性模型，对于非高斯和非线性模型的研究进展相对有限。我们提出了一种最大值-最小值双重逐步算法（MM-DUST）来高效地追踪广义拉索问题的完整解路径。最大值技术被整合进来，通过二次近似器处理不同的凸损失函数。利用原问题和对偶问题之间的联系及逐步学习中“慢炖”的思想，在对偶空间中通过一系列简单的坐标更新来执行最小化步骤，并使用较小的步长。因此，选择合适的步长可以在统计准确性和计算效率之间实现权衡。我们分析了MM-DUST的计算复杂性，并建立了近似解路径的一致收敛性。广泛的模拟研究和正则化逻辑回归和Cox模型的现实应用展示了该方法的有效性。
2501.02189	 | Benchmark Evaluations, Applications, and Challenges of Large Vision Language Models: A Survey	 | Zongxia Li,Xiyang Wu,Hongyang Du,Huy Nghiem,Guangyao Shi	 | 本文提供了一个全面的多模态视觉语言模型（VLMs）综述，涵盖了2019年至2024年间的主要VLM模型及其架构、训练方法、评估指标、应用领域以及面临的问题和挑战，并详细包含了相关论文和模型仓库链接。	 | Multimodal Vision Language Models (VLMs) have emerged as a transformative technology at the intersection of computer vision and natural language processing, enabling machines to perceive and reason about the world through both visual and textual modalities. For example, models such as CLIP, Claude, and GPT-4V demonstrate strong reasoning and understanding abilities on visual and textual data and beat classical single modality vision models on zero-shot classification. Despite their rapid advancements in research and growing popularity in applications, a comprehensive survey of existing studies on VLMs is notably lacking, particularly for researchers aiming to leverage VLMs in their specific domains. To this end, we provide a systematic overview of VLMs in the following aspects: model information of the major VLMs developed over the past five years (2019-2024); the main architectures and training methods of these VLMs; summary and categorization of the popular benchmarks and evaluation metrics of VLMs; the applications of VLMs including embodied agents, robotics, and video generation; the challenges and issues faced by current VLMs such as hallucination, fairness, and safety. Detailed collections including papers and model repository links are listed in https://github.com/zli12321/Awesome-VLM-Papers-And-Models.git.	 | 多模态视觉语言模型（VLMs）已成为计算机视觉和自然语言处理交叉领域的变革性技术，使机器能够通过视觉和文本两种模态来感知和理解世界。例如，CLIP、Claude和GPT-4V等模型在视觉和文本数据上的推理和理解能力非常出色，并且在零样本分类方面超过了传统的单模态视觉模型。尽管这些模型在研究上的快速进步和在应用中的日益流行，但对现有VLM研究的全面综述仍然明显缺乏，特别是对于希望在其特定领域利用VLM的研究人员而言。为此，我们从以下几个方面提供了一个系统的概述：过去五年（2019-2024）开发的主要VLM模型的信息；这些VLM模型的主要架构和训练方法；VLM模型的流行基准和评估指标的总结和分类；VLM模型的应用，包括具身代理、机器人技术和视频生成；当前VLM模型所面临的问题和挑战，如幻觉、公平性和安全性。详细的资料包括论文和模型仓库链接，可在https://github.com/zli12321/Awesome-VLM-Papers-And-Models.git 中找到。
2501.02181	 | SMDP-Based Dynamic Batching for Improving Responsiveness and Energy Efficiency of Batch Services	 | Yaodan Xu,Sheng Zhou,Zhisheng Niu	 | 该研究提出了一种动态批量处理方案，以平衡延迟和能耗，通过将系统建模为具有大小依赖性服务时间的批量服务队列，并将其设计形式化为半马尔可夫决策过程（SMDP）问题，从而在不同参数设置下实现了优越的性能，并在能耗和延迟之间表现出显著的灵活性。	 | For servers incorporating parallel computing resources, batching is a pivotal technique for providing efficient and economical services at scale. Parallel computing resources exhibit heightened computational and energy efficiency when operating with larger batch sizes. However, in the realm of online services, the adoption of a larger batch size may lead to longer response times. This paper aims to provide a dynamic batching scheme that delicately balances latency and efficiency. The system is modeled as a batch service queue with size-dependent service times. Then, the design of dynamic batching is formulated as a semi-Markov decision process (SMDP) problem, with the objective of minimizing the weighted sum of average response time and average power consumption. A method is proposed to derive an approximate optimal SMDP solution, representing the chosen dynamic batching policy. By introducing an abstract cost to reflect the impact of "tail" states, the space complexity and the time complexity of the procedure can decrease by 63.5% and 98%, respectively. Numerical results showcase the superiority of SMDP-based batching policies across various parameter setups. Additionally, the proposed scheme exhibits noteworthy flexibility in balancing power consumption and latency.	 | 对于包含并行计算资源的服务器，批量处理是一种关键的技术，用于在大规模范围内提供高效且经济的服务。当使用更大的批量大小时，平行计算资源显示出更高的计算和能源效率。然而，在在线服务领域，采用更大的批量大小可能导致响应时间更长。本文旨在提供一种动态批量处理方案，精细地平衡延迟和效率。系统被建模为具有大小依赖性服务时间的批量服务队列。然后，动态批量处理的设计被形式化为半马尔可夫决策过程（SMDP）问题，目标是使加权平均响应时间和加权平均能耗之和最小化。提出了一种方法来导出近似的最优SMDP解，代表所选的动态批量策略。通过引入表示“尾部”状态影响的抽象成本，程序的空间复杂度和时间复杂度分别减少了63.5%和98%。数值结果展示了基于SMDP的批量策略在不同参数设置下的优越性。此外，所提出的方法在平衡能耗和延迟方面表现出显著的灵活性。
2501.02176	 | Molecule-dynamic-based Aging Clock and Aging Roadmap Forecast with Sundial	 | Wei Wu,Zizhen Deng,Chi Zhang,Can Liao,Jinzhuo Wang	 | Sundial是一个新框架，通过扩散场建模分子动态，同时捕捉群体和个体水平的老化过程，能够实现无偏差的生物学年龄估计和老化路线图的预测。该框架识别出的加速老化个体具有更高的疾病风险，为研究年龄和性别特异性老化动力学提供了新途径。	 | Addressing the unavoidable bias inherent in supervised aging clocks, we introduce Sundial, a novel framework that models molecular dynamics through a diffusion field, capturing both the population-level aging process and the individual-level relative aging order. Sundial enables unbiasedestimation of biological age and the forecast of aging roadmap. Fasteraging individuals from Sundial exhibit a higher disease risk compared to those identified from supervised aging clocks. This framework opens new avenues for exploring key topics, including age- and sex-specific aging dynamics and faster yet healthy aging paths.	 | 针对监督老化时钟固有的不可避免偏差，我们提出了Sundial这一新型框架，该框架通过扩散场建模分子动态，同时捕捉群体水平的老化过程和个体水平的相对老化顺序。Sundial能够实现生物学年龄的无偏差估计和老化路线图的预测。来自Sundial鉴定出的加速老化个体相比由监督老化时钟识别出的个体，面临更高的疾病风险。该框架为探索关键主题，如年龄和性别特异性老化动力学以及更快而健康的老化路径，开辟了新的研究途径。
2501.02173	 | The Efficiency vs. Accuracy Trade-off: Optimizing RAG-Enhanced LLM Recommender Systems Using Multi-Head Early Exit	 | Huixue Zhou,Hengrui Gu,Xi Liu,Kaixiong Zhou,Mingfu Liang,Yongkang Xiao,Srinivas Govindan,Piyush Chawla,Jiyan Yang,Xiangfei Meng,Huayu Li,Buyun Zhang,Liang Luo,Wen-Yen Chen,Yiping Han,Bo Long,Rui Zhang,Tianlong Chen	 | 本文提出了一种优化框架，结合检索增强生成（RAG）和多头早期退出架构，并通过集成图卷积网络（GCNs）来减少数据检索时间，从而在保持高性能的同时提高大型语言模型（LLMs）在推荐系统中的计算效率。实验结果显示，该架构能够有效减少计算时间而不牺牲准确性，为实时应用中的LLM部署树立了新标准。	 | The deployment of Large Language Models (LLMs) in recommender systems for predicting Click-Through Rates (CTR) necessitates a delicate balance between computational efficiency and predictive accuracy. This paper presents an optimization framework that combines Retrieval-Augmented Generation (RAG) with an innovative multi-head early exit architecture to concurrently enhance both aspects. By integrating Graph Convolutional Networks (GCNs) as efficient retrieval mechanisms, we are able to significantly reduce data retrieval times while maintaining high model performance. The early exit strategy employed allows for dynamic termination of model inference, utilizing real-time predictive confidence assessments across multiple heads. This not only quickens the responsiveness of LLMs but also upholds or improves their accuracy, making it ideal for real-time application scenarios. Our experiments demonstrate how this architecture effectively decreases computation time without sacrificing the accuracy needed for reliable recommendation delivery, establishing a new standard for efficient, real-time LLM deployment in commercial systems.	 | 大型语言模型（LLMs）在推荐系统中的部署，用于预测点击率（CTR），需要在计算效率和预测准确性之间取得微妙的平衡。本文提出了一种优化框架，将检索增强生成（RAG）与创新的多头早期退出架构相结合，以同时提升这两个方面。通过将图卷积网络（GCNs）作为高效的检索机制进行集成，我们能够显著减少数据检索时间，同时保持高模型性能。采用的早期退出策略允许根据多个头的实时预测置信度评估动态终止模型推理。这不仅加快了LLMs的响应速度，还能够维持或提高其准确性，使其特别适用于实时应用场景。实验结果表明，该架构在不影响可靠推荐交付所需的准确性的前提下，有效地减少了计算时间，为商业系统中的高效、实时LLM部署树立了新标准。
2501.02147	 | Exploring Secure Machine Learning Through Payload Injection and FGSM Attacks on ResNet-50	 | Umesh Yadav,Suman Niraula,Gaurav Kumar Gupta,Bicky Yadav	 | 本文研究了ResNet-50模型在Fast Gradient Sign Method (FGSM)对抗攻击和恶意负载注入下的鲁棒性，发现模型在面对FGSM干扰时准确率保持不变但信心增加，而恶意负载注入在大部分样本中成功实施，展示了高性能神经网络仍存在安全漏洞。	 | This paper investigates the resilience of a ResNet-50 image classification model under two prominent security threats: Fast Gradient Sign Method (FGSM) adversarial attacks and malicious payload injection. Initially, the model attains a 53.33% accuracy on clean images. When subjected to FGSM perturbations, its overall accuracy remains unchanged; however, the model's confidence in incorrect predictions notably increases. Concurrently, a payload injection scheme is successfully executed in 93.33% of the tested samples, revealing how stealthy attacks can manipulate model predictions without degrading visual quality. These findings underscore the vulnerability of even high-performing neural networks and highlight the urgency of developing more robust defense mechanisms for security-critical applications.	 | 本文探讨了ResNet-50 图像分类模型在两种主要安全威胁下的鲁棒性：Fast Gradient Sign Method (FGSM) 对抗攻击和恶意负载注入。初始状态下，该模型在干净图像上的准确率为 53.33%。当受到 FGSM 干扰时，模型的整体准确率保持不变，但模型对错误预测的信心显著增加。同时，在 93.33% 的测试样本中成功执行了恶意负载注入方案，揭示了隐蔽攻击如何在不降低视觉质量的情况下操控模型预测。这些发现强调了即使高性能的神经网络也存在漏洞，并突显了为关键安全应用开发更 robust 防御机制的紧迫性。
2501.02143	 | SafeAug: Safety-Critical Driving Data Augmentation from Naturalistic Datasets	 | Zhaobin Mo,Yunlong Li,Xuan Di	 | 本文提出了一种新型框架，通过检测、深度估计和三维变换增强自然场景中的安全性关键驾驶数据，以生成更真实的、反映危险情况的数据，从而提高自动驾驶算法的安全性和可靠性。实验结果表明，使用增强数据集训练的算法在性能上优于其他基线方法。	 | Safety-critical driving data is crucial for developing safe and trustworthy self-driving algorithms. Due to the scarcity of safety-critical data in naturalistic datasets, current approaches primarily utilize simulated or artificially generated images. However, there remains a gap in authenticity between these generated images and naturalistic ones. We propose a novel framework to augment the safety-critical driving data from the naturalistic dataset to address this issue. In this framework, we first detect vehicles using YOLOv5, followed by depth estimation and 3D transformation to simulate vehicle proximity and critical driving scenarios better. This allows for targeted modification of vehicle dynamics data to reflect potentially hazardous situations. Compared to the simulated or artificially generated data, our augmentation methods can generate safety-critical driving data with minimal compromise on image authenticity. Experiments using KITTI datasets demonstrate that a downstream self-driving algorithm trained on this augmented dataset performs superiorly compared to the baselines, which include SMOGN and importance sampling.	 | 安全性关键的驾驶数据对于开发安全可靠的自动驾驶算法至关重要。由于自然场景数据集中安全性关键数据的稀缺性，当前方法主要依赖于模拟或人工生成的图像。然而，这些生成的图像与自然场景图像之间仍然存在真实的差距。我们提出了一种新型框架，以增强自然场景数据集中的安全性关键驾驶数据来解决这一问题。在这个框架中，我们首先使用YOLOv5检测车辆，然后进行深度估计和三维变换，以更好地模拟车辆接近和关键驾驶场景。这允许对车辆动力学数据进行针对性修改，以反映可能的危险情况。与模拟或人工生成的数据相比，我们的增强方法可以在最大限度保留图像真实性的同时生成安全性关键的驾驶数据。使用KITTI数据集进行的实验表明，使用该增强数据集训练的下游自动驾驶算法在性能上优于基线方法，包括SMOGN和重要性抽样。
2501.02114	 | Relaxation-assisted reverse annealing on nonnegative/binary matrix factorization	 | Renichiro Haba,Masayuki Ohzeki,Kazuyuki Tanaka	 | 该研究提出了一种将逆退火与线性规划松弛技术结合的新方法，以提高非负/二值矩阵分解的优化性能，并在面部图像数据集上显示出更好的收敛性；实验结果强调了结合量子退火和经典优化策略在提高优化性能方面的潜在价值。	 | Quantum annealing has garnered significant attention as meta-heuristics inspired by quantum physics for combinatorial optimization problems. Among its many applications, nonnegative/binary matrix factorization stands out for its complexity and relevance in unsupervised machine learning. The use of reverse annealing, a derivative procedure of quantum annealing to prioritize the search in a vicinity under a given initial state, helps improve its optimization performance in matrix factorization. This study proposes an improved strategy that integrates reverse annealing with a linear programming relaxation technique. Using relaxed solutions as the initial configuration for reverse annealing, we demonstrate improvements in optimization performance comparable to the exact optimization methods. Our experiments on facial image datasets show that our method provides better convergence than known reverse annealing methods. Furthermore, we investigate the effectiveness of relaxation-based initialization methods on randomized datasets, demonstrating a relationship between the relaxed solution and the optimal solution. This research underscores the potential of combining reverse annealing and classical optimization strategies to enhance optimization performance.	 | 量子退火因其受量子物理学启发而成为组合优化问题中的元启发式算法，受到了广泛关注。在众多应用中，非负/二值矩阵分解因其在无监督机器学习中的复杂性和相关性而尤为突出。逆退火是一种从给定初始状态优先搜索邻域的量子退火衍生技术，有助于提高矩阵分解中的优化性能。本研究提出了一种改进策略，将逆退火与线性规划松弛技术相结合。使用松弛解作为逆退火的初始配置，我们展示了与精确优化方法相当的优化性能改进。在面部图像数据集上的实验表明，我们的方法比已知的逆退火方法具有更好的收敛性。此外，我们还研究了基于松弛初始化方法在随机化数据集上的有效性，揭示了松弛解与最优解之间的关系。这项研究强调了结合逆退火和经典优化策略以提高优化性能的潜在价值。
2501.02105	 | Learning Fricke signs from Maass form Coefficients	 | Joanna Bieri,Giorgi Butbaia,Edgar Costa,Alyson Deines,Kyu-Hwan Lee,David Lowry-Duda,Thomas Oliver,Yidi Qi,Tamara Veenstra	 | 本文通过对Maass形式的傅里叶系数进行数据科学分析，发现具有相同弗里克标志的形式呈现出类似的“椋鸟集群”模式，并使用线性判别分析（LDA）和神经网络成功预测了约43%未知弗里克标志形式的标志，预测准确率较高。	 | In this paper, we conduct a data-scientific investigation of Maass forms. We find that averaging the Fourier coefficients of Maass forms with the same Fricke sign reveals patterns analogous to the recently discovered "murmuration" phenomenon, and that these patterns become more pronounced when parity is incorporated as an additional feature. Approximately 43% of the forms in our dataset have an unknown Fricke sign. For the remaining forms, we employ Linear Discriminant Analysis (LDA) to machine learn their Fricke sign, achieving 96% (resp. 94%) accuracy for forms with even (resp. odd) parity. We apply the trained LDA model to forms with unknown Fricke signs to make predictions. The average values based on the predicted Fricke signs are computed and compared to those for forms with known signs to verify the reasonableness of the predictions. Additionally, a subset of these predictions is evaluated against heuristic guesses provided by Hejhal's algorithm, showing a match approximately 95% of the time. We also use neural networks to obtain results comparable to those from the LDA model.	 | 在本文中，我们对Maass形式进行了数据科学调查。我们发现，对于具有相同弗里克标志的Maass形式，平均其傅里叶系数揭示了类似于最近发现的“椋鸟集群”现象的模式，并且在引入奇偶性作为额外特征时，这些模式会更加明显。在我们的数据集中，约43%的形式具有未知的弗里克标志。对于剩余的形式，我们使用线性判别分析（LDA）来机器学习它们的弗里克标志，对偶数（奇数）奇偶性的形式，准确率分别达到96%（94%）。我们使用训练好的LDA模型对具有未知弗里克标志的形式进行预测，并计算基于预测的弗里克标志的平均值，将其与已知标志的形式的平均值进行比较，以验证预测的合理性。此外，我们还对Hejhal算法提供的启发式猜测进行了一部分预测的评估，结果显示大约95%的时间匹配。我们还使用神经网络获得了与LDA模型结果相当的结果。
2501.02071	 | Laws of thermodynamics for exponential families	 | Akshay Balsubramani	 | 通过将学习问题重新表述为最大熵和统计力学的形式，研究将热力学定律以通用指数族的形式发展，并在AI和统计学中找到了功、热、热力学循环和能量均分等概念的对应，这些概念在应对分布偏移方面具有广泛应用意义。	 | We develop the laws of thermodynamics in terms of general exponential families. By casting learning (log-loss minimization) problems in max-entropy and statistical mechanics terms, we translate thermodynamics results to learning scenarios. We extend the well-known way in which exponential families characterize thermodynamic and learning equilibria. Basic ideas of work and heat, and advanced concepts of thermodynamic cycles and equipartition of energy, find exact and useful counterparts in AI / statistics terms. These ideas have broad implications for quantifying and addressing distribution shift.	 | 我们以通用指数族的形式发展热力学定律。通过将学习（对数损失最小化）问题重新表述为最大熵和统计力学的形式，我们将热力学结果转化为学习场景。我们扩展了指数族以表征热力学和学习平衡方式的经典方法。关于功和热的基本概念，以及热力学循环和能量均分等高级概念，在AI / 统计术语中找到了精确且实用的对应。这些概念在量化和应对分布偏移方面具有广泛的应用意义。
2501.02045	 | METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring	 | Ollie Liu,Sami Jaghouar,Johannes Hagemann,Shangshang Wang,Jason Wiemels,Jeff Kaufman,Willie Neiswanger	 | 研究人员开发了一种名为METAGENE-1的70亿参数自回归变压器模型，该模型在大规模人类废水中处理的DNA和RNA序列数据上预训练，旨在捕捉整个宏基因组信息，以支持传染病监测和病原体检测任务，并在多个基准测试中达到了最先进的性能。	 | We pretrain METAGENE-1, a 7-billion-parameter autoregressive transformer model, which we refer to as a metagenomic foundation model, on a novel corpus of diverse metagenomic DNA and RNA sequences comprising over 1.5 trillion base pairs. This dataset is sourced from a large collection of human wastewater samples, processed and sequenced using deep metagenomic (next-generation) sequencing methods. Unlike genomic models that focus on individual genomes or curated sets of specific species, the aim of METAGENE-1 is to capture the full distribution of genomic information present within this wastewater, to aid in tasks relevant to pandemic monitoring and pathogen detection. We carry out byte-pair encoding (BPE) tokenization on our dataset, tailored for metagenomic sequences, and then pretrain our model. In this paper, we first detail the pretraining dataset, tokenization strategy, and model architecture, highlighting the considerations and design choices that enable the effective modeling of metagenomic data. We then show results of pretraining this model on our metagenomic dataset, providing details about our losses, system metrics, and training stability over the course of pretraining. Finally, we demonstrate the performance of METAGENE-1, which achieves state-of-the-art results on a set of genomic benchmarks and new evaluations focused on human-pathogen detection and genomic sequence embedding, showcasing its potential for public health applications in pandemic monitoring, biosurveillance, and early detection of emerging health threats.	 | 我们预先训练了一个名为METAGENE-1的70亿参数自回归变压器模型，我们将其称为宏基因组基础模型，该模型是在一个新颖的宏基因组DNA和RNA序列数据集上训练的，该数据集包含超过1.5万亿个碱基对。这些数据集来源于大量的人类废水样本，通过深度宏基因组（下一代）测序方法进行了处理和测序。与专注于单一基因组或特定物种的标准化集合的基因组模型不同，METAGENE-1的目标是捕捉这些废水中存在的全部基因组信息，以辅助与传染病监测和病原体检测相关任务。我们对数据集进行了字节对编码（BPE）标记化，特别针对宏基因组序列进行了定制，并然后预训练了我们的模型。在本文中，我们首先详细介绍了预训练数据集、标记化策略和模型架构，突出了使宏基因组数据建模有效的考虑和设计选择。然后，我们展示了在我们的宏基因组数据集上预训练该模型的结果，提供了损失函数、系统指标和预训练过程中训练稳定性的详细信息。最后，我们展示了METAGENE-1的表现，该模型在一组基因组基准测试和新的关注人类-病原体检测及基因组序列嵌入的评估中达到了最先进的结果，展示了其在公共卫生领域应用于传染病监测、生物监视以及对新兴健康威胁的早期检测方面的潜力。
2501.02043	 | Modeling COVID-19 spread in the USA using metapopulation SIR models coupled with graph convolutional neural networks	 | Petr Kisselev,Padmanabhan Seshaiyer	 | 该研究将混合GCN-SIR方法应用于美国大陆的数据，通过调整移动模式和政策反应，提高了再生数的实时连续估计和预测准确性，并探讨了该方法在疾病动力学建模中的优势和局限性。	 | Graph convolutional neural networks (GCNs) have shown tremendous promise in addressing data-intensive challenges in recent years. In particular, some attempts have been made to improve predictions of Susceptible-Infected-Recovered (SIR) models by incorporating human mobility between metapopulations and using graph approaches to estimate corresponding hyperparameters. Recently, researchers have found that a hybrid GCN-SIR approach outperformed existing methodologies when used on the data collected on a precinct level in Japan. In our work, we extend this approach to data collected from the continental US, adjusting for the differing mobility patterns and varying policy responses. We also develop the strategy for real-time continuous estimation of the reproduction number and study the accuracy of model predictions for the overall population as well as individual states. Strengths and limitations of the GCN-SIR approach are discussed as a potential candidate for modeling disease dynamics.	 | 近年来，图卷积神经网络（GCNs）在应对数据密集型挑战方面展现出了巨大的潜力。特别是在改进SIR（易感-感染-恢复）模型的预测方面，一些研究人员尝试通过整合元群体间的人类移动性，并利用图方法估计相应的超参数来提高预测精度。最近，研究发现，在日本市辖区级别的数据上使用混合GCN-SIR方法时，这种方法优于现有方法。在我们的工作中，我们将这种方法扩展到从美国大陆收集的数据上，调整了不同的移动模式和政策反应。我们还开发了一种实时连续估计再生数的策略，并研究了模型对总体人口以及各州的预测准确性。我们讨论了GCN-SIR方法的优势和局限性，将其视为建模疾病动力学的潜在候选方法。
2501.02026	 | Recursive Decomposition of Logical Thoughts: Framework for Superior Reasoning and Knowledge Propagation in Large Language Models	 | Kaleem Ullah Qasim,Jiashu Zhang,Tariq Alsahfi,Ateeq Ur Rehman Butt	 | RDoLT（递归分解逻辑思考提示）通过递归分解复杂任务、优化推理选择机制并集成知识传播模块，显著提升了大型语言模型的推理能力，在多个基准测试中展现出了优越性能，特别是在GSM8K测试中将准确率提升了6.28%至90.98%。	 | Enhancing the reasoning capabilities of Large Language Models remains a critical challenge in artificial intelligence. We introduce RDoLT, Recursive Decomposition of Logical Thought prompting, a novel framework that significantly boosts LLM reasoning performance. RDoLT is built on three key innovations: (1) recursively breaking down complex reasoning tasks into sub-tasks of progressive complexity; (2) employing an advanced selection and scoring mechanism to identify the most promising reasoning thoughts; and (3) integrating a knowledge propagation module that mimics human learning by keeping track of strong and weak thoughts for information propagation. Our approach was evaluated across multiple benchmarks, including GSM8K, SVAMP, MultiArith, LastLetterConcatenation, and Gaokao2023 Math. The results demonstrate that RDoLT consistently outperforms existing state-of-the-art techniques, achieving a 90.98 percent accuracy on GSM8K with ChatGPT-4, surpassing state-of-the-art techniques by 6.28 percent. Similar improvements were observed on other benchmarks, with accuracy gains ranging from 5.5 percent to 6.75 percent. These findings highlight RDoLT's potential to advance prompt engineering, offering a more effective and generalizable approach to complex reasoning tasks.	 | 增强大型语言模型的推理能力仍然是人工智能领域的关键挑战。我们提出了RDoLT（递归分解逻辑思维提示）这一新颖框架，显著提升了大型语言模型的推理性能。RDoLT 基于三大创新：（1）递归地将复杂的推理任务分解为逐级复杂性的子任务；（2）采用先进的选择和评分机制来识别最有前景的推理思考；（3）整合一个知识传播模块，该模块模仿人类学习，跟踪强弱思维以促进信息传播。我们的方法在多个基准测试中进行了评估，包括GSM8K、SVAMP、MultiArith、LastLetterConcatenation 和 Gaokao2023 数学。结果显示，RDoLT 一致优于现有最先进的技术，在使用ChatGPT-4进行GSM8K测试时，准确率达到90.98%，比最先进的技术高出6.28%。在其他基准测试中也观察到类似改进，准确率提高了5.5%至6.75%。这些发现突显了RDoLT 在促进提示工程方面的潜力，提供了一种更有效且通用的方法来处理复杂的推理任务。
2501.02018	 | Safeguarding Large Language Models in Real-time with Tunable Safety-Performance Trade-offs	 | Joao Fonseca,Andrew Bell,Julia Stoyanovich	 | SafeNudge是一种新颖的保障方法，通过结合受控文本生成与“引导”，在执行越狱攻击时减少成功概率30%，同时对推理延迟和输出语义流畅性影响较小，允许调整安全与性能之间的权衡。	 | Large Language Models (LLMs) have been shown to be susceptible to jailbreak attacks, or adversarial attacks used to illicit high risk behavior from a model. Jailbreaks have been exploited by cybercriminals and blackhat actors to cause significant harm, highlighting the critical need to safeguard widely-deployed models. Safeguarding approaches, which include fine-tuning models or having LLMs "self-reflect", may lengthen the inference time of a model, incur a computational penalty, reduce the semantic fluency of an output, and restrict ``normal'' model behavior. Importantly, these Safety-Performance Trade-offs (SPTs) remain an understudied area. In this work, we introduce a novel safeguard, called SafeNudge, that combines Controlled Text Generation with "nudging", or using text interventions to change the behavior of a model. SafeNudge triggers during text-generation while a jailbreak attack is being executed, and can reduce successful jailbreak attempts by 30% by guiding the LLM towards a safe responses. It adds minimal latency to inference and has a negligible impact on the semantic fluency of outputs. Further, we allow for tunable SPTs. SafeNudge is open-source and available through https://pypi.org/, and is compatible with models loaded with the Hugging Face "transformers" library.	 | 大型语言模型（LLMs）已被证明容易受到“越狱”攻击，即利用对抗性攻击促使模型执行高风险行为。越狱已被网络犯罪分子和黑客利用，造成了重大危害，突显了保护广泛部署的模型的迫切需求。保障措施，包括对模型进行微调或让LLMs进行“自我反思”，可能会延长模型的推理时间，增加计算成本，降低输出的语义流畅性，并限制“正常”的模型行为。重要的是，这些安全-性能权衡（SPTs）仍是一个研究不足的领域。在这项工作中，我们介绍了一种名为SafeNudge的新颖保障方法，该方法结合了受控文本生成与“引导”，即通过使用文本干预来改变模型的行为。SafeNudge在执行越狱攻击时触发，在生成文本期间发挥作用，可将成功的越狱尝试减少30%，通过引导LLM产生安全的响应。它对推理延迟的影响很小，并且对输出的语义流畅性几乎没有影响。此外，SafeNudge允许调整安全-性能权衡。SafeNudge开源并通过https://pypi.org/提供，并且可以与通过Hugging Face“transformers”库加载的模型兼容。
2501.01999	 | On the Utility of Equivariance and Symmetry Breaking in Deep Learning Architectures on Point Clouds	 | Sharvaree Vadgama,Mohammad Mohaiminul Islam,Domas Buracus,Christian Shewmake,Erik Bekkers	 | 本文探讨了影响点云模型性能的关键因素，特别是在不同几何复杂度任务中的对称层的灵活性与权重共享之间的权衡，并通过多种数据集的基准测试发现，对称性在提高模型性能方面具有积极作用，特别是在复杂任务中更为显著。	 | This paper explores the key factors that influence the performance of models working with point clouds, across different tasks of varying geometric complexity. In this work, we explore the trade-offs between flexibility and weight-sharing introduced by equivariant layers, assessing when equivariance boosts or detracts from performance. It is often argued that providing more information as input improves a model's performance. However, if this additional information breaks certain properties, such as $\SE(3)$ equivariance, does it remain beneficial? We identify the key aspects of equivariant and non-equivariant architectures that drive success in different tasks by benchmarking them on segmentation, regression, and generation tasks across multiple datasets with increasing complexity. We observe a positive impact of equivariance, which becomes more pronounced with increasing task complexity, even when strict equivariance is not required.	 | 本文探讨了影响点云模型性能的关键因素，这些因素覆盖了不同几何复杂度的任务。本文研究了由对称层引入的灵活性与权重共享之间的权衡，评估了对称性是提升还是削弱性能的情况。通常认为，提供更多的输入信息可以改善模型的性能。然而，如果这种额外信息破坏了某些属性，如$\SE(3)$对称性，它是否仍然有益？我们通过在多个数据集上对分割、回归和生成任务进行基准测试，确定了在不同任务中推动成功的关键方面，这些数据集的复杂度逐步增加。我们观察到对称性具有积极影响，随着任务复杂度的增加，这种影响变得更加显著，即使不需要严格的对称性。
2501.01993	 | A Novel Convolution and Attention Mechanism-based Model for 6D Object Pose Estimation	 | Alexander Du,Yingwu Zhu	 | 该研究提出了一种基于图结构和注意力机制的新方法，直接从RGB图像中估计6D物体姿态，通过节点连接性和空间交互定义特征关系，并利用Legendre卷积层提高数值稳定性，实验结果在多个数据集上优于现有方法，达到了最先进的性能。	 | Estimating 6D object poses from RGB images is challenging because the lack of depth information requires inferring a three dimensional structure from 2D projections. Traditional methods often rely on deep learning with grid based data structures but struggle to capture complex dependencies among extracted features. To overcome this, we introduce a graph based representation derived directly from images, where spatial temporal features of each pixel serve as nodes, and relationships between them are defined through node connectivity and spatial interactions. We also employ feature selection mechanisms that use spatial attention and self attention distillation, along with a Legendre convolution layer leveraging the orthogonality of Legendre polynomials for numerical stability. Experiments on the LINEMOD, Occluded LINEMOD, and YCB Video datasets demonstrate that our method outperforms nine existing approaches and achieves state of the art benchmark in object pose estimation.	 | 从RGB图像估计6D物体姿态具有挑战性，因为缺乏深度信息要求从2D投影中推断三维结构。传统方法通常依赖基于网格的数据结构的深度学习，但在捕捉提取特征间的复杂依赖关系方面存在困难。为克服这一问题，我们提出了一种从图像直接获得的图基表示，其中每个像素的空间时域特征作为节点，节点之间的关系通过节点连接性和空间交互定义。我们还采用了使用空间注意和自我注意蒸馏进行特征选择的机制，并利用Legendre卷积层结合Legendre多项式的正交性以提高数值稳定性。在LINEMOD、Occluded LINEMOD和YCB Video数据集上的实验表明，我们的方法在现有九种方法中表现更优，并在物体姿态估计方面达到了最先进的基准。
2501.01987	 | Gender Bias in Text-to-Video Generation Models: A case study of Sora	 | Mohammad Nadeem,Shahab Saquib Sohail,Erik Cambria,Björn W. Schuller,Amir Hussain	 | 研究发现，OpenAI的Sora文本到视频生成模型存在性别偏见，该模型不均衡地将特定性别与刻板印象中的行为和职业联系起来，反映出其训练数据中的社会偏见。	 | The advent of text-to-video generation models has revolutionized content creation as it produces high-quality videos from textual prompts. However, concerns regarding inherent biases in such models have prompted scrutiny, particularly regarding gender representation. Our study investigates the presence of gender bias in OpenAI's Sora, a state-of-the-art text-to-video generation model. We uncover significant evidence of bias by analyzing the generated videos from a diverse set of gender-neutral and stereotypical prompts. The results indicate that Sora disproportionately associates specific genders with stereotypical behaviors and professions, which reflects societal prejudices embedded in its training data.	 | 文本到视频生成模型的出现已经彻底改变了内容创作的方式，因为它可以根据文本提示生成高质量的视频。然而，这些模型中固有偏见的问题引起了关注，特别是在性别代表方面。我们的研究探讨了OpenAI的Sora（一种最先进的文本到视频生成模型）中性别偏见的存在。我们通过分析从各种性别中立和刻板印象提示生成的视频，发现了明显的偏见证据。结果表明，Sora 不均衡地将特定性别与刻板印象中的行为和职业联系起来，这反映了其训练数据中嵌入的社会偏见。
2501.01974	 | Hawkes based Representation Learning for Reasoning over Scale-free Community-structured Temporal Knowledge Graphs	 | Yuwei Du,Xinyue Liu,Wenxin Liang,Linlin Zong,Xianchao Zhang	 | 本文提出了一种名为HERLN的新模型，该模型利用基于霍克斯过程的关系图卷积网络和条件解码方法，同时考虑TKG的社区结构、无标度分布和时间衰减特性，从而有效进行TKG推理，并在实验中取得了显著的性能提升。	 | Temporal knowledge graph (TKG) reasoning has become a hot topic due to its great value in many practical tasks. The key to TKG reasoning is modeling the structural information and evolutional patterns of the TKGs. While great efforts have been devoted to TKG reasoning, the structural and evolutional characteristics of real-world networks have not been considered. In the aspect of structure, real-world networks usually exhibit clear community structure and scale-free (long-tailed distribution) properties. In the aspect of evolution, the impact of an event decays with the time elapsing. In this paper, we propose a novel TKG reasoning model called Hawkes process-based Evolutional Representation Learning Network (HERLN), which learns structural information and evolutional patterns of a TKG simultaneously, considering the characteristics of real-world networks: community structure, scale-free and temporal decaying. First, we find communities in the input TKG to make the encoding get more similar intra-community embeddings. Second, we design a Hawkes process-based relational graph convolutional network to cope with the event impact-decaying phenomenon. Third, we design a conditional decoding method to alleviate biases towards frequent entities caused by long-tailed distribution. Experimental results show that HERLN achieves significant improvements over the state-of-the-art models.	 | 时序知识图谱（Temporal Knowledge Graph, TKG）推理成为热门话题，原因是其在许多实际任务中具有巨大的价值。TKG推理的关键在于建模TKG的结构性信息及其演化模式。尽管在TKG推理方面投入了大量努力，但现实世界网络的结构性和演化特性仍未得到充分考虑。在结构方面，现实世界网络通常表现出明显的社区结构和长尾分布（无标度）特性。在演化方面，事件的影响会随时间的推移而减弱。本文提出了一种名为基于霍克斯过程的演化表示学习网络（Hawkes Process-based Evolutional Representation Learning Network, HERLN）的新颖TKG推理模型，该模型同时学习TKG的结构性信息和演化模式，考虑到现实世界网络的特点：社区结构、无标度和时间衰减。首先，我们通过在输入TKG中找到社区来使编码获得更相似的社区嵌入。其次，我们设计了一个基于霍克斯过程的关系图卷积网络来应对事件影响减弱的现象。第三，我们设计了一种条件解码方法来缓解由长尾分布引起的对频繁实体的偏好。实验结果表明，HERLN在与现有最佳模型相比时，实现了显著的改进。
2501.01969	 | Optimal bounds for dissatisfaction in perpetual voting	 | Alexander Kozachinskiy,Alexander Shen,Tomasz Steifer	 | 本文探讨了一种可持续赞成投票方法，确保选民不会因不满意而感到过于频繁，并利用科莫戈罗夫复杂性技术证明了在“有界冲突”条件下不满意感的增长可以是亚线性的。此外，通过将二元选择的赞成投票与机器学习中的专家建议预测相类比，提出了一种新的投票方法，该方法在有界冲突条件下能保持不满增长的亚线性。	 | In perpetual voting, multiple decisions are made at different moments in time. Taking the history of previous decisions into account allows us to satisfy properties such as proportionality over periods of time. In this paper, we consider the following question: is there a perpetual approval voting method that guarantees that no voter is dissatisfied too many times? We identify a sufficient condition on voter behavior -- which we call 'bounded conflicts' condition -- under which a sublinear growth of dissatisfaction is possible. We provide a tight upper bound on the growth of dissatisfaction under bounded conflicts, using techniques from Kolmogorov complexity. We also observe that the approval voting with binary choices mimics the machine learning setting of prediction with expert advice. This allows us to present a voting method with sublinear guarantees on dissatisfaction under bounded conflicts, based on the standard techniques from prediction with expert advice.	 | 在永续投票中，多个决策会在不同的时间点上作出。将先前决策的历史考虑在内，可以使我们满足时间周期内的比例性等性质。在本文中，我们考虑了以下问题：是否存在一种永续赞成投票方法，可以保证没有选民会因不满意而感到太多次？我们确定了一个选民行为的充分条件——我们称之为“有界冲突”条件——在该条件下，不满意感的增长可以是亚线性的。我们使用科莫戈罗夫复杂性技术，提供了在有界冲突条件下不满意感增长的紧上界。我们还观察到，二元选择的赞成投票类似于机器学习中的专家建议预测设置。这使我们能够基于专家建议预测的标准技术，提出一种在有界冲突条件下不满意感增长亚线性的投票方法。
2501.01960	 | GAF-FusionNet: Multimodal ECG Analysis via Gramian Angular Fields and Split Attention	 | Jiahao Qin,Feng Liu	 | 本文介绍了一种名为GAF-FusionNet的新多模态框架，结合时间序列分析与Gramian Angular Fields（GAF）表示，用于ECG分类，并在三个不同ECG数据集上取得了优于现有最佳方法的分类准确率，分别为94.5%、96.9%和99.6%。	 | Electrocardiogram (ECG) analysis plays a crucial role in diagnosing cardiovascular diseases, but accurate interpretation of these complex signals remains challenging. This paper introduces a novel multimodal framework(GAF-FusionNet) for ECG classification that integrates time-series analysis with image-based representation using Gramian Angular Fields (GAF). Our approach employs a dual-layer cross-channel split attention module to adaptively fuse temporal and spatial features, enabling nuanced integration of complementary information. We evaluate GAF-FusionNet on three diverse ECG datasets: ECG200, ECG5000, and the MIT-BIH Arrhythmia Database. Results demonstrate significant improvements over state-of-the-art methods, with our model achieving 94.5\%, 96.9\%, and 99.6\% accuracy on the respective datasets. Our code will soon be available at https://github.com/Cross-Innovation-Lab/GAF-FusionNet.git.	 | 心电图（ECG）分析在诊断心血管疾病中起着至关重要的作用，但准确解读这些复杂的信号仍然具有挑战性。本文介绍了一种新的多模态框架（GAF-FusionNet），该框架将时间序列分析与基于图像的表示（Gramian Angular Fields，GAF）相结合，用于ECG分类。我们的方法采用了双层跨通道分割注意力模块，以自适应地融合时序和空间特征，从而使互补信息的整合更加细微。我们使用GAF-FusionNet在三个不同的ECG数据集上进行了评估：ECG200、ECG5000和MIT-BIH心律失常数据库。结果表明，与现有最佳方法相比，我们的模型在各个数据集上分别实现了94.5%，96.9%，和99.6%的准确率。我们的代码很快将在https://github.com/Cross-Innovation-Lab/GAF-FusionNet.git公开。
2412.19784	 | Can AI Help with Your Personal Finances?	 | Oudom Hean,Utsha Saha,Binita Saha	 | 研究探讨了大型语言模型（LLMs）在个人金融领域的应用潜力，评估了包括ChatGPT、Gemini、Claude和Llama在内的几种模型在提供财务建议方面的有效性，发现其平均准确率为70%，但仍存在处理复杂查询和不同主题表现不均的局限性。	 | In recent years, Large Language Models (LLMs) have emerged as a transformative development in artificial intelligence (AI), drawing significant attention from industry and academia. Trained on vast datasets, these sophisticated AI systems exhibit impressive natural language processing and content generation capabilities. This paper explores the potential of LLMs to address key challenges in personal finance, focusing on the United States. We evaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini, Anthropic's Claude, and Meta's Llama, to assess their effectiveness in providing accurate financial advice on topics such as mortgages, taxes, loans, and investments. Our findings show that while these models achieve an average accuracy rate of approximately 70%, they also display notable limitations in certain areas. Specifically, LLMs struggle to provide accurate responses for complex financial queries, with performance varying significantly across different topics. Despite these limitations, the analysis reveals notable improvements in newer versions of these models, highlighting their growing utility for individuals and financial advisors. As these AI systems continue to evolve, their potential for advancing AI-driven applications in personal finance becomes increasingly promising.	 | 近年来，大型语言模型（LLMs）在人工智能（AI）领域崭露头角，引起了行业和学术界的广泛关注。这些模型通过训练大量的数据集，展示了令人印象深刻的自然语言处理和内容生成能力。本文探讨了LLMs在个人金融领域解决关键挑战的可能性，重点关注美国市场。我们评估了几种领先的LLMs，包括OpenAI的ChatGPT、Google的Gemini、Anthropic的Claude以及Meta的Llama，以评估它们在提供有关抵押贷款、税收、贷款和投资等方面的准确财务建议方面的有效性。我们的研究发现，这些模型的平均准确率为约70%，但它们在某些领域也显示出明显的局限性。具体而言，LLMs在处理复杂的财务查询时表现不佳，不同主题的表现差异很大。尽管存在这些局限性，分析显示这些模型的新版本在某些方面取得了显著改进，突显了它们在个人和金融顾问方面的日益重要的应用价值。随着这些AI系统的不断进化，它们在个人金融领域推动AI驱动应用的潜力变得越来越有前景。
2412.14663	 | IOHunter: Graph Foundation Model to Uncover Online Information Operations	 | Marco Minici,Luca Luceri,Francesco Fabbri,Emilio Ferrara	 | 该研究介绍了一种名为IOHunter的方法论，利用语言模型和图神经网络结合的优势，用于识别社交媒体平台上策划信息操作的“IO驱动者”，并在多个数据集上实现了最先进的性能。这项工作对保护在线讨论的完整性具有重要意义。	 | Social media platforms have become vital spaces for public discourse, serving as modern agorás where a wide range of voices influence societal narratives. However, their open nature also makes them vulnerable to exploitation by malicious actors, including state-sponsored entities, who can conduct information operations (IOs) to manipulate public opinion. The spread of misinformation, false news, and misleading claims threatens democratic processes and societal cohesion, making it crucial to develop methods for the timely detection of inauthentic activity to protect the integrity of online discourse. In this work, we introduce a methodology designed to identify users orchestrating information operations, a.k.a. \textit{IO drivers}, across various influence campaigns. Our framework, named \texttt{IOHunter}, leverages the combined strengths of Language Models and Graph Neural Networks to improve generalization in \emph{supervised}, \emph{scarcely-supervised}, and \emph{cross-IO} contexts. Our approach achieves state-of-the-art performance across multiple sets of IOs originating from six countries, significantly surpassing existing approaches. This research marks a step toward developing Graph Foundation Models specifically tailored for the task of IO detection on social media platforms.	 | 社交媒体平台已成为公共讨论的重要空间，作为现代论坛，广泛的声音影响着社会叙事。然而，它们的开放性质也使其容易被恶意行为者利用，包括国家支持的实体，他们可以进行信息操作（IOs）来操控公众舆论。错误信息的传播、假新闻和误导性声明威胁着民主进程和社会凝聚力，因此，及时检测不实活动的方法对于保护在线讨论的完整性至关重要。在这项工作中，我们介绍了一种方法论，用于识别在各种影响活动中策划信息操作的用户，即所谓的“IO驱动者”。我们的框架名为IOHunter，利用语言模型和图神经网络的结合优势，在监督学习、少监督学习和跨信息操作（cross-IO）背景下提高泛化能力。我们的方法在源自六个不同国家的多个信息操作数据集上实现了最先进的性能，显著超越了现有方法。这项研究标志着为社交媒体平台上的信息操作检测任务开发图基础模型的一个步骤。
2411.08447	 | Learning Dynamic Cognitive Map with Autonomous Navigation	 | Daria de Tinguy,Tim Verbelen,Bart Dhoedt	 | 该研究受动物导航策略启发，提出了一种通过在Active Inference框架下动态扩展认知地图并结合预测姿势的计算模型，展示了在mini-grid环境中高效学习和导航复杂环境的能力，优于具有相似目标的Clone-Structured Cognitive Graph模型。	 | Inspired by animal navigation strategies, we introduce a novel computational model to navigate and map a space rooted in biologically inspired principles. Animals exhibit extraordinary navigation prowess, harnessing memory, imagination, and strategic decision-making to traverse complex and aliased environments adeptly. Our model aims to replicate these capabilities by incorporating a dynamically expanding cognitive map over predicted poses within an Active Inference framework, enhancing our agent's generative model plasticity to novelty and environmental changes. Through structure learning and active inference navigation, our model demonstrates efficient exploration and exploitation, dynamically expanding its model capacity in response to anticipated novel un-visited locations and updating the map given new evidence contradicting previous beliefs. Comparative analyses in mini-grid environments with the Clone-Structured Cognitive Graph model (CSCG), which shares similar objectives, highlight our model's ability to rapidly learn environmental structures within a single episode, with minimal navigation overlap. Our model achieves this without prior knowledge of observation and world dimensions, underscoring its robustness and efficacy in navigating intricate environments.	 | 受动物导航策略的启发，我们提出了一种新的计算模型，该模型在生物启发原理的基础上导航和映射空间。动物在导航方面表现出非凡的能力，利用记忆、想象和战略决策来熟练地穿越复杂且具欺骗性的环境。我们的模型旨在通过在Active Inference框架内动态扩展认知地图，以及将预测姿势纳入其中，来复制这些能力，从而增强我们的代理对新颖性和环境变化的生成模型的适应性。通过结构学习和主动推理导航，我们的模型展示了高效的探索和利用，能够根据预期的新未探索位置动态扩展其模型容量，并在新证据反驳先前信念时更新地图。在mini-grid环境中，与具有相似目标的Clone-Structured Cognitive Graph模型（CSCG）进行比较分析，突显了我们的模型能够在单个回合中快速学习环境结构的能力，且导航重叠极少。我们的模型无需先验的知识即可完成观察和世界维度的导航，这突显了其在导航复杂环境中的稳健性和有效性。
