| ID | Title | Authors | Summary (zh) | Abstract (en) | Abstract (zh) | 
| --- | --- | --- | --- | --- | --- |
2501.09004	 | Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails	 | Shaona Ghosh,Prasoon Varshney,Makesh Narsimhan Sreedhar,Aishwarya Padmakumar,Traian Rebedea,Jibin Rajan Varghese,Christopher Parisien	 | 该研究提出了一种全面的分类体系，将大型语言模型的安全风险分为12个顶级危害类别和9个子类别，并通过结合人类标注和多LLM系统评估生成了Aegis 2.0数据集，展示了其在安全模型性能上的有效性，并引入了新的训练混合方法以增强模型适应性。	 | As Large Language Models (LLMs) and generative AI become increasingly widespread, concerns about content safety have grown in parallel. Currently, there is a clear lack of high-quality, human-annotated datasets that address the full spectrum of LLM-related safety risks and are usable for commercial applications. To bridge this gap, we propose a comprehensive and adaptable taxonomy for categorizing safety risks, structured into 12 top-level hazard categories with an extension to 9 fine-grained subcategories. This taxonomy is designed to meet the diverse requirements of downstream users, offering more granular and flexible tools for managing various risk types. Using a hybrid data generation pipeline that combines human annotations with a multi-LLM "jury" system to assess the safety of responses, we obtain Aegis 2.0, a carefully curated collection of 34,248 samples of human-LLM interactions, annotated according to our proposed taxonomy. To validate its effectiveness, we demonstrate that several lightweight models, trained using parameter-efficient techniques on Aegis 2.0, achieve performance competitive with leading safety models fully fine-tuned on much larger, non-commercial datasets. In addition, we introduce a novel training blend that combines safety with topic following data.This approach enhances the adaptability of guard models, enabling them to generalize to new risk categories defined during inference. We plan to open-source Aegis 2.0 data and models to the research community to aid in the safety guardrailing of LLMs.	 | 随着大型语言模型（LLMs）和生成式AI的广泛应用，内容安全性问题也日益引起关注。目前，缺乏全面覆盖LLM相关安全风险且适用于商业应用的高质量、人工标注的数据集。为解决这一问题，我们提出了一种全面且可扩展的分类体系，将安全风险分类为12个顶级危害类别，并进一步细分为9个子类别。该分类体系旨在满足下游用户多样化的需求，提供更细粒度和灵活的工具来管理各种风险类型。通过结合人类标注和多LLM “陪审团”系统评估响应的安全性来生成混合数据生成管道，我们获得了Aegis 2.0数据集，该数据集包含34,248个人-LLM交互样本，并按照我们提出的分类体系进行了标注。为了验证其有效性，我们展示了使用参数高效技术训练的几个轻量级模型在Aegis 2.0数据集上达到与在更大规模非商业数据集上完全微调的安全模型相当的性能。此外，我们还引入了一种新的训练混合方法，结合了安全性和主题跟随数据。这种方法增强了防护模型的适应性，使其能够泛化到推理过程中定义的新风险类别。我们计划将Aegis 2.0数据集和模型开源给研究社区，以帮助LLM的安全管理。
2501.08985	 | Personality Modeling for Persuasion of Misinformation using AI Agent	 | Qianmin Lou,Wentao Xu	 | 本研究通过代理基于模型探讨了大五人格特质如何影响个体对虚假信息的易感性和传播，发现具有分析性和批判性人格特质的个体更能抵御虚假信息，而非侵略性的说服策略在纠正错误信息方面也表现出色。研究结果强调了在数字环境中开发干预措施时应注重情感连接和信任建设。	 | The proliferation of misinformation on social media platforms has highlighted the need to understand how individual personality traits influence susceptibility to and propagation of misinformation. This study employs an innovative agent-based modeling approach to investigate the relationship between personality traits and misinformation dynamics. Using six AI agents embodying different dimensions of the Big Five personality traits (Extraversion, Agreeableness, and Neuroticism), we simulated interactions across six diverse misinformation topics. The experiment, implemented through the AgentScope framework using the GLM-4-Flash model, generated 90 unique interactions, revealing complex patterns in how personality combinations affect persuasion and resistance to misinformation. Our findings demonstrate that analytical and critical personality traits enhance effectiveness in evidence-based discussions, while non-aggressive persuasion strategies show unexpected success in misinformation correction. Notably, agents with critical traits achieved a 59.4% success rate in HIV-related misinformation discussions, while those employing non-aggressive approaches maintained consistent persuasion rates above 40% across different personality combinations. The study also revealed a non-transitive pattern in persuasion effectiveness, challenging conventional assumptions about personality-based influence. These results provide crucial insights for developing personality-aware interventions in digital environments and suggest that effective misinformation countermeasures should prioritize emotional connection and trust-building over confrontational approaches. The findings contribute to both theoretical understanding of personality-misinformation dynamics and practical strategies for combating misinformation in social media contexts.	 | 社交媒体平台上虚假信息的泛滥凸显了理解个体人格特质如何影响其对虚假信息的易感性和传播的重要性。本研究采用创新的基于代理的建模方法，探讨人格特质与虚假信息动态之间的关系。通过六个代表大五人格特质不同维度的AI代理（外向性、宜人性和神经质），我们模拟了在六个不同虚假信息主题上的相互作用。实验通过基于AgentScope框架的GLM-4-Flash模型实施，生成了90种独特互动，揭示了人格组合如何影响说服力和对虚假信息的抵抗力的复杂模式。研究结果表明，分析性和批判性的人格特质增强了基于证据讨论的有效性，而非侵略性的说服策略在纠正虚假信息方面表现出意想不到的成功。值得注意的是，具有批判性特质的代理在与艾滋病相关的虚假信息讨论中实现了59.4%的成功率，而使用非侵略性方法的代理在不同人格组合中保持了40%以上的持续说服率。研究还揭示了说服力效果的非传递性模式，挑战了基于人格影响的传统假设。这些结果为在数字环境中开发人格意识干预措施提供了关键见解，并建议有效的虚假信息对策应优先考虑情感连接和信任建设，而非对抗性方法。本研究不仅增加了对人格与虚假信息动态的理解，还为社交媒体背景下打击虚假信息提供了实用策略。
2501.08974	 | Learning to Extract Cross-Domain Aspects and Understanding Sentiments Using Large Language Models	 | Karukriti Kaushik Ghosh,Chiranjib Sur	 | 基于方面的情感分析（ASBA）能够将文本分解为具体方面并分别评估其情感，提供更细致的顾客意见理解，帮助企业识别改进方向并做出数据驱动的决策。研究发现大型语言模型在跨领域的ASBA任务上表现良好，特别是在SemEval-2015任务12中的准确率达到92%。	 | Aspect-based sentiment analysis (ASBA) is a refined approach to sentiment analysis that aims to extract and classify sentiments based on specific aspects or features of a product, service, or entity. Unlike traditional sentiment analysis, which assigns a general sentiment score to entire reviews or texts, ABSA focuses on breaking down the text into individual components or aspects (e.g., quality, price, service) and evaluating the sentiment towards each. This allows for a more granular level of understanding of customer opinions, enabling businesses to pinpoint specific areas of strength and improvement. The process involves several key steps, including aspect extraction, sentiment classification, and aspect-level sentiment aggregation for a review paragraph or any other form that the users have provided. ABSA has significant applications in areas such as product reviews, social media monitoring, customer feedback analysis, and market research. By leveraging techniques from natural language processing (NLP) and machine learning, ABSA facilitates the extraction of valuable insights, enabling companies to make data-driven decisions that enhance customer satisfaction and optimize offerings. As ABSA evolves, it holds the potential to greatly improve personalized customer experiences by providing a deeper understanding of sentiment across various product aspects. In this work, we have analyzed the strength of LLMs for a complete cross-domain aspect-based sentiment analysis with the aim of defining the framework for certain products and using it for other similar situations. We argue that it is possible to that at an effectiveness of 92\% accuracy for the Aspect Based Sentiment Analysis dataset of SemEval-2015 Task 12.	 | 基于方面的情感分析（Aspect-Based Sentiment Analysis, ASBA）是一种精细化的情感分析方法，旨在根据产品、服务或实体的具体方面或特征提取和分类情感。与传统的情感分析不同，后者通常为整个评论或文本赋予一个通用的情感评分，ASBA则侧重于将文本分解为各个组成部分或方面（例如，质量、价格、服务），并分别评估每个方面的正面或负面情绪。这种方法可以提供更细致的顾客意见理解层次，使企业能够识别出具体的优势领域和改进方向。  ASBA的过程包括几个关键步骤，如方面提取、情感分类以及针对评论段落或其他用户提供的任何形式的方面级情感聚合。ASBA在产品评论、社交媒体监控、客户反馈分析和市场研究等领域具有广泛应用。通过结合自然语言处理（NLP）和机器学习技术，ASBA能够提取有价值的见解，帮助企业做出数据驱动的决策，从而提高客户满意度并优化产品。随着ASBA的发展，它有可能通过提供对不同产品方面情感的更深入理解来极大地改善个性化客户服务体验。  在本工作中，我们分析了大型语言模型（LLMs）在跨领域方面进行情感分析方面的表现，并旨在定义特定产品的框架，以便应用于类似情况。我们主张，ASBA数据集在SemEval-2015任务12中的准确率可以达到92%的有效性。
2501.08946	 | Applying General Turn-taking Models to Conversational Human-Robot Interaction	 | Gabriel Skantze,Bahar Irfan	 | 本文研究了将通用轮流对话模型TurnGPT和语音活动投影（VAP）应用于人机交互（HRI）系统，以改善对话动态，减少响应延迟和打断，实验结果表明参与者更偏好该系统。	 | Turn-taking is a fundamental aspect of conversation, but current Human-Robot Interaction (HRI) systems often rely on simplistic, silence-based models, leading to unnatural pauses and interruptions. This paper investigates, for the first time, the application of general turn-taking models, specifically TurnGPT and Voice Activity Projection (VAP), to improve conversational dynamics in HRI. These models are trained on human-human dialogue data using self-supervised learning objectives, without requiring domain-specific fine-tuning. We propose methods for using these models in tandem to predict when a robot should begin preparing responses, take turns, and handle potential interruptions. We evaluated the proposed system in a within-subject study against a traditional baseline system, using the Furhat robot with 39 adults in a conversational setting, in combination with a large language model for autonomous response generation. The results show that participants significantly prefer the proposed system, and it significantly reduces response delays and interruptions.	 | 轮流对话是交谈的基本要素，但目前的人机交互（HRI）系统往往依赖于简单的基于沉默的模型，导致对话中出现不自然的停顿和打断。本文首次研究了将通用轮流对话模型，特别是TurnGPT和语音活动投影（VAP）应用于提高HRI中的对话动态。这些模型通过自监督学习目标在人类对话数据上进行训练，无需特定领域的微调。我们提出了利用这些模型的方法，以预测机器人何时开始准备回应、何时轮换以及如何处理潜在的打断。我们通过一项针对39名成人的单被试研究，在Furhat机器人上测试了提出的系统，结合了一个大型语言模型来生成自主响应。结果显示，参与者明显更喜欢提出的系统，该系统显著减少了响应延迟和打断。
2501.08913	 | GenAI Content Detection Task 3: Cross-Domain Machine-Generated Text Detection Challenge	 | Liam Dugan,Andrew Zhu,Firoj Alam,Preslav Nakov,Marianna Apidianaki,Chris Callison-Burch	 | 该研究利用新发布的RAID基准，评估了多个团队开发的检测器在识别来自多种领域和大型语言模型生成文本方面的性能，发现多个检测器在准确率达到99%以上的同时保持了5%的假阳性率，表明这些检测器在检测多样性文本方面具有较高的稳健性。	 | Recently there have been many shared tasks targeting the detection of generated text from Large Language Models (LLMs). However, these shared tasks tend to focus either on cases where text is limited to one particular domain or cases where text can be from many domains, some of which may not be seen during test time. In this shared task, using the newly released RAID benchmark, we aim to answer whether or not models can detect generated text from a large, yet fixed, number of domains and LLMs, all of which are seen during training. Over the course of three months, our task was attempted by 9 teams with 23 detector submissions. We find that multiple participants were able to obtain accuracies of over 99% on machine-generated text from RAID while maintaining a 5% False Positive Rate -- suggesting that detectors are able to robustly detect text from many domains and models simultaneously. We discuss potential interpretations of this result and provide directions for future research.	 | 最近，有许多共享任务针对大型语言模型（LLMs）生成文本的检测。然而，这些共享任务往往要么集中在文本局限于一个特定领域的情况，要么集中在可以从多个领域取材的情况，部分领域可能在测试时未见过。在这个共享任务中，使用新发布的RAID基准，我们旨在回答模型是否能够检测来自大量但固定数量的领域和LLMs的生成文本，所有这些领域和模型在训练期间都能看到。在三个月的时间里，共有9个团队尝试了我们的任务，提交了23个检测器。我们发现，多个参与者能够在RAID生成的文本上达到超过99%的准确率，同时保持5%的假阳性率——这表明检测器能够同时稳健地检测来自多种领域和多种模型的文本。我们讨论了这一结果的潜在解释，并提供了未来研究的方向。
2501.08838	 | ToMATO: Verbalizing the Mental States of Role-Playing LLMs for Benchmarking Theory of Mind	 | Kazutoshi Shinoda,Nobukatsu Hojo,Kyosuke Nishida,Saki Mizuno,Keita Suzuki,Ryo Masumura,Hiroaki Sugiyama,Kuniko Saito	 | ToMATO是一个新的基于多选题问答形式的理论思维基准，通过利用信息不对称和多种个性特征，评估模型在信念、意图、欲望、情绪和知识等方面的表现，发现即使强大的语言模型如GPT-4o mini也存在局限性，尤其在理解虚假信念和适应不同人格特征方面。	 | Existing Theory of Mind (ToM) benchmarks diverge from real-world scenarios in three aspects: 1) they assess a limited range of mental states such as beliefs, 2) false beliefs are not comprehensively explored, and 3) the diverse personality traits of characters are overlooked. To address these challenges, we introduce ToMATO, a new ToM benchmark formulated as multiple-choice QA over conversations. ToMATO is generated via LLM-LLM conversations featuring information asymmetry. By employing a prompting method that requires role-playing LLMs to verbalize their thoughts before each utterance, we capture both first- and second-order mental states across five categories: belief, intention, desire, emotion, and knowledge. These verbalized thoughts serve as answers to questions designed to assess the mental states of characters within conversations. Furthermore, the information asymmetry introduced by hiding thoughts from others induces the generation of false beliefs about various mental states. Assigning distinct personality traits to LLMs further diversifies both utterances and thoughts. ToMATO consists of 5.4k questions, 753 conversations, and 15 personality trait patterns. Our analysis shows that this dataset construction approach frequently generates false beliefs due to the information asymmetry between role-playing LLMs, and effectively reflects diverse personalities. We evaluate nine LLMs on ToMATO and find that even GPT-4o mini lags behind human performance, especially in understanding false beliefs, and lacks robustness to various personality traits.	 | 现有的理论思维（Theory of Mind, ToM）基准在三个方面与现实世界的情景不符：1）它们评估的主要是有限的精神状态，如信念；2）虚假信念没有得到全面探索；3）角色的多样化人格特质被忽视。为了解决这些挑战，我们引入了ToMATO，一个新的基于多选题问答（multiple-choice QA）形式的ToM基准。ToMATO通过LLM-LLM对话生成，其中包含信息不对称。通过使用一种要求语言模型（LLM）在每条陈述之前表达其想法的提示方法，我们捕捉到了五大类精神状态：信念、意图、欲望、情绪和知识。这些表达的想法作为问题的答案，这些问题旨在评估对话中角色的精神状态。此外，通过隐藏LLM的想法来引入信息不对称，从而生成各种精神状态的虚假信念。给LLM分配不同的个性特征进一步丰富了陈述和想法的多样性。ToMATO包含5400个问题、753个对话和15种个性特征模式。我们的分析表明，由于角色扮演的LLM之间存在信息不对称，这种数据集构建方法经常生成虚假信念，并有效地反映了各种不同的人格。我们在ToMATO上评估了九个语言模型，发现即使是GPT-4o mini的人类表现也有所落后，尤其是在理解虚假信念方面，并且在应对各种个性特征方面缺乏鲁棒性。
2501.08769	 | Enhanced Large Language Models for Effective Screening of Depression and Anxiety	 | June M. Liu,Mengxia Gao,Sahand Sabour,Zhuang Chen,Minlie Huang,Tatia M.C. Lee	 | 该研究介绍了一个基于大型语言模型的情感障碍筛查系统EmoScan，通过合成临床访谈生成了一个包含1157个交互对话的PsyInterview管道，EmoScan在筛查情感障碍方面表现出色，优于基线模型和GPT-4等其他模型，并且在访谈技巧方面也优于基准模型。	 | Depressive and anxiety disorders are widespread, necessitating timely identification and management. Recent advances in Large Language Models (LLMs) offer potential solutions, yet high costs and ethical concerns about training data remain challenges. This paper introduces a pipeline for synthesizing clinical interviews, resulting in 1,157 interactive dialogues (PsyInterview), and presents EmoScan, an LLM-based emotional disorder screening system. EmoScan distinguishes between coarse (e.g., anxiety or depressive disorders) and fine disorders (e.g., major depressive disorders) and conducts high-quality interviews. Evaluations showed that EmoScan exceeded the performance of base models and other LLMs like GPT-4 in screening emotional disorders (F1-score=0.7467). It also delivers superior explanations (BERTScore=0.9408) and demonstrates robust generalizability (F1-score of 0.67 on an external dataset). Furthermore, EmoScan outperforms baselines in interviewing skills, as validated by automated ratings and human evaluations. This work highlights the importance of scalable data-generative pipelines for developing effective mental health LLM tools.	 | 抑郁和焦虑障碍广泛存在，需要及时识别和管理。近年来，大型语言模型（LLMs）的进展提供了潜在的解决方案，但仍存在成本高昂和训练数据的伦理问题。本文介绍了一个合成临床访谈的管道，生成了1157个交互对话（PsyInterview），并展示了基于LLM的情感障碍筛查系统EmoScan。EmoScan能够区分粗略的情感障碍（如焦虑或抑郁障碍）和细致的情感障碍（如重度抑郁症），并且能够进行高质量的访谈。评估结果显示，EmoScan在筛查情感障碍方面的性能超过了基线模型和其他LLM模型（如GPT-4）（F1分数=0.7467）。它还提供了更优的解释（BERTScore=0.9408），并展示了强大的泛化能力（外部数据集上的F1分数为0.67）。此外，EmoScan在访谈技巧方面也优于基准模型，这一结论得到了自动化评估和人类评估的验证。这项工作强调了为开发有效的心理健康LLM工具而构建可扩展的数据生成管道的重要性。
2501.08758	 | Expanding Vietnamese SentiWordNet to Improve Performance of Vietnamese Sentiment Analysis Models	 | Hong-Viet Tran,Van-Tan Bui,Lam-Quan Tran	 | 本文提出了一种结合PhoBERT-V2预训练模型和SentiWordnet词典的情感分析方法，用于越南语评论的情感极性分类，实验结果表明该方法在VLSP 2016和AIVIVN 2019数据集上的表现优于其他模型。	 | Sentiment analysis is one of the most crucial tasks in Natural Language Processing (NLP), involving the training of machine learning models to classify text based on the polarity of opinions. Pre-trained Language Models (PLMs) can be applied to downstream tasks through fine-tuning, eliminating the need to train the model from scratch. Specifically, PLMs have been employed for Sentiment Analysis, a process that involves detecting, analyzing, and extracting the polarity of text sentiments. Numerous models have been proposed to address this task, with pre-trained PhoBERT-V2 models standing out as the state-of-the-art language models for Vietnamese. The PhoBERT-V2 pre-training approach is based on RoBERTa, optimizing the BERT pre-training method for more robust performance. In this paper, we introduce a novel approach that combines PhoBERT-V2 and SentiWordnet for Sentiment Analysis of Vietnamese reviews. Our proposed model utilizes PhoBERT-V2 for Vietnamese, offering a robust optimization for the prominent BERT model in the context of Vietnamese language, and leverages SentiWordNet, a lexical resource explicitly designed to support sentiment classification applications. Experimental results on the VLSP 2016 and AIVIVN 2019 datasets demonstrate that our sentiment analysis system has achieved excellent performance in comparison to other models.	 | 情感分析是自然语言处理（NLP）中最关键的任务之一，涉及训练机器学习模型根据意见的极性对文本进行分类。预训练语言模型（PLMs）可以通过微调应用于下游任务，从而消除从头开始训练模型的需要。具体而言，PLMs 已被用于情感分析，这是一个涉及检测、分析和提取文本情感极性的过程。提出了许多模型来解决这一任务，其中 PhoBERT-V2 预训练模型因其在越南语中的最新技术水平而脱颖而出。PhoBERT-V2 的预训练方法基于 RoBERTa，对 BERT 的预训练方法进行了优化，以提高更稳健的性能。在本文中，我们提出了一种新颖的方法，将 PhoBERT-V2 与 SentiWordnet 结合起来用于越南语评论的情感分析。我们提出的模型利用了 PhoBERT-V2 对越南语的支持，为其提供了对 BERT 模型的稳健优化，并利用了 SentiWordnet 这一专门设计用于支持情感分类应用的词典资源。在 VLSP 2016 和 AIVIVN 2019 数据集上的实验结果表明，我们的情感分析系统在与其他模型的比较中表现出色。
2501.08716	 | The Inherent Limits of Pretrained LLMs: The Unexpected Convergence of Instruction Tuning and In-Context Learning Capabilities	 | Irina Bigoulaeva,Harish Tayyar Madabushi,Iryna Gurevych	 | 通过大量实验和对90个不同大型语言模型（LLMs）的指令微调研究，发现指令微调模型的性能与其基本版本的上下文提示性能之间存在显著相关性，表明基本模型的预训练数据和指令微调数据集共同决定了模型的能力边界。	 | Large Language Models (LLMs), trained on extensive web-scale corpora, have demonstrated remarkable abilities across diverse tasks, especially as they are scaled up. Nevertheless, even state-of-the-art models struggle in certain cases, sometimes failing at problems solvable by young children, indicating that traditional notions of task complexity are insufficient for explaining LLM capabilities. However, exploring LLM capabilities is complicated by the fact that most widely-used models are also "instruction-tuned" to respond appropriately to prompts. With the goal of disentangling the factors influencing LLM performance, we investigate whether instruction-tuned models possess fundamentally different capabilities from base models that are prompted using in-context examples. Through extensive experiments across various model families, scales and task types, which included instruction tuning 90 different LLMs, we demonstrate that the performance of instruction-tuned models is significantly correlated with the in-context performance of their base counterparts. By clarifying what instruction-tuning contributes, we extend prior research into in-context learning, which suggests that base models use priors from pretraining data to solve tasks. Specifically, we extend this understanding to instruction-tuned models, suggesting that their pretraining data similarly sets a limiting boundary on the tasks they can solve, with the added influence of the instruction-tuning dataset.	 | 大型语言模型（LLMs）在广泛网络规模语料库上训练后，展示了在各种任务上非凡的能力，尤其是在模型扩大的情况下。然而，即使是最先进的模型，在某些情况下也会遇到困难，有时甚至会解决不了简单的孩子都能解决的问题，这表明传统的任务复杂性概念不足以解释LLM的能力。然而，探索LLM的能力是复杂的，因为大多数广泛使用的模型也被“指令微调”以适当地响应提示。为了剖析影响LLM性能的因素，我们研究了指令微调模型是否与基于通过上下文示例进行提示的基本模型具有根本不同的能力。通过在各种模型家族、规模和任务类型上进行大量实验，包括对90个不同LLM进行指令微调，我们证明了指令微调模型的性能与它们的基本版本的上下文提示性能之间存在显著的相关性。通过阐明指令微调所贡献的内容，我们扩展了关于上下文学习的前期研究，表明基本模型使用预训练数据中的先验知识来解决任务。具体来说，我们将这种理解扩展到指令微调模型，表明它们的预训练数据同样为它们能够解决的任务设定了限制边界，而指令微调数据集则增加了额外的影响。
2501.08696	 | Deep Learning-Based Feature Fusion for Emotion Analysis and Suicide Risk Differentiation in Chinese Psychological Support Hotlines	 | Han Wang,Jianqiang Li,Qing Zhao,Zhonglong Chen,Changwei Song,Jing Tang,Yuning Huang,Wei Zhai,Yongsheng Tong,Guanghui Fu	 | 该研究提出了一种结合音高声学特征和深度学习的方法，用于分析心理支持热线通话中情感表达，其在情感识别任务上取得了良好的性能，并揭示了情感波动的强度和频率可能成为评估自杀风险的新特征。	 | Mental health is a critical global public health issue, and psychological support hotlines play a pivotal role in providing mental health assistance and identifying suicide risks at an early stage. However, the emotional expressions conveyed during these calls remain underexplored in current research. This study introduces a method that combines pitch acoustic features with deep learning-based features to analyze and understand emotions expressed during hotline interactions. Using data from China's largest psychological support hotline, our method achieved an F1-score of 79.13% for negative binary emotion classification.Additionally, the proposed approach was validated on an open dataset for multi-class emotion classification,where it demonstrated better performance compared to the state-of-the-art methods. To explore its clinical relevance, we applied the model to analysis the frequency of negative emotions and the rate of emotional change in the conversation, comparing 46 subjects with suicidal behavior to those without. While the suicidal group exhibited more frequent emotional changes than the non-suicidal group, the difference was not statistically significant.Importantly, our findings suggest that emotional fluctuation intensity and frequency could serve as novel features for psychological assessment scales and suicide risk prediction.The proposed method provides valuable insights into emotional dynamics and has the potential to advance early intervention and improve suicide prevention strategies through integration with clinical tools and assessments The source code is publicly available at https://github.com/Sco-field/Speechemotionrecognition/tree/main.	 | 心理健康是全球公共卫生的一个关键问题，而心理支持热线在提供心理健康援助和早期识别自杀风险方面发挥着重要作用。然而，这些热线通话中传达的情感表达在当前研究中仍是一个未充分探索的领域。本研究提出了一种结合音高声学特征和基于深度学习的特征的方法，用于分析和理解热线互动中表达的情感。利用中国最大的心理支持热线数据，我们的方法在负面二分类情感识别上的F1分数达到了79.13%。此外，该方法在多分类情感识别的开放数据集上进行了验证，并显示出了比最先进的方法更好的性能。为了探讨其临床相关性，我们将模型应用于分析对话中负面情感的频率及其情感变化的速率，比较了46名有自杀行为的个体和无自杀行为个体。结果显示，自杀组的情感变化频率高于非自杀组，但差异并无统计学意义。重要的是，我们的研究结果表明，情感波动的强度和频率可能成为心理评估量表和自杀风险预测的新特征。所提出的方法提供了关于情感动态的重要见解，并有可能通过与临床工具和评估的集成来促进早期干预和提高自杀预防策略。源代码已在 https://github.com/Sco-field/Speechemotionrecognition/tree/main 公开。
2501.08648	 | MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities	 | Savya Khosla,Kushal Kafle,Simon Jenni,Handong Zhao,John Collomosse,Jing Shi	 | MAGNET是一种改进的大语言模型，通过结合双向和因果注意力机制及三种自监督训练目标，增强了生成稳健表示和填充缺失文本片段的能力，同时保留了知识和文本生成能力。实验表明，MAGNET在表示学习、文本填充和知识保留方面优于现有模型。	 | While originally designed for unidirectional generative modeling, decoder-only large language models (LLMs) are increasingly being adapted for bidirectional modeling. However, unidirectional and bidirectional models are typically trained separately with distinct objectives (generation and representation learning, respectively). This separation overlooks the opportunity for developing a more versatile language model and for these objectives to complement each other. In this work, we introduce MAGNET, an adaptation of decoder-only LLMs that enhances their ability to generate robust representations and infill missing text spans, while preserving their knowledge and text generation capabilities. MAGNET employs three self-supervised training objectives and introduces an attention mechanism that combines bidirectional and causal attention, enabling unified training across all objectives. Our results demonstrate that LLMs adapted with MAGNET (1) surpass strong text encoders on token-level and sentence-level representation learning tasks, (2) generate contextually appropriate text infills by leveraging future context, (3) retain the ability for open-ended text generation without exhibiting repetition problem, and (4) preserve the knowledge gained by the LLM during pretraining.	 | 尽管最初设计用于单向生成建模，解码器主导的大语言模型（LLMs）现在越来越多地被改编为双向建模。然而，单向模型和双向模型通常采用不同的训练目标分别进行训练（分别为生成和表示学习）。这种分离忽视了开发更为通用的语言模型以及这些目标互相补充的机会。在这项研究中，我们引入了MAGNET，这是一种解码器主导LLMs的改进版本，它增强了生成稳健表示和填充缺失文本片段的能力，同时保留了其知识和文本生成能力。MAGNET采用了三种自监督训练目标，并引入了一种结合双向和因果注意力的注意力机制，使所有目标都能实现统一训练。我们的实验结果表明，使用MAGNET进行改进后的LLMs（1）在 token 级和 sentence 级表示学习任务中超越了强大的文本编码器，（2）通过利用未来上下文生成上下文适配的文本填充，（3）保留了开放生成文本的能力而没有重复问题，（4）保留了在预训练期间获得的知识。
2501.08641	 | Reassessing the Role of Chain-of-Thought in Sentiment Analysis: Insights and Limitations	 | Kaiyuan Zheng,Qinghua Zhao,Lei Li	 | 该研究探讨了大型语言模型的情感分析能力是否依赖于思维过程，并通过实验发现，链式思考提示对情感分析影响较小，模型主要依赖于示例中的信息，支持语言与思维独立的观点。	 | The relationship between language and thought remains an unresolved philosophical issue. Existing viewpoints can be broadly categorized into two schools: one asserting their independence, and another arguing that language constrains thought. In the context of large language models, this debate raises a crucial question: Does a language model's grasp of semantic meaning depend on thought processes? To explore this issue, we investigate whether reasoning techniques can facilitate semantic understanding. Specifically, we conceptualize thought as reasoning, employ chain-of-thought prompting as a reasoning technique, and examine its impact on sentiment analysis tasks. The experiments show that chain-of-thought has a minimal impact on sentiment analysis tasks. Both the standard and chain-of-thought prompts focus on aspect terms rather than sentiment in the generated content. Furthermore, counterfactual experiments reveal that the model's handling of sentiment tasks primarily depends on information from demonstrations. The experimental results support the first viewpoint.	 | 语言与思维之间的关系仍然是一个未解的哲学问题。现有的观点可以大致分为两类：一类认为它们是独立的，另一类则认为语言限制了思维。在大型语言模型的背景下，这种争论提出了一个关键问题：语言模型对语义意义的理解是否依赖于思维过程？为了探讨这个问题，我们研究了推理技术是否能够促进语义理解。具体来说，我们将思维概念化为推理，使用链式思考提示作为推理技术，并研究其对情感分析任务的影响。实验证明，链式思考对情感分析任务的影响很小。标准提示和链式思考提示都集中在生成内容中的方面词上，而非情感。此外，反事实实验表明，模型处理情感任务主要依赖于示例中的信息。实验结果支持第一种观点。
2501.08621	 | ViBidirectionMT-Eval: Machine Translation for Vietnamese-Chinese and Vietnamese-Lao language pair	 | Hong-Viet Tran,Minh-Quy Nguyen,Van-Vinh Nguyen	 | 该研究总结了2022-2023年VLSP机器翻译共享任务的结果，重点关注越南语到汉语和越南语到老挝语的翻译，使用BLEU和SacreBLEU等指标进行评估，并通过专家判断进行人工评估以确保全面性。	 | This paper presents an results of the VLSP 2022-2023 Machine Translation Shared Tasks, focusing on Vietnamese-Chinese and Vietnamese-Lao machine translation. The tasks were organized as part of the 9th, 10th annual workshop on Vietnamese Language and Speech Processing (VLSP 2022, VLSP 2023). The objective of the shared task was to build machine translation systems, specifically targeting Vietnamese-Chinese and Vietnamese-Lao translation (corresponding to 4 translation directions). The submission were evaluated on 1,000 pairs for testing (news and general domains) using established metrics like BLEU [11] and SacreBLEU [12]. Additionally, system outputs also were evaluated with human judgment provided by experts in Chinese and Lao languages. These human assessments played a crucial role in ranking the performance of the machine translation models, ensuring a more comprehensive evaluation.	 | 本文介绍了2022-2023年VLSP机器翻译共享任务的结果，重点关注越南语-汉语和越南语-老挝语机器翻译任务。这些任务作为第九届和第十届越南语言和语音处理研讨会（VLSP 2022, VLSP 2023）的一部分组织进行。共享任务的目标是构建机器翻译系统，特别针对越南语-汉语和越南语-老挝语翻译（对应4个翻译方向）。提交的作品使用如BLEU [11] 和SacreBLEU [12] 等公认指标，在1000对测试数据（新闻和一般领域）上进行了评估。此外，系统输出还通过汉语和老挝语专家提供的专家判断进行了评估。这些人工评估在排名机器翻译模型性能方面起到了关键作用，确保了更加全面的评估。
2501.08618	 | Disjoint Processing Mechanisms of Hierarchical and Linear Grammars in Large Language Models	 | Aruna Sankaranarayanan,Dylan Hadfield-Menell,Aaron Mueller	 | 研究发现，大规模语言模型在处理分层结构和线性结构的语言输入时表现出不同的行为，并且分层处理的组件与线性处理的组件在模型中功能上是独立的，这种分层敏感性甚至对无意义的语法也有效，表明这种敏感性与语义无关。	 | All natural languages are structured hierarchically. In humans, this structural restriction is neurologically coded: when two grammars are presented with identical vocabularies, brain areas responsible for language processing are only sensitive to hierarchical grammars. Using large language models (LLMs), we investigate whether such functionally distinct hierarchical processing regions can arise solely from exposure to large-scale language distributions. We generate inputs using English, Italian, Japanese, or nonce words, varying the underlying grammars to conform to either hierarchical or linear/positional rules. Using these grammars, we first observe that language models show distinct behaviors on hierarchical versus linearly structured inputs. Then, we find that the components responsible for processing hierarchical grammars are distinct from those that process linear grammars; we causally verify this in ablation experiments. Finally, we observe that hierarchy-selective components are also active on nonce grammars; this suggests that hierarchy sensitivity is not tied to meaning, nor in-distribution inputs.	 | 所有自然语言都是分层结构化的。在人类中，这种结构限制是神经编码的：当两个语法使用相同的词汇表时，负责语言处理的大脑区域只对分层语法敏感。利用大规模语言模型（LLMs），我们研究这些功能上独特的分层处理区域是否仅仅从大规模语言分布的暴露中产生。我们使用英语、意大利语、日语或无意义词生成输入，并改变底层语法以遵循分层或线性/位置规则。利用这些语法，我们首先观察到语言模型在处理分层结构和线性结构的输入时表现出不同的行为。然后，我们发现处理分层语法的组件与处理线性语法的组件不同；我们在消融实验中因果验证了这一点。最后，我们观察到，分层选择性组件也在无意义语法上活跃；这表明分层敏感性与意义无关，也不局限于分布内的输入。
2501.08613	 | Assessing the Alignment of FOL Closeness Metrics with Human Judgement	 | Ramya Keerthy Thatikonda,Wray Buntine,Ehsan Shareghi	 | 本文研究了现有评估指标在验证大语言模型生成的一阶逻辑语句正确性方面的敏感性和一致性，并发现不同类型的扰动对各种评估指标的敏感度不同，结合使用多个指标可以提高评估的可靠性和敏感性。	 | The recent successful paradigm of solving logical reasoning problems with tool-augmented large language models (LLMs) leverages translation of natural language statements into First-Order Logic~(FOL) and external theorem provers. However, the correctness of FOL statements, comprising operators and text predicates, often goes unverified due to the lack of a reliable evaluation metric for comparing generated and ground-truth FOLs. In this paper, we present a comprehensive study of sensitivity of existing metrics and their alignment with human judgement on FOL evaluation. Using ground-truth FOLs, we carefully designed various perturbations on the ground-truth to assess metric sensitivity. We sample FOL translation candidates for natural language statements and measure the ranking alignment between automatic metrics and human annotators. Our empirical findings highlight oversensitivity in the n-gram metric BLEU for text perturbations, the semantic graph metric Smatch++ for structural perturbations, and FOL metric for operator perturbation. We also observe a closer alignment between BertScore and human judgement. Additionally, we show that combining metrics enhances both alignment and sensitivity compared to using individual metrics.	 | 最近成功采用工具增强的大语言模型（LLMs）来解决逻辑推理问题的范式，依赖于将自然语言陈述转换为一阶逻辑（FOL）以及利用外部定理证明器。然而，由于缺乏可靠的评估指标来比较生成的FOL和真实FOL，FOL语句（包含操作符和文本谓词）的正确性往往得不到验证。在本文中，我们对现有评估指标的敏感性和其与人类判断的一致性进行了全面研究。利用真实FOL，我们精心设计了各种对真实FOL的扰动来评估指标的敏感性。我们从中选取自然语言陈述的一阶逻辑翻译候选，并测量自动指标和人类注释者之间的排名一致性。我们的实证研究结果显示，在文本扰动方面，n-gram度量BLEU过于敏感；在结构扰动方面，语义图度量Smatch++过于敏感；在操作符扰动方面，一阶逻辑度量过于敏感。此外，我们还观察到BertScore与人类判断更为一致。此外，我们将多个指标结合起来使用，与单独使用任一个指标相比，可以提高一致性和敏感性。
2501.08597	 | Dynamic Knowledge Integration for Enhanced Vision-Language Reasoning	 | Julian Perry,Surasakdi Siripong,Thanakorn Phonchai	 | 为提升大视觉-语言模型在知识密集型任务中的性能，本文提出了一种自适应知识引导预训练方法（AKGP），该方法通过动态整合外部知识来增强模型能力，并在多个基准数据集上显示出显著的优越性；此外，人类评估和详细分析进一步验证了AKGP-LVLM的鲁棒性、效率和可扩展性。	 | Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities in multimodal tasks, but their performance is often constrained by the lack of external knowledge integration, limiting their ability to handle knowledge-intensive tasks such as visual question answering and reasoning. To address this challenge, we propose a novel method, Adaptive Knowledge-Guided Pretraining for Large Vision-Language Models (AKGP-LVLM), which dynamically incorporates structured and unstructured knowledge into LVLMs during pretraining and fine-tuning. Our approach employs a knowledge encoder to represent external knowledge, a retrieval mechanism to select task-relevant information, and a dynamic adaptor to align multimodal and knowledge representations effectively. We evaluate our method on four benchmark datasets, demonstrating significant performance improvements over state-of-the-art models. Furthermore, human evaluations highlight the superior correctness and relevance of our model's outputs. Extensive analyses confirm the robustness, efficiency, and scalability of AKGP-LVLM, making it a compelling solution for real-world knowledge-intensive tasks.	 | 大视觉-语言模型（Large Vision-Language Models, LVLMs）在多模态任务中表现出色，但它们的性能常常受限于外部知识整合不足，限制了它们处理知识密集型任务（如视觉问答和推理）的能力。为解决这一挑战，我们提出了一种新颖的方法——自适应知识引导预训练（Adaptive Knowledge-Guided Pretraining, AKGP）方法，该方法在预训练和微调过程中动态地将结构化和非结构化的知识整合到LVLMs中。我们的方法采用知识编码器来表示外部知识，采用检索机制来选择与任务相关的信息，并采用动态适配器来有效对齐跨模态和知识表示。我们在四个基准数据集上评估了该方法，结果显示我们的方法在性能上显著优于最先进的模型。此外，人类评估表明，我们的模型输出具有更高的正确性和相关性。详尽的分析证实了AKGP-LVLM的鲁棒性、效率和可扩展性，使其成为解决实际世界知识密集型任务的理想解决方案。
2501.08582	 | LoRS: Efficient Low-Rank Adaptation for Sparse Large Language Model	 | Yuxuan Hu,Jing Zhang,Xiaodong Chen,Zhe Zhao,Cuiping Li,Hong Chen	 | LoRS 是一种旨在提高稀疏大型语言模型（LLMs）微调效率的方法，通过结合权重重计算和计算图重构策略来缓解保持稀疏性带来的高内存和计算开销问题，同时保持或提升性能。	 | Existing low-rank adaptation (LoRA) methods face challenges on sparse large language models (LLMs) due to the inability to maintain sparsity. Recent works introduced methods that maintain sparsity by augmenting LoRA techniques with additional masking mechanisms. Despite these successes, such approaches suffer from an increased memory and computation overhead, which affects efficiency of LoRA methods. In response to this limitation, we introduce LoRS, an innovative method designed to achieve both memory and computation efficiency when fine-tuning sparse LLMs. To mitigate the substantial memory and computation demands associated with preserving sparsity, our approach incorporates strategies of weight recompute and computational graph rearrangement. In addition, we also improve the effectiveness of LoRS through better adapter initialization. These innovations lead to a notable reduction in memory and computation consumption during the fine-tuning phase, all while achieving performance levels that outperform existing LoRA approaches.	 | 现有的低秩适应（LoRA）方法在稀疏大型语言模型（LLMs）上存在挑战，因为它们无法保持稀疏性。最近的研究引入了通过增加额外的遮罩机制来维持稀疏性的LoRA技术。尽管这些方法取得了一定的成功，但它们仍然存在内存和计算开销增加的问题，这影响了LoRA方法的效率。为应对这一局限性，我们提出了LoRS，一种旨在在微调稀疏LLMs时实现内存和计算效率的方法。为了缓解保持稀疏性所带来的大量内存和计算需求，我们的方法结合了权重重计算和计算图重构的策略。此外，我们还通过改进适配器初始化来提高LoRS的有效性。这些创新在微调阶段显著减少了内存和计算消耗，同时达到了优于现有LoRA方法的性能水平。
2501.08579	 | What Limits LLM-based Human Simulation: LLMs or Our Design?	 | Qian Wang,Jiaying Wu,Zhenheng Tang,Bingqiao Luo,Nuo Chen,Wei Chen,Bingsheng He	 | 该研究分析了大语言模型（LLM）在人类模拟中的局限性和设计挑战，并提出了对应的目标解决方案，同时探讨了未来的研究方向，特别是在数据收集、LLM生成和评估方面。为了支持进一步研究，作者提供了一个基于LLM的人类模拟资源集合。	 | We argue that advancing LLM-based human simulation requires addressing both LLM's inherent limitations and simulation framework design challenges. Recent studies have revealed significant gaps between LLM-based human simulations and real-world observations, highlighting these dual challenges. To address these gaps, we present a comprehensive analysis of LLM limitations and our design issues, proposing targeted solutions for both aspects. Furthermore, we explore future directions that address both challenges simultaneously, particularly in data collection, LLM generation, and evaluation. To support further research in this field, we provide a curated collection of LLM-based human simulation resources.\footnote{https://github.com/Persdre/llm-human-simulation}	 | 我们认为，推进基于大语言模型（LLM）的人类模拟需要同时解决LLM固有的局限性和模拟框架设计挑战。最近的研究揭示了基于LLM的人类模拟与现实世界观察之间的显著差距，突显了这两个方面的挑战。为了弥补这些差距，我们对LLM局限性和设计问题进行了全面分析，并提出了针对两个方面的目标解决方案。此外，我们探讨了同时应对这些挑战的未来方向，特别是在数据收集、LLM生成和评估方面。为了支持该领域的进一步研究，我们提供了一个精心筛选的基于LLM的人类模拟资源集合。\footnote{https://github.com/Persdre/llm-human-simulation}
2501.08570	 | Information Entropy Invariance: Enhancing Length Extrapolation in Attention Mechanisms	 | Kewei Li,Yanwen Kong,Yiping Xu,Lan Huang,Ruochi Zhang,Fengfeng Zhou	 | 本文提出了一种基于信息熵不变性的新方法来增强大型语言模型的长度外推能力，通过设计无需训练的InfoScale和理论分析的CosScale两种新的标度温度，该方法在将上下文窗口扩展至64倍的GAU-α模型上取得了最佳性能，优于现有七种方法。	 | Improving the length extrapolation capabilities of Large Language Models (LLMs) remains a critical challenge in natural language processing. Many recent efforts have focused on modifying the scaled dot-product attention mechanism, and often introduce scaled temperatures without rigorous theoretical justification. To fill this gap, we introduce a novel approach based on information entropy invariance. We propose two new scaled temperatures to enhance length extrapolation. First, a training-free method InfoScale is designed for dot-product attention, and preserves focus on original tokens during length extrapolation by ensuring information entropy remains consistent. Second, we theoretically analyze the impact of scaling (CosScale) on cosine attention. Experimental data demonstrates that combining InfoScale and CosScale achieves state-of-the-art performance on the GAU-α model with a context window extended to 64 times the training length, and outperforms seven existing methods. Our analysis reveals that significantly increasing CosScale approximates windowed attention, and highlights the significance of attention score dilution as a key challenge in long-range context handling. The code and data are available at https://github.com/HT-NEKO/InfoScale.	 | 改进大型语言模型（LLMs）的长度外推能力仍然是自然语言处理中的一个关键挑战。许多近期的努力集中在修改标度点积注意力机制上，常常引入未经严格理论验证的标度温度。为了填补这一空白，我们提出了一种基于信息熵不变性的新方法。我们提出了两种新的标度温度来增强长度外推能力。首先，我们设计了一种无需训练的方法 InfoScale 用于点积注意力，通过确保信息熵保持一致，确保在长度外推过程中对原始令牌保持关注。其次，我们从理论上分析了缩放（CosScale）对余弦注意力的影响。实验数据表明，将InfoScale和CosScale结合使用能够在将上下文窗口扩展至训练长度64倍的GAU-α模型上达到最先进的性能，且优于七种现有方法。我们的分析表明，显著增加CosScale逼近窗口注意力，并强调了注意分数稀释在处理长范围上下文时的关键挑战。相关代码和数据可在 https://github.com/HT-NEKO/InfoScale 获取。
2501.08540	 | Knowledge prompt chaining for semantic modeling	 | Ning Pei Ding,Jingge Du,Zaiwen Feng	 | 本文提出了一种新颖的自动语义建模框架——知识提示链，该框架能够通过提示链架构将图结构知识注入到大语言模型中，从而生成结构化数据的语义标签和语义图，实验结果表明其性能优于现有技术。	 | The task of building semantics for structured data such as CSV, JSON, and XML files is highly relevant in the knowledge representation field. Even though we have a vast of structured data on the internet, mapping them to domain ontologies to build semantics for them is still very challenging as it requires the construction model to understand and learn graph-structured knowledge. Otherwise, the task will require human beings' effort and cost. In this paper, we proposed a novel automatic semantic modeling framework: Knowledge Prompt Chaining. It can serialize the graph-structured knowledge and inject it into the LLMs properly in a Prompt Chaining architecture. Through this knowledge injection and prompting chaining, the model in our framework can learn the structure information and latent space of the graph and generate the semantic labels and semantic graphs following the chains' insturction naturally. Based on experimental results, our method achieves better performance than existing leading techniques, despite using reduced structured input data.	 | 构建像CSV、JSON和XML文件这样的结构化数据的语义是一项在知识表示领域极具相关性的任务。尽管互联网上有大量的结构化数据，将它们映射到领域本体以构建其语义仍然是一个极具挑战性的任务，因为这要求构建模型能够理解并学习图结构知识。否则，这项任务将需要人类的努力和成本。在本文中，我们提出了一种新颖的自动语义建模框架：知识提示链。该框架可以通过提示链架构将图结构知识序列化并正确注入到LLMs中。通过这种知识注入和提示链操作，我们框架中的模型可以学习图的结构信息和隐含空间，并自然地生成遵循链指令的语义标签和语义图。基于实验结果，尽管使用了较少的结构化输入数据，我们的方法在性能上优于现有领先技术。
2501.08537	 | Complexity Control Facilitates Reasoning-Based Compositional Generalization in Transformers	 | Zhongwang Zhang,Pengxiao Lin,Zhiwei Wang,Yaoyu Zhang,Zhi-Qin John Xu	 | 该研究探讨了变换器在组合性任务中的内部工作机制，发现复杂性控制策略影响模型学习推理规则或记忆映射的解决方案，并通过遮蔽策略和复杂性度量揭示了不同解决方案的内部差异，验证了这些发现适用于多种实际数据集，包括图像生成和自然语言处理任务。	 | Transformers have demonstrated impressive capabilities across various tasks, yet their performance on compositional problems remains a subject of debate. In this study, we investigate the internal mechanisms underlying Transformers' behavior in compositional tasks. We find that complexity control strategies significantly influence whether the model learns primitive-level rules that generalize out-of-distribution (reasoning-based solutions) or relies solely on memorized mappings (memory-based solutions). By applying masking strategies to the model's information circuits and employing multiple complexity metrics, we reveal distinct internal working mechanisms associated with different solution types. Further analysis reveals that reasoning-based solutions exhibit a lower complexity bias, which aligns with the well-studied neuron condensation phenomenon. This lower complexity bias is hypothesized to be the key factor enabling these solutions to learn reasoning rules. We validate these conclusions across multiple real-world datasets, including image generation and natural language processing tasks, confirming the broad applicability of our findings.	 | 变换器在各种任务中表现出令人印象深刻的性能，但在组合性问题上的表现仍存在争议。在这项研究中，我们探讨了变换器在组合性任务中的内部工作机制。我们发现，复杂性控制策略对模型学习普适性规则（基于推理的解决方案）或依赖于记忆映射（基于记忆的解决方案）有显著影响。通过在模型的信息电路中应用遮蔽策略并使用多种复杂性度量，我们揭示了与不同解决方案类型相关联的不同内部工作机制。进一步的分析表明，基于推理的解决方案表现出较低的复杂性偏好，这与广泛研究的神经元凝聚现象一致。这种较低的复杂性偏好被认为是使这些解决方案能够学习推理规则的关键因素。我们在多个实际数据集上验证了这些结论，包括图像生成和自然语言处理任务，证实了我们发现的广泛适用性。
2501.08523	 | Doc-Guided Sent2Sent++: A Sent2Sent++ Agent with Doc-Guided memory for Document-level Machine Translation	 | Jiaxin Guo,Yuanchang Luo,Daimeng Wei,Ling Zhang,Zongyao Li,Hengchao Shang,Zhiqiang Rao,Shaojun Li,Jinlong Yang,Zhanglin Wu,Hao Yang	 | 本文介绍了一种名为Sent2Sent++的新代理，它采用逐步级别的强制解码策略，确保每个句子都被翻译并增强相邻句子的流畅性，同时利用文档导向的记忆机制来提高文档级机器翻译的质量、一致性和流畅性。实验结果显示，Sent2Sent++在多个评估指标上显著优于现有方法。	 | The field of artificial intelligence has witnessed significant advancements in natural language processing, largely attributed to the capabilities of Large Language Models (LLMs). These models form the backbone of Agents designed to address long-context dependencies, particularly in Document-level Machine Translation (DocMT). DocMT presents unique challenges, with quality, consistency, and fluency being the key metrics for evaluation. Existing approaches, such as Doc2Doc and Doc2Sent, either omit sentences or compromise fluency. This paper introduces Doc-Guided Sent2Sent++, an Agent that employs an incremental sentence-level forced decoding strategy \textbf{to ensure every sentence is translated while enhancing the fluency of adjacent sentences.} Our Agent leverages a Doc-Guided Memory, focusing solely on the summary and its translation, which we find to be an efficient approach to maintaining consistency. Through extensive testing across multiple languages and domains, we demonstrate that Sent2Sent++ outperforms other methods in terms of quality, consistency, and fluency. The results indicate that, our approach has achieved significant improvements in metrics such as s-COMET, d-COMET, LTCR-$1_f$, and document-level perplexity (d-ppl). The contributions of this paper include a detailed analysis of current DocMT research, the introduction of the Sent2Sent++ decoding method, the Doc-Guided Memory mechanism, and validation of its effectiveness across languages and domains.	 | 人工智能领域的自然语言处理取得了显著进步，这主要归功于大型语言模型（LLMs）的能力。这些模型构成了设计用于解决长上下文依赖性的代理的基础，尤其是在文档级机器翻译（DocMT）方面。DocMT面临着独特的挑战，质量、一致性和流畅性是评价的关键指标。现有的方法，如Doc2Doc和Doc2Sent，要么省略句子，要么牺牲流畅性。本文介绍了一种名为Sent2Sent++的代理，该代理采用逐步级别的强制解码策略，**确保每个句子都被翻译，并增强相邻句子的流畅性。**我们的代理利用一种文档导向的记忆机制，仅关注总结及其翻译，我们发现这种方法在保持一致性方面非常有效。通过在多种语言和领域进行广泛的测试，我们证明Sent2Sent++在质量、一致性和流畅性方面优于其他方法。结果显示，我们的方法在s-COMET、d-COMET、LTCR-$1_f$和文档级困惑度（d-ppl）等指标上取得了显著改进。本文的贡献包括对现有DocMT研究的详细分析，Sent2Sent++解码方法的引入，文档导向记忆机制的介绍，以及其在多种语言和领域中的有效性验证。
2501.08502	 | Adapting Whisper for Regional Dialects: Enhancing Public Services for Vulnerable Populations in the United Kingdom	 | Melissa Torgbi,Andrew Clayman,Jordan J. Speight,Harish Tayyar Madabushi	 | 该研究评估了当前最先进的自动语音识别模型（如Whisper v3）在捕捉英国不同地区口音差异方面的表现，特别是在苏格兰方言上，并探讨了模型微调对改善地区口音识别准确性的影响。研究发现，通过微调可以提高模型在特定地区方言上的识别性能，但使用单词错误率作为评估指标存在局限性。	 | We collect novel data in the public service domain to evaluate the capability of the state-of-the-art automatic speech recognition (ASR) models in capturing regional differences in accents in the United Kingdom (UK), specifically focusing on two accents from Scotland with distinct dialects. This study addresses real-world problems where biased ASR models can lead to miscommunication in public services, disadvantaging individuals with regional accents particularly those in vulnerable populations. We first examine the out-of-the-box performance of the Whisper large-v3 model on a baseline dataset and our data. We then explore the impact of fine-tuning Whisper on the performance in the two UK regions and investigate the effectiveness of existing model evaluation techniques for our real-world application through manual inspection of model errors. We observe that the Whisper model has a higher word error rate (WER) on our test datasets compared to the baseline data and fine-tuning on a given data improves performance on the test dataset with the same domain and accent. The fine-tuned models also appear to show improved performance when applied to the test data outside of the region it was trained on suggesting that fine-tuned models may be transferable within parts of the UK. Our manual analysis of model outputs reveals the benefits and drawbacks of using WER as an evaluation metric and fine-tuning to adapt to regional dialects.	 | 我们收集了公共服务业领域的新型数据，以评估当前最先进的自动语音识别（ASR）模型在捕捉英国（UK）内部地区口音差异方面的能力，特别是重点关注苏格兰两种具有明显方言差异的口音。本研究针对在公共服中因为存在偏见的ASR模型而导致的沟通错误问题，这些偏见模型会尤其影响具有地区口音的个人，特别是弱势群体。首先，我们评估了Whisper大型v3模型在基线数据集和我们数据集上的开箱即用性能。然后，我们探索了对Whisper进行微调对其在两个英国地区的表现的影响，并通过人工检查模型错误来研究现有模型评估技术在我们实际情况中的有效性。我们观察到，Whisper模型在我们测试数据集上的单词错误率（WER）高于基线数据集，而在相同领域和口音的数据集上进行微调后，模型在测试数据集上的表现有所提升。微调后的模型在外地区域的表现也显得更好，这表明微调后的模型可能在英国的不同部分具有可转移性。我们对模型输出的手动分析揭示了使用WER作为评估指标和微调以适应地区方言的优缺点。
2501.08496	 | Quantifying the Importance of Data Alignment in Downstream Model Performance	 | Krrish Chawla,Aryan Sahai,Mario DePavia,Sudharsan Sundar,Brando Miranda	 | 该研究探讨了数据对齐在训练强大语言模型中的作用，通过使用Task2Vec数据对齐系数，发现训练数据与评估数据之间的对齐程度与模型在下游任务上的性能呈强烈负相关关系，特别是在专门化任务如Autoformalization中，表明数据对齐比数据量更为关键。	 | Contrary to the conventional emphasis on dataset size, we explore the role of data alignment -- an often overlooked aspect of data quality -- in training capable Large Language Models (LLMs). To do so, we use the Task2Vec-based alignment coefficient, a quantitative measure of the similarity between two datasets, to quantify the impact of alignment between training data and evaluation data on downstream performance. In particular, we conduct controlled \textit{interventional} experiments for two settings: 1. the impact of increased alignment coefficients between various pre-training (pt) against evaluation datasets, and 2. the impact of increased alignment coefficients between domain specific fine-tuning (ft) against domain specific evaluation. The domain specific task we explore is Autoformalization -- the machine translation task between natural language and code for formal verification. In both settings, we find a strong, predictable negative correlation between the alignment coefficient of a model's training and evaluation data and the model's loss/perplexity on the respective downstream task. These findings suggest a re-evaluation of LLM training approaches, demonstrating the relevance of data alignment compared to data quantity, especially in specialized downstream tasks such as Autoformalization.	 | 与传统的强调数据集大小不同，我们探讨了数据对齐——这一经常被忽视的数据质量方面——在训练强大语言模型（LLMs）中的作用。为此，我们使用了基于Task2Vec的数据对齐系数，这是一种衡量两个数据集相似性的定量指标，来量化训练数据与评估数据之间对齐程度对下游性能的影响。具体而言，我们在两个设置中进行了控制实验： 1. 各种预训练数据（pt）与评估数据之间对齐系数增加的影响； 2. 领域特定微调数据（ft）与领域特定评估数据之间对齐系数增加的影响。  我们探索的领域特定任务是Autoformalization——即自然语言与代码之间用于形式验证的机器翻译任务。在两种设置中，我们发现模型训练数据与评估数据之间的对齐系数与其在相应下游任务上的损失/困惑度之间存在强烈的、可预测的负相关关系。这些发现表明，需要重新评估LLM的训练方法，证明数据对齐相较于数据量而言更为相关，尤其是在像Autoformalization这样的专门化下游任务中。
2501.08474	 | The Theater Stage as Laboratory: Review of Real-Time Comedy LLM Systems for Live Performance	 | Piotr Wojciech Mirowski,Boyd Branch,Kory Wallace Mathewson	 | 本文回顾了计算系统生成幽默的历史，并强调现场表演是评估人工智能喜剧的理想环境，同时指出现场表演为计算幽默带来了机器人身体表现、喜剧节奏及观众理解等方面的挑战。	 | In this position paper, we review the eclectic recent history of academic and artistic works involving computational systems for humor generation, and focus specifically on live performance. We make the case that AI comedy should be evaluated in live conditions, in front of audiences sharing either physical or online spaces, and under real-time constraints. We further suggest that improvised comedy is therefore the perfect substrate for deploying and assessing computational humor systems. Using examples of successful AI-infused shows, we demonstrate that live performance raises three sets of challenges for computational humor generation: 1) questions around robotic embodiment, anthropomorphism and competition between humans and machines, 2) questions around comedic timing and the nature of audience interaction, and 3) questions about the human interpretation of seemingly absurd AI-generated humor. We argue that these questions impact the choice of methodologies for evaluating computational humor, as any such method needs to work around the constraints of live audiences and performance spaces. These interrogations also highlight different types of collaborative relationship of human comedians towards AI tools.	 | 在这篇立场文件中，我们回顾了涉及计算系统生成幽默的学术和艺术作品的丰富多彩的历史，并特别关注现场表演。我们主张人工智能喜剧应该在观众共享物理或在线空间的现场条件下进行评估，同时在实时条件下进行。此外，我们建议即兴喜剧是部署和评估计算幽默系统最理想的载体。通过成功的人工智能融合表演的例子，我们证明了现场表演为计算幽默生成带来了三组挑战：1) 关于机器人身体表现、拟人化以及人类与机器之间的竞争问题；2) 关于喜剧节奏以及观众互动的本质问题；3) 关于人类对看似荒谬的人工智能生成幽默的理解问题。我们认为，这些挑战影响了评估计算幽默的方法选择，因为任何这样的方法都需要围绕现场观众和表演空间的限制来运作。这些质疑还突显了人类喜剧演员与人工智能工具之间不同类型的协作关系。
2501.08468	 | Selective Attention Merging for low resource tasks: A case study of Child ASR	 | Natarajan Balaji Shankar,Zilai Wang,Eray Eren,Abeer Alwan	 | 本文探索了模型合并技术以提升低资源任务（如儿童ASR）的语音基础模型性能，并提出了一种新颖的选择性注意（SA）合并方法，能够显著降低词错误率，实验结果显示性能优于现有技术。	 | While Speech Foundation Models (SFMs) excel in various speech tasks, their performance for low-resource tasks such as child Automatic Speech Recognition (ASR) is hampered by limited pretraining data. To address this, we explore different model merging techniques to leverage knowledge from models trained on larger, more diverse speech corpora. This paper also introduces Selective Attention (SA) Merge, a novel method that selectively merges task vectors from attention matrices to enhance SFM performance on low-resource tasks. Experiments on the MyST database show significant reductions in relative word error rate of up to 14%, outperforming existing model merging and data augmentation techniques. By combining data augmentation techniques with SA Merge, we achieve a new state-of-the-art WER of 8.69 on the MyST database for the Whisper-small model, highlighting the potential of SA Merge for improving low-resource ASR.	 | 尽管语音基础模型（SFMs）在各种语音任务中表现出色，但在如儿童自动语音识别（ASR）等低资源任务上的表现受到有限预训练数据的限制。为了解决这一问题，我们探索了不同的模型合并技术，以便利用在更大、更多样化的语音语料库上训练的模型的知识。本文还介绍了选择性注意（SA）合并，这是一种新颖的方法，通过选择性地合并注意力矩阵中的任务向量来增强SFM在低资源任务上的性能。在MyST数据库上的实验显示，相对词错误率减少了高达14%，超过了现有的模型合并和数据增强技术。通过结合数据增强技术与SA合并，我们对MyST数据库中的Whisper-small模型实现了新的最佳词错误率（WER）8.69，突显了SA合并在提高低资源ASR方面的潜力。
2501.08457	 | Large Language Models For Text Classification: Case Study And Comprehensive Review	 | Arina Kostina,Marios D. Dikaiakos,Dimosthenis Stefanidis,George Pallis	 | 本研究评估了不同规模和架构的大型语言模型（LLMs）在多分类和二分类任务中的表现，并发现LLMs在复杂任务中优于传统方法，但代价是较长的推理时间，而在简单任务中，传统的机器学习模型则更具优势。	 | Unlocking the potential of Large Language Models (LLMs) in data classification represents a promising frontier in natural language processing. In this work, we evaluate the performance of different LLMs in comparison with state-of-the-art deep-learning and machine-learning models, in two different classification scenarios: i) the classification of employees' working locations based on job reviews posted online (multiclass classification), and 2) the classification of news articles as fake or not (binary classification). Our analysis encompasses a diverse range of language models differentiating in size, quantization, and architecture. We explore the impact of alternative prompting techniques and evaluate the models based on the weighted F1-score. Also, we examine the trade-off between performance (F1-score) and time (inference response time) for each language model to provide a more nuanced understanding of each model's practical applicability. Our work reveals significant variations in model responses based on the prompting strategies. We find that LLMs, particularly Llama3 and GPT-4, can outperform traditional methods in complex classification tasks, such as multiclass classification, though at the cost of longer inference times. In contrast, simpler ML models offer better performance-to-time trade-offs in simpler binary classification tasks.	 | 解锁大型语言模型（LLMs）在数据分类中的潜力代表了自然语言处理领域的一个有前景的研究前沿。在本研究中，我们评估了不同LLMs的表现，将其与最先进的深度学习和机器学习模型进行了比较，在两种不同的分类场景中：i) 根据在线发布的职位评论对员工的工作地点进行分类（多分类），以及ii) 将新闻文章分类为假新闻或真新闻（二分类）。我们的分析涵盖了不同规模、量化和架构的多种语言模型。我们探讨了替代提示技术的影响，并基于加权F1得分评估模型。此外，我们还研究了每个语言模型的性能（F1得分）与时间（推理响应时间）之间的权衡，以提供对每个模型实际应用性的更细致理解。我们的研究揭示了基于提示策略的模型响应之间存在显著差异。我们发现，特别是在复杂的分类任务，如多分类中，LLMs，尤其是Llama3和GPT-4，能够超越传统方法，尽管会付出较长的推理时间的代价。相比之下，在简单的二分类任务中，更简单的机器学习模型在性能和时间之间的权衡上表现更好。
2501.08442	 | Jochre 3 and the Yiddish OCR corpus	 | Assaf Urieli,Amber Clooney,Michelle Sigiel,Grisha Leyfer	 | 该研究描述了构建公开可用的意第绪语OCR语料库的过程，并评估了开源OCR工具套件Jochre 3，该工具套件在测试语料库上实现了优于现有公共模型的字符错误率。此外，Jochre 3被用于分析包含6.6亿单词的意第绪语书籍中心的全部内容，并可通过OCR搜索引擎进行访问。	 | We describe the construction of a publicly available Yiddish OCR Corpus, and describe and evaluate the open source OCR tool suite Jochre 3, including an Alto editor for corpus annotation, OCR software for Alto OCR layer generation, and a customizable OCR search engine. The current version of the Yiddish OCR corpus contains 658 pages, 186K tokens and 840K glyphs. The Jochre 3 OCR tool uses various fine-tuned YOLOv8 models for top-down page layout analysis, and a custom CNN network for glyph recognition. It attains a CER of 1.5% on our test corpus, far out-performing all other existing public models for Yiddish. We analyzed the full 660M word Yiddish Book Center with Jochre 3 OCR, and the new OCR is searchable through the Yiddish Book Center OCR search engine.	 | 我们描述了构建一个公开可用的意第绪语OCR语料库的过程，并介绍了并评估了开源OCR工具套件Jochre 3，包括一个用于语料库注释的Alto编辑器、用于生成Alto OCR层的OCR软件以及一个可定制的OCR搜索引擎。当前版本的意第绪语OCR语料库包含658页、186K个词素和840K个图形单元。Jochre 3 OCR工具使用各种微调的YOLOv8模型进行自上而下的页面布局分析，并使用自定义的CNN网络进行字符识别。它在我们的测试语料库上的CER为1.5%，远优于所有现有的公共意第绪语模型。我们使用Jochre 3 OCR分析了意第绪语书中心的全部6.6亿单词，新的OCR可以通过意第绪语书中心的OCR搜索引擎进行搜索。
2501.08441	 | Religious Bias Landscape in Language and Text-to-Image Models: Analysis, Detection, and Debiasing Strategies	 | Ajwad Abrar,Nafisa Tabassum Oeshy,Mohsinul Kabir,Sophia Ananiadou	 | 本研究系统地调查了语言模型和文本到图像生成模型中的宗教偏见，通过构建约400个独特的提示在多种任务中探索，并发现这些模型存在显著的偏见和刻板印象，强调了开发更公平的语言模型的紧迫需求。	 | Note: This paper includes examples of potentially offensive content related to religious bias, presented solely for academic purposes. The widespread adoption of language models highlights the need for critical examinations of their inherent biases, particularly concerning religion. This study systematically investigates religious bias in both language models and text-to-image generation models, analyzing both open-source and closed-source systems. We construct approximately 400 unique, naturally occurring prompts to probe language models for religious bias across diverse tasks, including mask filling, prompt completion, and image generation. Our experiments reveal concerning instances of underlying stereotypes and biases associated disproportionately with certain religions. Additionally, we explore cross-domain biases, examining how religious bias intersects with demographic factors such as gender, age, and nationality. This study further evaluates the effectiveness of targeted debiasing techniques by employing corrective prompts designed to mitigate the identified biases. Our findings demonstrate that language models continue to exhibit significant biases in both text and image generation tasks, emphasizing the urgent need to develop fairer language models to achieve global acceptability.	 | 注意：本文包括与宗教偏见相关的潜在冒犯内容的示例，仅用于学术目的。语言模型的广泛采纳突显了对其内在偏见进行批判性审查的必要性，特别是与宗教有关的偏见。本研究系统地调查了语言模型和文本到图像生成模型中的宗教偏见，分析了开源和封闭源系统。我们构建了约400个独特的自然出现的提示，以跨多种任务探究语言模型中的宗教偏见，包括掩码填充、提示完成和图像生成。我们的实验揭示了一些令人担忧的潜在刻板印象和与某些宗教不成比例相关的偏见。此外，我们还探讨了跨域偏见，研究了宗教偏见与性别、年龄和国籍等人口统计因素的交叉影响。本研究进一步评估了有针对性的去偏见技术的有效性，通过使用设计用于缓解已识别偏见的纠正提示进行评估。我们的发现表明，语言模型在文本和图像生成任务中仍然表现出显著的偏见，强调了开发更公平的语言模型以实现全球接受度的紧迫需求。
2501.08413	 | Ensemble of Large Language Models for Curated Labeling and Rating of Free-text Data	 | Jiaxing Qiu,Dongliang Guo,Papini Natalie,Peace Noelle,Levinson Cheri,Teague R. Henry	 | 本文提出了一种利用多种开源大型语言模型（LLM）在隐私约束下对自由文本数据中的预定义主题进行标记的框架，并通过饮食障碍相关的Reddit数据评估了其性能，发现集成多个LLM比单一LLM能更好地预测人类注释，并有效缓解了LLM标记结果的差异性。	 | Free-text responses are commonly collected in psychological studies, providing rich qualitative insights that quantitative measures may not capture. Labeling curated topics of research interest in free-text data by multiple trained human coders is typically labor-intensive and time-consuming. Though large language models (LLMs) excel in language processing, LLM-assisted labeling techniques relying on closed-source LLMs cannot be directly applied to free-text data, without explicit consent for external use.   In this study, we propose a framework of assembling locally-deployable LLMs to enhance the labeling of predetermined topics in free-text data under privacy constraints. Analogous to annotation by multiple human raters, this framework leverages the heterogeneity of diverse open-source LLMs. The ensemble approach seeks a balance between the agreement and disagreement across LLMs, guided by a relevancy scoring methodology that utilizes embedding distances between topic descriptions and LLMs' reasoning. We evaluated the ensemble approach using both publicly accessible Reddit data from eating disorder related forums, and free-text responses from eating disorder patients, both complemented by human annotations.   We found that: (1) there is heterogeneity in the performance of labeling among same-sized LLMs, with some showing low sensitivity but high precision, while others exhibit high sensitivity but low precision. (2) Compared to individual LLMs, the ensemble of LLMs achieved the highest accuracy and optimal precision-sensitivity trade-off in predicting human annotations. (3) The relevancy scores across LLMs showed greater agreement than dichotomous labels, indicating that the relevancy scoring method effectively mitigates the heterogeneity in LLMs' labeling.	 | 自由文本回应在心理学研究中常见，提供了丰富的定性见解，这些见解可能无法通过定量测量捕捉到。对研究感兴趣的特定主题进行标记通常需要多个经过训练的人类编码员花费大量时间和精力。虽然大型语言模型（LLM）在语言处理方面表现出色，但依赖于封闭源LLM的LLM辅助标记技术无法直接应用于自由文本数据，除非获得明确的外部使用许可。在本研究中，我们提出了一种框架，通过部署本地可部署的LLM来增强在隐私约束下自由文本数据中预定义主题的标记。类似于多人评分注释，该框架利用了多种开源LLM的多样性。集成方法寻求在LLM之间平衡一致性与分歧，由利用主题描述与LLM推理之间嵌入距离的相关性评分方法指导。我们使用可公开访问的与饮食障碍相关的Reddit数据以及饮食障碍患者自由文本回应进行了评估，两者都包含了人类注释。  我们发现以下几点：(1) 同等规模的LLM在标记方面存在差异性表现，有些显示较低的敏感性但较高的查准率，而另一些则表现出较高的敏感性但较低的查准率。(2) 相比单个LLM，LLM的集成在预测人类注释方面实现了最高准确度和最优的查准率与查全率权衡。(3) 跨LLM的相关性评分比二分类标签更具一致性，表明相关性评分方法有效地缓解了LLM标记中的差异性。
2501.08335	 | MERaLiON-TextLLM: Cross-Lingual Understanding of Large Language Models in Chinese, Indonesian, Malay, and Singlish	 | Xin Huang,Tarun Kumar Vangani,Minh Duc Pham,Xunlong Zou,Bin Wang,Zhengyuan Liu,Ai Ti Aw	 | MERaLiON-TextLLM是一系列针对汉语、印尼语、马来语和新加坡语优化的开源语言模型，通过专门的持续预训练和权重合并改进了这些语言的理解和生成能力，超过了官方Llama-3模型的表现。	 | Multilingual large language models (MLLMs) have shown impressive capabilities across a variety of languages. However, efficacy can differ greatly between different language families, especially for those with limited linguistic resources. This report presents MERaLiON-TextLLM, a series of open-source language models specifically tailored to improve understanding and generation in Chinese, Indonesian, Malay, and Singlish. The initial released model is built on Llama-3-8B-Base and refined through a meticulously crafted process of continued pre-training and weight merging. Our approach achieves performance improvements across benchmarks in these languages, exceeding the capabilities of the official Llama-3 models. We provide the model checkpoints as a resource to support further research and development in cross-lingual language understanding.	 | 多语言大规模语言模型（MLLMs）在多种语言中展现了令人印象深刻的性能。然而，不同语言家族之间效果差异很大，尤其是在那些资源有限的语言中。本报告介绍了一种名为MERaLiON-TextLLM的系列开源语言模型，专门针对提高汉语、印尼语、马来语和新加坡语的理解和生成能力进行优化。初始发布的模型基于Llama-3-8B-Base，并通过精细设计的持续预训练和权重合并过程进行完善。我们的方法在这些语言的基准测试中实现了性能提升，超过了官方的Llama-3模型的能力。我们提供模型检查点作为资源，支持跨语言语言理解的进一步研究和开发。
2501.09012	 | Multimodal LLMs Can Reason about Aesthetics in Zero-Shot	 | Ruixiang Jiang,Changwen Chen	 | 研究首次利用Multimodal LLMs评估艺术品的美学，并通过MM-StyleBench数据集和ArtCoT方法提升了MLLMs在艺术评价中的表现，揭示了其在美学推理中的固有主观性和挑战。	 | We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability shall be elicited to evaluate the aesthetics of artworks. To facilitate this investigation, we construct MM-StyleBench, a novel high-quality dataset for benchmarking artistic stylization. We then develop a principled method for human preference modeling and perform a systematic correlation analysis between MLLMs' responses and human preference. Our experiments reveal an inherent hallucination issue of MLLMs in art evaluation, associated with response subjectivity. ArtCoT is proposed, demonstrating that art-specific task decomposition and the use of concrete language boost MLLMs' reasoning ability for aesthetics. Our findings offer valuable insights into MLLMs for art and can benefit a wide range of downstream applications, such as style transfer and artistic image generation. Code available at https://github.com/songrise/MLLM4Art.	 | 我们首次探讨了如何通过Multimodal LLMs（MLLMs）的推理能力来评估艺术品的美学。为促进这一研究，我们构建了MM-StyleBench，这是一个新颖且高质量的数据集，用于评估艺术风格化。随后，我们开发了一种原理性的人类偏好建模方法，并系统地分析了MLLMs的响应与人类偏好的相关性。我们的实验揭示了MLLMs在艺术评价中固有的虚幻问题，与响应的主观性有关。我们提出了ArtCoT，表明特定于艺术的任务分解和使用具体语言可以提升MLLMs在美学上的推理能力。我们的发现为艺术领域的MLLMs提供了宝贵的见解，并能惠及一系列下游应用，如风格迁移和艺术图像生成。代码可在 https://github.com/songrise/MLLM4Art 获取。
2501.08925	 | Disentangling Exploration of Large Language Models by Optimal Exploitation	 | Tim Grams,Patrick Betz,Christian Bartelt	 | 这项研究将探索作为唯一的评估目标，要求代理提供能够提升未来收益的信息，并通过测量已探索状态的最优可实现回报来评估大型语言模型的探索能力，发现大多数模型在探索状态空间方面存在困难，但大型模型表现出更好的探索能力，并且提出的方法有助于揭示提示工程中的行为差异。	 | Exploration is a crucial skill for self-improvement and open-ended problem-solving. However, it remains uncertain whether large language models can effectively explore the state-space. Existing evaluations predominantly focus on the trade-off between exploration and exploitation, often assessed in multi-armed bandit problems. In contrast, this work isolates exploration as the sole objective, tasking the agent with delivering information that enhances future returns. For the evaluation, we propose to decompose missing rewards into exploration and exploitation components by measuring the optimal achievable return for the states already explored. Our experiments with various LLMs reveal that most models struggle to sufficiently explore the state-space and that weak exploration is insufficient. We observe a positive correlation between model size and exploration performance, with larger models demonstrating superior capabilities. Furthermore, we show that our decomposition provides insights into differences in behaviors driven by agent instructions during prompt engineering, offering a valuable tool for refining LLM performance in exploratory tasks.	 | 探索是自我提升和开放性问题解决的关键技能。然而，尚不确定大型语言模型是否能够有效地探索状态空间。现有的评估主要集中在探索与利用之间的权衡上，通常是在多臂老虎机问题中进行评估。相比之下，本工作将探索作为唯一的评估目标，要求代理提供能够提升未来收益的信息。为了进行评估，我们提出了一种将缺失的奖励分解为探索和利用两个部分的方法，通过测量已探索状态的最优可实现回报来进行衡量。我们的实验使用了多种LLM，结果显示大多数模型在探索状态空间方面存在困难，仅弱探索是不够的。我们观察到，模型大小与探索性能之间存在正相关性，大型模型展示了更好的能力。此外，我们展示了我们的分解方法能够揭示在提示工程过程中由代理指令驱动的行为差异，为改进LLM在探索性任务中的性能提供了有价值的工具。
2501.08828	 | MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents	 | Kuicai Dong,Yujing Chang,Xin Deik Goh,Dexun Li,Ruiming Tang,Yong Liu	 | 该研究提出了一种新的多模态文档检索基准MMDocIR，包含页面级和布局级检索任务，涵盖1,685个专家注释问题和173,843个-bootstrap标签，旨在改善多模态文档内容的识别和检索。实验结果表明，视觉检索器优于文本检索器，且利用视觉和语言模型的文本检索器表现最佳。	 | Multi-modal document retrieval is designed to identify and retrieve various forms of multi-modal content, such as figures, tables, charts, and layout information from extensive documents. Despite its significance, there is a notable lack of a robust benchmark to effectively evaluate the performance of systems in multi-modal document retrieval. To address this gap, this work introduces a new benchmark, named as MMDocIR, encompassing two distinct tasks: page-level and layout-level retrieval. The former focuses on localizing the most relevant pages within a long document, while the latter targets the detection of specific layouts, offering a more fine-grained granularity than whole-page analysis. A layout can refer to a variety of elements such as textual paragraphs, equations, figures, tables, or charts. The MMDocIR benchmark comprises a rich dataset featuring expertly annotated labels for 1,685 questions and bootstrapped labels for 173,843 questions, making it a pivotal resource for advancing multi-modal document retrieval for both training and evaluation. Through rigorous experiments, we reveal that (i) visual retrievers significantly outperform their text counterparts, (ii) MMDocIR train set can effectively benefit the training process of multi-modal document retrieval and (iii) text retrievers leveraging on VLM-text perform much better than those using OCR-text. These findings underscores the potential advantages of integrating visual elements for multi-modal document retrieval.	 | 多模态文档检索旨在识别和检索各种形式的多模态内容，如图表、表格、图表以及布局信息，从广泛的文档中选取。尽管其重要性不言而喻，但由于缺乏一个有效的基准来评估多模态文档检索系统的性能，这一领域仍然存在明显的不足。为解决这一问题，本研究引入了一个新的基准，名为MMDocIR，包含两个不同的任务：页面级检索和布局级检索。前者专注于在长文档中定位最相关页面，而后者则针对特定布局的检测，提供比整个页面分析更精细的粒度。布局可以是指文本段落、公式、图表、表格或图表等各种元素。MMDocIR基准包含了一个丰富数据集，其中包括1,685个问题的专家注释标签和173,843个问题的-bootstrap标签，这是一个重要的资源，可用于多模态文档检索的训练和评估。通过严格的实验，我们揭示了以下几点：（i）视觉检索器显著优于文本检索器；（ii）MMDocIR的训练集可以有效提升多模态文档检索的训练过程；（iii）利用VLM文本的文本检索器比使用OCR文本的检索器表现更好。这些发现强调了在多模态文档检索中整合视觉元素的潜在优势。
2501.08814	 | SAIF: A Comprehensive Framework for Evaluating the Risks of Generative AI in the Public Sector	 | Kyeongryul Lee,Heehyeon Kim,Joyce Jiyoung Whang	 | 该研究探讨了生成式AI在公共部门中的广泛应用及其潜在风险，并提出了一套系统化的数据生成框架（SAIF）来评估这些风险，确保AI的应用既安全又负责任。	 | The rapid adoption of generative AI in the public sector, encompassing diverse applications ranging from automated public assistance to welfare services and immigration processes, highlights its transformative potential while underscoring the pressing need for thorough risk assessments. Despite its growing presence, evaluations of risks associated with AI-driven systems in the public sector remain insufficiently explored. Building upon an established taxonomy of AI risks derived from diverse government policies and corporate guidelines, we investigate the critical risks posed by generative AI in the public sector while extending the scope to account for its multimodal capabilities. In addition, we propose a Systematic dAta generatIon Framework for evaluating the risks of generative AI (SAIF). SAIF involves four key stages: breaking down risks, designing scenarios, applying jailbreak methods, and exploring prompt types. It ensures the systematic and consistent generation of prompt data, facilitating a comprehensive evaluation while providing a solid foundation for mitigating the risks. Furthermore, SAIF is designed to accommodate emerging jailbreak methods and evolving prompt types, thereby enabling effective responses to unforeseen risk scenarios. We believe that this study can play a crucial role in fostering the safe and responsible integration of generative AI into the public sector.	 | 公共部门中生成式AI的迅速采用，涵盖了从自动化公共援助到福利服务和移民流程等多样化应用，突显了其变革潜力，同时也强调了进行全面风险评估的紧迫性。尽管生成式AI在公共部门中的存在日益增强，但对其相关风险的评估仍然不够深入。基于从各类政府政策和企业指南中提炼出的AI风险分类体系，我们研究了生成式AI在公共部门中所面临的关键风险，并将其范围扩展以涵盖其跨模态能力。此外，我们提出了一个系统化的数据生成框架（Systematic Data Generation Framework for Evaluating Generative AI, SAIF）以评估生成式AI的风险。SAIF 包含四个关键阶段：分解风险、设计场景、应用防御措施方法和探索提示类型。该框架确保了系统的和一致的生成提示数据，从而实现全面评估，并为风险缓解提供坚实基础。此外，SAIF 设计为能够适应不断出现的防御措施方法和变化中的提示类型，从而能够有效应对不可预见的风险情景。我们相信，这一研究可以在促进生成式AI在公共部门中的安全和负责任的应用方面发挥关键作用。
2501.08686	 | Knowledge Graph-based Retrieval-Augmented Generation for Schema Matching	 | Chuangtao Ma,Sriom Chakrabarti,Arijit Khan,Bálint Molnár	 | KG-RAG4SM 是一种基于知识图谱的检索增强生成模型，通过引入多种图检索方法和排序方案，有效解决了传统相似性模式匹配方法在处理语义歧义和冲突时的局限性，并且在多个数据集上的实验结果显著优于现有基于语言模型的方法。	 | Traditional similarity-based schema matching methods are incapable of resolving semantic ambiguities and conflicts in domain-specific complex mapping scenarios due to missing commonsense and domain-specific knowledge. The hallucination problem of large language models (LLMs) also makes it challenging for LLM-based schema matching to address the above issues. Therefore, we propose a Knowledge Graph-based Retrieval-Augmented Generation model for Schema Matching, referred to as the KG-RAG4SM. In particular, KG-RAG4SM introduces novel vector-based, graph traversal-based, and query-based graph retrievals, as well as a hybrid approach and ranking schemes that identify the most relevant subgraphs from external large knowledge graphs (KGs). We showcase that KG-based retrieval-augmented LLMs are capable of generating more accurate results for complex matching cases without any re-training. Our experimental results show that KG-RAG4SM outperforms the LLM-based state-of-the-art (SOTA) methods (e.g., Jellyfish-8B) by 35.89% and 30.50% in terms of precision and F1 score on the MIMIC dataset, respectively; KG-RAG4SM with GPT-4o-mini outperforms the pre-trained language model (PLM)-based SOTA methods (e.g., SMAT) by 69.20% and 21.97% in terms of precision and F1 score on the Synthea dataset, respectively. The results also demonstrate that our approach is more efficient in end-to-end schema matching, and scales to retrieve from large KGs. Our case studies on the dataset from the real-world schema matching scenario exhibit that the hallucination problem of LLMs for schema matching is well mitigated by our solution.	 | 传统的基于相似性模式匹配方法在处理特定领域复杂映射场景中的语义歧义和冲突时，由于缺乏常识和领域特定知识而能力有限。大型语言模型（LLMs）的幻觉问题也使得基于LLM的模式匹配难以解决上述问题。因此，我们提出了一种基于知识图谱的检索增强生成模型，称为KG-RAG4SM。特别是，KG-RAG4SM 引入了新颖的向量基于、图遍历基于和查询基于的图检索方法，以及一种混合方法和排序方案，能够从外部大知识图谱（KGs）中识别出最相关的子图。我们展示了基于知识图谱的检索增强LLM能够生成更准确的结果，而无需任何再训练。我们的实验结果表明，KG-RAG4SM 在MIMIC 数据集上的准确率和F1分数方面分别比基于LLM的当前最佳方法（例如，Jellyfish-8B）高出35.89%和30.50%；而KG-RAG4SM 使用GPT-4o-mini 在Synthea 数据集上的准确率和F1分数方面分别比基于预训练语言模型（PLM）的最佳方法（例如，SMAT）高出69.20%和21.97%。结果还表明，我们的方法在端到端模式匹配中更为高效，并能够扩展以检索大型KG中的数据。我们的案例研究展示了从真实世界模式匹配场景中获得的数据集的结果，表明我们的解决方案有效地缓解了LLMs在模式匹配中的幻觉问题。
2501.08631	 | SWSC: Shared Weight for Similar Channel in LLM	 | Binrui Zeng,Yongtao Tang,Xiaodong Liu,Xiaopeng Li	 | 该研究提出了SWSC方法，通过逐通道聚类和选择代表向量来压缩大型语言模型，同时通过奇异值分解保留重要信息以减少性能损失，从而有效降低存储和计算负担。	 | Large language models (LLMs) have spurred development in multiple industries. However, the growing number of their parameters brings substantial storage and computing burdens, making it essential to explore model compression techniques for parameter reduction and easier deployment. We propose SWSC, an LLM compression method based on the concept of Shared Weight for Similar Channel. It uses the K-Means clustering algorithm to cluster model weights channel-by-channel, generating clusters with highly similar vectors within each. A representative vector from each cluster is selected to approximately replace all vectors in the cluster, significantly reducing the number of model weight parameters. However, approximate restoration will inevitably cause damage to the performance of the model. To tackle this issue, we perform singular value decomposition on the weight error values before and after compression and retain the larger singular values and their corresponding singular vectors to compensate for the accuracy. The experimental results show that our method can effectively ensure the performance of the compressed LLM even under low-precision conditions.	 | 大型语言模型（LLMs）在多个行业的发展中发挥了重要作用。然而，随着其参数数量的增加，带来了显著的存储和计算负担，因此探索模型压缩技术以减少参数并便于部署变得至关重要。我们提出了SWSC，一种基于“共享相似通道权重”概念的LLM压缩方法。该方法使用K-Means聚类算法逐通道聚类模型权重，生成每个聚类内部具有高度相似向量的聚类。从每个聚类中选择一个代表向量来近似替换聚类中的所有向量，从而显著减少模型权重参数的数量。然而，这种近似恢复不可避免地会对模型的性能造成损害。为解决这一问题，我们在压缩前后对权重误差值进行奇异值分解，并保留较大的奇异值及其相应的奇异向量以补偿精度损失。实验结果表明，即使在低精度条件下，我们的方法也能有效保证压缩后的LLM性能。
2501.08617	 | RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation	 | Kaiqu Liang,Haimin Hu,Ryan Liu,Thomas L. Griffiths,Jaime Fernández Fisac	 | 该研究指出，现有的基于即时人类反馈的强化学习方法可能因Goodhart定律而引发不良行为，建议采用后视模拟强化学习（RLHS）来减轻未对齐问题，通过模拟下游后果并基于此收集反馈，从而提高模型的对齐程度和用户满意度。	 | Generative AI systems like foundation models (FMs) must align well with human values to ensure their behavior is helpful and trustworthy. While Reinforcement Learning from Human Feedback (RLHF) has shown promise for optimizing model performance using human judgments, existing RLHF pipelines predominantly rely on immediate feedback, which can fail to accurately reflect the downstream impact of an interaction on users' utility. We demonstrate that feedback based on evaluators' foresight estimates of downstream consequences systematically induces Goodhart's Law dynamics, incentivizing misaligned behaviors like sycophancy and deception and ultimately degrading user outcomes. To alleviate this, we propose decoupling evaluation from prediction by refocusing RLHF on hindsight feedback. Our theoretical analysis reveals that conditioning evaluator feedback on downstream observations mitigates misalignment and improves expected human utility, even when these observations are simulated by the AI system itself. To leverage this insight in a practical alignment algorithm, we introduce Reinforcement Learning from Hindsight Simulation (RLHS), which first simulates plausible consequences and then elicits feedback to assess what behaviors were genuinely beneficial in hindsight. We apply RLHS to two widely-employed online and offline preference optimization methods -- Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) -- and show empirically that misalignment is significantly reduced with both methods. Through an online human user study, we show that RLHS consistently outperforms RLHF in helping users achieve their goals and earns higher satisfaction ratings, despite being trained solely with simulated hindsight feedback. These results underscore the importance of focusing on long-term consequences, even simulated ones, to mitigate misalignment in RLHF.	 | 生成型人工智能系统，如基础模型（FMs），必须与人类价值观良好对齐，以确保其行为具有帮助性和可信度。尽管人类反馈强化学习（RLHF）已经展示了通过人类判断优化模型性能的潜力，但现有的RLHF管道主要依赖即时反馈，这可能无法准确反映交互对用户效用的下游影响。我们证明，基于评估者对下游后果的前瞻性估计的反馈系统系统性地引发了Goodhart定律动态，激励了诸如阿谀奉承和欺骗等未对齐行为，最终损害了用户结果。为了缓解这一问题，我们提出将评价与预测脱钩，重新将RLHF聚焦于后视反馈。我们的理论分析显示，根据下游观察调整评估者的反馈可以减轻未对齐并提高预期的人类效用，即使这些观察是AI系统自身模拟的。为了在实际对齐算法中利用这一见解，我们引入了后视模拟强化学习（RLHS），首先模拟可能的后果，然后收集反馈以评估哪些行为在事后看来是有益的。我们用RLHS对两种广泛使用的在线和离线偏好优化方法——近端策略优化（PPO）和直接偏好优化（DPO）——进行应用，并证明两种方法中未对齐显著减少。通过在线的人类用户研究，我们展示了即使仅使用模拟后视反馈进行训练，RLHS也比RLHF更有效地帮助用户实现目标，并获得更高的满意度评分。这些结果强调了在RLHF中重视长期后果（即使是模拟的）的重要性，以减轻未对齐。
2501.08460	 | Towards Zero-Shot & Explainable Video Description by Reasoning over Graphs of Events in Space and Time	 | Mihai Masala,Marius Leordeanu	 | 本文提出了一种基于空间和时间事件的方法，将视觉与语言的先进学习模型连接起来，以编程化且可解释的方式理解视频内容，并生成连贯且相关的文本描述，验证结果表明该方法在多个数据集上表现良好，能生成高质量的描述性文本。	 | In the current era of Machine Learning, Transformers have become the de facto approach across a variety of domains, such as computer vision and natural language processing. Transformer-based solutions are the backbone of current state-of-the-art methods for language generation, image and video classification, segmentation, action and object recognition, among many others. Interestingly enough, while these state-of-the-art methods produce impressive results in their respective domains, the problem of understanding the relationship between vision and language is still beyond our reach. In this work, we propose a common ground between vision and language based on events in space and time in an explainable and programmatic way, to connect learning-based vision and language state of the art models and provide a solution to the long standing problem of describing videos in natural language. We validate that our algorithmic approach is able to generate coherent, rich and relevant textual descriptions on videos collected from a variety of datasets, using both standard metrics (e.g. Bleu, ROUGE) and the modern LLM-as-a-Jury approach.	 | 在当前的机器学习时代，Transformer 已经在多个领域，如计算机视觉和自然语言处理中成为常规方法。基于 Transformer 的解决方案是当前语言生成、图像和视频分类、分割、动作和物体识别等多种先进方法的核心。有趣的是，尽管这些最先进的方法在各自领域产生了令人印象深刻的成果，但理解视觉与语言之间关系的问题仍然超出了我们的能力范围。在这项工作中，我们提出了一种基于空间和时间事件的视觉与语言之间的共同点，以一种可解释且编程化的方式将基于学习的视觉和语言先进模型连接起来，并提供了一种描述视频的自然语言的长期解决方案。我们验证了我们的算法方法能够通过使用标准指标（例如 Bleu、ROUGE）以及现代的“LLM作为陪审团”方法，生成来自各种数据集收集的视频的连贯、丰富且相关性的文本描述。
2501.08454	 | Tag&Tab: Pretraining Data Detection in Large Language Models Using Keyword-Based Membership Inference Attack	 | Sagiv Antebi,Edan Habler,Asaf Shabtai,Yuval Elovici	 | Tag&Tab 是一种新颖的方法，通过为输入文本中的关键词打标签并利用大型语言模型计算这些关键词的概率，从而有效检测 LLM 预训练数据中的成员身份推断攻击，相较于现有方法，其 AUC 分数平均提高了 4.1% 到 12.1%。	 | Large language models (LLMs) have become essential digital task assistance tools. Their training relies heavily on the collection of vast amounts of data, which may include copyright-protected or sensitive information. Recent studies on the detection of pretraining data in LLMs have primarily focused on sentence-level or paragraph-level membership inference attacks (MIAs), usually involving probability analysis of the target model prediction tokens. However, the proposed methods often demonstrate poor performance, specifically in terms of accuracy, failing to account for the semantic importance of textual content and word significance. To address these shortcomings, we propose Tag&Tab, a novel approach for detecting data that has been used as part of the LLM pretraining. Our method leverages advanced natural language processing (NLP) techniques to tag keywords in the input text - a process we term Tagging. Then, the LLM is used to obtain the probabilities of these keywords and calculate their average log-likelihood to determine input text membership, a process we refer to as Tabbing. Our experiments on three benchmark datasets (BookMIA, MIMIR, and the Pile) and several open-source LLMs of varying sizes demonstrate an average increase in the AUC scores ranging from 4.1% to 12.1% over state-of-the-art methods. Tag&Tab not only sets a new standard for data leakage detection in LLMs, but its outstanding performance is a testament to the importance of words in MIAs on LLMs.	 | 大型语言模型（LLMs）已经成为数字任务辅助的重要工具。它们的训练依赖于大量数据的收集，这些数据可能包括受版权保护或敏感信息。最近关于检测LLMs预训练数据的研究主要集中在句级或段落级的成员身份推断攻击（MIA），通常涉及目标模型预测令牌的概率分析。然而，提出的许多方法在性能上表现不佳，尤其是在准确性方面，未能考虑到文本内容的语义重要性和词的意义。为了解决这些不足，我们提出了Tag&Tab，一种新颖的方法来检测在LLM预训练过程中使用的数据。我们的方法利用先进的自然语言处理（NLP）技术为输入文本中的关键词打标签——我们将其称为“标记”过程。然后，使用LLM获取这些关键词的概率，并计算它们的平均对数似然性来确定输入文本的成员身份——我们将其称为“表征”过程。我们在三个基准数据集（BookMIA、MIMIR和Pile）以及多种不同规模的开源LLM上进行的实验表明，与最先进的方法相比，AUC分数平均提高了4.1%到12.1%。Tag&Tab不仅为LLMs中的数据泄漏检测设立了新标准，而且其卓越的性能证明了在LLMs中的MIA对词的重要性。
2501.08421	 | SEAL: Speaker Error Correction using Acoustic-conditioned Large Language Models	 | Anurag Kumar,Rohit Paturi,Amber Afshan,Sundararajan Srinivasan	 | 本文提出了一种新的声学条件化方法，利用发音分割器提供的精细信息改进大型语言模型的说话人错误修正，从而在Fisher、Callhome和RT03-CTS数据集上将说话人错误率降低了24-43%。此外，该方法通过使用简单的约束解码策略减少了语言模型的幻觉现象，避免了复杂的后处理。	 | Speaker Diarization (SD) is a crucial component of modern end-to-end ASR pipelines. Traditional SD systems, which are typically audio-based and operate independently of ASR, often introduce speaker errors, particularly during speaker transitions and overlapping speech. Recently, language models including fine-tuned large language models (LLMs) have shown to be effective as a second-pass speaker error corrector by leveraging lexical context in the transcribed output. In this work, we introduce a novel acoustic conditioning approach to provide more fine-grained information from the acoustic diarizer to the LLM. We also show that a simpler constrained decoding strategy reduces LLM hallucinations, while avoiding complicated post-processing. Our approach significantly reduces the speaker error rates by 24-43% across Fisher, Callhome, and RT03-CTS datasets, compared to the first-pass Acoustic SD.	 | 以下是对给定摘要的中文翻译：  说话人分割（SD）是现代端到端ASR流水线中的一个关键组成部分。传统的SD系统通常是基于音频的，并且独立于ASR系统操作，这往往会在说话人转换和重叠语音时引入说话人错误。最近，包括微调过的大型语言模型（LLMs）在内的语言模型已被证明可以通过利用转录输出中的词汇上下文有效地作为第二阶段的说话人错误修正器。在本研究中，我们介绍了一种新颖的声学条件化方法，以向LLM提供更加精细的信息来自发音分割器。我们还展示了使用更简单的约束解码策略可以减少LLM的幻觉现象，同时避免复杂的后处理。与第一次通过的声学SD相比，我们的方法在Fisher、Callhome和RT03-CTS数据集上显著降低了24-43%的说话人错误率。
2501.08406	 | OptiChat: Bridging Optimization Models and Practitioners with Large Language Models	 | Hao Chen,Gonzalo Esteban Constante-Flores,Krishna Sri Ipsit Mantri,Sai Madhukiran Kompalli,Akshdeep Singh Ahluwalia,Can Li	 | OptiChat是一种自然语言对话系统，旨在帮助非优化专家用户解释和交互优化模型，通过定制的功能调用和代码生成增强大型语言模型，实验表明其能有效提升用户的理解和使用体验。	 | Optimization models have been applied to solve a wide variety of decision-making problems. These models are usually developed by optimization experts but are used by practitioners without optimization expertise in various application domains. As a result, practitioners often struggle to interact with and draw useful conclusions from optimization models independently. To fill this gap, we introduce OptiChat, a natural language dialogue system designed to help practitioners interpret model formulation, diagnose infeasibility, analyze sensitivity, retrieve information, evaluate modifications, and provide counterfactual explanations. By augmenting large language models (LLMs) with functional calls and code generation tailored for optimization models, we enable seamless interaction and minimize the risk of hallucinations in OptiChat. We develop a new dataset to evaluate OptiChat's performance in explaining optimization models. Experiments demonstrate that OptiChat effectively bridges the gap between optimization models and practitioners, delivering autonomous, accurate, and instant responses.	 | 优化模型已被应用于解决各种决策问题。这些模型通常由优化专家开发，但被不同应用领域的非优化专家用户使用。这导致用户难以独立地与优化模型进行交互并从中得出有用结论。为了解决这个问题，我们提出了OptiChat，这是一种自然语言对话系统，旨在帮助用户解释模型公式、诊断不可行性、分析灵敏度、检索信息、评估修改并提供反事实解释。通过为优化模型量身定制的功能调用和代码生成增强大型语言模型（LLMs），我们使OptiChat能够无缝交互并最大限度地减少幻觉风险。我们开发了一个新的数据集来评估OptiChat在解释优化模型方面的性能。实验结果表明，OptiChat有效弥合了优化模型与用户之间的差距，提供了自主、准确且即时的响应。
2501.08365	 | Towards Best Practices for Open Datasets for LLM Training	 | Stefan Baack,Stella Biderman,Kasia Odrozek,Aviya Skowron,Ayah Bdeir,Jillian Bommarito,Jennifer Ding,Maximilian Gahntz,Paul Keller,Pierre-Carl Langlais,Greg Lindahl,Sebastian Majstorovic,Nik Marda,Guilherme Penedo,Maarten Van Segbroeck,Jennifer Wang,Leandro von Werra,Mitchell Baker,Julie Belião,Kasia Chmielinski,Marzieh Fadaee,Lisa Gutermuth,Hynek Kydlíček,Greg Leppert,EM Lewis-Jong,Solana Larsen,Shayne Longpre,Angela Oduor Lungati,Cullen Miller,Victor Miller,Max Ryabinin,Kathleen Siminyu,Andrew Strait,Mark Surman,Anna Tumadóttir,Maurice Weber,Rebecca Weiss,Lee White,Thomas Wolf	 | 许多人工智能公司在未经版权所有者许可的情况下训练大型语言模型，这引发了版权诉讼并导致了共享训练数据信息的减少，从而影响了透明度、问责制和创新；为了未来能够使用负责任地收集和管理的开放许可数据训练AI系统，需要法律、技术和政策领域的合作与投资。	 | Many AI companies are training their large language models (LLMs) on data without the permission of the copyright owners. The permissibility of doing so varies by jurisdiction: in countries like the EU and Japan, this is allowed under certain restrictions, while in the United States, the legal landscape is more ambiguous. Regardless of the legal status, concerns from creative producers have led to several high-profile copyright lawsuits, and the threat of litigation is commonly cited as a reason for the recent trend towards minimizing the information shared about training datasets by both corporate and public interest actors. This trend in limiting data information causes harm by hindering transparency, accountability, and innovation in the broader ecosystem by denying researchers, auditors, and impacted individuals access to the information needed to understand AI models.   While this could be mitigated by training language models on open access and public domain data, at the time of writing, there are no such models (trained at a meaningful scale) due to the substantial technical and sociological challenges in assembling the necessary corpus. These challenges include incomplete and unreliable metadata, the cost and complexity of digitizing physical records, and the diverse set of legal and technical skills required to ensure relevance and responsibility in a quickly changing landscape. Building towards a future where AI systems can be trained on openly licensed data that is responsibly curated and governed requires collaboration across legal, technical, and policy domains, along with investments in metadata standards, digitization, and fostering a culture of openness.	 | 许多人工智能公司正在未经版权所有者许可的情况下训练其大型语言模型（LLMs）。不同司法管辖区对此的许可程度有所不同：在欧盟和日本等国家，这在一定限制下是被允许的，而在美国，法律环境则更加模糊。无论从法律角度来看如何，创意生产者的担忧已经导致了一系列高调的版权诉讼案件，诉讼威胁通常被视为导致近期企业和公共利益行为体减少共享训练数据信息趋势的一个重要原因。这种限制数据信息的趋势导致了透明度、问责制和创新的减缓，因为它剥夺了研究人员、审计人员和受影响个人获取理解AI模型所需信息的渠道。  尽管可以通过使用开放访问和公共领域数据来训练语言模型来减轻这种影响，但截至撰写本文时，尚未有此类大规模训练的模型，因为组装必要的语料库面临着巨大的技术和社会挑战。这些挑战包括不完整和不可靠的元数据、物理记录数字化的成本和复杂性，以及在快速变化的环境中确保相关性和负责任性所需的多样化的法律和专业技术技能。为了在未来能够训练使用负责任地收集和管理的开放许可数据的AI系统，需要在法律、技术和政策领域进行合作，同时投资于元数据标准、数字化和培养开放文化。
