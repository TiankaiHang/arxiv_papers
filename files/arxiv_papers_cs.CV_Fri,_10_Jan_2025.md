| ID | Title | Authors | Summary (zh) | Abstract (en) | Abstract (zh) | 
| --- | --- | --- | --- | --- | --- |
2501.05453	 | An Empirical Study of Autoregressive Pre-training from Videos	 | Jathushan Rajasegaran,Ilija Radosavovic,Rahul Ravishankar,Yossi Gandelsman,Christoph Feichtenhofer,Jitendra Malik	 | 研究通过构建自回归视频模型Toto，在大量多样化视频和图像数据上进行预训练，并在多个下游任务中评估其表现，结果显示自回归预训练方法能够产生具有竞争力的视觉表示，且其扩展曲线与语言模型相似。	 | We empirically study autoregressive pre-training from videos. To perform our study, we construct a series of autoregressive video models, called Toto. We treat videos as sequences of visual tokens and train transformer models to autoregressively predict future tokens. Our models are pre-trained on a diverse dataset of videos and images comprising over 1 trillion visual tokens. We explore different architectural, training, and inference design choices. We evaluate the learned visual representations on a range of downstream tasks including image recognition, video classification, object tracking, and robotics. Our results demonstrate that, despite minimal inductive biases, autoregressive pre-training leads to competitive performance across all benchmarks. Finally, we find that scaling our video models results in similar scaling curves to those seen in language models, albeit with a different rate. More details at https://brjathu.github.io/toto/	 | 我们通过视频数据来实证研究自回归预训练方法。为了进行这项研究，我们构建了一系列自回归视频模型，称为Toto。我们将视频视为视觉标记序列，并训练变压器模型以自回归方式预测未来的标记。我们的模型在包含超过1万亿个视觉标记的多样化视频和图像数据集上进行了预训练。我们探索了不同架构、训练和推理设计的选择。我们在包括图像识别、视频分类、物体跟踪和机器人学等多个下游任务上评估了学到的视觉表示。我们的结果表明，尽管自回归预训练几乎没有先验偏置，但其在所有基准上的表现仍然具有竞争力。最后，我们发现，扩展我们的视频模型会呈现出与语言模型相似的扩展曲线，尽管扩展速度不同。更多细节请参见<https://brjathu.github.io/toto/>。
2501.05452	 | ReFocus: Visual Editing as a Chain of Thought for Structured Image Understanding	 | Xingyu Fu,Minqian Liu,Zhengyuan Yang,John Corring,Yijuan Lu,Jianwei Yang,Dan Roth,Dinei Florencio,Cha Zhang	 | ReFocus 是一种简单有效的框架，通过允许多模态大语言模型生成视觉编辑代码来增强它们的多跳选择性注意力能力，从而提高结构化图像理解任务的表现，特别是在处理表格和图表时显著提高了性能。此外，ReFocus 收集的包含视觉链式思维的训练集进一步提升了模型的推理能力。	 | Structured image understanding, such as interpreting tables and charts, requires strategically refocusing across various structures and texts within an image, forming a reasoning sequence to arrive at the final answer. However, current multimodal large language models (LLMs) lack this multihop selective attention capability. In this work, we introduce ReFocus, a simple yet effective framework that equips multimodal LLMs with the ability to generate "visual thoughts" by performing visual editing on the input image through code, shifting and refining their visual focuses. Specifically, ReFocus enables multimodal LLMs to generate Python codes to call tools and modify the input image, sequentially drawing boxes, highlighting sections, and masking out areas, thereby enhancing the visual reasoning process. We experiment upon a wide range of structured image understanding tasks involving tables and charts. ReFocus largely improves performance on all tasks over GPT-4o without visual editing, yielding an average gain of 11.0% on table tasks and 6.8% on chart tasks. We present an in-depth analysis of the effects of different visual edits, and reasons why ReFocus can improve the performance without introducing additional information. Further, we collect a 14k training set using ReFocus, and prove that such visual chain-of-thought with intermediate information offers a better supervision than standard VQA data, reaching a 8.0% average gain over the same model trained with QA pairs and 2.6% over CoT.	 | 结构化的图像理解，如解释表格和图表，需要在图像中的各种结构和文本之间有策略地重新聚焦，并形成一个推理序列以得出最终答案。然而，当前的多模态大型语言模型（LLMs）缺乏这种多跳选择性注意力的能力。在这项工作中，我们引入了ReFocus，这是一个简单而有效的框架，它通过代码对输入图像进行视觉编辑，从而使多模态LLMs具备生成“视觉思考”的能力，从而重新聚焦并细化其视觉关注点。具体而言，ReFocus使多模态LLMs能够生成Python代码来调用工具并修改输入图像，序列化地绘制框、突出显示部分并屏蔽区域，从而增强视觉推理过程。我们对涉及表格和图表的多种结构化图像理解任务进行了实验。ReFocus在所有任务上显著提高了性能，与GPT-4o相比，在表格任务上的平均提高11.0%，在图表任务上的平均提高6.8%。我们深入分析了不同视觉编辑的效果，并解释了为什么ReFocus可以在不引入额外信息的情况下提高性能。此外，我们使用ReFocus收集了一个包含14,000个样本的训练集，并证明了这种视觉的链式思维与中间信息提供的监督比标准的VQA数据更有效，与使用问题-答案对训练的模型相比，平均提高了8.0%，与使用CoT训练的模型相比，平均提高了2.6%。
2501.05450	 | Decentralized Diffusion Models	 | David McAllister,Matthew Tancik,Jiaming Song,Angjoo Kanazawa	 | 去中心化的扩散模型通过在独立集群之间分布训练过程，消除了对集中式高带宽网络架构的依赖，从而降低了基础设施成本并提高了对局部故障的弹性；该方法在多个“计算岛”上分散训练负担，适用于更小且成本效益更高的计算资源，并显著提高了训练效率和模型质量。	 | Large-scale AI model training divides work across thousands of GPUs, then synchronizes gradients across them at each step. This incurs a significant network burden that only centralized, monolithic clusters can support, driving up infrastructure costs and straining power systems. We propose Decentralized Diffusion Models, a scalable framework for distributing diffusion model training across independent clusters or datacenters by eliminating the dependence on a centralized, high-bandwidth networking fabric. Our method trains a set of expert diffusion models over partitions of the dataset, each in full isolation from one another. At inference time, the experts ensemble through a lightweight router. We show that the ensemble collectively optimizes the same objective as a single model trained over the whole dataset. This means we can divide the training burden among a number of "compute islands," lowering infrastructure costs and improving resilience to localized GPU failures. Decentralized diffusion models empower researchers to take advantage of smaller, more cost-effective and more readily available compute like on-demand GPU nodes rather than central integrated systems. We conduct extensive experiments on ImageNet and LAION Aesthetics, showing that decentralized diffusion models FLOP-for-FLOP outperform standard diffusion models. We finally scale our approach to 24 billion parameters, demonstrating that high-quality diffusion models can now be trained with just eight individual GPU nodes in less than a week.	 | 以下是该摘要的中文翻译：  大规模AI模型训练将工作分散到数千个GPU上，然后在每一步中同步梯度。这会产生巨大的网络负担，只有集中式的大型集群才能支持，从而推高了基础设施成本并给电力系统带来了压力。我们提出了去中心化的扩散模型，这是一种在独立集群或数据中心之间分布扩散模型训练的可扩展框架，通过消除对集中式高带宽网络架构的依赖。我们的方法在数据集的不同分区上训练一系列专家扩散模型，彼此完全独立。在推理阶段，专家模型通过一个轻量级路由器进行ensemble。我们证明，ensemble的整体效用与在完整数据集上训练的单一模型相同。这意味着我们可以将训练负担分布在多个“计算岛”上，从而降低基础设施成本并提高对局部GPU故障的弹性。去中心化的扩散模型使研究人员能够利用更小、成本效益更高且更易获得的计算资源（如按需的GPU节点），而不仅仅是集中式集成系统。我们在ImageNet和LAION美学数据集上进行了广泛实验，证明去中心化的扩散模型在FLOP对FLOP的基础上优于标准扩散模型。我们最终将该方法扩展到240亿参数，展示了现在只需八个多GPU节点便可以在不到一周的时间内训练出高质量的扩散模型。
2501.05449	 | Explainable AI-Enhanced Deep Learning for Pumpkin Leaf Disease Detection: A Comparative Analysis of CNN Architectures	 | Md. Arafat Alam Khandaker,Ziyan Shirin Raha,Shifat Islam,Tashreef Muhammad	 | 本研究利用“南瓜叶片疾病数据集”中的2000张高分辨率图像，通过多种深度学习架构比较，发现ResNet50表现最佳，准确率达到90.5%，并结合可解释AI方法提高了对模型决策的理解和信任，有望实现更早更准确的南瓜叶片疾病检测。	 | Pumpkin leaf diseases are significant threats to agricultural productivity, requiring a timely and precise diagnosis for effective management. Traditional identification methods are laborious and susceptible to human error, emphasizing the necessity for automated solutions. This study employs on the "Pumpkin Leaf Disease Dataset", that comprises of 2000 high-resolution images separated into five categories. Downy mildew, powdery mildew, mosaic disease, bacterial leaf spot, and healthy leaves. The dataset was rigorously assembled from several agricultural fields to ensure a strong representation for model training. We explored many proficient deep learning architectures, including DenseNet201, DenseNet121, DenseNet169, Xception, ResNet50, ResNet101 and InceptionResNetV2, and observed that ResNet50 performed most effectively, with an accuracy of 90.5% and comparable precision, recall, and F1-Score. We used Explainable AI (XAI) approaches like Grad-CAM, Grad-CAM++, Score-CAM, and Layer-CAM to provide meaningful representations of model decision-making processes, which improved understanding and trust in automated disease diagnostics. These findings demonstrate ResNet50's potential to revolutionize pumpkin leaf disease detection, allowing for earlier and more accurate treatments.	 | 南瓜叶片疾病是农业生产中的重大威胁，需要及时而精准的诊断以便有效管理。传统识别方法耗时且容易出错，强调了自动解决方案的必要性。本研究采用“南瓜叶片疾病数据集”，该数据集包含2000张高分辨率图像，分为五个类别：灰霉病、白粉病、花叶病、细菌性斑点病以及健康叶片。数据集从多个农业田地精心收集，以确保模型训练时有足够的代表性。我们探索了多种高效的深度学习架构，包括DenseNet201、DenseNet121、DenseNet169、Xception、ResNet50、ResNet101和InceptionResNetV2，并观察到ResNet50表现最佳，准确率为90.5%，且在精确率、召回率和F1分数方面具有可比性。我们使用可解释AI（XAI）方法，如Grad-CAM、Grad-CAM++、Score-CAM和Layer-CAM，为模型决策过程提供有意义的表示，从而提高了对自动疾病诊断的理解和信任。这些发现表明，ResNet50 有可能彻底改变南瓜叶片疾病的检测，实现更早、更准确的治疗。
2501.05446	 | Relative Pose Estimation through Affine Corrections of Monocular Depth Priors	 | Yifan Yu,Shaohui Liu,Rémi Pautrat,Marc Pollefeys,Viktor Larsson	 | 本文开发了三种专门用于相对姿态估计的求解器，考虑了仿射不变的尺度和位移模态性，并提出了一种混合估计管道结合经典点基求解器和极线约束，显著提高了标定和非标定条件下的相对姿态估计性能。	 | Monocular depth estimation (MDE) models have undergone significant advancements over recent years. Many MDE models aim to predict affine-invariant relative depth from monocular images, while recent developments in large-scale training and vision foundation models enable reasonable estimation of metric (absolute) depth. However, effectively leveraging these predictions for geometric vision tasks, in particular relative pose estimation, remains relatively under explored. While depths provide rich constraints for cross-view image alignment, the intrinsic noise and ambiguity from the monocular depth priors present practical challenges to improving upon classic keypoint-based solutions. In this paper, we develop three solvers for relative pose estimation that explicitly account for independent affine (scale and shift) ambiguities, covering both calibrated and uncalibrated conditions. We further propose a hybrid estimation pipeline that combines our proposed solvers with classic point-based solvers and epipolar constraints. We find that the affine correction modeling is beneficial to not only the relative depth priors but also, surprisingly, the ``metric" ones. Results across multiple datasets demonstrate large improvements of our approach over classic keypoint-based baselines and PnP-based solutions, under both calibrated and uncalibrated setups. We also show that our method improves consistently with different feature matchers and MDE models, and can further benefit from very recent advances on both modules. Code is available at https://github.com/MarkYu98/madpose.	 | 单目深度估计（MDE）模型在过去几年中取得了显著的进步。许多MDE模型致力于从单目图像中预测仿射不变的相对深度，而大规模训练和视觉基础模型的最新发展使绝对深度的合理估计成为可能。然而，有效地利用这些预测以几何视觉任务，特别是相对姿态估计来说，仍然相对未被充分探索。尽管深度提供了丰富的跨视图图像对齐约束，但单目深度的固有噪声和模态性给基于经典关键点的解决方案带来了实际挑战。在本文中，我们开发了三种用于相对姿态估计的求解器，这些求解器明确考虑了独立的仿射（尺度和位移）模态性，涵盖了标定和非标定条件。我们进一步提出了一种混合估计管道，将我们提出的求解器与经典基于点的求解器和极线约束相结合。我们发现仿射校正建模不仅对相对深度先验有利，而且还意外地对“度量”先验也有益。在多个数据集上的结果表明，我们的方法在标定和非标定设置下均大幅优于经典的关键点基础基线和PnP基础解决方案。我们还展示了我们的方法在不同特征匹配器和MDE模型下的一致改进，并且可以从这两个模块的最新进展中进一步受益。代码可在https://github.com/MarkYu98/madpose获得。
2501.05445	 | Consistent Flow Distillation for Text-to-3D Generation	 | Runjie Yan,Yinbo Chen,Xiaolong Wang	 | 持续流动提炼（CFD）通过引入多视角一致的高斯噪声改进了3D生成模型，解决了Score Distillation Sampling (SDS)带来的视觉质量下降和多样性降低问题，在文本到3D生成任务中显著优于之前的方法。	 | Score Distillation Sampling (SDS) has made significant strides in distilling image-generative models for 3D generation. However, its maximum-likelihood-seeking behavior often leads to degraded visual quality and diversity, limiting its effectiveness in 3D applications. In this work, we propose Consistent Flow Distillation (CFD), which addresses these limitations. We begin by leveraging the gradient of the diffusion ODE or SDE sampling process to guide the 3D generation. From the gradient-based sampling perspective, we find that the consistency of 2D image flows across different viewpoints is important for high-quality 3D generation. To achieve this, we introduce multi-view consistent Gaussian noise on the 3D object, which can be rendered from various viewpoints to compute the flow gradient. Our experiments demonstrate that CFD, through consistent flows, significantly outperforms previous methods in text-to-3D generation.	 | Score Distillation Sampling (SDS) 在3D生成的图像生成模型提炼方面取得了显著进展。然而，其最大似然估计行为往往导致视觉质量下降和多样性降低，限制了其在3D应用中的有效性。在这项工作中，我们提出了持续流动提炼（Consistent Flow Distillation, CFD），以解决这些限制。我们首先利用扩散ODE或SDE采样过程的梯度来指导3D生成。从基于梯度的采样角度来看，我们发现不同视角下2D图像流的一致性对于高质量3D生成至关重要。为了实现这一点，我们在3D物体上引入了多视角一致的高斯噪声，可以从不同视角渲染这些噪声来计算流的梯度。我们的实验表明，通过一致的流动，CFD 在文本到3D生成方面显著优于之前的 方法。
2501.05444	 | Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark	 | Yunzhuo Hao,Jiawei Gu,Huichen Will Wang,Linjie Li,Zhengyuan Yang,Lijuan Wang,Yu Cheng	 | EMMA基准测试旨在评估多模态大型语言模型在数学、物理、化学和编码领域进行有机文本-图像推理的能力，发现现有模型在处理复杂多模态和多步骤推理任务时存在显著局限性，需要改进多模态架构和训练范式。	 | The ability to organically reason over and with both text and images is a pillar of human intelligence, yet the ability of Multimodal Large Language Models (MLLMs) to perform such multimodal reasoning remains under-explored. Existing benchmarks often emphasize text-dominant reasoning or rely on shallow visual cues, failing to adequately assess integrated visual and textual reasoning. We introduce EMMA (Enhanced MultiModal reAsoning), a benchmark targeting organic multimodal reasoning across mathematics, physics, chemistry, and coding. EMMA tasks demand advanced cross-modal reasoning that cannot be addressed by reasoning independently in each modality, offering an enhanced test suite for MLLMs' reasoning capabilities. Our evaluation of state-of-the-art MLLMs on EMMA reveals significant limitations in handling complex multimodal and multi-step reasoning tasks, even with advanced techniques like Chain-of-Thought prompting and test-time compute scaling underperforming. These findings underscore the need for improved multimodal architectures and training paradigms to close the gap between human and model reasoning in multimodality.	 | 有机地在文本和图像之间进行推理是人类智能的关键特征之一，然而，多模态大型语言模型（MLLMs）在这种多模态推理方面的能力仍被忽视。现有的基准测试往往强调以文本为主导的推理或依赖于浅层视觉线索，未能充分评估综合视觉和文本推理的能力。我们引入了EMMA（Enhanced MultiModal reAsoning）基准测试，旨在评估数学、物理、化学和编码领域中的有机多模态推理。EMMA任务要求进行复杂的跨模态推理，不能仅通过在每个模态中独立推理来解决，从而为MLLMs的推理能力提供了更高级的测试套件。我们对当前最先进的MLLMs在EMMA上的评估显示，它们在处理复杂多模态和多步骤推理任务时存在显著局限性，即使使用诸如思维链提示和测试时计算扩展等高级技术也未能改进表现。这些发现强调了改进多模态架构和训练范式的需求，以缩小人类与模型在多模态推理方面的差距。
2501.05442	 | Progressive Growing of Video Tokenizers for Highly Compressed Latent Spaces	 | Aniruddha Mahapatra,Long Mai,Yitian Zhang,David Bourgin,Feng Liu	 | 本文提出了一种新的方法来提高视频分词器的时域压缩比，通过在已训练的低压缩模型上逐步训练高压缩模块，并结合跨层特征混合模块，有效提升了重建质量和时间压缩比，同时减少了分词数量，适用于潜在视频扩散模型的高效训练。	 | Video tokenizers are essential for latent video diffusion models, converting raw video data into spatiotemporally compressed latent spaces for efficient training. However, extending state-of-the-art video tokenizers to achieve a temporal compression ratio beyond 4x without increasing channel capacity poses significant challenges. In this work, we propose an alternative approach to enhance temporal compression. We find that the reconstruction quality of temporally subsampled videos from a low-compression encoder surpasses that of high-compression encoders applied to original videos. This indicates that high-compression models can leverage representations from lower-compression models. Building on this insight, we develop a bootstrapped high-temporal-compression model that progressively trains high-compression blocks atop well-trained lower-compression models. Our method includes a cross-level feature-mixing module to retain information from the pretrained low-compression model and guide higher-compression blocks to capture the remaining details from the full video sequence. Evaluation of video benchmarks shows that our method significantly improves reconstruction quality while increasing temporal compression compared to direct extensions of existing video tokenizers. Furthermore, the resulting compact latent space effectively trains a video diffusion model for high-quality video generation with a reduced token budget.	 | 视频分词器是用于潜在视频扩散模型的核心组件，它们能够将原始视频数据转换为时空压缩的潜在空间，从而实现高效训练。然而，将最先进的视频分词器扩展到超过4倍的时域压缩比而不增加通道容量，面临着重大挑战。在本文中，我们提出了一种替代方法来提升时域压缩。我们发现，低压缩编码器生成的时域下采样视频的重建质量超过了高压缩编码器应用于原始视频的质量。这表明，高压缩模型可以从低压缩模型中利用其表示。基于这一洞察，我们开发了一种逐步训练的方法，该方法在已训练好的低压缩模型之上依次训练高压缩模块。我们的方法包含一个跨层特征混合模块，用于保留预训练低压缩模型的信息，并指导高压缩模块捕捉完整视频序列中的剩余细节。对视频基准的评估表明，与现有视频分词器的直接扩展相比，我们的方法在提升重建质量的同时，还增加了时域压缩比。此外，生成的紧凑潜在空间有效地训练了一个用于高质量视频生成的视频扩散模型，同时减少了所需的分词数量。
2501.05436	 | $DPF^*$: improved Depth Potential Function for scale-invariant sulcal depth estimation	 | Maxime Dieudonné(1),Guillaume Auzias(1),Julien Lefèvre(1) ((1) Institut de Neurosciences de la Timone, UMR 7289, CNRS, Aix-Marseille Université, 13005, Marseille, France)	 | 本研究首次定量分析了大脑大小对沟回深度测量的影响，并提出了一种新的、尺度不变的沟回深度估计方法，通过包含26周胎龄到成年期1,987名受试者的大型样本，展示了新方法的生物学意义。	 | The shape of human brain is complex and highly variable, with interactions between brain size, cortical folding, and age well-documented in the literature. However, few studies have explored how global brain size influences geometric features of the cortical surface derived from anatomical MRI. In this work, we focus on sulcal depth, an imaging phenotype that has gained significant attention in both basic research and clinical applications. We make key contributions to the field by: 1) providing the first quantitative analysis of how brain size affects sulcal depth measurements; 2) introducing a novel, scale-invariant method for sulcal depth estimation based on an original formalization of the problem; 3) presenting a validation framework and sharing our code and benchmark data with the community; and 4) demonstrating the biological relevance of our new sulcal depth measure using a large sample of 1,987 subjects spanning the developmental period from 26 weeks post-conception to adulthood.	 | 人类大脑的形状复杂且高度变化，大脑大小、皮层折叠和年龄之间的相互作用已经在文献中得到了充分记录。然而，很少有研究探讨全球大脑大小如何影响从解剖MRI中获得的皮层表面的几何特征。在这项工作中，我们重点关注沟回深度这一成像表型，它在基础研究和临床应用中都引起了广泛关注。我们为该领域做出了重要贡献，具体包括：1）首次定量分析大脑大小如何影响沟回深度测量；2）提出了一种新的、尺度不变的沟回深度估计方法，基于对该问题的原始形式化；3）提供了一个验证框架，并与社区分享了我们的代码和基准数据；4）通过包含26周胎龄到成年期1,987名受试者的大型样本，展示了我们新提出的沟回深度测量的生物学意义。
2501.05427	 | Zero-1-to-G: Taming Pretrained 2D Diffusion Model for Direct 3D Generation	 | Xuyi Meng,Chen Wang,Jiahui Lei,Kostas Daniilidis,Jiatao Gu,Lingjie Liu	 | 本文提出了一种名为Zero-1-to-G的新方法，通过利用预训练的二维扩散模型在高斯斑点上实现直接单视图生成，并引入跨视图和跨属性注意力层来捕捉三维一致性，从而实现高效的三维物体生成并改善泛化能力。	 | Recent advances in 2D image generation have achieved remarkable quality,largely driven by the capacity of diffusion models and the availability of large-scale datasets. However, direct 3D generation is still constrained by the scarcity and lower fidelity of 3D datasets. In this paper, we introduce Zero-1-to-G, a novel approach that addresses this problem by enabling direct single-view generation on Gaussian splats using pretrained 2D diffusion models. Our key insight is that Gaussian splats, a 3D representation, can be decomposed into multi-view images encoding different attributes. This reframes the challenging task of direct 3D generation within a 2D diffusion framework, allowing us to leverage the rich priors of pretrained 2D diffusion models. To incorporate 3D awareness, we introduce cross-view and cross-attribute attention layers, which capture complex correlations and enforce 3D consistency across generated splats. This makes Zero-1-to-G the first direct image-to-3D generative model to effectively utilize pretrained 2D diffusion priors, enabling efficient training and improved generalization to unseen objects. Extensive experiments on both synthetic and in-the-wild datasets demonstrate superior performance in 3D object generation, offering a new approach to high-quality 3D generation.	 | 近年来，二维图像生成取得了显著的进步，这主要得益于扩散模型的能力增强以及大规模数据集的可用性。然而，直接的三维生成仍然受到三维数据集稀缺且精度较低的限制。本文提出了一种名为Zero-1-to-G的新方法，通过利用预训练的二维扩散模型，在高斯斑点上实现直接单视图生成。我们的核心见解是，高斯斑点作为一种三维表示，可以分解成多视图图像，编码不同的属性。这将直接三维生成的挑战性任务重新框架为二维扩散框架，使我们可以利用预训练二维扩散模型的丰富先验知识。为了引入三维意识，我们引入了跨视图和跨属性注意力层，这些层捕捉复杂的关联性，并在生成的斑点之间强制执行三维一致性。这使得Zero-1-to-G成为第一个能够有效利用预训练二维扩散先验的直接图像到三维生成模型，实现高效的训练并改善对未见物体的泛化能力。在合成和野外数据集上的大量实验表明，Zero-1-to-G在三维物体生成方面表现出优越的性能，提供了一种高质量三维生成的新方法。
2501.05409	 | A Novel Pathology Foundation Model by Mayo Clinic, Charit\'e, and Aignostics	 | Maximilian Alber,Stephan Tietz,Jonas Dippel,Timo Milbich,Timothée Lesort,Panos Korfiatis,Moritz Krügener,Beatriz Perez Cancer,Neelay Shah,Alexander Möllers,Philipp Seegerer,Alexandra Carpen-Amarie,Kai Standvoss,Gabriel Dernbach,Edwin de Jong,Simon Schallenberg,Andreas Kunft,Helmut Hoffer von Ankershoffen,Gavin Schaeferle,Patrick Duffy,Matt Redlon,Philipp Jurmeister,David Horst,Lukas Ruff,Klaus-Robert Müller,Frederick Klauschen,Andrew Norgan	 | 该研究提出了一种基于RudolfV方法的新视觉基础模型，并在来自两家医疗机构的120万张组织病理学全切片图像上进行训练，该模型在多个公开基准数据集上实现了最先进的性能。	 | Recent advances in digital pathology have demonstrated the effectiveness of foundation models across diverse applications. In this report, we present a novel vision foundation model based on the RudolfV approach. Our model was trained on a dataset comprising 1.2 million histopathology whole slide images, collected from two medical institutions: Mayo Clinic and Charité - Universtätsmedizin Berlin. Comprehensive evaluations show that our model achieves state-of-the-art performance across twenty-one public benchmark datasets, even though it is neither the largest model by parameter count nor by training dataset size.	 | 近年来，数字病理学领域的进展展示了基础模型在多种应用中的有效性。在本报告中，我们提出了一种基于RudolfV方法的新视觉基础模型。该模型在来自两家医疗机构（梅奥诊所和查理特—洪堡医学中心）的120万张组织病理学全切片图像数据集上进行了训练。全面的评估结果显示，尽管该模型的参数量和训练数据集规模都不是最大的，但它在二十一个公开基准数据集上实现了最先进的性能。
2501.05399	 | Performance of YOLOv7 in Kitchen Safety While Handling Knife	 | Athulya Sundaresan Geetha	 | 研究利用YOLOv7模型识别厨房刀具使用中的安全风险，结果显示在第31个周期时，模型的mAP50-95得分为0.7879，显著降低了食物准备过程中的受伤风险，促进了厨房安全的提升。	 | Safe knife practices in the kitchen significantly reduce the risk of cuts, injuries, and serious accidents during food preparation. Using YOLOv7, an advanced object detection model, this study focuses on identifying safety risks during knife handling, particularly improper finger placement and blade contact with hand. The model's performance was evaluated using metrics such as precision, recall, mAP50, and mAP50-95. The results demonstrate that YOLOv7 achieved its best performance at epoch 31, with a mAP50-95 score of 0.7879, precision of 0.9063, and recall of 0.7503. These findings highlight YOLOv7's potential to accurately detect knife-related hazards, promoting the development of improved kitchen safety.	 | 厨房中的安全刀具使用显著降低了在食物准备过程中被割伤、受伤以及发生严重事故的风险。本研究利用先进的目标检测模型YOLOv7，专注于识别刀具使用过程中的安全风险，特别是不正确的手指位置和刀刃与手部接触的情况。模型的性能通过精度、召回率、mAP50和mAP50-95等指标进行评估。结果表明，YOLOv7在第31个周期时表现最佳，mAP50-95得分为0.7879，精度为0.9063，召回率为0.7503。这些发现突显了YOLOv7在准确检测刀具相关危险方面的潜力，促进了厨房安全的改进。
2501.05379	 | Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID Guidance	 | Dimitrios Gerogiannis,Foivos Paraperas Papantoniou,Rolandos Alexandros Potamias,Alexandros Lattas,Stefanos Zafeiriou	 | Arc2Avatar 是一种利用人类面部基础模型的 SDS 方法，仅通过单张图像生成高保真3D头像，通过微调3D高斯散射并在合成数据上进行条件修改，实现了多样视角的人头生成，并通过连接性正则化器和策略性初始化提高了生成的逼真度和多样性。	 | Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in reconstructing detailed 3D scenes within multi-view setups and the emergence of large 2D human foundation models, we introduce Arc2Avatar, the first SDS-based method utilizing a human face foundation model as guidance with just a single image as input. To achieve that, we extend such a model for diverse-view human head generation by fine-tuning on synthetic data and modifying its conditioning. Our avatars maintain a dense correspondence with a human face mesh template, allowing blendshape-based expression generation. This is achieved through a modified 3DGS approach, connectivity regularizers, and a strategic initialization tailored for our task. Additionally, we propose an optional efficient SDS-based correction step to refine the blendshape expressions, enhancing realism and diversity. Experiments demonstrate that Arc2Avatar achieves state-of-the-art realism and identity preservation, effectively addressing color issues by allowing the use of very low guidance, enabled by our strong identity prior and initialization strategy, without compromising detail.	 | 受3D高斯散列（3DGS）在多视图设置中重建详细3D场景的有效性启发，以及大规模2D人类基础模型的兴起，我们提出了Arc2Avatar，这是首个利用人类面部基础模型作为指导的SDS（Sparse Deformation Space）方法，仅使用单张图像作为输入。为了实现这一点，我们通过在合成数据上进行微调并修改其条件，将该模型扩展到多样视角的人头生成。我们的avatar与人类面部网格模板保持密集对应，允许基于混合形状的表情生成。这通过修改后的3DGS方法、连接性正则化器和针对我们任务的战略初始化实现。此外，我们提出了一种可选的高效的SDS基纠正步骤，以细化混合形状表情，增强真实感和多样性。实验表明，Arc2Avatar实现了最先进的真实感和身份保留，能够通过使用非常低的指导信息有效解决颜色问题，这得益于我们强大的身份先验和初始化策略，同时不牺牲细节。
2501.05369	 | 1-2-1: Renaissance of Single-Network Paradigm for Virtual Try-On	 | Shuliang Ning,Yipeng Qin,Xiaoguang Han	 | 本文提出了一种新的单网络虚拟试穿（VTON）方法——MNVTON，该方法通过模态特定归一化策略处理文本、图像和视频输入，能够在共享注意力层的同时保持高质量的细节。实验结果表明，MNVTON在图像和视频VTON任务中能够获得更高质量、更详细的结果，提供了与双网络方法相媲美的高效替代方案。	 | Virtual Try-On (VTON) has become a crucial tool in ecommerce, enabling the realistic simulation of garments on individuals while preserving their original appearance and pose. Early VTON methods relied on single generative networks, but challenges remain in preserving fine-grained garment details due to limitations in feature extraction and fusion. To address these issues, recent approaches have adopted a dual-network paradigm, incorporating a complementary "ReferenceNet" to enhance garment feature extraction and fusion. While effective, this dual-network approach introduces significant computational overhead, limiting its scalability for high-resolution and long-duration image/video VTON applications. In this paper, we challenge the dual-network paradigm by proposing a novel single-network VTON method that overcomes the limitations of existing techniques. Our method, namely MNVTON, introduces a Modality-specific Normalization strategy that separately processes text, image and video inputs, enabling them to share the same attention layers in a VTON network. Extensive experimental results demonstrate the effectiveness of our approach, showing that it consistently achieves higher-quality, more detailed results for both image and video VTON tasks. Our results suggest that the single-network paradigm can rival the performance of dualnetwork approaches, offering a more efficient alternative for high-quality, scalable VTON applications.	 | 虚拟试穿（VTON）已成为电子商务中的关键工具，能够实现在保留个体原有外观和姿态的前提下，对服装进行逼真的模拟。早期的VTON方法依赖单一生成网络，但在提取和融合细节特征方面仍存在挑战，难以保持细微的服装细节。为了解决这些问题，最近的方法采用了双网络范式，引入了一个补充的“ReferenceNet”来增强服装特征的提取和融合。虽然有效，但双网络方法引入了较大的计算开销，限制了其在高分辨率和长时间图像/视频VTON应用中的可扩展性。在本文中，我们挑战了双网络范式，提出了一种新的单网络VTON方法，克服了现有技术的局限性。我们的方法，即MNVTON，提出了模态特定归一化策略，分别处理文本、图像和视频输入，使它们能够在VTON网络中共享相同的注意力层。大量的实验结果证明了我们方法的有效性，显示它在图像和视频VTON任务中始终能够获得更高质量、更详细的成果。我们的结果表明，单网络范式可以与双网络方法相媲美，提供了一种更高效的替代方案，适用于高质量和可扩展的VTON应用。
2501.05359	 | CROPS: Model-Agnostic Training-Free Framework for Safe Image Synthesis with Latent Diffusion Models	 | Junha Park,Ian Ryu,Jaehui Hwang,Hyungkeun Park,Jiyoon Kim,Jong-Seok Lee	 | 本文针对扩散模型生成不适合工作场所（NSFW）内容的问题，提出了一种名为CROPS的模型无关框架，通过改变文本提示或输入潜在变量来防御对抗性攻击，同时开发了CROPS-1方法以提高NSFW检测的效率和性能。	 | With advances in diffusion models, image generation has shown significant performance improvements. This raises concerns about the potential abuse of image generation, such as the creation of explicit or violent images, commonly referred to as Not Safe For Work (NSFW) content. To address this, the Stable Diffusion model includes several safety checkers to censor initial text prompts and final output images generated from the model. However, recent research has shown that these safety checkers have vulnerabilities against adversarial attacks, allowing them to generate NSFW images. In this paper, we find that these adversarial attacks are not robust to small changes in text prompts or input latents. Based on this, we propose CROPS (Circular or RandOm Prompts for Safety), a model-agnostic framework that easily defends against adversarial attacks generating NSFW images without requiring additional training. Moreover, we develop an approach that utilizes one-step diffusion models for efficient NSFW detection (CROPS-1), further reducing computational resources. We demonstrate the superiority of our method in terms of performance and applicability.	 | 随着扩散模型的进步，图像生成已经展示出显著的性能改进。然而，这引发了对其潜在滥用的关注，如生成明确的或暴力图像，通常称为不适合工作场所（NSFW）内容。为应对这一问题，Stable Diffusion 模型包括了多个安全检查器来审查初始文本提示和最终生成的图像。然而，最近的研究表明这些安全检查器对对抗性攻击存在漏洞，允许它们生成 NSFW 图像。本文中，我们发现这些对抗性攻击对文本提示或输入潜在变量的小变化并不鲁棒。基于此，我们提出了一种名为 CROPS（Circular or RandOm Prompts for Safety）的模型无关框架，可以在不进行额外训练的情况下轻松防御生成 NSFW 图像的对抗性攻击。此外，我们开发了一种利用一步扩散模型进行高效 NSFW 检测的方法（CROPS-1），进一步减少了计算资源的消耗。我们展示了该方法在性能和适用性方面的优越性。
2501.05339	 | JAQ: Joint Efficient Architecture Design and Low-Bit Quantization with Hardware-Software Co-Exploration	 | Mingzi Wang,Yuan Meng,Chen Tang,Weixiang Zhang,Yijian Qin,Yang Yao,Yingxin Li,Tongtong Feng,Xin Wang,Xun Guan,Zhi Wang,Wenwu Zhu	 | JAQ框架同时优化神经网络架构、量化精度和硬件加速器，以在边缘设备上实现性能和效率的最佳平衡。该框架通过通道级稀疏量化减少内存开销，并利用BatchTile加速硬件搜索，从而在ImageNet上达到更高的Top-1精度并显著缩短每次迭代的硬件搜索时间。	 | The co-design of neural network architectures, quantization precisions, and hardware accelerators offers a promising approach to achieving an optimal balance between performance and efficiency, particularly for model deployment on resource-constrained edge devices. In this work, we propose the JAQ Framework, which jointly optimizes the three critical dimensions. However, effectively automating the design process across the vast search space of those three dimensions poses significant challenges, especially when pursuing extremely low-bit quantization. Specifical, the primary challenges include: (1) Memory overhead in software-side: Low-precision quantization-aware training can lead to significant memory usage due to storing large intermediate features and latent weights for back-propagation, potentially causing memory exhaustion. (2) Search time-consuming in hardware-side: The discrete nature of hardware parameters and the complex interplay between compiler optimizations and individual operators make the accelerator search time-consuming. To address these issues, JAQ mitigates the memory overhead through a channel-wise sparse quantization (CSQ) scheme, selectively applying quantization to the most sensitive components of the model during optimization. Additionally, JAQ designs BatchTile, which employs a hardware generation network to encode all possible tiling modes, thereby speeding up the search for the optimal compiler mapping strategy. Extensive experiments demonstrate the effectiveness of JAQ, achieving approximately 7% higher Top-1 accuracy on ImageNet compared to previous methods and reducing the hardware search time per iteration to 0.15 seconds.	 | 神经网络架构、量化精度和硬件加速器的协同设计为在资源受限的边缘设备上实现性能和效率的最佳平衡提供了有前途的方法。本文提出了一种名为JAQ框架的方法，该框架同时优化这三个关键维度。然而，有效地自动化这三个维度的庞大搜索空间中的设计过程面临着巨大挑战，特别是在追求极低位量化时尤为明显。具体而言，主要挑战包括：（1）软件方面的内存开销：低精度量化感知训练会因存储大量的中间特征和用于反向传播的潜在权重而导致显著的内存使用，这可能导致内存耗尽。（2）硬件方面的搜索耗时：硬件参数的离散性质以及编译器优化与个体操作之间的复杂交互使加速器搜索变得耗时。为了解决这些问题，JAQ通过通道级稀疏量化（CSQ）方案减轻了内存开销，并在优化过程中选择性地对模型中最敏感的组件进行量化。此外，JAQ设计了BatchTile，该设计利用硬件生成网络编码所有可能的分块模式，从而加快了找到最佳编译映射策略的搜索速度。广泛的实验表明，JAQ的有效性，与之前的方法相比，在ImageNet上实现了约7%更高的Top-1精度，并将每次迭代的硬件搜索时间缩短至0.15秒。
2501.05281	 | Comparison Study: Glacier Calving Front Delineation in Synthetic Aperture Radar Images With Deep Learning	 | Nora Gourmelon,Konrad Heidler,Erik Loebel,Daniel Cheng,Julian Klink,Anda Dong,Fei Wu,Noah Maul,Moritz Koch,Marcel Dreier,Dakota Pyles,Thorsten Seehaus,Matthias Braun,Andreas Maier,Vincent Christlein	 | 该研究首次对比了深度学习系统在从合成孔径雷达图像中自动提取冰川前沿位置方面的性能，并发现最佳DL模型的精度仍低于人类注释者，表明需要进一步改进以实现完全自动化监测。未来研究应聚焦于视觉变换器、基础模型和信息处理策略的优化。	 | Calving front position variation of marine-terminating glaciers is an indicator of ice mass loss and a crucial parameter in numerical glacier models. Deep Learning (DL) systems can automatically extract this position from Synthetic Aperture Radar (SAR) imagery, enabling continuous, weather- and illumination-independent, large-scale monitoring. This study presents the first comparison of DL systems on a common calving front benchmark dataset. A multi-annotator study with ten annotators is performed to contrast the best-performing DL system against human performance. The best DL model's outputs deviate 221 m on average, while the average deviation of the human annotators is 38 m. This significant difference shows that current DL systems do not yet match human performance and that further research is needed to enable fully automated monitoring of glacier calving fronts. The study of Vision Transformers, foundation models, and the inclusion and processing strategy of more information are identified as avenues for future research.	 | 海洋终端冰川的后退前沿位置变化是冰质量损失的一个指标，并且是数值冰川模型中至关重要的参数。深度学习（DL）系统可以从合成孔径雷达（SAR）图像中自动提取这一位置，从而使持续的、不受天气和光照影响的大型规模监测成为可能。本研究首次对DL系统在共同的冰川前沿基准数据集上进行了比较。使用十名注释者进行多注释者研究，比较最佳性能的DL系统与人类表现。最佳DL模型的输出平均偏差为221米，而人类注释者的平均偏差为38米。这一显著差异表明当前的DL系统尚未达到人类的表现水平，需要进一步研究以实现冰川前沿的完全自动化监测。研究发现，视觉变换器、基础模型的研究以及更多信息的纳入和处理策略是未来研究的方向。
2501.05272	 | Solving the Catastrophic Forgetting Problem in Generalized Category Discovery	 | Xinzi Cao,Xiawu Zheng,Guanhong Wang,Weijiang Yu,Yunhang Shen,Ke Li,Yutong Lu,Yonghong Tian	 | LegoGCD 是一种通过局部熵正则化和双视图 Kullback-Leibler 散度约束来增强新型别类区分能力并保留已知类别性能的新方法，有效解决了广义类别发现中的已知类别遗忘问题。	 | Generalized Category Discovery (GCD) aims to identify a mix of known and novel categories within unlabeled data sets, providing a more realistic setting for image recognition. Essentially, GCD needs to remember existing patterns thoroughly to recognize novel categories. Recent state-of-the-art method SimGCD transfers the knowledge from known-class data to the learning of novel classes through debiased learning. However, some patterns are catastrophically forgot during adaptation and thus lead to poor performance in novel categories classification. To address this issue, we propose a novel learning approach, LegoGCD, which is seamlessly integrated into previous methods to enhance the discrimination of novel classes while maintaining performance on previously encountered known classes. Specifically, we design two types of techniques termed as Local Entropy Regularization (LER) and Dual-views Kullback Leibler divergence constraint (DKL). The LER optimizes the distribution of potential known class samples in unlabeled data, thus ensuring the preservation of knowledge related to known categories while learning novel classes. Meanwhile, DKL introduces Kullback Leibler divergence to encourage the model to produce a similar prediction distribution of two view samples from the same image. In this way, it successfully avoids mismatched prediction and generates more reliable potential known class samples simultaneously. Extensive experiments validate that the proposed LegoGCD effectively addresses the known category forgetting issue across all datasets, eg, delivering a 7.74% and 2.51% accuracy boost on known and novel classes in CUB, respectively. Our code is available at: https://github.com/Cliffia123/LegoGCD.	 | 广义类别发现（GCD）旨在识别未标记数据集中已知类别和新型别类的混合，为图像识别提供更具现实意义的环境。本质上，GCD 需要彻底记住现有的模式，以便识别新型别类。最近的先进方法 SimGCD 通过去偏学习将已知类别数据的知识转移到新型别类的学习中。然而，在适应过程中，一些模式可能会被灾难性地遗忘，从而导致新型别类分类性能不佳。为解决这一问题，我们提出了一种新颖的学习方法——LegoGCD，该方法可以无缝集成到以前的方法中，增强对新型别类的区分能力，同时保持对已遇见过的已知类别性能的提升。具体而言，我们设计了两类技术，分别为局部熵正则化（LER）和双视图Kullback-Leibler散度约束（DKL）。LER 优化潜在已知类别样本在未标记数据中的分布，从而确保在学习新型别类时保留与已知类别相关的信息。同时，DKL 引入Kullback-Leibler散度，鼓励模型生成同一图像的两个视图样本的相似预测分布。这样，它能够成功避免预测不匹配，并同时生成更可靠的潜在已知类别样本。广泛实验验证了所提出的LegoGCD在所有数据集上有效解决了已知类别遗忘问题，例如在CUB数据集上分别提高了已知类别和新型别类7.74%和2.51%的准确性。我们的代码可以在 https://github.com/Cliffia123/LegoGCD 获取。
2501.05269	 | CellViT++: Energy-Efficient and Adaptive Cell Segmentation and Classification Using Foundation Models	 | Fabian Hörst,Moritz Rempe,Helmut Becker,Lukas Heine,Julius Keyl,Jens Kleesiek	 | 该研究提出了$\text{CellViT}^{\scriptscriptstyle ++}$框架，用于数字病理学中的通用细胞分割，该框架能够适应未见过的细胞类型，减少碳足迹，并在多个数据集上表现出色。此外，$\text{CellViT}^{\scriptscriptstyle ++}$还能够利用免疫荧光染色自动生成训练数据集，无需人工标注，提高了数据效率和性能。	 | Digital Pathology is a cornerstone in the diagnosis and treatment of diseases. A key task in this field is the identification and segmentation of cells in hematoxylin and eosin-stained images. Existing methods for cell segmentation often require extensive annotated datasets for training and are limited to a predefined cell classification scheme. To overcome these limitations, we propose $\text{CellViT}^{\scriptscriptstyle ++}$, a framework for generalized cell segmentation in digital pathology. $\text{CellViT}^{\scriptscriptstyle ++}$ utilizes Vision Transformers with foundation models as encoders to compute deep cell features and segmentation masks simultaneously. To adapt to unseen cell types, we rely on a computationally efficient approach. It requires minimal data for training and leads to a drastically reduced carbon footprint. We demonstrate excellent performance on seven different datasets, covering a broad spectrum of cell types, organs, and clinical settings. The framework achieves remarkable zero-shot segmentation and data-efficient cell-type classification. Furthermore, we show that $\text{CellViT}^{\scriptscriptstyle ++}$ can leverage immunofluorescence stainings to generate training datasets without the need for pathologist annotations. The automated dataset generation approach surpasses the performance of networks trained on manually labeled data, demonstrating its effectiveness in creating high-quality training datasets without expert annotations. To advance digital pathology, $\text{CellViT}^{\scriptscriptstyle ++}$ is available as an open-source framework featuring a user-friendly, web-based interface for visualization and annotation. The code is available under https://github.com/TIO-IKIM/CellViT-plus-plus.	 | 数字病理学是疾病诊断和治疗的基础。在这个领域中，识别和分割苏木精和伊红染色图像中的细胞是一项关键任务。现有细胞分割方法通常需要大量标注数据进行训练，并且局限于预定义的细胞分类方案。为了克服这些限制，我们提出了$\text{CellViT}^{\scriptscriptstyle ++}$框架，用于数字病理学中的通用细胞分割。$\text{CellViT}^{\scriptscriptstyle ++}$利用基础模型作为编码器的视觉变换器来同时计算深层细胞特征和分割掩码。为了适应未见过的细胞类型，我们采用了计算高效的方案。这种方法在训练时所需数据很少，从而大幅减少了碳足迹。我们在七个不同的数据集上展示了优异的表现，这些数据集涵盖了广泛的细胞类型、器官和临床场景。该框架实现了出色的零样本分割和数据高效细胞类型分类。此外，我们证明$\text{CellViT}^{\scriptscriptstyle ++}$可以利用免疫荧光染色生成训练数据集，而无需病理学家的标注。自动化数据集生成方法在性能上超过了在手动标注数据上训练的网络，证明了其在无需专家标注的情况下创建高质量训练数据集的有效性。为了推进数字病理学的发展，$\text{CellViT}^{\scriptscriptstyle ++}$作为一个开源框架提供，它配备了用户友好的基于Web的界面用于可视化和标注。代码可在https://github.com/TIO-IKIM/CellViT-plus-plus 下载。
2501.05265	 | Patch-GAN Transfer Learning with Reconstructive Models for Cloud Removal	 | Wanli Ma,Oktay Karakus,Paul L. Rosin	 | 本文提出了一种基于生成对抗网络（GAN）框架的深度迁移学习方法，通过使用块判别器来实现云消除，该方法在云消除性能上显著优于其他基于GAN的方法。尽管缺乏清晰的训练/测试数据分割细节限制了与某些先进云消除技术的直接对比，所提出模型仍能达到竞争力的结果。	 | Cloud removal plays a crucial role in enhancing remote sensing image analysis, yet accurately reconstructing cloud-obscured regions remains a significant challenge. Recent advancements in generative models have made the generation of realistic images increasingly accessible, offering new opportunities for this task. Given the conceptual alignment between image generation and cloud removal tasks, generative models present a promising approach for addressing cloud removal in remote sensing. In this work, we propose a deep transfer learning approach built on a generative adversarial network (GAN) framework to explore the potential of the novel masked autoencoder (MAE) image reconstruction model in cloud removal. Due to the complexity of remote sensing imagery, we further propose using a patch-wise discriminator to determine whether each patch of the image is real or not. The proposed reconstructive transfer learning approach demonstrates significant improvements in cloud removal performance compared to other GAN-based methods. Additionally, whilst direct comparisons with some of the state-of-the-art cloud removal techniques are limited due to unclear details regarding their train/test data splits, the proposed model achieves competitive results based on available benchmarks.	 | 云消除在提高遥感图像分析方面发挥着关键作用，但准确重构被云遮蔽的区域仍然是一个重大挑战。近年来生成模型的发展使得生成逼真图像变得更加可行，为云消除任务提供了新的机会。鉴于图像生成任务与云消除任务在概念上的契合性，生成模型为解决遥感中的云消除问题提供了一种有前途的方法。在本研究中，我们提出了一种基于生成对抗网络（GAN）框架的深度迁移学习方法，以探索新型掩码自编码器（MAE）图像重建模型在云消除中的潜在应用。由于遥感图像的复杂性，我们进一步提出使用块判别器来判断图像的每个块是真实的还是合成的。所提出的重构迁移学习方法在云消除性能上显著优于其他基于GAN的方法。此外，由于缺乏清晰的训练/测试数据分割细节，与一些最先进的云消除技术的直接对比受到限制，但根据现有基准，所提出模型仍能达到竞争力的结果。
2501.05264	 | Towards Balanced Continual Multi-Modal Learning in Human Pose Estimation	 | Jiaxuan Peng,Mengshi Qi,Dong Zhao,Huadong Ma	 | 本文提出了一种新的平衡持续多模态学习方法，结合RGB、LiDAR、毫米波和WiFi等模态，通过贡献算法识别模态不平衡并采用重新学习策略和去噪持续学习方法来改进3D人体姿态估计，实验结果表明该方法在性能提升和灾难性遗忘缓解方面具有优势。	 | 3D human pose estimation (3D HPE) has emerged as a prominent research topic, particularly in the realm of RGB-based methods. However, RGB images are susceptible to limitations such as sensitivity to lighting conditions and potential user discomfort. Consequently, multi-modal sensing, which leverages non-intrusive sensors, is gaining increasing attention. Nevertheless, multi-modal 3D HPE still faces challenges, including modality imbalance and the imperative for continual learning. In this work, we introduce a novel balanced continual multi-modal learning method for 3D HPE, which harnesses the power of RGB, LiDAR, mmWave, and WiFi. Specifically, we propose a Shapley value-based contribution algorithm to quantify the contribution of each modality and identify modality imbalance. To address this imbalance, we employ a re-learning strategy. Furthermore, recognizing that raw data is prone to noise contamination, we develop a novel denoising continual learning approach. This approach incorporates a noise identification and separation module to mitigate the adverse effects of noise and collaborates with the balanced learning strategy to enhance optimization. Additionally, an adaptive EWC mechanism is employed to alleviate catastrophic forgetting. We conduct extensive experiments on the widely-adopted multi-modal dataset, MM-Fi, which demonstrate the superiority of our approach in boosting 3D pose estimation and mitigating catastrophic forgetting in complex scenarios. We will release our codes.	 | 三维人体姿态估计（3D HPE）已成为研究领域的热门话题，尤其是在基于RGB的方法中。然而，RGB图像存在光照条件敏感和潜在用户不适的局限性。因此，多模态传感逐渐受到关注，这种传感方式利用了非侵入性传感器。尽管如此，多模态3D HPE仍然面临挑战，包括模态不平衡和持续学习的需求。在这项工作中，我们提出了一种新的平衡持续多模态学习方法，用于3D HPE，该方法利用了RGB、LiDAR、毫米波和WiFi等模态。具体而言，我们提出了一种基于Shapley值的贡献算法来量化每个模态的贡献并识别模态不平衡。为了解决这种不平衡，我们采用了一种重新学习策略。此外，考虑到原始数据容易受到噪声污染的影响，我们开发了一种新颖的去噪持续学习方法。该方法结合了一个噪声识别和分离模块，以减轻噪声的不良影响，并与均衡学习策略协作，以增强优化。此外，我们还采用了自适应EWC机制来缓解灾难性遗忘。我们在广泛采用的多模态数据集MM-Fi上进行了大量实验，结果表明我们的方法在提升3D姿态估计性能和在复杂场景中缓解灾难性遗忘方面具有优势。我们将发布我们的代码。
2501.05246	 | Domain-Incremental Semantic Segmentation for Autonomous Driving under Adverse Driving Conditions	 | Shishir Muralidhara,René Schuster,Didier Stricker	 | 该研究提出了一种基于架构的领域增量学习方法——渐进式语义分割（PSS），该方法能够在恶劣驾驶条件下提高自动驾驶中的语义分割性能，同时避免灾难性遗忘，并展示了在类似和未见过的领域中的泛化能力。	 | Semantic segmentation for autonomous driving is an even more challenging task when faced with adverse driving conditions. Standard models trained on data recorded under ideal conditions show a deteriorated performance in unfavorable weather or illumination conditions. Fine-tuning on the new task or condition would lead to overwriting the previously learned information resulting in catastrophic forgetting. Adapting to the new conditions through traditional domain adaption methods improves the performance on the target domain at the expense of the source domain. Addressing these issues, we propose an architecture-based domain-incremental learning approach called Progressive Semantic Segmentation (PSS). PSS is a task-agnostic, dynamically growing collection of domain-specific segmentation models. The task of inferring the domain and subsequently selecting the appropriate module for segmentation is carried out using a collection of convolutional autoencoders. We extensively evaluate our proposed approach using several datasets at varying levels of granularity in the categorization of adverse driving conditions. Furthermore, we demonstrate the generalization of the proposed approach to similar and unseen domains.	 | 自动驾驶中的语义分割在面对恶劣驾驶条件时是一个更加具有挑战性的任务。标准模型在理想条件下采集的数据集上进行训练，会在不利的天气或光照条件下表现出性能下降。在新的任务或条件下进行微调会导致以前学到的信息被覆盖，从而引发灾难性的遗忘。通过传统的领域适应方法来适应新条件可以提高目标领域的性能，但会牺牲源领域的性能。为了解决这些问题，我们提出了一种基于架构的领域增量学习方法，称为渐进式语义分割（PSS）。PSS 是一个任务无关、动态增长的领域特定分割模型集合。通过一组卷积自编码器来推断领域并选择合适的分割模块。我们使用多个具有不同粒度分类的恶劣驾驶条件数据集，对提出的方法进行了广泛评估，并展示了该方法在类似和未见过的领域中的泛化能力。
2501.05242	 | Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and Photorealistic Mapping	 | Wen Tianci,Liu Zhiang,Lu Biao,Fang Yongchun	 | Scaffold-SLAM 是一种新型的 SLAM 方法，能够在单目、立体和 RGB-D 相机上实现高质量的 3D 视觉合成，通过运动外观嵌入和频率正则化金字塔创新显著提升了渲染质量，特别是在单目相机上的表现，PSNR 指标提高了 16.76%。	 | 3D Gaussian Splatting (3DGS) has recently revolutionized novel view synthesis in the Simultaneous Localization and Mapping (SLAM). However, existing SLAM methods utilizing 3DGS have failed to provide high-quality novel view rendering for monocular, stereo, and RGB-D cameras simultaneously. Notably, some methods perform well for RGB-D cameras but suffer significant degradation in rendering quality for monocular cameras. In this paper, we present Scaffold-SLAM, which delivers simultaneous localization and high-quality photorealistic mapping across monocular, stereo, and RGB-D cameras. We introduce two key innovations to achieve this state-of-the-art visual quality. First, we propose Appearance-from-Motion embedding, enabling 3D Gaussians to better model image appearance variations across different camera poses. Second, we introduce a frequency regularization pyramid to guide the distribution of Gaussians, allowing the model to effectively capture finer details in the scene. Extensive experiments on monocular, stereo, and RGB-D datasets demonstrate that Scaffold-SLAM significantly outperforms state-of-the-art methods in photorealistic mapping quality, e.g., PSNR is 16.76% higher in the TUM RGB-D datasets for monocular cameras.	 | 3D高斯绘制（3DGS） recently 重塑了Simultaneous Localization and Mapping（SLAM）中的新型视角合成。然而，现有的利用3DGS的SLAM方法未能同时为单目、立体和RGB-D相机提供高质量的新型视角渲染。值得注意的是，一些方法在RGB-D相机上表现良好，但在单目相机上却表现出显著的渲染质量下降。在本文中，我们提出了Scaffold-SLAM，它能够在单目、立体和RGB-D相机上实现同时定位和高质量的摄影现实感绘图。我们引入了两项关键创新以达到这一最先进的视觉质量。首先，我们提出了运动外观嵌入，使3D高斯更好地建模不同相机姿态下的图像外观变化。其次，我们引入了频率正则化金字塔来引导高斯分布，从而使模型能够有效地捕捉场景中的更细小细节。在单目、立体和RGB-D数据集上的广泛实验表明，Scaffold-SLAM 在摄影现实感绘图质量上显著优于现有方法，例如，在TUM RGB-D数据集上，单目相机的峰值信噪比（PSNR）提高了16.76%。
2501.05238	 | FOCUS: Towards Universal Foreground Segmentation	 | Zuyao You,Lingyu Kong,Lingchen Meng,Zuxuan Wu	 | 本文提出了FOCUS框架，旨在统一处理多种前景分割任务，通过引入多尺度语义网络和新颖的蒸馏方法，FOCUS在多个数据集上表现出优于特定任务模型的性能。	 | Foreground segmentation is a fundamental task in computer vision, encompassing various subdivision tasks. Previous research has typically designed task-specific architectures for each task, leading to a lack of unification. Moreover, they primarily focus on recognizing foreground objects without effectively distinguishing them from the background. In this paper, we emphasize the importance of the background and its relationship with the foreground. We introduce FOCUS, the Foreground ObjeCts Universal Segmentation framework that can handle multiple foreground tasks. We develop a multi-scale semantic network using the edge information of objects to enhance image features. To achieve boundary-aware segmentation, we propose a novel distillation method, integrating the contrastive learning strategy to refine the prediction mask in multi-modal feature space. We conduct extensive experiments on a total of 13 datasets across 5 tasks, and the results demonstrate that FOCUS consistently outperforms the state-of-the-art task-specific models on most metrics.	 | 前景分割是计算机视觉中的一个基本任务，涵盖了多种细分任务。以往的研究通常为每个任务设计特定的任务架构，导致缺乏统一性。此外，这些研究主要集中在识别前景对象上，但未能有效地将它们与背景区分开来。在本文中，我们强调了背景及其与前景关系的重要性。我们引入了FOCUS（Foreground ObjeCts Universal Segmentation）框架，该框架能够处理多个前景任务。我们使用对象的边缘信息开发了一种多尺度语义网络，以增强图像特征。为了实现边界感知的分割，我们提出了一种新颖的蒸馏方法，将对比学习策略集成到多模态特征空间中，以精炼预测掩码。我们在五个任务的总共13个数据集上进行了广泛的实验，结果表明，FOCUS在大多数指标上都优于最先进的特定任务模型。
2501.05236	 | Automated external cervical resorption segmentation in cone-beam CT using local texture features	 | Sadhana Ravikumar,Asma A. Khan,Matthew C. Davis,Beatriz Paniagua	 | 该研究提出了一种基于自动二元分类局部提取的体素级纹理特征方法，用于分割和量化外部颈椎吸收病变，以期通过识别和测量CBCT信号变化来预测ECR的发展趋势，从而辅助治疗决策。	 | External cervical resorption (ECR) is a resorptive process affecting teeth. While in some patients, active resorption ceases and gets replaced by osseous tissue, in other cases, the resorption progresses and ultimately results in tooth loss. For proper ECR assessment, cone-beam computed tomography (CBCT) is the recommended imaging modality, enabling a 3-D characterization of these lesions. While it is possible to manually identify and measure ECR resorption in CBCT scans, this process can be time intensive and highly subject to human error. Therefore, there is an urgent need to develop an automated method to identify and quantify the severity of ECR resorption using CBCT. Here, we present a method for ECR lesion segmentation that is based on automatic, binary classification of locally extracted voxel-wise texture features. We evaluate our method on 6 longitudinal CBCT datasets and show that certain texture-features can be used to accurately detect subtle CBCT signal changes due to ECR. We also present preliminary analyses clustering texture features within a lesion to stratify the defects and identify patterns indicative of calcification. These methods are important steps in developing prognostic biomarkers to predict whether ECR will continue to progress or cease, ultimately informing treatment decisions.	 | 外部颈椎吸收（ECR）是一种影响牙齿的吸收过程。在一些患者中，活性吸收会停止并被骨组织替代；而在另一些患者中，吸收过程会继续并最终导致牙齿丧失。为了正确评估ECR，推荐使用锥束计算机断层扫描（CBCT）作为成像模态，它能够对这些病变进行三维表征。尽管可以通过手动识别和测量CBCT扫描中的ECR吸收，但这一过程耗时且容易出错。因此，迫切需要开发一种自动方法，利用CBCT来识别和量化ECR吸收的严重程度。在这里，我们提出了一种基于自动二元分类局部提取的体素级纹理特征的方法来进行ECR病损分割。我们对6个纵向CBCT数据集进行了评估，并展示了某些纹理特征可以用于准确检测由于ECR引起的CBCT信号变化。我们还进行了初步分析，将纹理特征在病损内聚类，以分层缺陷并识别与钙化相关的模式。这些方法是开发预测ECR是否将继续进展或停止的预后生物标志物的重要步骤，最终有助于指导治疗决策。
2501.05228	 | Harnessing Large Language and Vision-Language Models for Robust Out-of-Distribution Detection	 | Pei-Kang Lee,Jun-Cheng Chen,Ja-Ling Wu	 | 该研究提出了一种新的零样本超出分布（OOD）检测方法，通过结合大型语言模型（LLMs）和视觉语言模型（VLMs），同时提升远超出分布和近超出分布场景的检测性能。实验结果表明，该方法在多个基准测试中优于现有方法，显著提高了AUROC和降低了FPR95，并在不同领域中表现出更强的协变量漂移鲁棒性。	 | Out-of-distribution (OOD) detection has seen significant advancements with zero-shot approaches by leveraging the powerful Vision-Language Models (VLMs) such as CLIP. However, prior research works have predominantly focused on enhancing Far-OOD performance, while potentially compromising Near-OOD efficacy, as observed from our pilot study. To address this issue, we propose a novel strategy to enhance zero-shot OOD detection performances for both Far-OOD and Near-OOD scenarios by innovatively harnessing Large Language Models (LLMs) and VLMs. Our approach first exploit an LLM to generate superclasses of the ID labels and their corresponding background descriptions followed by feature extraction using CLIP. We then isolate the core semantic features for ID data by subtracting background features from the superclass features. The refined representation facilitates the selection of more appropriate negative labels for OOD data from a comprehensive candidate label set of WordNet, thereby enhancing the performance of zero-shot OOD detection in both scenarios. Furthermore, we introduce novel few-shot prompt tuning and visual prompt tuning to adapt the proposed framework to better align with the target distribution. Experimental results demonstrate that the proposed approach consistently outperforms current state-of-the-art methods across multiple benchmarks, with an improvement of up to 2.9% in AUROC and a reduction of up to 12.6% in FPR95. Additionally, our method exhibits superior robustness against covariate shift across different domains, further highlighting its effectiveness in real-world scenarios.	 | 超出分布（OOD）检测通过利用强大的视觉语言模型（VLMs）如CLIP，在零样本方法中取得了显著进展。然而，先前的研究工作主要集中在提高远超出分布（Far-OOD）性能上，而可能会在近超出分布（Near-OOD）效果上妥协，这是我们初步研究中观察到的。为了解决这一问题，我们提出了一种新的策略，通过创新地利用大型语言模型（LLMs）和VLMs，同时增强远超出分布和近超出分布场景的零样本OOD检测性能。我们的方法首先利用一个LLM生成ID标签的超类别及其相应的背景描述，然后使用CLIP进行特征提取。接下来，通过从超类别特征中减去背景特征来隔离ID数据的核心语义特征。这种细化的表示使得从WordNet的全面候选标签集中选择更合适的负标签变得更加容易，从而在两种场景中提升了零样本OOD检测的性能。此外，我们引入了新颖的少量样本提示调优和视觉提示调优，以使提出的框架更好地与目标分布对齐。实验结果表明，该方法在多个基准测试中均优于当前最先进的方法，AUROC提高了高达2.9%，FPR95降低了高达12.6%。此外，我们的方法在不同领域中表现出更强的协变量漂移鲁棒性，进一步突显了其在现实场景中的有效性。
2501.05226	 | Light Transport-aware Diffusion Posterior Sampling for Single-View Reconstruction of 3D Volumes	 | Ludwic Leonard,Nils Thuerey,Ruediger Westermann	 | 该研究提出了一种适用于多光散射现象的单视图重建技术，利用无条件扩散模型和新型基准数据集建模未知体素场分布，并结合物理可微体积渲染，实现了高质量的单视图云朵体积重建。	 | We introduce a single-view reconstruction technique of volumetric fields in which multiple light scattering effects are omnipresent, such as in clouds. We model the unknown distribution of volumetric fields using an unconditional diffusion model trained on a novel benchmark dataset comprising 1,000 synthetically simulated volumetric density fields. The neural diffusion model is trained on the latent codes of a novel, diffusion-friendly, monoplanar representation. The generative model is used to incorporate a tailored parametric diffusion posterior sampling technique into different reconstruction tasks. A physically-based differentiable volume renderer is employed to provide gradients with respect to light transport in the latent space. This stands in contrast to classic NeRF approaches and makes the reconstructions better aligned with observed data. Through various experiments, we demonstrate single-view reconstruction of volumetric clouds at a previously unattainable quality.	 | 我们介绍了一种单视图重建技术，该技术适用于多光散射现象普遍存在的情况，例如云朵。我们使用一种无条件扩散模型来建模未知的体素场分布，该模型基于包含1000个合成模拟体素密度场的新型基准数据集进行训练。神经扩散模型是在一种新的、对扩散模型友好的单平面表示的潜在代码上进行训练的。生成模型被用于将定制的参数化扩散后验采样技术整合到不同的重建任务中。我们采用基于物理的可微体积渲染器来提供与潜在空间中的光线传输相关的梯度。这与经典的NeRF方法不同，使得重建结果与观测数据更好地对齐。通过各种实验，我们展示了在以前无法达到的质量水平上实现单视图云朵的体积重建。
2501.05209	 | MHAFF: Multi-Head Attention Feature Fusion of CNN and Transformer for Cattle Identification	 | Rabin Dulal,Lihong Zheng,Muhammad Ashad Kabir	 | 本文提出了一种多头注意力特征融合（MHAFF）方法，结合了卷积神经网络（CNNs）和变换器的优势，以提高基于牛鼻部图像的身份识别精度，在两个公开数据集上分别达到了99.88%和99.52%的最优识别率。	 | Convolutional Neural Networks (CNNs) have drawn researchers' attention to identifying cattle using muzzle images. However, CNNs often fail to capture long-range dependencies within the complex patterns of the muzzle. The transformers handle these challenges. This inspired us to fuse the strengths of CNNs and transformers in muzzle-based cattle identification. Addition and concatenation have been the most commonly used techniques for feature fusion. However, addition fails to preserve discriminative information, while concatenation results in an increase in dimensionality. Both methods are simple operations and cannot discover the relationships or interactions between fusing features. This research aims to overcome the issues faced by addition and concatenation. This research introduces a novel approach called Multi-Head Attention Feature Fusion (MHAFF) for the first time in cattle identification. MHAFF captures relations between the different types of fusing features while preserving their originality. The experiments show that MHAFF outperformed addition and concatenation techniques and the existing cattle identification methods in accuracy on two publicly available cattle datasets. MHAFF demonstrates excellent performance and quickly converges to achieve optimum accuracy of 99.88% and 99.52% in two cattle datasets simultaneously.	 | 卷积神经网络（CNNs）因利用牛鼻部图像进行身份识别而引起了研究人员的注意。然而，CNNs在捕捉复杂鼻部图案中的长程依赖关系方面往往表现不佳。变换器能够应对这些挑战。这激发了我们结合CNNs和变换器的优势，以实现基于鼻部的牛的身份识别。特征融合中常用的两种方法是相加和拼接。然而，相加无法保留区分性信息，而拼接则会导致维度增加。这两种方法过于简单，无法发现融合特征之间的关系或交互。  本文旨在克服相加和拼接方法所面临的问题。本文首次提出了一个多头注意力特征融合（MHAFF）的新方法，用于牛的身份识别。MHAFF能够捕捉不同类型的融合特征之间的关系并保留其原始性。实验结果显示，MHAFF在两个公开可用的牛数据集上精度超过了相加和拼接方法以及现有的牛身份识别方法。MHAFF展现了出色的表现，并能迅速收敛，同时在两个牛数据集上分别实现了99.88%和99.52%的最优精度。
2501.05205	 | Discovering Hidden Visual Concepts Beyond Linguistic Input in Infant Learning	 | Xueyi Ke,Satoshi Tsutsui,Yayun Zhang,Bihan Wen	 | 本文提出了一项跨学科研究，旨在探究是否可以构建一个模仿婴儿学习过程的计算模型，使其能够发展出超越初始词汇范围的更广泛的视觉概念，并通过分析Vong等人模型中的内部表示，将认知科学与计算机视觉联系起来。	 | Infants develop complex visual understanding rapidly, even preceding of the acquisition of linguistic inputs. As computer vision seeks to replicate the human vision system, understanding infant visual development may offer valuable insights. In this paper, we present an interdisciplinary study exploring this question: can a computational model that imitates the infant learning process develop broader visual concepts that extend beyond the vocabulary it has heard, similar to how infants naturally learn? To investigate this, we analyze a recently published model in Science by Vong et al.,which is trained on longitudinal, egocentric images of a single child paired with transcribed parental speech. We introduce a training-free framework that can discover visual concept neurons hidden in the model's internal representations. Our findings show that these neurons can classify objects outside its original vocabulary. Furthermore, we compare the visual representations in infant-like models with those in moder computer vision models, such as CLIP or ImageNet pre-trained model, highlighting key similarities and differences. Ultimately, our work bridges cognitive science and computer vision by analyzing the internal representations of a computational model trained on an infant's visual and linguistic inputs.	 | 婴儿能够快速发展出复杂的视觉理解能力，甚至在获得语言输入之前就已经具备这种能力。随着计算机视觉试图复制人类视觉系统，理解婴儿的视觉发展可能会提供宝贵的观点。在本文中，我们提出了一项跨学科研究来探讨这一问题：一个模仿婴儿学习过程的计算模型能否发展出超越其听到词汇范围的更广泛的视觉概念，类似于婴儿自然学习的过程？为了研究这个问题，我们分析了Vong等人最近在《科学》杂志上发表的一个模型，该模型是在单一儿童的纵向主观图像与转录的父母亲言语配对的数据上进行训练的。我们引入了一个无需训练的框架，可以发现模型内部表示中隐藏的视觉概念神经元。我们的研究发现这些神经元可以对模型原始词汇范围之外的对象进行分类。此外，我们将婴儿样模型中的视觉表示与现代计算机视觉模型（如CLIP或ImageNet预训练模型）中的视觉表示进行比较，突显了关键的相似性和差异性。最终，我们的工作通过分析一个基于婴儿视觉和语言输入训练的计算模型的内部表示，将认知科学与计算机视觉联系起来。
2501.05195	 | HipyrNet: Hypernet-Guided Feature Pyramid network for mixed-exposure correction	 | Shaurya Singh Rathore,Aravind Shenoy,Krish Didwania,Aditya Kasliwal,Ujjwal Verma	 | 本文提出了一种名为HipyreNet的新方法，通过结合拉普拉斯金字塔框架和HyperNetwork来增强混合曝光图像，从而有效应对极端曝光差异，表现出优于现有方法的性能。HipyreNet通过动态生成权重和预测最佳核来实现针对每个输入图像的定制分解过程，在定性和定量评估中均取得了最优结果。	 | Recent advancements in image translation for enhancing mixed-exposure images have demonstrated the transformative potential of deep learning algorithms. However, addressing extreme exposure variations in images remains a significant challenge due to the inherent complexity and contrast inconsistencies across regions. Current methods often struggle to adapt effectively to these variations, resulting in suboptimal performance. In this work, we propose HipyrNet, a novel approach that integrates a HyperNetwork within a Laplacian Pyramid-based framework to tackle the challenges of mixed-exposure image enhancement. The inclusion of a HyperNetwork allows the model to adapt to these exposure variations. HyperNetworks dynamically generates weights for another network, allowing dynamic changes during deployment. In our model, the HyperNetwork employed is used to predict optimal kernels for Feature Pyramid decomposition, which enables a tailored and adaptive decomposition process for each input image. Our enhanced translational network incorporates multiscale decomposition and reconstruction, leveraging dynamic kernel prediction to capture and manipulate features across varying scales. Extensive experiments demonstrate that HipyrNet outperforms existing methods, particularly in scenarios with extreme exposure variations, achieving superior results in both qualitative and quantitative evaluations. Our approach sets a new benchmark for mixed-exposure image enhancement, paving the way for future research in adaptive image translation.	 | 近年来，图像翻译技术在增强混合曝光图像方面取得了显著进展，显示出深度学习算法的巨大潜力。然而，由于图像中固有的复杂性和对比度不一致性，应对极端曝光差异仍然是一个重大挑战。当前的方法往往难以有效适应这些差异，导致性能不佳。在本文中，我们提出了一种名为HipyreNet的新方法，该方法结合了基于拉普拉斯金字塔的框架和HyperNetwork，以应对混合曝光图像增强的挑战。HyperNetwork的引入使模型能够适应这些曝光差异。HyperNetwork能够动态生成另一个网络的权重，在部署过程中允许动态变化。在我们的模型中，使用的HyperNetwork用于预测特征金字塔分解的最佳核，从而实现针对每个输入图像的定制和适应性分解过程。增强的翻译网络结合了多尺度分解和重构，利用动态核预测来捕捉和操控不同尺度的特征。通过广泛的实验表明，HipyreNet在极端曝光差异的情况下优于现有方法，并在定性和定量评估中均获得了更优的结果。我们的方法为混合曝光图像增强设定了新的基准，为未来自适应图像翻译的研究铺平了道路。
2501.05179	 | Compression with Global Guidance: Towards Training-free High-Resolution MLLMs Acceleration	 | Xuyang Liu,Ziming Wang,Yuhang Han,Yingyao Wang,Jiale Yuan,Jun Song,Bo Zheng,Linfeng Zhang,Siteng Huang,Honggang Chen	 | 本文提出了一种名为GlobalCom$^2$的新标记压缩方法，专门适用于高分辨率图像理解的多模态大型语言模型，通过将缩略图的令牌作为“指挥官”指导压缩过程，实现了性能与效率的最佳平衡。实验表明，GlobalCom$^2$在使用LLaVA-NeXT-7B/13B模型时优于其他最先进的标记压缩方法。	 | Multimodal large language models (MLLMs) have attracted considerable attention due to their exceptional performance in visual content understanding and reasoning. However, their inference efficiency has been a notable concern, as the increasing length of multimodal contexts leads to quadratic complexity. Token compression techniques, which reduce the number of visual tokens, have demonstrated their effectiveness in reducing computational costs. Yet, these approaches have struggled to keep pace with the rapid advancements in MLLMs, especially the AnyRes strategy in the context of high-resolution image understanding. In this paper, we propose a novel token compression method, GlobalCom$^2$, tailored for high-resolution MLLMs that receive both the thumbnail and multiple crops. GlobalCom$^2$ treats the tokens derived from the thumbnail as the ``commander'' of the entire token compression process, directing the allocation of retention ratios and the specific compression for each crop. In this way, redundant tokens are eliminated while important local details are adaptively preserved to the highest extent feasible. Empirical results across 10 benchmarks reveal that GlobalCom$^2$ achieves an optimal balance between performance and efficiency, and consistently outperforms state-of-the-art token compression methods with LLaVA-NeXT-7B/13B models. Our code is released at \url{https://github.com/xuyang-liu16/GlobalCom2}.	 | 多模态大型语言模型（MLLMs）由于在视觉内容理解和推理方面表现出色而引起了广泛关注。然而，它们的推理效率一直是显著的关切点，因为随着多模态上下文长度的增加，计算复杂度呈现平方增长。通过减少视觉令牌数量的标记压缩技术已经显示出降低计算成本的有效性。然而，这些方法难以跟上MLLMs的快速发展，尤其是在高分辨率图像理解的背景下，AnyRes策略尤为突出。在本文中，我们提出了一种名为GlobalCom$^2$的新颖标记压缩方法，专门适用于同时接收缩略图和多个裁剪的高分辨率MLLMs。GlobalCom$^2$将从缩略图中得到的令牌视为整个标记压缩过程的“指挥官”，指导保留比率的分配以及每个裁剪的具体压缩。这样，冗余令牌被消除，同时重要局部细节被尽可能适当地保留。在10个基准测试中，实验证明GlobalCom$^2$在性能与效率之间达到了最佳平衡，并且在使用LLaVA-NeXT-7B/13B模型时，始终优于最先进的标记压缩方法。我们的代码已发布在\url{https://github.com/xuyang-liu16/GlobalCom2}。
2501.05177	 | FaceMe: Robust Blind Face Restoration with Personal Identification	 | Siyu Liu,Zheng-Peng Duan,Jia OuYang,Jiayi Fu,Hyunhee Park,Zikun Liu,Chun-Le Guo,Chongyi Li	 | FaceMe是一种基于扩散模型的个性化面部修复方法，通过使用身份编码器提取与身份相关的特点来指导扩散模型生成高质量且身份一致的面部图像，能够在保持身份一致性的同时恢复高质量的面部图像，表现出卓越的性能和鲁棒性。	 | Blind face restoration is a highly ill-posed problem due to the lack of necessary context. Although existing methods produce high-quality outputs, they often fail to faithfully preserve the individual's identity. In this paper, we propose a personalized face restoration method, FaceMe, based on a diffusion model. Given a single or a few reference images, we use an identity encoder to extract identity-related features, which serve as prompts to guide the diffusion model in restoring high-quality and identity-consistent facial images. By simply combining identity-related features, we effectively minimize the impact of identity-irrelevant features during training and support any number of reference image inputs during inference. Additionally, thanks to the robustness of the identity encoder, synthesized images can be used as reference images during training, and identity changing during inference does not require fine-tuning the model. We also propose a pipeline for constructing a reference image training pool that simulates the poses and expressions that may appear in real-world scenarios. Experimental results demonstrate that our FaceMe can restore high-quality facial images while maintaining identity consistency, achieving excellent performance and robustness.	 | 盲脸修复是一个高度病态的问题，因为缺乏必要的上下文。尽管现有的方法能够生成高质量的输出，但在保留个体身份方面往往不尽如人意。在这篇论文中，我们提出了一种基于扩散模型的个性化面部修复方法——FaceMe。给定一张或多张参考图像，我们使用身份编码器提取与身份相关的特点，这些特点是引导扩散模型修复高质量且身份一致的面部图像的提示。通过简单地结合与身份相关的特点，我们在训练过程中有效减少了无关身份特点的影响，并且在推理时可以接受任意数量的参考图像输入。此外，由于身份编码器的鲁棒性，合成的图像可以在训练期间用作参考图像，而在推理时身份的变化不需要重新调整模型。我们还提出了一种构建参考图像训练池的流水线，以模拟可能出现的真实世界场景中的姿态和表情。实验结果表明，我们的FaceMe能够在保持身份一致性的同时恢复高质量的面部图像，实现了卓越的性能和鲁棒性。
2501.05147	 | A Systematic Literature Review on Deep Learning-based Depth Estimation in Computer Vision	 | Ali Rohan,Md Junayed Hasan,Andrei Petrovski	 | 这项系统性文献综述（SLR）研究了基于深度学习的深度估计方法，发现这些方法能够有效处理各种场景条件，并在未见过的环境中表现出良好的泛化能力。研究涵盖了128篇相关文献，涉及单目、多目和多视角三种类型的深度估计，使用了多种数据集和评估指标，其中ResNet-50、ResNet-18、ResNet-101、U-Net和VGG-16是最常用的基础模型，而缺乏高质量的地面真实数据仍是主要挑战之一。	 | Depth estimation (DE) provides spatial information about a scene and enables tasks such as 3D reconstruction, object detection, and scene understanding. Recently, there has been an increasing interest in using deep learning (DL)-based methods for DE. Traditional techniques rely on handcrafted features that often struggle to generalise to diverse scenes and require extensive manual tuning. However, DL models for DE can automatically extract relevant features from input data, adapt to various scene conditions, and generalise well to unseen environments. Numerous DL-based methods have been developed, making it necessary to survey and synthesize the state-of-the-art (SOTA). Previous reviews on DE have mainly focused on either monocular or stereo-based techniques, rather than comprehensively reviewing DE. Furthermore, to the best of our knowledge, there is no systematic literature review (SLR) that comprehensively focuses on DE. Therefore, this SLR study is being conducted. Initially, electronic databases were searched for relevant publications, resulting in 1284 publications. Using defined exclusion and quality criteria, 128 publications were shortlisted and further filtered to select 59 high-quality primary studies. These studies were analysed to extract data and answer defined research questions. Based on the results, DL methods were developed for mainly three different types of DE: monocular, stereo, and multi-view. 20 publicly available datasets were used to train, test, and evaluate DL models for DE, with KITTI, NYU Depth V2, and Make 3D being the most used datasets. 29 evaluation metrics were used to assess the performance of DE. 35 base models were reported in the primary studies, and the top five most-used base models were ResNet-50, ResNet-18, ResNet-101, U-Net, and VGG-16. Finally, the lack of ground truth data was among the most significant challenges reported by primary studies.	 | 深度估计（DE）提供了场景的空间信息，并使3D重建、物体检测和场景理解等任务成为可能。最近，人们越来越关注使用深度学习（DL）方法进行深度估计。传统技术依赖于手工设计的特征，这些特征往往难以泛化到多样的场景中，并且需要大量的手动调整。然而，用于深度估计的DL模型可以从输入数据中自动提取相关特征，适应各种场景条件，并在未见过的环境中表现出良好的泛化能力。许多基于DL的方法已经被开发出来，因此需要对其进行综述和综合。之前的深度估计综述主要集中在单目或多目技术上，而非全面地综述深度估计。此外，据我们所知，没有系统性的文献综述（SLR）全面关注深度估计。因此，正在进行这项SLR研究。首先，在电子数据库中搜索相关文献，共检索到1284篇文献。通过定义的排除标准和质量标准，筛选出128篇文献，并进一步筛选出59篇高质量的原始研究。对这些研究进行了分析，提取数据并回答定义的研究问题。根据研究结果，基于DL的方法主要用于三种不同类型的深度估计：单目、多目和多视角。共有20个公开可用的数据集用于训练、测试和评估深度估计的DL模型，其中KITTI、NYU Depth V2和Make 3D是最常用的数据集。共使用了29种评估指标来评估深度估计的性能。在原始研究中有35个基础模型被报告，使用频率最高的五个基础模型分别是ResNet-50、ResNet-18、ResNet-101、U-Net和VGG-16。最后，原始研究中报告的最大挑战之一是没有地面真实数据。
2501.05132	 | CorrDiff: Adaptive Delay-aware Detector with Temporal Cue Inputs for Real-time Object Detection	 | Xiang Zhang,Chenchen Fu,Yufei Cui,Lan Yi,Yuyang Sun,Weiwei Wu,Xue Liu	 | 本文提出了一种名为CorrDiff的新颖实时流式感知方法，能够预测多个未来帧中物体的位置，有效补偿延迟，从而在自动驾驶等应用中提升安全性与可靠性，并在多种设备上实现了最高感知准确性。	 | Real-time object detection takes an essential part in the decision-making process of numerous real-world applications, including collision avoidance and path planning in autonomous driving systems. This paper presents a novel real-time streaming perception method named CorrDiff, designed to tackle the challenge of delays in real-time detection systems. The main contribution of CorrDiff lies in its adaptive delay-aware detector, which is able to utilize runtime-estimated temporal cues to predict objects' locations for multiple future frames, and selectively produce predictions that matches real-world time, effectively compensating for any communication and computational delays. The proposed model outperforms current state-of-the-art methods by leveraging motion estimation and feature enhancement, both for 1) single-frame detection for the current frame or the next frame, in terms of the metric mAP, and 2) the prediction for (multiple) future frame(s), in terms of the metric sAP (The sAP metric is to evaluate object detection algorithms in streaming scenarios, factoring in both latency and accuracy). It demonstrates robust performance across a range of devices, from powerful Tesla V100 to modest RTX 2080Ti, achieving the highest level of perceptual accuracy on all platforms. Unlike most state-of-the-art methods that struggle to complete computation within a single frame on less powerful devices, CorrDiff meets the stringent real-time processing requirements on all kinds of devices. The experimental results emphasize the system's adaptability and its potential to significantly improve the safety and reliability for many real-world systems, such as autonomous driving. Our code is completely open-sourced and is available at https://anonymous.4open.science/r/CorrDiff.	 | 实时目标检测在许多实际应用的决策过程中占据重要地位，包括自动驾驶系统中的碰撞避免和路径规划。本文提出了一种名为CorrDiff的新颖实时流式感知方法，旨在应对实时检测系统中的延迟挑战。CorrDiff的主要贡献在于其适应性延迟感知检测器，该检测器能够利用运行时估计的时间线索来预测多个未来帧中物体的位置，并仅在实际时间范围内生成预测，有效地补偿任何通信和计算延迟。所提出模型通过利用运动估计和特征增强，在1）当前帧或下一帧的单帧检测方面（以mAP为指标）和2）对未来帧的预测方面（以sAP为指标），均优于当前最先进的方法。它在各种设备上均表现出稳健的性能，从强大的Tesla V100到适度的RTX 2080Ti，均实现了所有平台上的最高感知准确性。与大多数最先进的方法在较弱设备上难以在单帧内完成计算不同，CorrDiff能够在各种设备上满足严格的实时处理要求。实验结果强调了该系统的适应性和其在提高许多实际系统（如自动驾驶）的安全性和可靠性方面的重要潜力。我们的代码完全开源，并可在https://anonymous.4open.science/r/CorrDiff获取。
2501.05131	 | 3DIS-FLUX: simple and efficient multi-instance generation with DiT rendering	 | Dewei Zhou,Ji Xie,Zongxin Yang,Yi Yang	 | 该研究提出了一种名为3DIS-FLUX的方法，通过将多实例生成分解为基于深度的场景构建和细节渲染两个阶段，利用FLUX模型提高了生成文本到图像的性能和图像质量，实现了对实例布局和属性的精确控制。	 | The growing demand for controllable outputs in text-to-image generation has driven significant advancements in multi-instance generation (MIG), enabling users to define both instance layouts and attributes. Currently, the state-of-the-art methods in MIG are primarily adapter-based. However, these methods necessitate retraining a new adapter each time a more advanced model is released, resulting in significant resource consumption. A methodology named Depth-Driven Decoupled Instance Synthesis (3DIS) has been introduced, which decouples MIG into two distinct phases: 1) depth-based scene construction and 2) detail rendering with widely pre-trained depth control models. The 3DIS method requires adapter training solely during the scene construction phase, while enabling various models to perform training-free detail rendering. Initially, 3DIS focused on rendering techniques utilizing U-Net architectures such as SD1.5, SD2, and SDXL, without exploring the potential of recent DiT-based models like FLUX. In this paper, we present 3DIS-FLUX, an extension of the 3DIS framework that integrates the FLUX model for enhanced rendering capabilities. Specifically, we employ the FLUX.1-Depth-dev model for depth map controlled image generation and introduce a detail renderer that manipulates the Attention Mask in FLUX's Joint Attention mechanism based on layout information. This approach allows for the precise rendering of fine-grained attributes of each instance. Our experimental results indicate that 3DIS-FLUX, leveraging the FLUX model, outperforms the original 3DIS method, which utilized SD2 and SDXL, and surpasses current state-of-the-art adapter-based methods in terms of both performance and image quality. Project Page: https://limuloo.github.io/3DIS/.	 | 随着对可控生成输出的需求不断增长，文本到图像生成领域取得了显著进展，推动了多实例生成（MIG）技术的发展，让用户能够定义实例布局和属性。目前，最先进的MIG方法主要基于适配器。然而，这些方法需要每次在发布更先进的模型时重新训练一个新的适配器，导致资源消耗巨大。为此，提出了一种名为深度驱动的解耦实例合成（3DIS）的方法，将MIG分解为两个独立阶段：1）基于深度的场景构建和2）使用广泛预训练的深度控制模型进行细节渲染。3DIS方法仅在场景构建阶段需要适配器训练，而允许各种模型在不进行训练的情况下进行细节渲染。最初，3DIS专注于使用U-Net架构如SD1.5、SD2和SDXL的渲染技术，而没有探索基于DiT模型如FLUX的最新潜力。在本文中，我们介绍了一种名为3DIS-FLUX的3DIS框架的扩展，它集成了FLUX模型以增强渲染能力。具体来说，我们使用FLUX.1-Depth-dev模型进行深度图控制图像生成，并引入了一个细节渲染器，该渲染器根据布局信息调整FLUX联合注意机制中的注意力掩码。这种方法使得能够精确地渲染每个实例的细粒度属性。实验结果表明，3DIS-FLUX利用FLUX模型在性能和图像质量方面均优于原3DIS方法，该方法使用了SD2和SDXL模型。项目页面：https://limuloo.github.io/3DIS/。
2501.05108	 | Optimizing Multitask Industrial Processes with Predictive Action Guidance	 | Naval Kishore Mehta,Arvind,Shyam Sunder Prasad,Sumeet Saurav,Sanjay Singh	 | 该研究提出了一个多模态变换器融合和递归单元（MMTFRU）网络，用于提高自我中心活动预测的准确性，并结合操作员动作监控单元（OAMU）提供实时指导，防止装配过程中的偏差，从而有效提升生产力和任务完成效率。	 | Monitoring complex assembly processes is critical for maintaining productivity and ensuring compliance with assembly standards. However, variability in human actions and subjective task preferences complicate accurate task anticipation and guidance. To address these challenges, we introduce the Multi-Modal Transformer Fusion and Recurrent Units (MMTFRU) Network for egocentric activity anticipation, utilizing multimodal fusion to improve prediction accuracy. Integrated with the Operator Action Monitoring Unit (OAMU), the system provides proactive operator guidance, preventing deviations in the assembly process. OAMU employs two strategies: (1) Top-5 MMTF-RU predictions, combined with a reference graph and an action dictionary, for next-step recommendations; and (2) Top-1 MMTF-RU predictions, integrated with a reference graph, for detecting sequence deviations and predicting anomaly scores via an entropy-informed confidence mechanism. We also introduce Time-Weighted Sequence Accuracy (TWSA) to evaluate operator efficiency and ensure timely task completion. Our approach is validated on the industrial Meccano dataset and the largescale EPIC-Kitchens-55 dataset, demonstrating its effectiveness in dynamic environments.	 | 监测复杂的装配过程对于保持生产力并确保符合装配标准至关重要。然而，人类动作的变异性及其主观任务偏好复杂了准确的任务预测和指导。为了解决这些挑战，我们引入了利用多模态融合提高预测准确性的多模态变换器融合和递归单元（MMTFRU）网络，以进行自我中心的活动预测。系统结合了操作员动作监控单元（OAMU），提供了提前的操作员指导，防止装配过程中的偏差。OAMU采用两种策略：(1) 基于多模态变换器融合和递归单元（MMTF-RU）的前五预测，结合参考图和动作字典，用于下一步推荐；(2) 基于多模态变换器融合和递归单元（MMTF-RU）的最显著预测，结合参考图，用于检测序列偏差并通过基于熵的信息置信机制预测异常分数。我们还引入了时间加权序列准确性（TWSA）来评估操作员效率并确保及时完成任务。我们的方法在工业Meccano数据集和大规模EPIC-Kitchens-55数据集上得到了验证，证明了其在动态环境中的有效性。
2501.05098	 | Motion-X++: A Large-Scale Multimodal 3D Whole-body Human Motion Dataset	 | Yuhong Zhang,Jing Lin,Ailing Zeng,Guanlin Wu,Shunlin Lu,Yurong Fu,Yuanhao Cai,Ruimao Zhang,Haoqian Wang,Lei Zhang	 | 本文介绍了Motion-X++，一个包含1.95亿个3D全身姿态标注和多模态标签的数据集，覆盖了12.05万个运动序列、8.08万个RGB视频、4.53万个音频等，显著提高了表达性、精确性和自然度，支持多种下游任务。	 | In this paper, we introduce Motion-X++, a large-scale multimodal 3D expressive whole-body human motion dataset. Existing motion datasets predominantly capture body-only poses, lacking facial expressions, hand gestures, and fine-grained pose descriptions, and are typically limited to lab settings with manually labeled text descriptions, thereby restricting their scalability. To address this issue, we develop a scalable annotation pipeline that can automatically capture 3D whole-body human motion and comprehensive textural labels from RGB videos and build the Motion-X dataset comprising 81.1K text-motion pairs. Furthermore, we extend Motion-X into Motion-X++ by improving the annotation pipeline, introducing more data modalities, and scaling up the data quantities. Motion-X++ provides 19.5M 3D whole-body pose annotations covering 120.5K motion sequences from massive scenes, 80.8K RGB videos, 45.3K audios, 19.5M frame-level whole-body pose descriptions, and 120.5K sequence-level semantic labels. Comprehensive experiments validate the accuracy of our annotation pipeline and highlight Motion-X++'s significant benefits for generating expressive, precise, and natural motion with paired multimodal labels supporting several downstream tasks, including text-driven whole-body motion generation,audio-driven motion generation, 3D whole-body human mesh recovery, and 2D whole-body keypoints estimation, etc.	 | 在本文中，我们介绍了Motion-X++，这是一个大规模多模态3D全身表现性人体运动数据集。现有的运动数据集主要捕捉身体姿态，缺乏面部表情、手势和精细的姿态描述，并且通常局限于实验室环境，使用人工标注的文字描述，从而限制了其可扩展性。为了解决这一问题，我们开发了一种可扩展的标注流水线，能够自动捕捉3D全身人体运动和全面的纹理标签，并从RGB视频中构建包含81.1万文本-运动配对的Motion-X数据集。此外，我们通过改进标注流水线、引入更多数据模态和扩大数据量，将Motion-X扩展为Motion-X++。Motion-X++提供了1.95亿个3D全身姿态标注，涵盖了12.05万个运动序列，8.08万个RGB视频，4.53万个音频，1.95亿个帧级全身姿态描述，以及12.05万个序列级语义标签。全面的实验验证了我们标注流水线的准确性，并突出了Motion-X++在生成具有配对多模态标签的表达性、精确性和自然运动方面的显著优势，支持下游任务，包括文本驱动的全身运动生成、音频驱动的运动生成、3D全身人体网格恢复和2D全身关键点估计等。
2501.05097	 | A 1Mb mixed-precision quantized encoder for image classification and patch-based compression	 | Van Thien Nguyen,William Guicquero,Gilles Sicard	 | 本文展示了一种专用于图像处理的ASIC神经网络加速器，能够处理多种任务如图像分类和压缩，同时仅需有限的硬件资源。通过采用可重构的混合精度编码器和自动调整的量化因子，该加速器不仅实现了高分类精度，还在端到端的无块状伪影图像压缩方面优于现有技术。	 | Even if Application-Specific Integrated Circuits (ASIC) have proven to be a relevant choice for integrating inference at the edge, they are often limited in terms of applicability. In this paper, we demonstrate that an ASIC neural network accelerator dedicated to image processing can be applied to multiple tasks of different levels: image classification and compression, while requiring a very limited hardware. The key component is a reconfigurable, mixed-precision (3b/2b/1b) encoder that takes advantage of proper weight and activation quantizations combined with convolutional layer structural pruning to lower hardware-related constraints (memory and computing). We introduce an automatic adaptation of linear symmetric quantizer scaling factors to perform quantized levels equalization, aiming at stabilizing quinary and ternary weights training. In addition, a proposed layer-shared Bit-Shift Normalization significantly simplifies the implementation of the hardware-expensive Batch Normalization. For a specific configuration in which the encoder design only requires 1Mb, the classification accuracy reaches 87.5% on CIFAR-10. Besides, we also show that this quantized encoder can be used to compress image patch-by-patch while the reconstruction can performed remotely, by a dedicated full-frame decoder. This solution typically enables an end-to-end compression almost without any block artifacts, outperforming patch-based state-of-the-art techniques employing a patch-constant bitrate.	 | 即使专用集成电路（ASIC）已被证实是将推理功能集成到边缘的合适选择，它们在适用性方面往往有限制。在本文中，我们证明了一种专门用于图像处理的ASIC神经网络加速器可以应用于不同级别的多种任务：图像分类和压缩，同时只需要非常有限的硬件资源。关键组件是一种可重构的混合精度（3b/2b/1b）编码器，该编码器利用适当的权重和激活量化，结合卷积层结构剪枝，以降低硬件相关的限制（内存和计算）。我们引入了一种自动调整线性对称量化因子的方法，旨在稳定五元和三元权重训练。此外，我们提出了一个分层共享的位移归一化，显著简化了昂贵的批归一化硬件实现。在一个特定的配置中，编码器设计只需要1兆字节的存储空间，分类精度在CIFAR-10数据集上达到了87.5%。此外，我们还展示了这种量化编码器可以用于逐块压缩图像，而重建可以通过专用的全帧解码器在远程进行。这种解决方案通常可以实现几乎无块状伪影的端到端压缩，优于基于块的最先进的技术，这些技术使用固定的比特率。
2501.05095	 | Advancing ALS Applications with Large-Scale Pre-training: Dataset Development and Downstream Assessment	 | Haoyi Xiu,Xin Liu,Taehoon Kim,Kyoung-Sook Kim	 | 该研究通过构建大规模的航空激光扫描（ALS）点云数据集，填补了预训练和微调范式在ALS领域的空白，并评估其对下游应用的影响，结果显示预训练模型在树木分类、地形识别和点云分割等任务上显著优于从零开始训练的模型，且地理空间采样方法进一步提升了模型性能。	 | The pre-training and fine-tuning paradigm has revolutionized satellite remote sensing applications. However, this approach remains largely underexplored for airborne laser scanning (ALS), an important technology for applications such as forest management and urban planning. In this study, we address this gap by constructing a large-scale ALS point cloud dataset and evaluating its impact on downstream applications. Our dataset comprises ALS point clouds collected across the contiguous United States, provided by the United States Geological Survey's 3D Elevation Program. To ensure efficient data collection while capturing diverse land cover and terrain types, we introduce a geospatial sampling method that selects point cloud tiles based on land cover maps and digital elevation models. As a baseline self-supervised learning model, we adopt BEV-MAE, a state-of-the-art masked autoencoder for 3D outdoor point clouds, and pre-train it on the constructed dataset. The pre-trained models are subsequently fine-tuned for downstream tasks, including tree species classification, terrain scene recognition, and point cloud semantic segmentation. Our results show that the pre-trained models significantly outperform their scratch counterparts across all downstream tasks, demonstrating the transferability of the representations learned from the proposed dataset. Furthermore, we observe that scaling the dataset using our geospatial sampling method consistently enhances performance, whereas pre-training on datasets constructed with random sampling fails to achieve similar improvements. These findings highlight the utility of the constructed dataset and the effectiveness of our sampling strategy in the pre-training and fine-tuning paradigm. The source code and pre-trained models will be made publicly available at \url{https://github.com/martianxiu/ALS_pretraining}.	 | 该预训练和微调范式已经彻底改变了卫星遥感应用。然而，这种方法在航空激光扫描（ALS）领域中的应用仍然相对较少，ALS 是森林管理和城市规划等应用的重要技术。在这项研究中，我们通过构建大规模的 ALS 点云数据集，来填补这一空白，并评估其对下游应用的影响。我们的数据集包括美国本土范围内由美国地质调查局 3D 地形计划提供的 ALS 点云数据。为了确保高效的数据采集并涵盖不同类型的土地覆盖和地形，我们引入了一种地理空间采样方法，该方法基于土地覆盖图和数字高程模型选择点云切片。作为基线的自监督学习模型，我们采用了 BEV-MAE，这是一种用于 3D 室外点云的最先进的掩码自编码器，并在构建的数据集上对其进行预训练。预训练的模型随后被微调以完成下游任务，包括树木种类分类、地形场景识别和点云语义分割。结果显示，预训练的模型在所有下游任务上显著优于从零开始训练的模型，证明了所提出数据集学习到的表示的可迁移性。此外，我们观察到，使用我们的地理空间采样方法扩展数据集能够持续增强性能，而使用随机采样构建的数据集进行预训练无法达到类似的改进效果。这些发现突显了构建数据集的实用性和我们采样策略在预训练和微调范式中的有效性。源代码和预训练模型将在 \url{https://github.com/martianxiu/ALS_pretraining} 上公开。
2501.05091	 | ResPanDiff: Diffusion Model with Disentangled Modulations for Image Fusion	 | Shiqi Cao,Liangjian Deng,Shangqi Deng	 | 该研究提出了一种名为ResPanDiff的新颖扩散模型，通过引入一种新颖的马尔可夫链和改进的潜在空间设计，显著减少了采样步骤而不牺牲性能，特别适用于图像增强锐化任务。实验结果表明，ResPanDiff仅需15个采样步骤，相比基准模型减少了超过90%的步骤数，并在性能上优于现有最先进的技术。	 | The implementation of diffusion-based pansharpening task is predominantly constrained by its slow inference speed, which results from numerous sampling steps. Despite the existing techniques aiming to accelerate sampling, they often compromise performance when fusing multi-source images. To ease this limitation, we introduce a novel and efficient diffusion model named Diffusion Model for Pansharpening by Inferring Residual Inference (ResPanDiff), which significantly reduces the number of diffusion steps without sacrificing the performance to tackle pansharpening task. In ResPanDiff, we innovatively propose a Markov chain that transits from noisy residuals to the residuals between the LRMS and HRMS images, thereby reducing the number of sampling steps and enhancing performance. Additionally, we design the latent space to help model extract more features at the encoding stage, Shallow Cond-Injection~(SC-I) to help model fetch cond-injected hidden features with higher dimensions, and loss functions to give a better guidance for the residual generation task. enabling the model to achieve superior performance in residual generation. Furthermore, experimental evaluations on pansharpening datasets demonstrate that the proposed method achieves superior outcomes compared to recent state-of-the-art~(SOTA) techniques, requiring only 15 sampling steps, which reduces over $90\%$ step compared with the benchmark diffusion models. Our experiments also include thorough discussions and ablation studies to underscore the effectiveness of our approach.	 | 扩散增强锐化任务的实现主要受限于其缓慢的推理速度，这源于大量的采样步骤。尽管现有技术旨在加速采样，但它们往往在多源图像融合时会牺牲性能。为了解决这一限制，我们引入了一种新颖且高效的扩散模型——基于残差推断的扩散模型（ResPanDiff），该模型在显著减少扩散步骤的同时，不牺牲性能来应对增强锐化任务。在ResPanDiff中，我们创新地提出了一种马尔可夫链，该链从噪声残差过渡到低分辨率多光谱（LRMS）和高分辨率多光谱（HRMS）图像之间的残差，从而减少采样步骤并提高性能。此外，我们设计了潜在空间以帮助模型在编码阶段提取更多特征，引入了浅层条件注射（SC-I）以帮助模型获取更高维度的条件注入隐特征，并设计了损失函数以更好地指导残差生成任务，从而使模型在残差生成方面取得优越性能。此外，对增强锐化数据集的实验评估表明，所提出的方法在与最近的最先进的（SOTA）技术相比时表现更优，仅需15个采样步骤，这比基准扩散模型减少了超过90%的步骤数。我们的实验还包括详尽的讨论和消融研究，以强调我们方法的有效性。
2501.05076	 | TipSegNet: Fingertip Segmentation in Contactless Fingerprint Imaging	 | Laurenz Ruzicka,Bernhard Kohn,Clemens Heitzinger	 | 本文介绍了一种名为TipSegNet的深度学习模型，该模型能够从灰度手掌图像中实现精确的指尖分割，并在多种条件下表现出色，达到了平均交并比（mIoU）为0.987和准确率为0.999的良好性能。	 | Contactless fingerprint recognition systems offer a hygienic, user-friendly, and efficient alternative to traditional contact-based methods. However, their accuracy heavily relies on precise fingertip detection and segmentation, particularly under challenging background conditions. This paper introduces TipSegNet, a novel deep learning model that achieves state-of-the-art performance in segmenting fingertips directly from grayscale hand images. TipSegNet leverages a ResNeXt-101 backbone for robust feature extraction, combined with a Feature Pyramid Network (FPN) for multi-scale representation, enabling accurate segmentation across varying finger poses and image qualities. Furthermore, we employ an extensive data augmentation strategy to enhance the model's generalizability and robustness. TipSegNet outperforms existing methods, achieving a mean Intersection over Union (mIoU) of 0.987 and an accuracy of 0.999, representing a significant advancement in contactless fingerprint segmentation. This enhanced accuracy has the potential to substantially improve the reliability and effectiveness of contactless biometric systems in real-world applications.	 | 无接触指纹识别系统提供了一种卫生、用户友好且高效的替代传统接触式方法的选择。然而，它们的准确性高度依赖于对指尖的精确检测和分割，尤其是在复杂背景条件下。本文介绍了一种名为TipSegNet的新颖深度学习模型，该模型能够直接从灰度手掌图像中实现最先进的指尖分割性能。TipSegNet利用ResNeXt-101骨干网络进行稳健的特征提取，并结合Feature Pyramid Network (FPN)多尺度表示，从而能够跨越不同手指姿态和图像质量实现准确分割。此外，我们采用了一种广泛的图像增强策略来增强模型的泛化能力和鲁棒性。TipSegNet在现有方法中表现出色，达到平均交并比（mIoU）为0.987和准确率为0.999，这代表了无接触指纹分割领域的重要进展。这种增强的准确性有可能在实际应用中显著提高无接触生物识别系统的可靠性和有效性。
2501.05069	 | Commonsense Video Question Answering through Video-Grounded Entailment Tree Reasoning	 | Huabin Liu,Filip Ilievski,Cees G. M. Snoek	 | 本文提出了一种新的视频引导推理树方法，用于常识视频问答，通过构建推理树、视频-语言蕴含验证、树推理和动态树扩展四步将VQA任务与视频片段明确对接，克服了视觉语言模型在视频与答案间学习虚假关联的问题，并通过去偏差程序重新编写基准答案集以支持公平评估。	 | This paper proposes the first video-grounded entailment tree reasoning method for commonsense video question answering (VQA). Despite the remarkable progress of large visual-language models (VLMs), there are growing concerns that they learn spurious correlations between videos and likely answers, reinforced by their black-box nature and remaining benchmarking biases. Our method explicitly grounds VQA tasks to video fragments in four steps: entailment tree construction, video-language entailment verification, tree reasoning, and dynamic tree expansion. A vital benefit of the method is its generalizability to current video and image-based VLMs across reasoning types. To support fair evaluation, we devise a de-biasing procedure based on large-language models that rewrites VQA benchmark answer sets to enforce model reasoning. Systematic experiments on existing and de-biased benchmarks highlight the impact of our method components across benchmarks, VLMs, and reasoning types.	 | 本文提出了第一个用于常识视频问答（VQA）的视频引导推理树方法。尽管大型视觉语言模型（VLMs）取得了显著的进步，人们对其在视频与可能答案之间学习虚假关联的现象越来越担忧，尤其是由于这些模型的黑盒性质以及剩余的基准测试偏差。我们的方法通过四个步骤明确将VQA任务与视频片段进行对接：构建推理树、视频-语言蕴含验证、树推理和动态树扩展。该方法的一个重要优势在于它能够跨各种推理类型，将当前基于视频和图像的VLMs泛化。为了支持公平评估，我们基于大型语言模型设计了一种去偏差程序，重新编写VQA基准答案集，以强制模型进行推理。在现有基准和去偏差基准上的系统实验突显了我们方法组件在不同基准、VLMs和推理类型下的影响。
2501.05067	 | LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding	 | Jiaxing Zhao,Boyuan Sun,Xiang Chen,Xihan Wei,Qibin Hou	 | LLaVA-Octopus是一种新型的视频多模态大语言模型，能够根据用户指令自适应地加权不同视觉投影器的特征，从而显著提高多模态任务的性能，尤其在多模态理解、视觉问答和视频理解等任务中表现优异。	 | In this paper, we introduce LLaVA-Octopus, a novel video multimodal large language model. LLaVA-Octopus adaptively weights features from different visual projectors based on user instructions, enabling us to leverage the complementary strengths of each projector. We observe that different visual projectors exhibit distinct characteristics when handling specific tasks. For instance, some projectors excel at capturing static details, while others are more effective at processing temporal information, and some are better suited for tasks requiring temporal coherence. By dynamically adjusting feature weights according to user instructions, LLaVA-Octopus dynamically selects and combines the most suitable features, significantly enhancing the model's performance in multimodal tasks. Experimental results demonstrate that LLaVA-Octopus achieves excellent performance across multiple benchmarks, especially in tasks such as multimodal understanding, visual question answering, and video understanding, highlighting its broad application potential.	 | 在本文中，我们介绍了LLaVA-Octopus，一种新型的视频多模态大语言模型。LLaVA-Octopus 根据用户指令自适应地加权来自不同视觉投影器的特征，使我们能够充分利用每个投影器的互补优势。我们观察到，不同的视觉投影器在处理特定任务时表现出不同的特性。例如，一些投影器擅长捕捉静态细节，而另一些则更适合处理时间信息，还有一些则更适合需要时间连贯的任务。通过根据用户指令动态调整特征权重，LLaVA-Octopus 动态地选择并组合最合适的特征，显著提高了模型在多模态任务中的性能。实验结果表明，LLaVA-Octopus 在多个基准测试中表现出色，特别是在多模态理解、视觉问答和视频理解等任务中，突显了其广泛的应用潜力。
2501.05066	 | Improving Skeleton-based Action Recognition with Interactive Object Information	 | Hao Wen,Ziqian Lu,Fengli Shen,Zhe-Ming Lu,Jialin Cui	 | 该研究提出了一种新的动作识别框架，通过引入对象节点来补充现有的基于骨架的动作识别方法，有效解决了在交互动作识别中的不足，并采用空间时变可变图卷积网络（ST-VGCN）建模包含对象的可变图，显著提升了识别性能。实验结果显示，该方法在多个基准测试中超越了现有先进技术，特别是在NTU RGB+D 60的交叉行人分割中达到96.7%的准确率。	 | Human skeleton information is important in skeleton-based action recognition, which provides a simple and efficient way to describe human pose. However, existing skeleton-based methods focus more on the skeleton, ignoring the objects interacting with humans, resulting in poor performance in recognizing actions that involve object interactions. We propose a new action recognition framework introducing object nodes to supplement absent interactive object information. We also propose Spatial Temporal Variable Graph Convolutional Networks (ST-VGCN) to effectively model the Variable Graph (VG) containing object nodes. Specifically, in order to validate the role of interactive object information, by leveraging a simple self-training approach, we establish a new dataset, JXGC 24, and an extended dataset, NTU RGB+D+Object 60, including more than 2 million additional object nodes. At the same time, we designe the Variable Graph construction method to accommodate a variable number of nodes for graph structure. Additionally, we are the first to explore the overfitting issue introduced by incorporating additional object information, and we propose a VG-based data augmentation method to address this issue, called Random Node Attack. Finally, regarding the network structure, we introduce two fusion modules, CAF and WNPool, along with a novel Node Balance Loss, to enhance the comprehensive performance by effectively fusing and balancing skeleton and object node information. Our method surpasses the previous state-of-the-art on multiple skeleton-based action recognition benchmarks. The accuracy of our method on NTU RGB+D 60 cross-subject split is 96.7\%, and on cross-view split, it is 99.2\%.	 | 人类骨架信息在基于骨架的动作识别中非常重要，它提供了一种简单而高效的方式来描述人类姿态。然而，现有的基于骨架的方法更多地关注骨架本身，而忽略了与人类互动的物体，这导致在识别涉及物体交互的动作时表现不佳。我们提出了一种新的动作识别框架，引入了对象节点以补充缺失的交互对象信息。同时，我们还提出了一种空间时变可变图卷积网络（ST-VGCN），以有效地建模包含对象节点的可变图（VG）。具体来说，为了验证交互对象信息的作用，我们利用一个简单的自训练方法建立了新的数据集JXGC 24和扩展数据集NTU RGB+D+Object 60，其中包括超过200万个额外的对象节点。同时，我们设计了一种可变图构建方法，以适应图结构中的不同节点数量。此外，我们首次探索了引入额外对象信息所产生的过拟合问题，并提出了一种基于可变图的数据增强方法，称为随机节点攻击（Random Node Attack）。最后，在网络结构方面，我们引入了两种融合模块CAF和WNPool，以及一种新颖的节点平衡损失，以通过有效地融合和平衡骨架节点和对象节点信息来增强综合性能。我们的方法在多个基于骨架的动作识别基准测试中超越了先前的最先进方法。在NTU RGB+D 60的交叉行人分割中，我们的方法的准确率为96.7%，而在交叉视角分割中，准确率为99.2%。
2501.05037	 | LongViTU: Instruction Tuning for Long-Form Video Understanding	 | Rujie Wu,Xiaojian Ma,Hai Ci,Yue Fan,Yuxuan Wang,Haozhe Zhao,Qing Li,Yizhou Wang	 | LongViTU是一个大规模自动生成的数据集，包含约121,000个问答对和900小时的视频，用于长格式视频理解，并展示了显著的挑战性；基于LongViTU的基准测试表明其数据具有高质量和良好的开集泛化能力。	 | This paper introduce LongViTU, a large-scale (~121k QA pairs, ~900h videos), automatically generated dataset for long-form video understanding. We developed a systematic approach that organizes videos into a hierarchical tree structure and incorporates self-revision mechanisms to ensure high-quality QA pairs. Each QA pair in LongViTU features: 1) long-term context (average certificate length of 4.6 minutes); 2) rich knowledge and condensed reasoning (commonsense, causality, planning, etc.); and 3) explicit timestamp labels for relevant events. LongViTU also serves as a benchmark for instruction following in long-form and streaming video understanding. We evaluate the open-source state-of-the-art long video understanding model, LongVU, and the commercial model, Gemini-1.5-Pro, on our benchmark. They achieve GPT-4 scores of 49.9 and 52.3, respectively, underscoring the substantial challenge posed by our benchmark. Further supervised fine-tuning (SFT) on LongVU led to performance improvements of 12.0% on our benchmark, 2.2% on the in-distribution (ID) benchmark EgoSchema, 1.0%, 2.2% and 1.2% on the out-of-distribution (OOD) benchmarks VideoMME (Long), WorldQA and OpenEQA, respectively. These outcomes demonstrate LongViTU's high data quality and robust OOD generalizability.	 | 本文介绍了LongViTU，这是一个大规模的自动生成数据集（约121,000个问答对，约900小时视频），用于长格式视频理解。我们开发了一种系统的方法，将视频组织成层次树结构，并引入了自我修订机制以确保高质量的问答对。LongViTU 中的每个问答对具有以下特点：1）长期上下文（平均证书长度为4.6分钟）；2）丰富的知识和浓缩的推理（常识、因果关系、计划等）；以及3）相关事件的显式时间戳标签。LongViTU 还作为长格式和流媒体视频理解的基准测试数据集。我们对开源的最先进的长视频理解模型LongVU和商业模型Gemini-1.5-Pro进行了基准测试。它们分别获得了GPT-4得分49.9和52.3，突显了我们基准测试的显著挑战性。进一步的监督微调（SFT）提高了LongVU在我们基准测试中的性能12.0%，在同分布（ID）基准测试EgoSchema中提高了2.2%，以及在不同分布（OOD）基准测试VideoMME（长）、WorldQA和OpenEQA中的性能分别提高了1.0%、2.2%和1.2%。这些结果证明了LongViTU的数据质量高和稳健的OOD泛化能力。
2501.05034	 | Towards Fingerprint Mosaicking Artifact Detection: A Self-Supervised Deep Learning Approach	 | Laurenz Ruzicka,Alexander Spenke,Stephan Bergmann,Gerd Nolden,Bernhard Kohn,Clemens Heitzinger	 | 本文提出了一种基于深度学习的方法，用于检测和评分指纹图像中的马赛克错误，并在多种指纹模式下实现了高精度，提高了指纹识别系统的准确性和可靠性。该方法利用自主监督学习框架在大量未标记数据上训练模型，无需手动标注，并引入了一种新的马赛克错误评分方法以量化错误程度。	 | Fingerprint mosaicking, which is the process of combining multiple fingerprint images into a single master fingerprint, is an essential process in modern biometric systems. However, it is prone to errors that can significantly degrade fingerprint image quality. This paper proposes a novel deep learning-based approach to detect and score mosaicking artifacts in fingerprint images. Our method leverages a self-supervised learning framework to train a model on large-scale unlabeled fingerprint data, eliminating the need for manual artifact annotation. The proposed model effectively identifies mosaicking errors, achieving high accuracy on various fingerprint modalities, including contactless, rolled, and pressed fingerprints and furthermore proves to be robust to different data sources. Additionally, we introduce a novel mosaicking artifact score to quantify the severity of errors, enabling automated evaluation of fingerprint images. By addressing the challenges of mosaicking artifact detection, our work contributes to improving the accuracy and reliability of fingerprint-based biometric systems.	 | 指纹马赛克拼接是将多个指纹图像合成一个主指纹图像的过程，在现代生物特征识别系统中是一项关键步骤。然而，这一过程容易出现错误，严重影响指纹图像的质量。本文提出了一种基于深度学习的新型方法，用于检测和评分指纹图像中的马赛克错误。我们的方法利用自主监督学习框架，在大量未标记的指纹数据上训练模型，消除了手动标注伪影的需要。所提出的方法能够有效识别马赛克错误，在接触式、卷式和压印等多种指纹模式下都取得了高精度，并且证明了对不同数据源的鲁棒性。此外，我们还引入了一种新的马赛克错误评分方法，用于量化错误的严重程度，使指纹图像的自动化评估成为可能。通过解决马赛克错误检测的挑战，我们的研究为提高指纹生物特征识别系统的准确性和可靠性做出了贡献。
2501.05031	 | ECBench: Can Multi-modal Foundation Models Understand the Egocentric World? A Holistic Embodied Cognition Benchmark	 | Ronghao Dang,Yuqian Yuan,Wenqi Zhang,Yifei Xin,Boqiang Zhang,Long Li,Liuyi Wang,Qinyang Zeng,Xin Li,Lidong Bing	 | ECBench 是一个高质量的基准测试框架，旨在系统性地评估大型视觉-语言模型在机器人自我认知、动态场景感知和幻觉等关键体感认知任务上的能力，通过多样化的场景视频来源和严格的问题筛选策略，确保评估的高质量和公平性。	 | The enhancement of generalization in robots by large vision-language models (LVLMs) is increasingly evident. Therefore, the embodied cognitive abilities of LVLMs based on egocentric videos are of great interest. However, current datasets for embodied video question answering lack comprehensive and systematic evaluation frameworks. Critical embodied cognitive issues, such as robotic self-cognition, dynamic scene perception, and hallucination, are rarely addressed. To tackle these challenges, we propose ECBench, a high-quality benchmark designed to systematically evaluate the embodied cognitive abilities of LVLMs. ECBench features a diverse range of scene video sources, open and varied question formats, and 30 dimensions of embodied cognition. To ensure quality, balance, and high visual dependence, ECBench uses class-independent meticulous human annotation and multi-round question screening strategies. Additionally, we introduce ECEval, a comprehensive evaluation system that ensures the fairness and rationality of the indicators. Utilizing ECBench, we conduct extensive evaluations of proprietary, open-source, and task-specific LVLMs. ECBench is pivotal in advancing the embodied cognitive capabilities of LVLMs, laying a solid foundation for developing reliable core models for embodied agents. All data and code are available at https://github.com/Rh-Dang/ECBench.	 | 通过大型视觉-语言模型（LVLMs）增强机器人的泛化能力越来越明显。因此，基于第一人称视频的LVLMs的体现认知能力备受关注。然而，当前用于体感视频问答的数据集缺乏全面而系统的评估框架。诸如机器人自我认知、动态场景感知和幻觉等关键体感认知问题鲜有涉及。为应对这些挑战，我们提出了ECBench，一个高质量的基准测试，旨在系统性地评估LVLMs的体感认知能力。ECBench 包含多样化的场景视频来源、开放且多样的问题格式，以及体感认知的30个维度。为了确保高质量、平衡性和高视觉依赖性，ECBench 使用了类无独立的人工注释和多轮问题筛选策略。此外，我们引入了ECEval，一个全面的评估系统，确保指标的公平性和合理性。利用ECBench，我们对专有的、开源的和任务特定的LVLMs进行了广泛的评估。ECBench 对于推动LVLMs的体感认知能力至关重要，为开发可靠的体感代理核心模型奠定了坚实的基础。所有数据和代码均在 https://github.com/Rh-Dang/ECBench 可用。
2501.05020	 | Perception-as-Control: Fine-grained Controllable Image Animation with 3D-aware Motion Representation	 | Yingjie Chen,Yifang Men,Yuan Yao,Miaomiao Cui,Liefeng Bo	 | 本文提出了一种基于3D感知的运动表示和“感知即控制”框架，实现了细粒度的相机和对象协作运动控制，适用于多种视频合成任务。该方法通过从参考图像构建3D运动表示、操作用户意图并从不同视角感知，将运动控制转化为直观的视觉变化。	 | Motion-controllable image animation is a fundamental task with a wide range of potential applications. Recent works have made progress in controlling camera or object motion via various motion representations, while they still struggle to support collaborative camera and object motion control with adaptive control granularity. To this end, we introduce 3D-aware motion representation and propose an image animation framework, called Perception-as-Control, to achieve fine-grained collaborative motion control. Specifically, we construct 3D-aware motion representation from a reference image, manipulate it based on interpreted user intentions, and perceive it from different viewpoints. In this way, camera and object motions are transformed into intuitive, consistent visual changes. Then, the proposed framework leverages the perception results as motion control signals, enabling it to support various motion-related video synthesis tasks in a unified and flexible way. Experiments demonstrate the superiority of the proposed framework. For more details and qualitative results, please refer to our project webpage: https://chen-yingjie.github.io/projects/Perception-as-Control.	 | 运动可控制的图像动画是一个具有广泛潜在应用的基本任务。最近的工作通过各种运动表示控制相机或对象的运动方面取得了进展，但仍难以实现具有适应性控制粒度的协作相机和对象运动控制。为了解决这一问题，我们引入了3D感知的运动表示，并提出了一种称为“感知即控制”的图像动画框架，以实现细粒度的协作运动控制。具体来说，我们从参考图像中构建3D感知的运动表示，基于解释的用户意图对其进行操作，并从不同的视角感知它。通过这种方式，相机和对象的运动被转换为直观且一致的视觉变化。然后，提出的框架利用感知结果作为运动控制信号，使其能够以统一且灵活的方式支持各种与运动相关的视频合成任务。实验表明，所提出的框架具有优势。欲了解更多信息和定性结果，请参阅我们的项目网页：https://chen-yingjie.github.io/projects/Perception-as-Control。
2501.05017	 | Continuous Knowledge-Preserving Decomposition for Few-Shot Continual Learning	 | Xiaojie Li,Yibo Yang,Jianlong Wu,David A. Clifton,Yue Yu,Bernard Ghanem,Min Zhang	 | CKPD-FSCIL框架将模型权重分解为知识敏感组件和冗余容量组件，通过仅调整冗余容量组件来促进适应性并最小化干扰，从而有效解决了少量样本类别增量学习中的灾难性遗忘问题，并在多种基准测试中优于现有方法。	 | Few-shot class-incremental learning (FSCIL) involves learning new classes from limited data while retaining prior knowledge, and often results in catastrophic forgetting. Existing methods either freeze backbone networks to preserve knowledge, which limits adaptability, or rely on additional modules or prompts, introducing inference overhead. To this end, we propose Continuous Knowledge-Preserving Decomposition for FSCIL (CKPD-FSCIL), a framework that decomposes a model's weights into two parts: one that compacts existing knowledge (knowledge-sensitive components) and another that carries redundant capacity to accommodate new abilities (redundant-capacity components). The decomposition is guided by a covariance matrix from replay samples, ensuring principal components align with classification abilities. During adaptation, we freeze the knowledge-sensitive components and only adapt the redundant-capacity components, fostering plasticity while minimizing interference without changing the architecture or increasing overhead. Additionally, CKPD introduces an adaptive layer selection strategy to identify layers with redundant capacity, dynamically allocating adapters. Experiments on multiple benchmarks show that CKPD-FSCIL outperforms state-of-the-art methods.	 | 以下是该摘要的中文翻译：  少量样本类别增量学习（Few-shot Class-Incremental Learning, FSCIL）涉及从有限数据中学习新类别并保留先前知识，通常会导致灾难性遗忘。现有方法要么冻结骨干网络以保留知识，从而限制了适应性，要么依赖于额外模块或提示，增加了推理开销。为了解决这些问题，我们提出了连续知识保留分解用于FSCIL（Continuous Knowledge-Preserving Decomposition for FSCIL，CKPD-FSCIL）框架，该框架将模型的权重分解为两部分：一部分用于压缩现有知识（知识敏感组件），另一部分携带冗余容量以适应新能力（冗余容量组件）。分解过程由重放样本的协方差矩阵引导，确保主成分与分类能力对齐。在适应过程中，我们冻结知识敏感组件，仅调整冗余容量组件，从而促进可塑性并最小化干扰，而不需要改变架构或增加开销。此外，CKPD引入了一种自适应层选择策略，用于识别具有冗余容量的层，并动态分配适配器。在多种基准测试上的实验结果显示，CKPD-FSCIL优于现有最先进的方法。
2501.04995	 | IPDN: Image-enhanced Prompt Decoding Network for 3D Referring Expression Segmentation	 | Qi Chen,Changli Wu,Jiayi Ji,Yiwei Ma,Danni Yang,Xiaoshuai Sun	 | 本文提出了一种图像增强提示解码网络（IPDN），通过引入多视角语义嵌入（MSE）模块和提示感知解码器（PAD）来解决3D表达分割中的特征和意图歧义性，显著提高了3D-RES和3D-GRES任务上的mIoU指标，分别比最先进的方法高出1.9和4.2个百分点。	 | 3D Referring Expression Segmentation (3D-RES) aims to segment point cloud scenes based on a given expression. However, existing 3D-RES approaches face two major challenges: feature ambiguity and intent ambiguity. Feature ambiguity arises from information loss or distortion during point cloud acquisition due to limitations such as lighting and viewpoint. Intent ambiguity refers to the model's equal treatment of all queries during the decoding process, lacking top-down task-specific guidance. In this paper, we introduce an Image enhanced Prompt Decoding Network (IPDN), which leverages multi-view images and task-driven information to enhance the model's reasoning capabilities. To address feature ambiguity, we propose the Multi-view Semantic Embedding (MSE) module, which injects multi-view 2D image information into the 3D scene and compensates for potential spatial information loss. To tackle intent ambiguity, we designed a Prompt-Aware Decoder (PAD) that guides the decoding process by deriving task-driven signals from the interaction between the expression and visual features. Comprehensive experiments demonstrate that IPDN outperforms the state-ofthe-art by 1.9 and 4.2 points in mIoU metrics on the 3D-RES and 3D-GRES tasks, respectively.	 | 3D表达分割（3D-RES）旨在根据给定的表达对点云场景进行分割。然而，现有的3D-RES方法面临着两大主要挑战：特征歧义性和意图歧义性。特征歧义性源于在点云获取过程中由于光照和视角等限制导致的信息损失或失真。意图歧义性指的是模型在解码过程中对所有查询等同处理，缺乏自上而下的任务特定指导。在本文中，我们引入了一种图像增强提示解码网络（IPDN），该网络利用多视图图像和任务驱动信息来增强模型的推理能力。为了解决特征歧义性，我们提出了多视角语义嵌入（MSE）模块，该模块将多视角的2D图像信息注入3D场景，并补偿潜在的空间信息损失。为解决意图歧义性，我们设计了一种提示感知解码器（PAD），它通过表达与视觉特征的交互中提取任务驱动信号来引导解码过程。全面的实验表明，IPDN在3D-RES和3D-GRES任务上的mIoU指标中分别比最先进的方法高出1.9和4.2个百分点。
2501.04975	 | V2C-CBM: Building Concept Bottlenecks with Vision-to-Concept Tokenizer	 | Hangzhou He,Lei Zhu,Xinliang Zhang,Shuang Zeng,Qian Chen,Yanye Lu	 | 该研究提出了一种名为V2C-CBM的新概念瓶颈模型，通过直接从多模态模型构建概念瓶颈，利用常用词作为基础概念词汇表，并构建一个视觉到概念的分词器，从而提高模型的训练效率和准确性。V2C-CBM在多种视觉分类任务中表现出与大型语言模型监督的CBMs相当或更优的效果，证明了该方法的有效性。	 | Concept Bottleneck Models (CBMs) offer inherent interpretability by initially translating images into human-comprehensible concepts, followed by a linear combination of these concepts for classification. However, the annotation of concepts for visual recognition tasks requires extensive expert knowledge and labor, constraining the broad adoption of CBMs. Recent approaches have leveraged the knowledge of large language models to construct concept bottlenecks, with multimodal models like CLIP subsequently mapping image features into the concept feature space for classification. Despite this, the concepts produced by language models can be verbose and may introduce non-visual attributes, which hurts accuracy and interpretability. In this study, we investigate to avoid these issues by constructing CBMs directly from multimodal models. To this end, we adopt common words as base concept vocabulary and leverage auxiliary unlabeled images to construct a Vision-to-Concept (V2C) tokenizer that can explicitly quantize images into their most relevant visual concepts, thus creating a vision-oriented concept bottleneck tightly coupled with the multimodal model. This leads to our V2C-CBM which is training efficient and interpretable with high accuracy. Our V2C-CBM has matched or outperformed LLM-supervised CBMs on various visual classification benchmarks, validating the efficacy of our approach.	 | 概念瓶颈模型（CBMs）通过首先将图像转换为人类可理解的概念，然后再对这些概念进行线性组合来进行分类，从而提供内在的可解释性。然而，对视觉识别任务的概念进行注释需要大量专家知识和劳动，限制了CBMs的广泛应用。最近的方法利用了大型语言模型的知识来构建概念瓶颈，随后使用多模态模型如CLIP将图像特征映射到概念特征空间进行分类。尽管如此，语言模型生成的概念可能冗长，并且可能引入非视觉属性，这影响了准确性和可解释性。在本研究中，我们通过直接从多模态模型构建CBMs来避免这些问题。为此，我们采用常用词作为基础概念词汇表，并利用辅助的未标注图像构建一个视觉到概念（V2C）分词器，可以显式地将图像量化为其最相关的视觉概念，从而创建一个紧密结合多模态模型的视觉导向的概念瓶颈。这导致了我们的V2C-CBM，该模型在训练效率和准确性方面均具有很高的可解释性。我们的V2C-CBM在各种视觉分类基准测试中达到了与大型语言模型监督的CBMs相当或更优的效果，验证了我们方法的有效性。
2501.04966	 | Emergence of Painting Ability via Recognition-Driven Evolution	 | Yi Lin,Lin Gu,Ziteng Cui,Shenghan Su,Yumo Hao,Yingtao Tian,Tatsuya Harada,Jianfei Yang	 | 该研究通过模拟提升视觉交流效率的进化压力，提出了一种包含笔触分支和调色板分支的模型，以优化人类绘画的艺术表达和识别效率，实验结果显示该模型在高层识别任务中表现出色，并且具有作为高效位级图像压缩技术的潜力。	 | From Paleolithic cave paintings to Impressionism, human painting has evolved to depict increasingly complex and detailed scenes, conveying more nuanced messages. This paper attempts to emerge this artistic capability by simulating the evolutionary pressures that enhance visual communication efficiency. Specifically, we present a model with a stroke branch and a palette branch that together simulate human-like painting. The palette branch learns a limited colour palette, while the stroke branch parameterises each stroke using Bézier curves to render an image, subsequently evaluated by a high-level recognition module. We quantify the efficiency of visual communication by measuring the recognition accuracy achieved with machine vision. The model then optimises the control points and colour choices for each stroke to maximise recognition accuracy with minimal strokes and colours. Experimental results show that our model achieves superior performance in high-level recognition tasks, delivering artistic expression and aesthetic appeal, especially in abstract sketches. Additionally, our approach shows promise as an efficient bit-level image compression technique, outperforming traditional methods.	 | 从旧石器时代的洞穴绘画到印象派，人类绘画经历了不断进化，以描绘更加复杂和详细的场景，并传达更加细腻的信息。本文试图通过模拟提升视觉交流效率的进化压力，来展现这种艺术能力。具体而言，我们提出了一种包含笔触分支和调色板分支的模型，共同模拟人类绘画的方式。调色板分支学习一个有限的颜色调色板，而笔触分支则使用贝塞尔曲线参数化每条笔触，从而绘制出图像，然后由高级识别模块进行评估。我们通过测量使用机器视觉实现的识别准确率来量化视觉交流的效率。该模型随后针对每条笔触优化其控制点和颜色选择，以实现使用最少的笔触和颜色来最大化识别准确率。实验结果表明，我们的模型在高层识别任务中表现出色，不仅能够传达艺术表达和审美吸引力，特别是在抽象草图中。此外，我们的方法还显示出作为高效位级图像压缩技术的潜力，优于传统方法。
2501.04958	 | Addressing Domain Shift via Imbalance-Aware Domain Adaptation in Embryo Development Assessment	 | Lei Li,Xinglin Zhang,Jun Liang,Tao Chen	 | 不平衡感知领域适应（IADA）是一种新型框架，通过自适应特征学习、动态加权的平衡领域对齐和自适应阈值优化来同时解决医学成像领域的领域迁移和类别不平衡问题，显著提高了模型的性能和泛化能力。	 | Deep learning models in medical imaging face dual challenges: domain shift, where models perform poorly when deployed in settings different from their training environment, and class imbalance, where certain disease conditions are naturally underrepresented. We present Imbalance-Aware Domain Adaptation (IADA), a novel framework that simultaneously tackles both challenges through three key components: (1) adaptive feature learning with class-specific attention mechanisms, (2) balanced domain alignment with dynamic weighting, and (3) adaptive threshold optimization. Our theoretical analysis establishes convergence guarantees and complexity bounds. Through extensive experiments on embryo development assessment across four imaging modalities, IADA demonstrates significant improvements over existing methods, achieving up to 25.19\% higher accuracy while maintaining balanced performance across classes. In challenging scenarios with low-quality imaging systems, IADA shows robust generalization with AUC improvements of up to 12.56\%. These results demonstrate IADA's potential for developing reliable and equitable medical imaging systems for diverse clinical settings. The code is made public available at \url{https://github.com/yinghemedical/imbalance-aware_domain_adaptation}	 | 深度学习模型在医学成像领域面临双重挑战：领域迁移问题，即模型在部署到与训练环境不同的环境时表现不佳；以及类别不平衡问题，其中某些疾病条件在自然状态下被过度代表。我们提出了不平衡感知领域适应（IADA），这是一种新型框架，通过三个关键组件同时解决这两个挑战：（1）具有类别特定注意力机制的自适应特征学习；（2）动态加权的平衡领域对齐；（3）自适应阈值优化。我们的理论分析建立了收敛保证和复杂性界。通过在四种成像模态下的胚胎发育评估的广泛实验，IADA在现有方法上表现出显著的改进，准确率最高提高了25.19%，同时在类别上保持了平衡性能。在低质量成像系统的挑战性场景中，IADA展示了稳健的一般化能力，AUC提高了最多12.56%。这些结果展示了IADA在为各种临床设置开发可靠且公平的医学成像系统方面的潜力。代码已公开发布，可在 \url{https://github.com/yinghemedical/imbalance-aware_domain_adaptation} 获取。
2501.04950	 | MORDA: A Synthetic Dataset to Facilitate Adaptation of Object Detectors to Unseen Real-target Domain While Preserving Performance on Real-source Domain	 | Hojun Lim,Heecheol Yoo,Jinwoo Lee,Seungmin Jeon,Hyeongseok Jeon	 | 本文提出了一种利用合成环境作为辅助域的方法，以减轻自动驾驶车辆在目标域数据不足的问题，成功构建了MORDA数据集，并在韩国收集的真实世界数据上验证了其在2D/3D检测器性能提升方面的有效性。	 | Deep neural network (DNN) based perception models are indispensable in the development of autonomous vehicles (AVs). However, their reliance on large-scale, high-quality data is broadly recognized as a burdensome necessity due to the substantial cost of data acquisition and labeling. Further, the issue is not a one-time concern, as AVs might need a new dataset if they are to be deployed to another region (real-target domain) that the in-hand dataset within the real-source domain cannot incorporate. To mitigate this burden, we propose leveraging synthetic environments as an auxiliary domain where the characteristics of real domains are reproduced. This approach could enable indirect experience about the real-target domain in a time- and cost-effective manner. As a practical demonstration of our methodology, nuScenes and South Korea are employed to represent real-source and real-target domains, respectively. That means we construct digital twins for several regions of South Korea, and the data-acquisition framework of nuScenes is reproduced. Blending the aforementioned components within a simulator allows us to obtain a synthetic-fusion domain in which we forge our novel driving dataset, MORDA: Mixture Of Real-domain characteristics for synthetic-data-assisted Domain Adaptation. To verify the value of synthetic features that MORDA provides in learning about driving environments of South Korea, 2D/3D detectors are trained solely on a combination of nuScenes and MORDA. Afterward, their performance is evaluated on the unforeseen real-world dataset (AI-Hub) collected in South Korea. Our experiments present that MORDA can significantly improve mean Average Precision (mAP) on AI-Hub dataset while that on nuScenes is retained or slightly enhanced.	 | 基于深度神经网络（DNN）的感知模型在自动驾驶车辆（AVs）的发展中不可或缺。然而，它们对大规模、高质量数据的依赖性广受认可，因为数据采集和标注的成本非常高。此外，这一问题并非一次性问题，因为如果自动驾驶车辆要部署到另一个地区（即目标域），现有数据可能无法涵盖该地区的情况，从而需要新的数据集。为减轻这一负担，我们提出了利用合成环境作为辅助域，以再现真实域的特征。这种做法可以在时间和成本上以间接方式为真实目标域提供经验。作为我们方法的实际演示，我们使用nuScenes和韩国分别代表真实源域和真实目标域。这意味着我们为韩国的几个地区构建了数字孪生，并复制了nuScenes的数据采集框架。在一个模拟器中结合上述组件，使我们能够获得一个合成融合域，在其中我们构建了我们的新型驾驶数据集MORDA：Mixture Of Real-domain characteristics for synthetic-data-assisted Domain Adaptation。为了验证MORDA提供的合成特征在学习韩国驾驶环境方面的重要性，我们仅使用nuScenes和MORDA的组合对2D/3D检测器进行训练。之后，我们将其性能评估在韩国收集的未预见真实世界数据集（AI-Hub）上。我们的实验结果显示，MORDA可以显著提高AI-Hub数据集上的平均精度均值（mAP），而nuScenes上的性能保持不变或略有提升。
2501.04947	 | Seeing with Partial Certainty: Conformal Prediction for Robotic Scene Recognition in Built Environments	 | Yifan Xu,Vineet Kamat,Carol Menassa	 | 本文提出了一种名为“部分确定性下的视觉识别”（SwPC）的框架，旨在通过一种轻量级的方法测量和校准基于视觉语言模型的地点识别中的不确定性，并在必要时寻求帮助，从而提高残障辅助机器人在室内环境中的导航和互动能力。实验表明，SwPC在准确率和所需人类干预量方面显著优于现有方法。	 | In assistive robotics serving people with disabilities (PWD), accurate place recognition in built environments is crucial to ensure that robots navigate and interact safely within diverse indoor spaces. Language interfaces, particularly those powered by Large Language Models (LLM) and Vision Language Models (VLM), hold significant promise in this context, as they can interpret visual scenes and correlate them with semantic information. However, such interfaces are also known for their hallucinated predictions. In addition, language instructions provided by humans can also be ambiguous and lack precise details about specific locations, objects, or actions, exacerbating the hallucination issue. In this work, we introduce Seeing with Partial Certainty (SwPC) - a framework designed to measure and align uncertainty in VLM-based place recognition, enabling the model to recognize when it lacks confidence and seek assistance when necessary. This framework is built on the theory of conformal prediction to provide statistical guarantees on place recognition while minimizing requests for human help in complex indoor environment settings. Through experiments on the widely used richly-annotated scene dataset Matterport3D, we show that SwPC significantly increases the success rate and decreases the amount of human intervention required relative to the prior art. SwPC can be utilized with any VLMs directly without requiring model fine-tuning, offering a promising, lightweight approach to uncertainty modeling that complements and scales alongside the expanding capabilities of foundational models.	 | 在为残疾人提供辅助的机器人技术中，准确的地点识别对于确保机器人能够在多样的室内环境中安全导航和互动至关重要。特别是由大规模语言模型（LLM）和视觉语言模型（VLM）驱动的语言接口，在这一领域具有巨大潜力，因为它们能够解释视觉场景并将其与语义信息相关联。然而，这些接口也常常会出现幻觉性预测。此外，人类提供的语言指令也可能模糊不清，缺乏对具体位置、物体或动作的精确细节描述，进一步加剧了幻觉性预测的问题。在本文中，我们引入了“部分确定性下的视觉识别”（SwPC）框架——一种旨在测量和校准基于VLM的地点识别中的不确定性，在必要时寻求帮助的框架。该框架基于一致预测理论，提供了在复杂室内环境设置中地点识别的统计保证，同时尽量减少对人类帮助的需求。通过在广泛使用的详细注释场景数据集Matterport3D上进行实验，我们证明了SwPC在成功率达到和所需人类干预量方面显著优于先前的方法。SwPC可以与任何VLM直接结合使用，无需进行模型微调，提供了一种轻量级且具有前景的不确定性建模方法，能够与基础模型不断扩大的能力相补充和扩展。
2501.04944	 | MambaHSI: Spatial-Spectral Mamba for Hyperspectral Image Classification	 | Yapeng Li,Yong Luo,Lefei Zhang,Zengmao Wang,Bo Du	 | MambaHSI 是一种基于 Mamba 模型的新高光谱图像分类方法，通过引入像素级的空域 Mamba 块、光谱 Mamba 块和空域-光谱融合模块，实现了长距离交互建模和空域-光谱信息的有效整合，从而在多个高光谱图像数据集上展示了优越的分类性能。	 | Transformer has been extensively explored for hyperspectral image (HSI) classification. However, transformer poses challenges in terms of speed and memory usage because of its quadratic computational complexity. Recently, the Mamba model has emerged as a promising approach, which has strong long-distance modeling capabilities while maintaining a linear computational complexity. However, representing the HSI is challenging for the Mamba due to the requirement for an integrated spatial and spectral understanding. To remedy these drawbacks, we propose a novel HSI classification model based on a Mamba model, named MambaHSI, which can simultaneously model long-range interaction of the whole image and integrate spatial and spectral information in an adaptive manner. Specifically, we design a spatial Mamba block (SpaMB) to model the long-range interaction of the whole image at the pixel-level. Then, we propose a spectral Mamba block (SpeMB) to split the spectral vector into multiple groups, mine the relations across different spectral groups, and extract spectral features. Finally, we propose a spatial-spectral fusion module (SSFM) to adaptively integrate spatial and spectral features of a HSI. To our best knowledge, this is the first image-level HSI classification model based on the Mamba. We conduct extensive experiments on four diverse HSI datasets. The results demonstrate the effectiveness and superiority of the proposed model for HSI classification. This reveals the great potential of Mamba to be the next-generation backbone for HSI models. Codes are available at https://github.com/li-yapeng/MambaHSI .	 | Transformer 在高光谱图像（HSI）分类领域得到了广泛的研究。然而，Transformer 因其计算复杂性呈平方级增长而面临着速度和内存使用方面的挑战。最近，Mamba模型作为一种有前景的方法出现，它具有强大的长距离建模能力，并且保持了线性计算复杂性。然而，Mamba在表示HSI时面临着挑战，因为需要一种整合空域和光谱理解的方式。为了弥补这些不足，我们提出了一种基于Mamba模型的新HSI分类模型MambaHSI，该模型可以同时建模整个图像的长距离交互，并以自适应的方式整合空域和光谱信息。具体而言，我们设计了一个像素级的空域Mamba块（SpaMB）来在像素级建模整个图像的长距离交互。然后，我们提出了一个光谱Mamba块（SpeMB），将其光谱矢量拆分为多个组，挖掘不同光谱组之间的关系，并提取光谱特征。最后，我们提出了一个空域-光谱融合模块（SSFM），以自适应地整合HSI的空域和光谱特征。据我们所知，这是第一个基于Mamba的图像级HSI分类模型。我们在四个不同的HSI数据集上进行了广泛的实验。结果表明，所提出模型在HSI分类上的有效性和优越性。这揭示了Mamba在HSI模型下一代骨干网络的巨大潜力。代码可在https://github.com/li-yapeng/MambaHSI 获取。
2501.04939	 | Multi-Context Temporal Consistent Modeling for Referring Video Object Segmentation	 | Sun-Hyuk Choi,Hayoung Jo,Seong-Whan Lee	 | 该研究针对视频对象分割中的查询不一致和对上下文考虑有限的问题，提出了一种多上下文时序一致性模块（MTCM），该模块包括对齐器和多上下文增强器，以提高查询一致性和上下文理解能力，从而改善分割性能；实验结果显示MTCM在多种模型上都提升了性能，特别是在MeViS上取得了47.6的J&F分数。	 | Referring video object segmentation aims to segment objects within a video corresponding to a given text description. Existing transformer-based temporal modeling approaches face challenges related to query inconsistency and the limited consideration of context. Query inconsistency produces unstable masks of different objects in the middle of the video. The limited consideration of context leads to the segmentation of incorrect objects by failing to adequately account for the relationship between the given text and instances. To address these issues, we propose the Multi-context Temporal Consistency Module (MTCM), which consists of an Aligner and a Multi-Context Enhancer (MCE). The Aligner removes noise from queries and aligns them to achieve query consistency. The MCE predicts text-relevant queries by considering multi-context. We applied MTCM to four different models, increasing performance across all of them, particularly achieving 47.6 J&F on the MeViS. Code is available at https://github.com/Choi58/MTCM.	 | 参考视频对象分割的目标是根据给定的文本描述对视频中的相应对象进行分割。现有的基于Transformer的时序建模方法面临查询不一致和对上下文考虑有限的问题。查询不一致会在视频中间产生不同对象的不稳定掩码。对上下文考虑有限会导致系统因未能充分考虑给定文本与实例之间的关系而将错误的对象进行分割。为了解决这些问题，我们提出了多上下文时序一致性模块（MTCM），该模块包括一个对齐器（Aligner）和一个多上下文增强器（MCE）。对齐器去除查询中的噪声并对其进行对齐，以实现查询一致性。MCE通过考虑多上下文来预测与文本相关查询。我们将MTCM应用到四种不同的模型中，在所有模型中都提高了性能，特别是在MeViS上达到了47.6的J&F分数。代码可在https://github.com/Choi58/MTCM获取。
2501.04934	 | Plug-and-Play DISep: Separating Dense Instances for Scene-to-Pixel Weakly-Supervised Change Detection in High-Resolution Remote Sensing Images	 | Zhenghui Zhao,Chen Wu,Lixiang Ru,Di Wang,Hongruixuan Chen,Cuiqun Chen	 | 该研究提出了一种名为DISep的即插即用密集实例分离方法，旨在解决现有弱监督变化检测方法中常见的“实例合并”问题，通过在场景级监督下细化像素特征，实现变化实例的精确分离，从而提高变化检测的准确性。该方法在多个变化检测数据集上展示了优越性能，且训练和推理成本较低，能够兼容现有的变化检测框架。	 | Existing Weakly-Supervised Change Detection (WSCD) methods often encounter the problem of "instance lumping" under scene-level supervision, particularly in scenarios with a dense distribution of changed instances (i.e., changed objects). In these scenarios, unchanged pixels between changed instances are also mistakenly identified as changed, causing multiple changes to be mistakenly viewed as one. In practical applications, this issue prevents the accurate quantification of the number of changes. To address this issue, we propose a Dense Instance Separation (DISep) method as a plug-and-play solution, refining pixel features from a unified instance perspective under scene-level supervision. Specifically, our DISep comprises a three-step iterative training process: 1) Instance Localization: We locate instance candidate regions for changed pixels using high-pass class activation maps. 2) Instance Retrieval: We identify and group these changed pixels into different instance IDs through connectivity searching. Then, based on the assigned instance IDs, we extract corresponding pixel-level features on a per-instance basis. 3) Instance Separation: We introduce a separation loss to enforce intra-instance pixel consistency in the embedding space, thereby ensuring separable instance feature representations. The proposed DISep adds only minimal training cost and no inference cost. It can be seamlessly integrated to enhance existing WSCD methods. We achieve state-of-the-art performance by enhancing {three Transformer-based and four ConvNet-based methods} on the LEVIR-CD, WHU-CD, DSIFN-CD, SYSU-CD, and CDD datasets. Additionally, our DISep can be used to improve fully-supervised change detection methods. Code is available at https://github.com/zhenghuizhao/Plug-and-Play-DISep-for-Change-Detection.	 | 现有弱监督变化检测（WSCD）方法在场景级监督下常遇到“实例合并”问题，特别是在变化实例（即变化对象）密集分布的场景中尤为明显。在这些场景中，未变化像素也被错误地识别为变化像素，导致多个变化被错误地视为一个。在实际应用中，这一问题阻碍了变化数量的精确量化。为解决这一问题，我们提出了一个即插即用的密集实例分离（DISep）方法，该方法在场景级监督下从统一实例视角细化像素特征。具体而言，我们的DISep包含一个三步迭代训练过程：1) 实例定位：我们使用高通类激活图来定位变化像素的实例候选区域。2) 实例检索：我们通过连通性搜索识别并分组这些变化像素到不同的实例ID上，基于分配的实例ID，我们逐实例提取对应的像素级特征。3) 实例分离：我们引入分离损失来强制实例内像素在嵌入空间中的一致性，从而确保实例特征的可分性。所提出的DISep仅增加了少量的训练成本，无额外的推理成本，可以无缝集成到现有的WSCD方法中，从而提升其性能。我们在LEVIR-CD、WHU-CD、DSIFN-CD、SYSU-CD和CDD数据集上通过增强三种基于Transformer的方法和四种基于ConvNet的方法，获得了最先进的性能。此外，我们的DISep还可以用于改进全监督变化检测方法。代码可在https://github.com/zhenghuizhao/Plug-and-Play-DISep-for-Change-Detection获取。
2501.04928	 | Image2CADSeq: Computer-Aided Design Sequence and Knowledge Inference from Product Images	 | Xingang Li,Zhenghui Sha	 | 该研究提出了一种名为Image2CADSeq的神经网络模型，用于将2D图像转换为CAD序列，从而重建3D CAD模型，该方法能够提供对模型构建过程的更深入理解。通过多层次的评估框架和优化的网络架构，实验结果表明该模型在预测性能上表现出色，展示了其在从2D图像生成CAD序列方面的巨大潜力。	 | Computer-aided design (CAD) tools empower designers to design and modify 3D models through a series of CAD operations, commonly referred to as a CAD sequence. In scenarios where digital CAD files are not accessible, reverse engineering (RE) has been used to reconstruct 3D CAD models. Recent advances have seen the rise of data-driven approaches for RE, with a primary focus on converting 3D data, such as point clouds, into 3D models in boundary representation (B-rep) format. However, obtaining 3D data poses significant challenges, and B-rep models do not reveal knowledge about the 3D modeling process of designs. To this end, our research introduces a novel data-driven approach with an Image2CADSeq neural network model. This model aims to reverse engineer CAD models by processing images as input and generating CAD sequences. These sequences can then be translated into B-rep models using a solid modeling kernel. Unlike B-rep models, CAD sequences offer enhanced flexibility to modify individual steps of model creation, providing a deeper understanding of the construction process of CAD models. To quantitatively and rigorously evaluate the predictive performance of the Image2CADSeq model, we have developed a multi-level evaluation framework for model assessment. The model was trained on a specially synthesized dataset, and various network architectures were explored to optimize the performance. The experimental and validation results show great potential for the model in generating CAD sequences from 2D image data.	 | 计算机辅助设计（CAD）工具使设计师能够通过一系列CAD操作设计和修改3D模型，这些操作通常被称为CAD序列。在无法访问数字CAD文件的情况下，逆向工程（RE）已被用于重建3D CAD模型。最近的进步看到了基于数据的方法在逆向工程中的兴起，主要集中在将3D数据，如点云，转换为边界表示（B-rep）格式的3D模型。然而，获取3D数据存在巨大挑战，且B-rep模型无法揭示设计的3D建模过程知识。为此，我们的研究引入了一种新颖的数据驱动方法，即使用Image2CADSeq神经网络模型。该模型旨在通过处理图像作为输入并生成CAD序列来进行逆向工程。这些序列随后可以借助实体建模内核转换为B-rep模型。与B-rep模型不同，CAD序列提供了修改模型创建步骤中的灵活性，从而提供对CAD模型构建过程的更深入理解。为了定量和严格地评估Image2CADSeq模型的预测性能，我们开发了一个多层次的评估框架来评估模型。该模型在特别合成的数据集上进行了训练，并探索了各种网络架构以优化性能。实验和验证结果表明，该模型在从2D图像数据生成CAD序列方面具有巨大潜力。
2501.04914	 | From Mesh Completion to AI Designed Crown	 | Golriz Hosseinimanesh,Farnoosh Ghadiri,Francois Guibault,Farida Cheriet,Julia Keren	 | 研究人员提出了一种端到端的深度学习方法——Dental Mesh Completion (DMC)，用于根据点云上下文自动生成牙冠网格，简化牙冠设计流程并提高准确性，实验结果表明该方法在Chamfer距离上表现出色，平均值为0.062。	 | Designing a dental crown is a time-consuming and labor intensive process. Our goal is to simplify crown design and minimize the tediousness of making manual adjustments while still ensuring the highest level of accuracy and consistency. To this end, we present a new end- to-end deep learning approach, coined Dental Mesh Completion (DMC), to generate a crown mesh conditioned on a point cloud context. The dental context includes the tooth prepared to receive a crown and its surroundings, namely the two adjacent teeth and the three closest teeth in the opposing jaw. We formulate crown generation in terms of completing this point cloud context. A feature extractor first converts the input point cloud into a set of feature vectors that represent local regions in the point cloud. The set of feature vectors is then fed into a transformer to predict a new set of feature vectors for the missing region (crown). Subsequently, a point reconstruction head, followed by a multi-layer perceptron, is used to predict a dense set of points with normals. Finally, a differentiable point-to-mesh layer serves to reconstruct the crown surface mesh. We compare our DMC method to a graph-based convolutional neural network which learns to deform a crown mesh from a generic crown shape to the target geometry. Extensive experiments on our dataset demonstrate the effectiveness of our method, which attains an average of 0.062 Chamfer Distance.The code is available at:https://github.com/Golriz-code/DMC.gi	 | 设计牙冠是一项耗时且劳动力密集的工作。我们的目标是简化牙冠设计流程，减少手动调整的繁琐性，同时仍能确保最高水平的准确性和一致性。为此，我们提出了一种新的端到端深度学习方法，称为牙冠网格完成（Dental Mesh Completion，DMC），以根据点云上下文生成牙冠网格。牙冠的上下文包括准备接受牙冠的牙齿及其周围的牙齿，即相邻的两颗牙齿和对面下颌中最接近的三颗牙齿。我们将牙冠生成问题表述为完成这个点云上下文。首先，特征提取器将输入的点云转换为一组特征向量，这些特征向量表示点云中的局部区域。然后，这组特征向量被输入到变压器中，以预测缺失区域（即牙冠）的新特征向量。接着，使用一个点重建头和多层感知机来预测密集的带法线的点集。最后，一个可微分的点到网格层用于重建牙冠表面网格。我们将我们的DMC方法与一种基于图的卷积神经网络进行比较，该网络从通用牙冠形状学习变形以匹配目标几何形状。在我们数据集上的广泛实验表明，我们的方法非常有效，平均Chamfer距离为0.062。代码可在以下地址获取：https://github.com/Golriz-code/DMC.gi
2501.04911	 | A Machine Learning Model for Crowd Density Classification in Hajj Video Frames	 | Afnan A.Shah	 | 本文提出了一种基于机器学习的模型，该模型通过分析朝圣期间拍摄的视频帧来检测和分类不同的人群密度，并在发现高风险的非常密集人群时发出警报，以提高公共安全；该模型在多种关键朝圣地点的视频数据上测试，显示出较高的准确性和较低的误分类率，有助于有效管理朝觐期间的人群。	 | Managing the massive annual gatherings of Hajj and Umrah presents significant challenges, particularly as the Saudi government aims to increase the number of pilgrims. Currently, around two million pilgrims attend Hajj and 26 million attend Umrah making crowd control especially in critical areas like the Grand Mosque during Tawaf, a major concern. Additional risks arise in managing dense crowds at key sites such as Arafat where the potential for stampedes, fires and pandemics poses serious threats to public safety. This research proposes a machine learning model to classify crowd density into three levels: moderate crowd, overcrowded and very dense crowd in video frames recorded during Hajj, with a flashing red light to alert organizers in real-time when a very dense crowd is detected. While current research efforts in processing Hajj surveillance videos focus solely on using CNN to detect abnormal behaviors, this research focuses more on high-risk crowds that can lead to disasters. Hazardous crowd conditions require a robust method, as incorrect classification could trigger unnecessary alerts and government intervention, while failure to classify could result in disaster. The proposed model integrates Local Binary Pattern (LBP) texture analysis, which enhances feature extraction for differentiating crowd density levels, along with edge density and area-based features. The model was tested on the KAU-Smart Crowd 'HAJJv2' dataset which contains 18 videos from various key locations during Hajj including 'Massaa', 'Jamarat', 'Arafat' and 'Tawaf'. The model achieved an accuracy rate of 87% with a 2.14% error percentage (misclassification rate), demonstrating its ability to detect and classify various crowd conditions effectively. That contributes to enhanced crowd management and safety during large-scale events like Hajj.	 | 管理朝圣和朝觐巨大的年度集会带来了显著的挑战，尤其是在沙特政府希望增加朝圣人数的情况下。目前，大约有200万朝圣者参加朝觐，2600万人参加朝觐，特别是在禁寺（麦加大清真寺）的朝拜巡礼（Tawaf）区域等关键区域，人群控制成为特别关注的问题。在关键地点如阿拉法特（Arafat）等人流密集区域，踩踏、火灾和疫情等潜在风险对公共安全构成了严重威胁。本文提出了一种机器学习模型，该模型在朝圣期间拍摄的视频帧中将人群密度分为三种等级：中等人群、过密人群和非常密集人群，并在检测到非常密集人群时发出红色闪光警报，以实时提醒组织者。目前针对朝圣监控视频的研究主要集中在使用CNN检测异常行为，而本文则更侧重于高风险人群，这些人群可能导致灾难。危险的人群状况需要一种稳健的方法，因为错误分类可能导致不必要的警报和政府干预，而未能分类可能导致灾难。该模型结合了局部二值模式（LBP）纹理分析，增强了不同人群密度级别的特征提取，并结合边缘密度和基于区域的特征。该模型在KAU-Smart Crowd 'HAJJv2'数据集上进行了测试，该数据集包含18段来自朝圣期间多个关键地点的视频，包括“麦加大清真寺”、“石踐礼”、“阿拉法特”和“朝拜巡礼”。模型的准确率为87%，误分类率为2.14%，证明了其能够有效检测和分类各种人群状况的能力。这有助于提高在朝觐等大规模活动中的人群管理和安全性。
2501.04878	 | Topological Classification of points in $Z^2$ by using Topological Numbers for $2$D discrete binary images	 | Christophe Lohou	 | 本文提出了二维离散二值图像中点的六种拓扑分类，包括孤立点、内部点、简单点、曲线点以及不同数量曲线相交的点，并给出了每类点的配置数量。	 | In this paper, we propose a topological classification of points for 2D discrete binary images. This classification is based on the values of the calculus of topological numbers. Six classes of points are proposed: isolated point, interior point, simple point, curve point, point of intersection of 3 curves, point of intersection of 4 curves. The number of configurations of each class is also given.	 | 在本文中，我们提出了二维离散二值图像点的拓扑分类。这种分类基于拓扑数算子的值。提出了六类点：孤立点、内部点、简单点、曲线点、三条曲线相交的点、四条曲线相交的点。每类点的配置数量也一并给出。
2501.04873	 | Back Home: A Machine Learning Approach to Seashell Classification and Ecosystem Restoration	 | Alexander Valverde,Luis Solano	 | 该研究开发了一种卷积神经网络（CNN）来识别来自哥斯达黎加太平洋和加勒比海岸的贝壳，并将其集成到一个用户友好的应用程序中，该程序已成功分类了超过36,000个贝壳，准确率超过85%，并能实时返回结果。	 | In Costa Rica, an average of 5 tons of seashells are extracted from ecosystems annually. Confiscated seashells, cannot be returned to their ecosystems due to the lack of origin recognition. To address this issue, we developed a convolutional neural network (CNN) specifically for seashell identification. We built a dataset from scratch, consisting of approximately 19000 images from the Pacific and Caribbean coasts. Using this dataset, the model achieved a classification accuracy exceeding 85%. The model has been integrated into a user-friendly application, which has classified over 36,000 seashells to date, delivering real-time results within 3 seconds per image. To further enhance the system's accuracy, an anomaly detection mechanism was incorporated to filter out irrelevant or anomalous inputs, ensuring only valid seashell images are processed.	 | 在哥斯达黎加，每年从生态系统中平均开采约5吨的贝壳。由于无法识别这些贝壳的来源，被扣押的贝壳无法被返回到它们的生态系统中。为解决这一问题，我们开发了一种专门用于贝壳识别的卷积神经网络（CNN）。我们从头开始构建了一个数据集，包含约19000张来自太平洋和加勒比海岸的贝壳图像。利用这个数据集，模型的分类准确率超过了85%。我们还将该模型集成到了一个用户友好的应用程序中，该应用程序至今已分类了超过36,000个贝壳，并能在每张图像上实时返回结果，时间不到3秒。为了进一步提高系统的准确性，我们还引入了一种异常检测机制，以过滤掉无关或异常的输入，确保仅处理有效的贝壳图像。
2501.04861	 | LayerMix: Enhanced Data Augmentation through Fractal Integration for Robust Deep Learning	 | Hafiz Mughees Ahmad,Dario Morle,Afshin Rahimi	 | LayerMix 是一种创新的数据增强方法，通过结构化分形图像合成增强模型的鲁棒性，显著提高了神经网络在多种基准数据集上的分类准确性和安全性指标。该方法在多个视觉任务中表现出色，提升了对自然损毁、对抗性攻击和模型校准等方面的抗性。	 | Deep learning models have demonstrated remarkable performance across various computer vision tasks, yet their vulnerability to distribution shifts remains a critical challenge. Despite sophisticated neural network architectures, existing models often struggle to maintain consistent performance when confronted with Out-of-Distribution (OOD) samples, including natural corruptions, adversarial perturbations, and anomalous patterns. We introduce LayerMix, an innovative data augmentation approach that systematically enhances model robustness through structured fractal-based image synthesis. By meticulously integrating structural complexity into training datasets, our method generates semantically consistent synthetic samples that significantly improve neural network generalization capabilities. Unlike traditional augmentation techniques that rely on random transformations, LayerMix employs a structured mixing pipeline that preserves original image semantics while introducing controlled variability. Extensive experiments across multiple benchmark datasets, including CIFAR-10, CIFAR-100, ImageNet-200, and ImageNet-1K demonstrate LayerMixs superior performance in classification accuracy and substantially enhances critical Machine Learning (ML) safety metrics, including resilience to natural image corruptions, robustness against adversarial attacks, improved model calibration and enhanced prediction consistency. LayerMix represents a significant advancement toward developing more reliable and adaptable artificial intelligence systems by addressing the fundamental challenges of deep learning generalization. The code is available at https://github.com/ahmadmughees/layermix.	 | 深度学习模型在各种计算机视觉任务中展现了卓越的性能，但它们对分布变化的脆弱性仍然是一个关键挑战。尽管神经网络架构日趋复杂，现有的模型在面对分布外（OOD）样本时，如自然损毁、对抗性扰动和异常模式时，往往难以保持一致的性能。我们提出了LayerMix，一种创新的数据增强方法，通过结构化的分形图像合成系统地增强模型的鲁棒性。通过在训练数据集中精心整合结构复杂性，我们的方法生成了语义一致的合成样本，显著提高了神经网络的泛化能力。不同于依赖随机变换的传统增强技术，LayerMix 使用了一种结构化的混合流水线，能够保留原始图像的语义同时引入可控的变异。在CIFAR-10、CIFAR-100、ImageNet-200 和 ImageNet-1K 等多个基准数据集上的广泛实验表明，LayerMix 在分类准确性和增强关键机器学习（ML）安全指标方面表现出色，包括提高了对自然图像损毁的抗性、对抗性攻击的鲁棒性、模型校准和预测一致性。LayerMix 代表了向开发更可靠和适应性强的人工智能系统迈进的重要一步，通过解决深度学习泛化的基本挑战。相关代码可在 https://github.com/ahmadmughees/layermix 获取。
2501.04846	 | EDMB: Edge Detector with Mamba	 | Yachuan Li,Xavier Soria Poma,Yun Bai,Qian Xiao,Chaozhi Yang,Guanlin Li,Zongmin Li	 | 本文提出了一种新型边缘检测器EDMB，结合了Mamba模型和全局-局部架构，能够高效生成高质量的多粒度边缘。在BSDS500数据集上，EDMB在单粒度和多粒度ODS上分别取得了0.837和0.851的优异性能，且无需额外数据支持或多尺度测试。	 | Transformer-based models have made significant progress in edge detection, but their high computational cost is prohibitive. Recently, vision Mamba have shown excellent ability in efficiently capturing long-range dependencies. Drawing inspiration from this, we propose a novel edge detector with Mamba, termed EDMB, to efficiently generate high-quality multi-granularity edges. In EDMB, Mamba is combined with a global-local architecture, therefore it can focus on both global information and fine-grained cues. The fine-grained cues play a crucial role in edge detection, but are usually ignored by ordinary Mamba. We design a novel decoder to construct learnable Gaussian distributions by fusing global features and fine-grained features. And the multi-grained edges are generated by sampling from the distributions. In order to make multi-granularity edges applicable to single-label data, we introduce Evidence Lower Bound loss to supervise the learning of the distributions. On the multi-label dataset BSDS500, our proposed EDMB achieves competitive single-granularity ODS 0.837 and multi-granularity ODS 0.851 without multi-scale test or extra PASCAL-VOC data. Remarkably, EDMB can be extended to single-label datasets such as NYUDv2 and BIPED. The source code is available at https://github.com/Li-yachuan/EDMB.	 | 基于变压器的模型在边缘检测方面取得了显著进展，但其高昂的计算成本使其难以承受。最近，Vision Mamba展示了在有效捕捉长距离依赖方面出色的能力。受到这一启发，我们提出了一种新型边缘检测器，称为EDMB，能够高效生成高质量的多粒度边缘。在EDMB中，Mamba与全局-局部架构结合，从而可以在关注全局信息的同时，捕捉到精细的线索。精细的线索在边缘检测中起着至关重要的作用，但在普通的Mamba中通常会被忽略。我们设计了一种新型解码器，通过融合全局特征和精细特征来构建可学习的高斯分布，并通过从这些分布中采样生成多粒度边缘。为了使多粒度边缘适用于单标签数据，我们引入了证据下界损失来监督分布的学习。在多标签数据集BSDS500上，我们提出的EDMB在单粒度ODS上达到了0.837，在多粒度ODS上达到了0.851，无需进行多尺度测试或额外的PASCAL-VOC数据支持。值得注意的是，EDMB可以扩展到如NYUDv2和BIPED等单标签数据集。源代码可在https://github.com/Li-yachuan/EDMB获取。
2501.04815	 | Towards Generalizable Trajectory Prediction Using Dual-Level Representation Learning And Adaptive Prompting	 | Kaouther Messaoud,Matthieu Cord,Alexandre Alahi	 | PerReg+ 是一种新的轨迹预测框架，通过引入自我蒸馏和掩码重构实现双层表示学习，增强多模态性并通过注册查询优化泛化能力；在多个数据集上达到了最佳性能，并且预训练模型在泛化能力和跨领域测试中表现出显著提升。	 | Existing vehicle trajectory prediction models struggle with generalizability, prediction uncertainties, and handling complex interactions. It is often due to limitations like complex architectures customized for a specific dataset and inefficient multimodal handling. We propose Perceiver with Register queries (PerReg+), a novel trajectory prediction framework that introduces: (1) Dual-Level Representation Learning via Self-Distillation (SD) and Masked Reconstruction (MR), capturing global context and fine-grained details. Additionally, our approach of reconstructing segmentlevel trajectories and lane segments from masked inputs with query drop, enables effective use of contextual information and improves generalization; (2) Enhanced Multimodality using register-based queries and pretraining, eliminating the need for clustering and suppression; and (3) Adaptive Prompt Tuning during fine-tuning, freezing the main architecture and optimizing a small number of prompts for efficient adaptation. PerReg+ sets a new state-of-the-art performance on nuScenes [1], Argoverse 2 [2], and Waymo Open Motion Dataset (WOMD) [3]. Remarkable, our pretrained model reduces the error by 6.8% on smaller datasets, and multi-dataset training enhances generalization. In cross-domain tests, PerReg+ reduces B-FDE by 11.8% compared to its non-pretrained variant.	 | 现有的车辆轨迹预测模型在泛化能力、预测不确定性以及处理复杂交互方面存在困难。这通常归因于针对特定数据集定制的复杂架构以及不高效的多模态处理。我们提出了Perceiver with Register queries (PerReg+)，一种新颖的轨迹预测框架，引入了以下内容：(1) 通过自我蒸馏（Self-Distillation, SD）和掩码重构（Masked Reconstruction, MR）实现双层表示学习，捕捉全局上下文和细粒度细节。此外，我们通过从掩码输入中重建段级轨迹和车道段来构建查询剔除的方法，有效利用上下文信息并提高泛化能力；(2) 基于注册查询的增强多模态性，并通过预训练消除聚类和抑制的需要；以及(3) 在微调期间进行自适应提示调整，冻结主要架构并优化少量提示以实现高效的适应。PerReg+ 在 nuScenes [1]、Argoverse 2 [2] 和 Waymo Open Motion Dataset (WOMD) [3] 上达到了新的最佳性能。值得注意的是，我们预训练的模型在较小的数据集上将误差降低了6.8%，而跨数据集训练增强了泛化能力。在跨领域测试中，PerReg+ 相比于其无预训练的变体将B-FDE降低了11.8%。
2501.04784	 | Leveraging Registers in Vision Transformers for Robust Adaptation	 | Srikar Yellapragada,Kowshik Thopalli,Vivek Narayanaswamy,Wesam Sakla,Yang Liu,Yamen Mubarka,Dimitris Samaras,Jayaraman J. Thiagarajan	 | 本文提出了一种将特殊CLS令牌嵌入与寄存器平均池化嵌入结合的方法，以提高视觉变换器在分布外场景下的泛化能力和异常检测性能，实验结果表明该方法在多个ViT骨干网络上显著提升了OOD准确率和降低了假阳率，且未增加额外计算开销。	 | Vision Transformers (ViTs) have shown success across a variety of tasks due to their ability to capture global image representations. Recent studies have identified the existence of high-norm tokens in ViTs, which can interfere with unsupervised object discovery. To address this, the use of "registers" which are additional tokens that isolate high norm patch tokens while capturing global image-level information has been proposed. While registers have been studied extensively for object discovery, their generalization properties particularly in out-of-distribution (OOD) scenarios, remains underexplored. In this paper, we examine the utility of register token embeddings in providing additional features for improving generalization and anomaly rejection. To that end, we propose a simple method that combines the special CLS token embedding commonly employed in ViTs with the average-pooled register embeddings to create feature representations which are subsequently used for training a downstream classifier. We find that this enhances OOD generalization and anomaly rejection, while maintaining in-distribution (ID) performance. Extensive experiments across multiple ViT backbones trained with and without registers reveal consistent improvements of 2-4\% in top-1 OOD accuracy and a 2-3\% reduction in false positive rates for anomaly detection. Importantly, these gains are achieved without additional computational overhead.	 | 视觉变换器（ViTs）由于其能够捕捉全局图像表示的能力，在多种任务中取得了成功。最近的研究发现，ViTs 中存在高范数令牌，这些令牌会干扰无监督对象发现。为了解决这一问题，提出了一种称为“寄存器”的附加令牌，它们可以隔离高范数的补丁令牌，同时捕捉全局图像级别信息。虽然寄存器在对象发现方面得到了广泛研究，但它们在分布外（OOD）场景中的泛化性质仍然未被充分探索。在本文中，我们探讨了寄存器令牌嵌入在提供额外特征以提高泛化能力和异常检测方面的作用。为此，我们提出了一种简单的方法，将ViTs 中常用的一种特殊的CLS令牌嵌入与寄存器的平均池化嵌入相结合，从而创建用于训练下游分类器的特征表示。我们发现，这种方法提高了OOD泛化性能和异常检测的真阳率，同时保持了分布内（ID）性能。跨多个ViT骨干网络的广泛实验表明，在有寄存器和无寄存器训练的情况下，top-1 OOD准确率提高了2-4%，异常检测的假阳率降低了2-3%，而这些增益并未增加额外的计算开销。
2501.04782	 | GaussianVideo: Efficient Video Representation via Hierarchical Gaussian Splatting	 | Andrew Bond,Jui-Hsien Wang,Long Mai,Erkut Erdem,Aykut Erdem	 | 本文提出了一种结合3D高斯散斑技术和神经ODEs的新神经视频表示方法，通过时空分层学习策略提高渲染质量和加速收敛，有效解决了现有方法在内存使用和训练时间上的问题，并在多个视频数据集上达到了最先进的性能，具有良好的时间一致性。	 | Efficient neural representations for dynamic video scenes are critical for applications ranging from video compression to interactive simulations. Yet, existing methods often face challenges related to high memory usage, lengthy training times, and temporal consistency. To address these issues, we introduce a novel neural video representation that combines 3D Gaussian splatting with continuous camera motion modeling. By leveraging Neural ODEs, our approach learns smooth camera trajectories while maintaining an explicit 3D scene representation through Gaussians. Additionally, we introduce a spatiotemporal hierarchical learning strategy, progressively refining spatial and temporal features to enhance reconstruction quality and accelerate convergence. This memory-efficient approach achieves high-quality rendering at impressive speeds. Experimental results show that our hierarchical learning, combined with robust camera motion modeling, captures complex dynamic scenes with strong temporal consistency, achieving state-of-the-art performance across diverse video datasets in both high- and low-motion scenarios.	 | 高效的神经表示对于视频动态场景的应用至关重要，包括视频压缩和交互式模拟等。然而，现有的方法往往面临着高内存使用、长训练时间和时间一致性差等问题。为了解决这些问题，我们提出了一种新的神经视频表示方法，该方法结合了3D高斯散斑技术和连续相机运动建模。利用神经ODEs，我们的方法学习平滑的相机轨迹，并通过高斯函数保持明确的3D场景表示。此外，我们引入了一种时空分层学习策略，逐步细化空间和时间特征，以提高重建质量和加速收敛。这种内存高效的方案能够在令人印象深刻的高速下实现高质量的渲染。实验结果表明，我们的分层学习与稳健的相机运动建模相结合，能够捕捉复杂动态场景，并在高动态和低动态的视频数据集上实现最先进的性能，具有很强的时间一致性。
2501.04765	 | TREAD: Token Routing for Efficient Architecture-agnostic Diffusion Training	 | Felix Krause,Timy Phan,Vincent Tao Hu,Björn Ommer	 | 本文提出了一种名为TREAD的方法，通过使用预先定义的路由来提高扩散模型的训练效率，这些路由在模型的深层重新引入信息，而不是完全丢弃标记，并结合了多个路由及适应性辅助损失，从而减少计算成本并提升模型性能，特别是在ImageNet-1K 256x256的条件分类合成中表现出色。	 | Diffusion models have emerged as the mainstream approach for visual generation. However, these models usually suffer from sample inefficiency and high training costs. This issue is particularly pronounced in the standard diffusion transformer architecture due to its quadratic complexity relative to input length. Recent works have addressed this by reducing the number of tokens processed in the model, often through masking. In contrast, this work aims to improve the training efficiency of the diffusion backbone by using predefined routes that store this information until it is reintroduced to deeper layers of the model, rather than discarding these tokens entirely. Further, we combine multiple routes and introduce an adapted auxiliary loss that accounts for all applied routes. Our method is not limited to the common transformer-based model - it can also be applied to state-space models. Unlike most current approaches, TREAD achieves this without architectural modifications. Finally, we show that our method reduces the computational cost and simultaneously boosts model performance on the standard benchmark ImageNet-1K 256 x 256 in class-conditional synthesis. Both of these benefits multiply to a convergence speedup of 9.55x at 400K training iterations compared to DiT and 25.39x compared to the best benchmark performance of DiT at 7M training iterations.	 | 扩散模型已成为视觉生成的主要方法。然而，这些模型通常存在样本效率低和高训练成本的问题。这种问题在标准的扩散变换器架构中尤为明显，因为其复杂度与输入长度成平方关系。最近的研究通过减少模型中处理的标记数量来解决这一问题，通常通过掩码实现。与此相反，本工作旨在通过使用预先定义的路由来提高扩散骨干的训练效率，这些路由存储这些信息，直到将其重新引入模型的更深层，而不是完全丢弃这些标记。此外，我们结合了多个路由，并引入了一种适应性的辅助损失，以考虑所有应用的路由。我们的方法不仅限于常见的基于变换器的模型，也可以应用于状态空间模型。与大多数当前方法不同，TREAD 在无需进行架构修改的情况下实现了这一点。最后，我们展示了我们的方法降低了计算成本，并在标准基准 ImageNet-1K 256x256 的条件分类合成中提升了模型性能。这两个好处在 40 万个训练迭代时使收敛速度提高了 9.55 倍，与 DiT 相比，在 700 万个训练迭代时使最佳基准性能提高了 25.39 倍。
2501.04764	 | Video Summarisation with Incident and Context Information using Generative AI	 | Ulindu De Silva,Leon Fernando,Kalinga Bandara,Rashmika Nawaratne	 | 本文提出了一种利用生成人工智能（GenAI）的新方法，通过结合YOLO-V8物体检测和Gemini分析，为用户定义的查询提供定制化文本摘要，从而提高海量视频数据的分析精度和效率，并通过定量和定性评估证明了其可行性和有效性。	 | The proliferation of video content production has led to vast amounts of data, posing substantial challenges in terms of analysis efficiency and resource utilization. Addressing this issue calls for the development of robust video analysis tools. This paper proposes a novel approach leveraging Generative Artificial Intelligence (GenAI) to facilitate streamlined video analysis. Our tool aims to deliver tailored textual summaries of user-defined queries, offering a focused insight amidst extensive video datasets. Unlike conventional frameworks that offer generic summaries or limited action recognition, our method harnesses the power of GenAI to distil relevant information, enhancing analysis precision and efficiency. Employing YOLO-V8 for object detection and Gemini for comprehensive video and text analysis, our solution achieves heightened contextual accuracy. By combining YOLO with Gemini, our approach furnishes textual summaries extracted from extensive CCTV footage, enabling users to swiftly navigate and verify pertinent events without the need for exhaustive manual review. The quantitative evaluation revealed a similarity of 72.8%, while the qualitative assessment rated an accuracy of 85%, demonstrating the capability of the proposed method.	 | 视频内容生产的普及导致了大量数据的存在，这在数据分析效率和资源利用方面带来了巨大挑战。为应对这一问题，需要开发强大的视频分析工具。本文提出了一种利用生成人工智能（GenAI）的新颖方法以简化视频分析流程。我们的工具旨在为用户定义的查询提供定制化的文本摘要，从而在海量视频数据中提供聚焦的洞察。与传统框架提供的通用摘要或有限的动作识别相比，我们的方法利用GenAI的力量提取相关信息，从而提高分析精度和效率。我们使用YOLO-V8进行物体检测，并使用Gemini进行全面的视频和文本分析，我们的解决方案实现了更高的上下文准确性。通过将YOLO与Gemini结合，我们的方法可以从大量的CCTV录像中提取文本摘要，使用户能够快速导航和验证关键事件，无需进行繁琐的手动检查。定量评估结果显示相似度为72.8%，而定性评估的准确性为85%，表明所提方法的可行性和有效性。
2501.04750	 | Efficient License Plate Recognition in Videos Using Visual Rhythm and Accumulative Line Analysis	 | Victor Nascimento Ribeiro,Nina S. T. Hirata	 | 本文提出了两种基于单帧图像的自动车牌识别方法，即通过视觉节奏生成时空图像和使用累积线分析处理单线视频，从而显著降低计算需求，实验证明这两种方法处理速度比传统逐帧方法快三倍且识别结果与传统方法相当。	 | Video-based Automatic License Plate Recognition (ALPR) involves extracting vehicle license plate text information from video captures. Traditional systems typically rely heavily on high-end computing resources and utilize multiple frames to recognize license plates, leading to increased computational overhead. In this paper, we propose two methods capable of efficiently extracting exactly one frame per vehicle and recognizing its license plate characters from this single image, thus significantly reducing computational demands. The first method uses Visual Rhythm (VR) to generate time-spatial images from videos, while the second employs Accumulative Line Analysis (ALA), a novel algorithm based on single-line video processing for real-time operation. Both methods leverage YOLO for license plate detection within the frame and a Convolutional Neural Network (CNN) for Optical Character Recognition (OCR) to extract textual information. Experiments on real videos demonstrate that the proposed methods achieve results comparable to traditional frame-by-frame approaches, with processing speeds three times faster.	 | 基于视频的自动车牌识别（Automatic License Plate Recognition, ALPR）涉及从视频捕获中提取车辆车牌的文字信息。传统的系统通常依赖高端计算资源，并利用多帧来识别车牌，导致计算开销增加。在本文中，我们提出了两种方法，这些方法能够在每辆车辆的图像中精确提取一帧，并从此单帧图像中识别其车牌字符，从而显著降低计算需求。第一种方法使用视觉节奏（Visual Rhythm, VR）从视频中生成时空图像，第二种方法则使用一种基于单线视频处理的新颖算法——累积线分析（Accumulative Line Analysis, ALA），以实现实时操作。这两种方法都利用YOLO进行帧内的车牌检测，并使用卷积神经网络（Convolutional Neural Network, CNN）进行光学字符识别（Optical Character Recognition, OCR）以提取文本信息。实验证明，所提出的方法在处理速度上比传统的逐帧方法快三倍，且结果可与传统方法媲美。
2501.05441	 | The GAN is dead; long live the GAN! A Modern GAN Baseline	 | Yiwen Huang,Aaron Gokaslan,Volodymyr Kuleshov,James Tompkin	 | 该研究反驳了GAN难以训练的说法，通过提出一种新的正则化相对GAN损失，解决了模式缺失和不收敛问题，并简化了现有架构，从而在多个数据集上取得了优于StyleGAN2的性能。	 | There is a widely-spread claim that GANs are difficult to train, and GAN architectures in the literature are littered with empirical tricks. We provide evidence against this claim and build a modern GAN baseline in a more principled manner. First, we derive a well-behaved regularized relativistic GAN loss that addresses issues of mode dropping and non-convergence that were previously tackled via a bag of ad-hoc tricks. We analyze our loss mathematically and prove that it admits local convergence guarantees, unlike most existing relativistic losses. Second, our new loss allows us to discard all ad-hoc tricks and replace outdated backbones used in common GANs with modern architectures. Using StyleGAN2 as an example, we present a roadmap of simplification and modernization that results in a new minimalist baseline -- R3GAN. Despite being simple, our approach surpasses StyleGAN2 on FFHQ, ImageNet, CIFAR, and Stacked MNIST datasets, and compares favorably against state-of-the-art GANs and diffusion models.	 | 关于生成对抗网络（GANs）难以训练的说法在广泛流传，文献中的GAN架构中充斥着各种经验性技巧。我们提供了对这一说法的反驳，并以更为严谨的方式建立了一个现代GAN基线。首先，我们推导出一种行为良好的正则化相对GAN损失，解决了之前通过一系列临时性的技巧来应对的模式缺失和不收敛问题。我们从数学上分析了这种损失，并证明它允许局部收敛保证，而大多数现有的相对损失则不具备这一点。其次，我们的新损失使我们能够抛弃所有临时性的技巧，并用现代架构替换常见GAN中过时的基础结构。以StyleGAN2为例，我们展示了简化和现代化的道路，最终得出一个新的极简主义基线——R3GAN。尽管方法简单，我们的方法在FFHQ、ImageNet、CIFAR和Stacked MNIST数据集上优于StyleGAN2，并且在对抗生成网络（GANs）和扩散模型中表现也很出色。
2501.05426	 | From Images to Insights: Transforming Brain Cancer Diagnosis with Explainable AI	 | Md. Arafat Alam Khandaker,Ziyan Shirin Raha,Salehin Bin Iqbal,M.F. Mridha,Jungpil Shin	 | 本研究介绍了一个包含6,056张MRI图像的孟加拉脑癌数据集，并利用DenseNet169取得了高准确率，同时应用了可解释的人工智能方法以提高诊断透明度。	 | Brain cancer represents a major challenge in medical diagnostics, requisite precise and timely detection for effective treatment. Diagnosis initially relies on the proficiency of radiologists, which can cause difficulties and threats when the expertise is sparse. Despite the use of imaging resources, brain cancer remains often difficult, time-consuming, and vulnerable to intraclass variability. This study conveys the Bangladesh Brain Cancer MRI Dataset, containing 6,056 MRI images organized into three categories: Brain Tumor, Brain Glioma, and Brain Menin. The dataset was collected from several hospitals in Bangladesh, providing a diverse and realistic sample for research. We implemented advanced deep learning models, and DenseNet169 achieved exceptional results, with accuracy, precision, recall, and F1-Score all reaching 0.9983. In addition, Explainable AI (XAI) methods including GradCAM, GradCAM++, ScoreCAM, and LayerCAM were employed to provide visual representations of the decision-making processes of the models. In the context of brain cancer, these techniques highlight DenseNet169's potential to enhance diagnostic accuracy while simultaneously offering transparency, facilitating early diagnosis and better patient outcomes.	 | 脑癌在医学诊断中构成了重大挑战，准确且及时的检测是有效治疗的必要前提。诊断最初依赖于放射科医生的专业技能，当专业知识稀缺时，这会带来困难和风险。尽管使用了成像资源，脑癌的诊断仍然常常具有挑战性、耗时且容易受到类别内变异的影响。本研究介绍了孟加拉脑癌MRI数据集，包含6,056张MRI图像，分为三类：脑肿瘤、脑胶质瘤和脑膜瘤。该数据集是从孟加拉国多个医院收集的，提供了多样且真实的样本，适用于研究。我们应用了先进的深度学习模型，DenseNet169取得了卓越的成果，准确率、精确率、召回率和F1分数均达到0.9983。此外，还使用了可解释的人工智能（XAI）方法，如GradCAM、GradCAM++、ScoreCAM和LayerCAM，以提供模型决策过程的可视化表示。在脑癌诊断的背景下，这些方法突显了DenseNet169提高诊断准确度的潜力，同时提供了透明度，有助于早期诊断和更好的患者结果。
2501.05413	 | Seeing Sound: Assembling Sounds from Visuals for Audio-to-Image Generation	 | Darius Petermann,Mahdi M. Kalayeh	 | 本文提出了一种可扩展的图像声化框架，通过利用视觉-语言模型的推理能力将不相关的单模态数据进行人工配对，从而训练音频到图像生成模型，并展示了模型在语义混合、插值、响度校准和声学空间建模等方面的有趣能力。	 | Training audio-to-image generative models requires an abundance of diverse audio-visual pairs that are semantically aligned. Such data is almost always curated from in-the-wild videos, given the cross-modal semantic correspondence that is inherent to them. In this work, we hypothesize that insisting on the absolute need for ground truth audio-visual correspondence, is not only unnecessary, but also leads to severe restrictions in scale, quality, and diversity of the data, ultimately impairing its use in the modern generative models. That is, we propose a scalable image sonification framework where instances from a variety of high-quality yet disjoint uni-modal origins can be artificially paired through a retrieval process that is empowered by reasoning capabilities of modern vision-language models. To demonstrate the efficacy of this approach, we use our sonified images to train an audio-to-image generative model that performs competitively against state-of-the-art. Finally, through a series of ablation studies, we exhibit several intriguing auditory capabilities like semantic mixing and interpolation, loudness calibration and acoustic space modeling through reverberation that our model has implicitly developed to guide the image generation process.	 | 训练音频到图像生成模型需要大量的多样且语义对齐的音频-视觉配对数据。这些数据通常是从野外视频中精心筛选出来的，因为它们在跨模态语义对应方面具有内在的优势。在这项工作中，我们假设严格要求绝对的音频-视觉对应关系不仅是不必要的，还会导致数据在规模、质量和多样性方面的严重限制，最终影响其在现代生成模型中的应用。也就是说，我们提出了一种可扩展的图像声化框架，在这种框架中，可以通过利用现代视觉-语言模型的推理能力，将多种高质量但又不相关的单模态数据实例进行人工配对。为了证明这种方法的有效性，我们使用我们的声化图像来训练一个音频到图像的生成模型，该模型在性能上能够与最先进的方法竞争。最后，通过一系列消融研究，我们展示了模型已隐式开发出的一些有趣的听觉能力，如语义混合和插值、响度校准以及通过混响进行声学空间建模，这些能力指导了图像生成过程。
2501.05244	 | Optimized Sampling for Non-Line-of-Sight Imaging Using Modified Fast Fourier Transforms	 | Talha Sultan,Alex Bocchieri,Chaoying Gu,Xiaochun Liu,Pavel Polynkin,Andreas Velten	 | 该研究利用相位场框架表明，非视距成像系统通常在中继表面上过采样，从而可以通过非均匀快速傅里叶变换从稀疏测量中重构图像，同时保持计算效率，适用于实际NLOS成像应用。	 | Non-line-of-Sight (NLOS) imaging systems collect light at a diffuse relay surface and input this measurement into computational algorithms that output a 3D volumetric reconstruction. These algorithms utilize the Fast Fourier Transform (FFT) to accelerate the reconstruction process but require both input and output to be sampled spatially with uniform grids. However, the geometry of NLOS imaging inherently results in non-uniform sampling on the relay surface when using multi-pixel detector arrays, even though such arrays significantly reduce acquisition times. Furthermore, using these arrays increases the data rate required for sensor readout, posing challenges for real-world deployment. In this work, we utilize the phasor field framework to demonstrate that existing NLOS imaging setups typically oversample the relay surface spatially, explaining why the measurement can be compressed without significantly sacrificing reconstruction quality. This enables us to utilize the Non-Uniform Fast Fourier Transform (NUFFT) to reconstruct from sparse measurements acquired from irregularly sampled relay surfaces of arbitrary shapes. Furthermore, we utilize the NUFFT to reconstruct at arbitrary locations in the hidden volume, ensuring flexible sampling schemes for both the input and output. Finally, we utilize the Scaled Fast Fourier Transform (SFFT) to reconstruct larger volumes without increasing the number of samples stored in memory. All algorithms introduced in this paper preserve the computational complexity of FFT-based methods, ensuring scalability for practical NLOS imaging applications.	 | 非视距（NLOS）成像系统在散射中继表面收集光线，并将此测量值输入到计算算法中，输出三维体积重构结果。这些算法利用快速傅里叶变换（FFT）加速重构过程，但要求输入和输出都以均匀网格进行空间采样。然而，使用多像素探测器阵列进行NLOS成像时，几何形状会导致在中继表面上进行非均匀采样，尽管如此，这些阵列显著减少了获取时间。此外，使用这些阵列会增加传感器读取所需的数据速率，从而对实际部署提出了挑战。在本研究中，我们利用相位场框架表明，现有的NLOS成像设置通常在空间上过采样中继表面，解释了为何可以压缩测量值而不显著牺牲重构质量。这使我们能够利用非均匀快速傅里叶变换（NUFFT）从任意形状的不规则采样中继表面上获取稀疏测量值进行重构。此外，我们利用NUFFT从隐藏体积的任意位置进行重构，确保输入和输出的灵活采样方案。最后，我们利用缩放快速傅里叶变换（SFFT）重构更大体积而不增加存储在内存中的样本数量。本文中介绍的所有算法保持了基于FFT方法的计算复杂性，确保了实际NLOS成像应用的可扩展性。
2501.05241	 | Contrast-Free Myocardial Scar Segmentation in Cine MRI using Motion and Texture Fusion	 | Guang Yang,Jingkun Chen,Xicheng Sheng,Shan Yang,Xiahai Zhuang,Betty Raman,Lei Li,Vicente Grau	 | 该研究提出了一种新框架，利用心脏运动跟踪和图像纹理信息来分割心肌梗死后的心肌和疤痕组织，该方法基于非对比电影MRI图像能够达到与晚期钆增强MRI类似的心肌疤痕分割准确性，有望替代现有的对比增强技术。	 | Late gadolinium enhancement MRI (LGE MRI) is the gold standard for the detection of myocardial scars for post myocardial infarction (MI). LGE MRI requires the injection of a contrast agent, which carries potential side effects and increases scanning time and patient discomfort. To address these issues, we propose a novel framework that combines cardiac motion observed in cine MRI with image texture information to segment the myocardium and scar tissue in the left ventricle. Cardiac motion tracking can be formulated as a full cardiac image cycle registration problem, which can be solved via deep neural networks. Experimental results prove that the proposed method can achieve scar segmentation based on non-contrasted cine images with comparable accuracy to LGE MRI. This demonstrates its potential as an alternative to contrast-enhanced techniques for scar detection.	 | 晚期钆增强磁共振成像（Late Gadolinium Enhancement MRI，LGE MRI）是检测心肌疤痕的金标准，特别是在心肌梗死（Myocardial Infarction，MI）后。LGE MRI 需要注射对比剂，这带来了潜在的副作用，并增加了扫描时间和患者的不适。为了解决这些问题，我们提出了一种新的框架，结合心脏在电影 MRI 中观察到的运动与图像纹理信息，以分割左心室的心肌和疤痕组织。心脏运动跟踪可以被表述为一个完整的心脏图像周期注册问题，可以利用深度神经网络来解决。实验结果证明，所提出的方法可以基于非对比电影 MRI 图像实现与 LGE MRI 相当的疤痕分割准确度。这表明它可能成为对比增强技术的替代方案，用于疤痕检测。
2501.05239	 | Is Your Autonomous Vehicle Safe? Understanding the Threat of Electromagnetic Signal Injection Attacks on Traffic Scene Perception	 | Wenhao Liao,Sineng Yan,Youqian Zhang,Xinwei Zhai,Yuanyuan Wang,Eugene Yujun Fu	 | 该研究分析了不同模型在电磁信号注入攻击下的表现，并开发了一种新的仿真方法以生成模拟攻击数据集，从而增强自主驾驶车辆的鲁棒性和安全性。	 | Autonomous vehicles rely on camera-based perception systems to comprehend their driving environment and make crucial decisions, thereby ensuring vehicles to steer safely. However, a significant threat known as Electromagnetic Signal Injection Attacks (ESIA) can distort the images captured by these cameras, leading to incorrect AI decisions and potentially compromising the safety of autonomous vehicles. Despite the serious implications of ESIA, there is limited understanding of its impacts on the robustness of AI models across various and complex driving scenarios. To address this gap, our research analyzes the performance of different models under ESIA, revealing their vulnerabilities to the attacks. Moreover, due to the challenges in obtaining real-world attack data, we develop a novel ESIA simulation method and generate a simulated attack dataset for different driving scenarios. Our research provides a comprehensive simulation and evaluation framework, aiming to enhance the development of more robust AI models and secure intelligent systems, ultimately contributing to the advancement of safer and more reliable technology across various fields.	 | 自主驾驶车辆依赖于基于摄像头的感知系统来理解其驾驶环境并做出关键决策，从而确保车辆能够安全行驶。然而，一种名为电磁信号注入攻击（ESIA）的重大威胁可以扭曲摄像头捕捉到的图像，导致AI做出错误决策，并可能危及自主驾驶车辆的安全。尽管ESIA的影响非常严重，但在各种复杂驾驶场景中的鲁棒性影响方面，人们对它的理解仍然有限。为了弥补这一差距，我们的研究分析了不同模型在ESIA攻击下的表现，揭示了它们对攻击的脆弱性。此外，由于获取真实世界攻击数据的挑战，我们开发了一种新的ESIA仿真方法，并生成了适用于不同驾驶场景的模拟攻击数据集。我们的研究提供了全面的仿真和评估框架，旨在增强更稳健的AI模型和安全智能系统的开发，最终为各种领域的更安全、更可靠技术的进步做出贡献。
2501.05122	 | Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model	 | Gregor Geigle,Florian Schneider,Carolin Holtermann,Chris Biemann,Radu Timofte,Anne Lauscher,Goran Glavaš	 | 该研究全面调查了大规模多语言视觉-语言模型的训练策略，通过多阶段实验在13个下游任务和43种语言上分析了不同训练语言组合对性能的影响，并提出了一种包含100种语言的高性能模型Centurio。研究表明，即使只有25-50%的非英语数据，模型也能同时保持强大的英语性能和多语言能力。	 | Most Large Vision-Language Models (LVLMs) to date are trained predominantly on English data, which makes them struggle to understand non-English input and fail to generate output in the desired target language. Existing efforts mitigate these issues by adding multilingual training data, but do so in a largely ad-hoc manner, lacking insight into how different training mixes tip the scale for different groups of languages. In this work, we present a comprehensive investigation into the training strategies for massively multilingual LVLMs. First, we conduct a series of multi-stage experiments spanning 13 downstream vision-language tasks and 43 languages, systematically examining: (1) the number of training languages that can be included without degrading English performance and (2) optimal language distributions of pre-training as well as (3) instruction-tuning data. Further, we (4) investigate how to improve multilingual text-in-image understanding, and introduce a new benchmark for the task. Surprisingly, our analysis reveals that one can (i) include as many as 100 training languages simultaneously (ii) with as little as 25-50\% of non-English data, to greatly improve multilingual performance while retaining strong English performance. We further find that (iii) including non-English OCR data in pre-training and instruction-tuning is paramount for improving multilingual text-in-image understanding. Finally, we put all our findings together and train Centurio, a 100-language LVLM, offering state-of-the-art performance in an evaluation covering 14 tasks and 56 languages.	 | 目前大多数大型视觉-语言模型（LVLMs）主要是在英语数据上进行训练，这使得它们在处理非英语输入时难以理解，并且难以生成所需的目标语言的输出。现有的努力通过增加多语言训练数据来缓解这些问题，但这种方式往往是随意的，缺乏对不同训练组合如何影响不同语言群体的深入了解。在这项工作中，我们对大规模多语言LVLMs的训练策略进行了全面调查。首先，我们在13个下游视觉-语言任务和43种语言上进行了多阶段实验，系统地探讨了：（1）可以包含多少种训练语言而不损害英语性能，以及（2）预训练和（3）指令调优数据的最佳语言分布。此外，我们（4）研究了如何提高多语言文本在图像中的理解能力，并提出了一个新基准任务。出人意料的是，我们的分析表明，通过以下方式可以大大提高多语言性能并保持强大的英语性能：（i）可以同时包含多达100种训练语言；（ii）只需要25-50%的非英语数据。我们还发现，（iii）在预训练和指令调优中包含非英语OCR数据对于提高多语言文本在图像中的理解能力至关重要。最后，我们将所有这些发现结合起来训练了Centurio，这是一个包含100种语言的LVLM，其性能在涵盖14个任务和56种语言的评估中达到最新水平。
2501.05120	 | Improving the U-Net Configuration for Automated Delineation of Head and Neck Cancer on MRI	 | Andrei Iantsen	 | 本文提出了一种改进的传统U-Net架构方法，用于自动分割MRI扫描中的头颈部肿瘤，通过块内归一化、数据增强和滑动窗口推理等策略优化模型性能，并在MICCAI HNTS-MRG挑战中取得了良好的Dice相似系数（DSCagg）结果。	 | Tumor volume segmentation on MRI is a challenging and time-consuming process that is performed manually in typical clinical settings. This work presents an approach to automated delineation of head and neck tumors on MRI scans, developed in the context of the MICCAI Head and Neck Tumor Segmentation for MR-Guided Applications (HNTS-MRG) 2024 Challenge. Rather than designing a new, task-specific convolutional neural network, the focus of this research was to propose improvements to the configuration commonly used in medical segmentation tasks, relying solely on the traditional U-Net architecture. The empirical results presented in this article suggest the superiority of patch-wise normalization used for both training and sliding window inference. They also indicate that the performance of segmentation models can be enhanced by applying a scheduled data augmentation policy during training. Finally, it is shown that a small improvement in quality can be achieved by using Gaussian weighting to combine predictions for individual patches during sliding window inference. The model with the best configuration obtained an aggregated Dice Similarity Coefficient (DSCagg) of 0.749 in Task 1 and 0.710 in Task 2 on five cross-validation folds. The ensemble of five models (one best model per validation fold) showed consistent results on a private test set of 50 patients with an DSCagg of 0.752 in Task 1 and 0.718 in Task 2 (team name: andrei.iantsen). The source code and model weights are freely available at www.github.com/iantsen/hntsmrg.	 | MRI肿瘤体积分割是一项具有挑战性和耗时的过程，通常在临床环境中通过手动操作完成。本文介绍了一种在MICCAI头颈部肿瘤分割用于MR引导应用（HNTS-MRG）2024挑战背景下开发的方法，用于自动分割MRI扫描中的头颈部肿瘤。不同于设计专门的任务特定卷积神经网络，这项研究的重点是提出改进传统在医学分割任务中常用配置的方法，完全依赖传统的U-Net架构。本文中呈现的经验结果表明，用于训练和滑动窗口推理的块内归一化在性能上具有优势。同时，研究还指出，在训练过程中应用计划的数据增强策略可以提升分割模型的性能。最后，通过在滑动窗口推理期间使用高斯加权结合单个块的预测，可以实现质量的小幅提升。配置最优的模型在任务1和任务2上的平均Dice相似系数（DSCagg）分别为0.749和0.710（基于五次交叉验证）。五个模型的集成（每个验证折一个最佳模型）在私有测试集的50个患者上显示一致的结果，任务1和任务2的DSCagg分别为0.752和0.718（团队名称：andrei.iantsen）。源代码和模型权重可在www.github.com/iantsen/hntsmrg免费获得。
2501.05085	 | End-to-End Deep Learning for Interior Tomography with Low-Dose X-ray CT	 | Yoseob Han,Dufan Wu,Kyungsang Kim,Quanzheng Li	 | 本文提出了一种双域卷积神经网络方法，用于同时解决低剂量CT和区域兴趣CT中的耦合伪影问题，通过分别在图像域和投影域中处理噪声和截断投影，该方法优于传统的图像域深度学习方法和投影域卷积神经网络。	 | Objective: There exist several X-ray computed tomography (CT) scanning strategies to reduce a radiation dose, such as (1) sparse-view CT, (2) low-dose CT, and (3) region-of-interest (ROI) CT (called interior tomography). To further reduce the dose, the sparse-view and/or low-dose CT settings can be applied together with interior tomography. Interior tomography has various advantages in terms of reducing the number of detectors and decreasing the X-ray radiation dose. However, a large patient or small field-of-view (FOV) detector can cause truncated projections, and then the reconstructed images suffer from severe cupping artifacts. In addition, although the low-dose CT can reduce the radiation exposure dose, analytic reconstruction algorithms produce image noise. Recently, many researchers have utilized image-domain deep learning (DL) approaches to remove each artifact and demonstrated impressive performances, and the theory of deep convolutional framelets supports the reason for the performance improvement. Approach: In this paper, we found that the image-domain convolutional neural network (CNN) is difficult to solve coupled artifacts, based on deep convolutional framelets. Significance: To address the coupled problem, we decouple it into two sub-problems: (i) image domain noise reduction inside truncated projection to solve low-dose CT problem and (ii) extrapolation of projection outside truncated projection to solve the ROI CT problem. The decoupled sub-problems are solved directly with a novel proposed end-to-end learning using dual-domain CNNs. Main results: We demonstrate that the proposed method outperforms the conventional image-domain deep learning methods, and a projection-domain CNN shows better performance than the image-domain CNNs which are commonly used by many researchers.	 | 目标：目前存在几种X射线计算机断层扫描（CT）扫描策略以减少辐射剂量，例如（1）稀视角CT，（2）低剂量CT，以及（3）区域兴趣（ROI）CT（称为内部断层摄影）。为了进一步减少剂量，可以将稀视角和/或低剂量CT设置与内部断层摄影结合使用。内部断层摄影在减少探测器数量和降低X射线辐射剂量方面具有多种优势。然而，大型患者或小视野（FOV）探测器会导致投影截断，从而导致重建图像出现严重的杯形伪影。此外，虽然低剂量CT可以减少辐射暴露剂量，但分析重建算法会产生图像噪声。近年来，许多研究人员利用图像域深度学习（DL）方法消除了各种伪影并展示了出色的表现，而深度卷积框架理论支持了性能提升的原因。  方法：在本文中，我们发现基于深度卷积框架的图像域卷积神经网络（CNN）难以解决耦合伪影问题。  意义：为了解决耦合问题，我们将问题分解为两个子问题：（i）在截断投影内减少图像域噪声以解决低剂量CT问题；（ii）在截断投影外外推投影以解决ROI CT问题。这两个子问题通过新型端到端学习方法使用双域CNN直接求解。  主要结果：我们证明了所提出的方法优于传统的图像域深度学习方法，而投影域CNN的表现优于许多研究人员常用的图像域CNN。
2501.05072	 | A Flexible and Scalable Framework for Video Moment Search	 | Chongzhi Zhang,Xizhou Zhu,Aixin Sun	 | 本文提出了一种名为片段-提案-排序（SPR）的框架，用于高效地从任意长度的视频集合中检索与文本查询匹配的已排序片段列表，该框架通过离线预计算嵌入和在线近似最近邻搜索来降低计算成本和处理时间，并在TVR-Ranking数据集上达到了最先进的性能。	 | Video moment search, the process of finding relevant moments in a video corpus to match a user's query, is crucial for various applications. Existing solutions, however, often assume a single perfect matching moment, struggle with inefficient inference, and have limitations with hour-long videos. This paper introduces a flexible and scalable framework for retrieving a ranked list of moments from collection of videos in any length to match a text query, a task termed Ranked Video Moment Retrieval (RVMR). Our framework, called Segment-Proposal-Ranking (SPR), simplifies the search process into three independent stages: segment retrieval, proposal generation, and moment refinement with re-ranking. Specifically, videos are divided into equal-length segments with precomputed embeddings indexed offline, allowing efficient retrieval regardless of video length. For scalable online retrieval, both segments and queries are projected into a shared feature space to enable approximate nearest neighbor (ANN) search. Retrieved segments are then merged into coarse-grained moment proposals. Then a refinement and re-ranking module is designed to reorder and adjust timestamps of the coarse-grained proposals. Evaluations on the TVR-Ranking dataset demonstrate that our framework achieves state-of-the-art performance with significant reductions in computational cost and processing time. The flexible design also allows for independent improvements to each stage, making SPR highly adaptable for large-scale applications.	 | 视频片段搜索是指在视频库中查找与用户查询相关的片段的过程，对于各种应用至关重要。现有的解决方案通常假设存在一个完美的匹配片段，处理效率低下，并且难以应对长达一小时的视频。本文提出了一种灵活且可扩展的框架，用于从任意长度的视频集合中检索与文本查询匹配的已排序片段列表，该任务称为排序视频片段检索（RVMR）。我们的框架称为片段-提案-排序（SPR），将搜索过程简化为三个独立阶段：片段检索、提案生成和重新排序后的片段细化。具体而言，视频被划分为等长片段，并通过离线预计算的嵌入进行索引，使得无需考虑视频长度即可进行高效的检索。为了实现可扩展的在线检索，将片段和查询投影到共享特征空间，以启用近似最近邻（ANN）搜索。检索到的片段随后合并为粗粒度的提案。然后设计了一个细化和重新排序模块，以重新排列并调整粗粒度提案的时间戳。在TVR-Ranking数据集上的评估表明，我们的框架在显著降低计算成本和处理时间的情况下达到了最先进的性能。这种灵活的设计还可以独立改进每个阶段，使SPR在大规模应用中具有高度适应性。
2501.05009	 | A Scalable System for Visual Analysis of Ocean Data	 | Toshit Jain,Upkar Singh,Varun Singh,Vijay Kumar Boda,Ingrid Hotz,Sathish S. Vadhiyar,P. N. Vinayachandran,Vijay Natarajan	 | pyParaOcean是一种可扩展且交互式的可视化系统，专门用于支持海洋学分析任务，如涡旋识别和盐度运动追踪，并无缝集成到ParaView中，以实现高效的并行化和通用可视化功能。通过孟加拉湾案例研究，该系统展示了其在实际应用中的实用性和效率。	 | Oceanographers rely on visual analysis to interpret model simulations, identify events and phenomena, and track dynamic ocean processes. The ever increasing resolution and complexity of ocean data due to its dynamic nature and multivariate relationships demands a scalable and adaptable visualization tool for interactive exploration. We introduce pyParaOcean, a scalable and interactive visualization system designed specifically for ocean data analysis. pyParaOcean offers specialized modules for common oceanographic analysis tasks, including eddy identification and salinity movement tracking. These modules seamlessly integrate with ParaView as filters, ensuring a user-friendly and easy-to-use system while leveraging the parallelization capabilities of ParaView and a plethora of inbuilt general-purpose visualization functionalities. The creation of an auxiliary dataset stored as a Cinema database helps address I/O and network bandwidth bottlenecks while supporting the generation of quick overview visualizations. We present a case study on the Bay of Bengal (BoB) to demonstrate the utility of the system and scaling studies to evaluate the efficiency of the system.	 | 海洋学家依赖视觉分析来解释模型模拟、识别事件和现象，并追踪海洋动态过程。由于海洋数据的动态性质和多元关系，其不断增加的分辨率和复杂性要求一种可扩展且适应性强的可视化工具来支持交互式探索。我们介绍了一种名为pyParaOcean的可扩展且交互式的可视化系统，专门用于海洋数据分析。pyParaOcean提供专门模块以支持常见的海洋学分析任务，包括涡旋识别和盐度运动追踪。这些模块无缝集成到ParaView中作为滤波器，确保系统用户友好且易于使用，同时利用ParaView的并行化能力和丰富的内置通用可视化功能。创建一个作为Cinema数据库存储的辅助数据集有助于解决I/O和网络带宽瓶颈问题，并支持生成快速概览可视化。我们通过孟加拉湾（BoB）案例研究展示了系统的实用性，并通过扩展研究评估了系统的效率。
2501.04969	 | AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data	 | Haoran Zhu,Zhenyuan Dong,Kristi Topollai,Anna Choromanska	 | 本文提出了一种名为AD-L-JEPA的新颖自动驾驶自我监督预训练框架，该框架通过联合嵌入预测架构学习LiDAR数据的世界模型，无需生成或对比方法中常见的掩蔽或正负样本对，从而提高数据效率和实现简单性，实验结果显示其在多个下游任务中优于现有方法。	 | As opposed to human drivers, current autonomous driving systems still require vast amounts of labeled data to train. Recently, world models have been proposed to simultaneously enhance autonomous driving capabilities by improving the way these systems understand complex real-world environments and reduce their data demands via self-supervised pre-training. In this paper, we present AD-L-JEPA (aka Autonomous Driving with LiDAR data via a Joint Embedding Predictive Architecture), a novel self-supervised pre-training framework for autonomous driving with LiDAR data that, as opposed to existing methods, is neither generative nor contrastive. Our method learns spatial world models with a joint embedding predictive architecture. Instead of explicitly generating masked unknown regions, our self-supervised world models predict Bird's Eye View (BEV) embeddings to represent the diverse nature of autonomous driving scenes. Our approach furthermore eliminates the need to manually create positive and negative pairs, as is the case in contrastive learning. AD-L-JEPA leads to simpler implementation and enhanced learned representations. We qualitatively and quantitatively demonstrate high-quality of embeddings learned with AD-L-JEPA. We furthermore evaluate the accuracy and label efficiency of AD-L-JEPA on popular downstream tasks such as LiDAR 3D object detection and associated transfer learning. Our experimental evaluation demonstrates that AD-L-JEPA is a plausible approach for self-supervised pre-training in autonomous driving applications and is the best available approach outperforming SOTA, including most recently proposed Occupancy-MAE [1] and ALSO [2]. The source code of AD-L-JEPA is available at https://github.com/HaoranZhuExplorer/AD-L-JEPA-Release.	 | 与人类驾驶员不同，当前的自动驾驶系统仍然需要大量的标注数据进行训练。最近，已经提出了世界模型，通过改进这些系统对复杂现实环境的理解方式，并通过自我监督预训练减少数据需求。在本文中，我们提出了AD-L-JEPA（全名：基于联合嵌入预测架构的LiDAR数据自动驾驶，即Autonomous Driving with LiDAR data via a Joint Embedding Predictive Architecture），这是一种新颖的基于LiDAR数据的自动驾驶自我监督预训练框架，与现有方法不同，我们的方法既不是生成性的，也不是对比性的。我们的方法使用联合嵌入预测架构学习空间世界模型。与显式生成掩蔽的未知区域不同，我们的自我监督世界模型预测鸟瞰图（BEV）嵌入来表示自动驾驶场景的多样性。此外，我们的方法还消除了对比学习中需要手动创建正负样本对的需求。AD-L-JEPA 导致了更简单的实现和更高质量的学习表示。我们通过定性和定量的方式展示了使用AD-L-JEPA学习的嵌入的质量。我们还评估了AD-L-JEPA在流行下游任务（如LiDAR 3D物体检测及其关联的迁移学习）上的准确性和标签效率。我们的实验评估表明，AD-L-JEPA 是一种可行的自我监督预训练方法，是目前最佳的方法，超越了SOTA，包括最近提出的Occupancy-MAE[1]和ALSO[2]。AD-L-JEPA的源代码可以在 https://github.com/HaoranZhuExplorer/AD-L-JEPA-Release 获取。
2501.04794	 | A Steerable Deep Network for Model-Free Diffusion MRI Registration	 | Gianfranco Cortes,Baba C. Vemuri	 | 本文提出了一种新型深度学习框架，用于无需显式重新定向的原始扩散MRI数据的非刚性配准，通过将配准问题形式化为位置和方向空间的群等变微分同胚，并采用等变$\mathsf{SE}(3)$ U-Net生成速度场，从而实现几何特性保持。实验结果表明，该方法在人类连接组计划的dMRI数据上具有竞争力，并且无需估计衍生表示。	 | Nonrigid registration is vital to medical image analysis but remains challenging for diffusion MRI (dMRI) due to its high-dimensional, orientation-dependent nature. While classical methods are accurate, they are computationally demanding, and deep neural networks, though efficient, have been underexplored for nonrigid dMRI registration compared to structural imaging. We present a novel, deep learning framework for model-free, nonrigid registration of raw diffusion MRI data that does not require explicit reorientation. Unlike previous methods relying on derived representations such as diffusion tensors or fiber orientation distribution functions, in our approach, we formulate the registration as an equivariant diffeomorphism of position-and-orientation space. Central to our method is an $\mathsf{SE}(3)$-equivariant UNet that generates velocity fields while preserving the geometric properties of a raw dMRI's domain. We introduce a new loss function based on the maximum mean discrepancy in Fourier space, implicitly matching ensemble average propagators across images. Experimental results on Human Connectome Project dMRI data demonstrate competitive performance compared to state-of-the-art approaches, with the added advantage of bypassing the overhead for estimating derived representations. This work establishes a foundation for data-driven, geometry-aware dMRI registration directly in the acquisition space.	 | 非刚性配准对于医学图像分析至关重要，但由于扩散磁共振成像（dMRI）的高维度和方向依赖特征，这一过程仍极具挑战性。尽管经典方法非常准确，但计算成本较高，而深度神经网络虽然高效，但在非刚性dMRI配准方面与结构成像相比仍然未得到充分探索。我们提出了一种新型的深度学习框架，用于无需显式重新定向的原始扩散MRI数据的无模型非刚性配准。不同于依赖于衍生表示（如扩散张量或纤维方向分布函数）的先前方法，我们通过将配准形式化为位置和方向空间的群等变微分同胚来实现这一目标。我们方法的核心是一个等变$\mathsf{SE}(3)$ U-Net，该网络能够生成速度场，同时保持原始dMRI数据域的几何特性。我们提出了一种基于傅里叶空间最大均值差异的新损失函数，该函数隐式地在图像间匹配群体平均传播器。在人类连接组计划（Human Connectome Project）的dMRI数据上的实验结果表明，该方法与最先进的方法相比具有竞争力，且具有无需估计衍生表示的额外优势。这项工作为直接在采集空间中实现数据驱动的几何感知dMRI配准奠定了基础。
2501.04735	 | Topology-based deep-learning segmentation method for deep anterior lamellar keratoplasty (DALK) surgical guidance using M-mode OCT data	 | J. Yu,H. Yi,Y. Wang,J. D. Opfermann,W. G. Gensheimer,A. Krieger,J. U. Kang	 | 该研究开发了一种基于拓扑的深度学习分割方法，用于改进光学相干断层扫描引导的眼部安装机器人在深板层角膜移植术中的应用，该方法能有效减少噪声影响，提高分割速度和精度，从而更准确地分离角膜层，指导手术进行。	 | Deep Anterior Lamellar Keratoplasty (DALK) is a partial-thickness corneal transplant procedure used to treat corneal stromal diseases. A crucial step in this procedure is the precise separation of the deep stroma from Descemet's membrane (DM) using the Big Bubble technique. To simplify the tasks of needle insertion and pneumo-dissection in this technique, we previously developed an Optical Coherence Tomography (OCT)-guided, eye-mountable robot that uses real-time tracking of corneal layers from M-mode OCT signals for control. However, signal noise and instability during manipulation of the OCT fiber sensor-integrated needle have hindered the performance of conventional deep-learning segmentation methods, resulting in rough and inaccurate detection of corneal layers. To address these challenges, we have developed a topology-based deep-learning segmentation method that integrates a topological loss function with a modified network architecture. This approach effectively reduces the effects of noise and improves segmentation speed, precision, and stability. Validation using in vivo, ex vivo, and hybrid rabbit eye datasets demonstrates that our method outperforms traditional loss-based techniques, providing fast, accurate, and robust segmentation of the epithelium and DM to guide surgery.	 | 深板层角膜移植术（DALK）是一种用于治疗角膜基质疾病的部分厚度角膜移植手术。该手术中的关键步骤是利用“大泡”技术精确分离深层基质与德西梅特膜（DM）。为了简化此技术中的针插入和气腔分离任务，我们之前开发了一种基于光学相干断层扫描（OCT）引导的眼部安装机器人。该机器人使用M模式OCT信号实时跟踪角膜层进行控制。然而，OCT光纤传感器集成针在操作过程中的信号噪声和不稳定状态阻碍了传统深度学习分割方法的性能，导致角膜层检测粗糙且不准确。为解决这些问题，我们开发了一种基于拓扑的深度学习分割方法，该方法结合了拓扑损失函数和修改后的网络架构。这一方法有效地减少了噪声的影响，并提高了分割速度、精确度和稳定性。使用活体、离体和混合兔眼数据集的验证结果显示，我们的方法优于传统的基于损失的方法，能够快速、准确且稳健地分割上皮层和DM，从而指导手术。
2501.04699	 | EditAR: Unified Conditional Generation with Autoregressive Models	 | Jiteng Mu,Nuno Vasconcelos,Xiaolong Wang	 | 本文提出了一种名为EditAR的统一自回归框架，用于多种条件图像生成任务，如图像编辑和分割图到图像生成等，该模型结合图像和指令作为输入，通过传统的下一个标记预测方式生成编辑后的图像标记，并在多个基准任务上取得了与先进特定任务方法相当的性能。	 | Recent progress in controllable image generation and editing is largely driven by diffusion-based methods. Although diffusion models perform exceptionally well in specific tasks with tailored designs, establishing a unified model is still challenging. In contrast, autoregressive models inherently feature a unified tokenized representation, which simplifies the creation of a single foundational model for various tasks. In this work, we propose EditAR, a single unified autoregressive framework for a variety of conditional image generation tasks, e.g., image editing, depth-to-image, edge-to-image, segmentation-to-image. The model takes both images and instructions as inputs, and predicts the edited images tokens in a vanilla next-token paradigm. To enhance the text-to-image alignment, we further propose to distill the knowledge from foundation models into the autoregressive modeling process. We evaluate its effectiveness across diverse tasks on established benchmarks, showing competitive performance to various state-of-the-art task-specific methods. Project page: https://jitengmu.github.io/EditAR/	 | 基于扩散的方法在可控图像生成和编辑方面的最新进展起到了主要推动作用。尽管定制设计的扩散模型在特定任务中表现出色，但建立一个统一的模型仍然颇具挑战性。相比之下，自回归模型本质上具备统一的标记表示，这简化了为各种任务创建单一基础模型的过程。在此项工作中，我们提出了一种名为EditAR的统一自回归框架，用于多种条件图像生成任务，例如图像编辑、深度图到图像、边缘图到图像、分割图到图像。该模型同时接受图像和指令作为输入，并以传统的下一个标记预测方式来预测编辑后的图像标记。为了增强文本到图像的对齐，我们进一步提出将基础模型的知识输送到自回归建模过程中。我们在多个基准任务上评估其有效性，结果显示其性能与各种最先进的特定任务方法相当。项目页面：https://jitengmu.github.io/EditAR/
