| ID | Title | Authors | Summary (zh) | Abstract (en) | Abstract (zh) | 
| --- | --- | --- | --- | --- | --- |
2501.03229	 | Gaussian Masked Autoencoders	 | Jathushan Rajasegaran,Xinlei Chen,Rulilong Li,Christoph Feichtenhofer,Jitendra Malik,Shiry Ginosar	 | 本文提出了高斯掩码自编码器（GMAE），该方法通过引入基于3D高斯的中间表示，旨在同时学习语义抽象和空间理解，从而在保留MAE的高质量自监督表示的同时，增强零样本学习的空间理解能力，如轮廓背景分割和边缘检测等。	 | This paper explores Masked Autoencoders (MAE) with Gaussian Splatting. While reconstructive self-supervised learning frameworks such as MAE learns good semantic abstractions, it is not trained for explicit spatial awareness. Our approach, named Gaussian Masked Autoencoder, or GMAE, aims to learn semantic abstractions and spatial understanding jointly. Like MAE, it reconstructs the image end-to-end in the pixel space, but beyond MAE, it also introduces an intermediate, 3D Gaussian-based representation and renders images via splatting. We show that GMAE can enable various zero-shot learning capabilities of spatial understanding (e.g., figure-ground segmentation, image layering, edge detection, etc.) while preserving the high-level semantics of self-supervised representation quality from MAE. To our knowledge, we are the first to employ Gaussian primitives in an image representation learning framework beyond optimization-based single-scene reconstructions. We believe GMAE will inspire further research in this direction and contribute to developing next-generation techniques for modeling high-fidelity visual data. More details at https://brjathu.github.io/gmae	 | 本文探讨了使用高斯点进行掩码自编码器（MAE）的研究。虽然诸如MAE之类的重建自监督学习框架能够学习良好的语义抽象，但它并不专门训练以获得明确的空间意识。我们的方法，名为高斯掩码自编码器（GMAE），旨在共同学习语义抽象和空间理解。与MAE类似，它在像素空间中端到端地重建图像，但在MAE之上，它引入了一种中间的基于3D高斯的表示，并通过高斯点进行图像渲染。我们展示了GMAE可以在保留来自MAE的自监督表示质量的高级语义的同时，实现各种零样本学习的空间理解能力（例如，轮廓背景分割、图像分层、边缘检测等）。据我们所知，我们是首次在图像表示学习框架中使用高斯基元，而不仅仅是基于优化的单场景重建。我们相信GMAE将激励在此方向上的进一步研究，并为开发下一代高保真视觉数据建模技术做出贡献。更多详情请参阅 https://brjathu.github.io/gmae
2501.03225	 | Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation	 | Yuhui Zhang,Yuchang Su,Yiming Liu,Xiaohan Wang,James Burgess,Elaine Sui,Chenyu Wang,Josiah Aklilu,Alejandro Lozano,Anjiang Wei,Ludwig Schmidt,Serena Yeung-Levy	 | 该研究提出了AutoConverter，一种自动将开放性视觉问答问题转换为多项选择题的框架，以提高评估的客观性和效率，并构建了VMCBench基准，用于全面评估视觉语言模型。	 | The rapid development of vision language models (VLMs) demands rigorous and reliable evaluation. However, current visual question answering (VQA) benchmarks often depend on open-ended questions, making accurate evaluation difficult due to the variability in natural language responses. To address this, we introduce AutoConverter, an agentic framework that automatically converts these open-ended questions into multiple-choice format, enabling objective evaluation while reducing the costly question creation process. Our experiments demonstrate that AutoConverter can generate correct and challenging multiple-choice questions, with VLMs demonstrating consistently similar or lower accuracy on these questions compared to human-created ones. Using AutoConverter, we construct VMCBench, a benchmark created by transforming 20 existing VQA datasets into a unified multiple-choice format, totaling 9,018 questions. We comprehensively evaluate 33 state-of-the-art VLMs on VMCBench, setting a new standard for scalable, consistent, and reproducible VLM evaluation.	 | 随着视觉语言模型（VLMs）的迅速发展，需要严格的和可靠的评估方法。然而，当前的视觉问答（VQA）基准通常依赖于开放性问题，这使得准确评估变得困难，因为自然语言回答的变异性大。为了解决这个问题，我们提出了AutoConverter，这是一种自主框架，可以自动将这些开放性问题转换为多项选择题格式，从而使评估更加客观，并减少创建问题的成本。我们的实验表明，AutoConverter能够生成正确且具有挑战性的多项选择题，VLMs在这些问题上的准确率与人类创建的问题相当甚至更低。使用AutoConverter，我们构建了VMCBench，这是一个通过将20个现有的VQA数据集统一转换为多项选择题格式而形成的基准，共计9,018个问题。我们全面评估了33个最先进的VLMs在VMCBench上的表现，确立了大规模、一致性和可再现性VLM评估的新标准。
2501.03223	 | Rate-My-LoRA: Efficient and Adaptive Federated Model Tuning for Cardiac MRI Segmentation	 | Xiaoxiao He,Haizhou Shi,Ligong Han,Chaowei Tan,Bo Liu,Zihao Xu,Meng Ye,Leon Axel,Kang Li,Dimitris Metaxas	 | 本文提出了一种新颖的适应性联邦学习方法，利用低秩适应（LoRA）减少带宽需求并提高模型性能，同时提出了一种自适应聚合技术以解决数据异质性问题，从而在心脏MRI图像分析中实现了更好的泛化性能和快速适应。	 | Cardiovascular disease (CVD) and cardiac dyssynchrony are major public health problems in the United States. Precise cardiac image segmentation is crucial for extracting quantitative measures that help categorize cardiac dyssynchrony. However, achieving high accuracy often depends on centralizing large datasets from different hospitals, which can be challenging due to privacy concerns. To solve this problem, Federated Learning (FL) is proposed to enable decentralized model training on such data without exchanging sensitive information. However, bandwidth limitations and data heterogeneity remain as significant challenges in conventional FL algorithms. In this paper, we propose a novel efficient and adaptive federate learning method for cardiac segmentation that improves model performance while reducing the bandwidth requirement. Our method leverages the low-rank adaptation (LoRA) to regularize model weight update and reduce communication overhead. We also propose a \mymethod{} aggregation technique to address data heterogeneity among clients. This technique adaptively penalizes the aggregated weights from different clients by comparing the validation accuracy in each client, allowing better generalization performance and fast local adaptation. In-client and cross-client evaluations on public cardiac MR datasets demonstrate the superiority of our method over other LoRA-based federate learning approaches.	 | 心血管疾病（CVD）和心脏异步是美国重要的公共卫生问题。精确的心脏图像分割对于提取有助于心脏异步分类的定量指标至关重要。然而，实现高精度往往依赖于集中来自不同医院的大规模数据集，这由于隐私问题可能会面临挑战。为了解决这一问题，联邦学习（FL）被提出，能够在不交换敏感信息的情况下，在分散的数据上进行模型训练。然而，带宽限制和数据异质性仍然是传统FL算法的重大挑战。在本文中，我们提出了一种新颖的、高效的适应性联邦学习方法，该方法在减少带宽需求的同时提高了模型性能。我们的方法利用低秩适应（LoRA）来正则化模型权重更新并减少通信开销。我们还提出了一种\mymethod{}聚合技术，以解决客户端之间数据异质性的问题。该技术通过比较每个客户端的验证准确率适当地惩罚不同客户端聚合后的权重，从而实现更好的泛化性能和快速的本地适应。在公共心脏MRI数据集上的客户端内和跨客户端评估表明，我们的方法在其他基于LoRA的联邦学习方法中具有明显的优势。
2501.03221	 | RW-Net: Enhancing Few-Shot Point Cloud Classification with a Wavelet Transform Projection-based Network	 | Haosheng Zhang,Hao Huang	 | 该研究提出了RW-Net，这是一种创新框架，通过结合率失真解释和小波变换，有效提升了3D物体分类在少样本学习场景中的泛化能力和鲁棒性。实验结果表明，RW-Net在多个数据集上显著优于现有方法，特别是在少量标注样本的情况下。	 | In the domain of 3D object classification, a fundamental challenge lies in addressing the scarcity of labeled data, which limits the applicability of traditional data-intensive learning paradigms. This challenge is particularly pronounced in few-shot learning scenarios, where the objective is to achieve robust generalization from minimal annotated samples. To overcome these limitations, it is crucial to identify and leverage the most salient and discriminative features of 3D objects, thereby enhancing learning efficiency and reducing dependency on large-scale labeled datasets. This work introduces RW-Net, a novel framework designed to address the challenges above by integrating Rate-Distortion Explanation (RDE) and wavelet transform into a state-of-the-art projection-based 3D object classification architecture. The proposed method capitalizes on RDE to extract critical features by identifying and preserving the most informative data components while reducing redundancy. This process ensures the retention of essential information for effective decision-making, optimizing the model's ability to learn from limited data. Complementing RDE, incorporating the wavelet transform further enhances the framework's capability to generalize in low-data regimes. By emphasizing low-frequency components of the input data, the wavelet transform captures fundamental geometric and structural attributes of 3D objects. These attributes are instrumental in mitigating overfitting and improving the robustness of the learned representations across diverse tasks and domains. To validate the effectiveness of our RW-Net, we conduct extensive experiments on three datasets: ModelNet40, ModelNet40-C, and ScanObjectNN for few-shot 3D object classification. The results demonstrate that our approach achieves state-of-the-art performance and exhibits superior generalization and robustness in few-shot learning scenarios.	 | 在3D物体分类领域，一个基本的挑战在于标注数据的稀缺性，这限制了传统数据密集型学习范式的适用性。这一挑战在少样本学习场景中尤为突出，其目标是从少量标注样本中实现稳健的泛化。为了克服这些限制，识别和利用3D物体中最显著和区分性特征至关重要，从而提升学习效率并减少对大规模标注数据集的依赖。本工作引入了RW-Net，这是一种新颖的框架，通过将率失真解释（RDE）和小波变换集成到最先进的投影基3D物体分类架构中，来解决上述挑战。所提出的方法利用RDE提取关键特征，通过识别并保留最具有信息性的数据成分来减少冗余。这一过程确保了保留了有效决策所需的基本信息，优化了模型从小样本数据中学习的能力。在RDE的基础上，结合小波变换进一步增强了框架在数据稀缺情况下的泛化能力。通过强调输入数据的低频分量，小波变换捕获了3D物体的基本几何和结构属性。这些属性在减轻过拟合和提高学习表示的鲁棒性方面至关重要，适用于各种任务和领域。为了验证RW-Net的有效性，我们在三个数据集上进行了广泛的实验：ModelNet40、ModelNet40-C和ScanObjectNN，用于少样本3D物体分类。实验结果表明，我们的方法实现了最先进的性能，并在少样本学习场景中展示了更好的泛化能力和鲁棒性。
2501.03220	 | ProTracker: Probabilistic Integration for Robust and Accurate Point Tracking	 | Tingyang Zhang,Chen Wang,Zhiyang Dou,Qingzhe Gao,Jiahui Lei,Baoquan Chen,Lingjie Liu	 | 本文提出了一种名为ProTracker的新框架，用于实现视频中任意点的稳健且准确的长期密集跟踪，通过结合概率集成优化光学流和语义特征预测，有效应对遮挡等挑战，并在多个基准测试中达到或超越了现有最先进的方法。	 | In this paper, we propose ProTracker, a novel framework for robust and accurate long-term dense tracking of arbitrary points in videos. The key idea of our method is incorporating probabilistic integration to refine multiple predictions from both optical flow and semantic features for robust short-term and long-term tracking. Specifically, we integrate optical flow estimations in a probabilistic manner, producing smooth and accurate trajectories by maximizing the likelihood of each prediction. To effectively re-localize challenging points that disappear and reappear due to occlusion, we further incorporate long-term feature correspondence into our flow predictions for continuous trajectory generation. Extensive experiments show that ProTracker achieves the state-of-the-art performance among unsupervised and self-supervised approaches, and even outperforms supervised methods on several benchmarks. Our code and model will be publicly available upon publication.	 | 在本文中，我们提出了一种名为ProTracker的新颖框架，用于视频中任意点的稳健且准确的长期密集跟踪。我们方法的核心思想是通过结合概率集成，对来自光学流和语义特征的多个预测进行细化，从而实现稳健的短期和长期跟踪。具体而言，我们以概率的方式整合光学流估计，通过最大化每个预测的似然性来生成平滑且准确的轨迹。为了有效重新定位由于遮挡而消失和重新出现的具有挑战性的点，我们进一步将长期特征对应关系整合到我们的流动预测中，用于连续轨迹的生成。广泛实验表明，ProTracker在无监督和自监督方法中达到了最先进的性能，并且在某些基准测试中甚至超越了监督方法。我们将在发表时公开我们的代码和模型。
2501.03218	 | Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction	 | Rui Qian,Shuangrui Ding,Xiaoyi Dong,Pan Zhang,Yuhang Zang,Yuhang Cao,Dahua Lin,Jiaqi Wang	 | Dispider系统提出了一种解耦主动实时视频交互的方法，通过轻量级的流视频处理模块进行实时监控和决策，并在触发交互时采用异步处理来实现及时、上下文准确且计算高效的响应。这种设计使得Dispider在保持传统视频问答任务性能的同时，显著优于前在线模型在流媒体场景中的响应表现。	 | Active Real-time interaction with video LLMs introduces a new paradigm for human-computer interaction, where the model not only understands user intent but also responds while continuously processing streaming video on the fly. Unlike offline video LLMs, which analyze the entire video before answering questions, active real-time interaction requires three capabilities: 1) Perception: real-time video monitoring and interaction capturing. 2) Decision: raising proactive interaction in proper situations, 3) Reaction: continuous interaction with users. However, inherent conflicts exist among the desired capabilities. The Decision and Reaction require a contrary Perception scale and grain, and the autoregressive decoding blocks the real-time Perception and Decision during the Reaction. To unify the conflicted capabilities within a harmonious system, we present Dispider, a system that disentangles Perception, Decision, and Reaction. Dispider features a lightweight proactive streaming video processing module that tracks the video stream and identifies optimal moments for interaction. Once the interaction is triggered, an asynchronous interaction module provides detailed responses, while the processing module continues to monitor the video in the meantime. Our disentangled and asynchronous design ensures timely, contextually accurate, and computationally efficient responses, making Dispider ideal for active real-time interaction for long-duration video streams. Experiments show that Dispider not only maintains strong performance in conventional video QA tasks, but also significantly surpasses previous online models in streaming scenario responses, thereby validating the effectiveness of our architecture. The code and model are released at \url{https://github.com/Mark12Ding/Dispider}.	 | 主动实时与视频交互的LLM引入了一种新的人类计算机交互范式，其中模型不仅理解用户意图，还在不断处理实时视频流的同时响应。与分析整段视频后再回答问题的离线视频LLM不同，主动实时交互需要三种能力：1）感知：实时视频监控和交互捕捉；2）决策：在适当情况下主动发起交互；3）反应：持续与用户交互。然而，这些期望的能力之间存在固有的冲突。决策和反应需要相反的感知尺度和细节，自回归解码在反应过程中阻塞了实时感知和决策。为了在一个和谐的系统中统一这些冲突的能力，我们提出了Dispider系统，该系统解开了感知、决策和反应。Dispider的特点是一个轻量级的主动流视频处理模块，用于跟踪视频流并识别最佳交互时刻。一旦触发交互，异步交互模块将提供详细的响应，同时处理模块继续监控视频。我们的解耦和异步设计确保了及时、上下文准确且计算高效的响应，使Dispider适用于长时间视频流的主动实时交互。实验显示，Dispider不仅在传统的视频问答任务中保持了强大的性能，还在流媒体场景中的响应表现上显著优于之前的在线模型，从而验证了我们架构的有效性。代码和模型已在https://github.com/Mark12Ding/Dispider上发布。
2501.03173	 | MObI: Multimodal Object Inpainting Using Diffusion Models	 | Alexandru Buburuzan,Anuj Sharma,John Redford,Puneet K. Dokania,Romain Mueller	 | 本文提出了一种名为MObI的新框架，利用扩散模型在多模态数据中生成逼真且可控的对象补全，特别适用于自动驾驶等安全性关键应用的严格测试。该方法能够将单张参考RGB图像中的对象无缝插入多模态场景中，保持语义一致性和多模态连贯性，并提供准确的空间定位和比例。	 | Safety-critical applications, such as autonomous driving, require extensive multimodal data for rigorous testing. Methods based on synthetic data are gaining prominence due to the cost and complexity of gathering real-world data but require a high degree of realism and controllability in order to be useful. This paper introduces MObI, a novel framework for Multimodal Object Inpainting that leverages a diffusion model to create realistic and controllable object inpaintings across perceptual modalities, demonstrated for both camera and lidar simultaneously. Using a single reference RGB image, MObI enables objects to be seamlessly inserted into existing multimodal scenes at a 3D location specified by a bounding box, while maintaining semantic consistency and multimodal coherence. Unlike traditional inpainting methods that rely solely on edit masks, our 3D bounding box conditioning gives objects accurate spatial positioning and realistic scaling. As a result, our approach can be used to insert novel objects flexibly into multimodal scenes, providing significant advantages for testing perception models.	 | 安全性关键应用，如自动驾驶，需要大量的多模态数据进行严格的测试。基于合成数据的方法由于收集真实世界数据的成本和复杂性而受到青睐，但它们需要高度的真实感和可控性才能发挥作用。本文提出了一种名为MObI的新颖框架，该框架利用扩散模型在感知模态之间生成逼真且可控的对象补全，同时实现了摄像头和激光雷达数据的同步展示。使用单张参考RGB图像，MObI能够将对象无缝地插入指定三维位置的现有多模态场景中，同时保持语义一致性和多模态的连贯性。与传统的仅依赖编辑掩膜的补全方法不同，我们的三维边界框条件赋予对象准确的空间定位和现实的比例。因此，我们的方法可以灵活地将新对象插入多模态场景中，为感知模型的测试提供了显著的优势。
2501.03153	 | Segment Anything Model for Zero-shot Single Particle Tracking in Liquid Phase Transmission Electron Microscopy	 | Risha Goel,Zain Shabeeb,Isabel Panicker,Vida Jamali	 | 该研究利用Meta发布的Segment Anything Model 2（SAM 2）开发了SAM4EM框架，该框架能够自动分割和追踪液相传输电子显微镜（LPTEM）视频中的纳米颗粒，显著提高了单颗粒追踪的准确度，推动了LPTEM在纳米尺度成像中的应用。	 | Liquid phase transmission electron microscopy (LPTEM) offers an unparalleled combination of spatial and temporal resolution, making it a promising tool for single particle tracking at the nanoscale. However, the absence of a standardized framework for identifying and tracking nanoparticles in noisy LPTEM videos has impeded progress in the field to develop this technique as a single particle tracking tool. To address this, we leveraged Segment Anything Model 2 (SAM 2), released by Meta, which is a foundation model developed for segmenting videos and images. Here, we demonstrate that SAM 2 can successfully segment LPTEM videos in a zero-shot manner and without requiring fine-tuning. Building on this capability, we introduce SAM4EM, a comprehensive framework that integrates promptable video segmentation with particle tracking and statistical analysis, providing an end-to-end LPTEM analysis framework for single particle tracking. SAM4EM achieves nearly 50-fold higher accuracy in segmenting and analyzing LPTEM videos compared to state-of-the-art methods, paving the way for broader applications of LPTEM in nanoscale imaging.	 | 液相传输电子显微镜（LPTEM）提供了前所未有的空间和时间分辨率组合，使其成为纳米尺度单颗粒追踪的有前途的工具。然而，由于缺乏标准化框架来识别和追踪LPTEM视频中的纳米颗粒，阻碍了该技术在单颗粒追踪方面的进展。为了解决这个问题，我们利用了Meta发布的Segment Anything Model 2（SAM 2），该模型是一个用于分割视频和图像的基础模型。在这里，我们展示了SAM 2能够以零样本的方式自动分割LPTEM视频，无需微调。在此基础上，我们引入了SAM4EM，这是一个全面的框架，结合了可提示视频分割、颗粒追踪和统计分析，为单颗粒追踪提供了一个端到端的LPTEM分析框架。SAM4EM在分割和分析LPTEM视频方面的准确度比最先进的方法高近50倍，为LPTEM在纳米尺度成像中的广泛应用铺平了道路。
2501.03145	 | Geometry Restoration and Dewarping of Camera-Captured Document Images	 | Valery Istomin,Oleg Pereziabov,Ilya Afanasyev	 | 本研究开发了一种结合深度学习和计算机视觉的流水线方法，用于恢复由相机拍摄的纸文档图像的拓扑结构，该方法在视觉效果、OCR可读性和几何恢复指标上优于现有技术，并显著减少了计算资源和内存需求。	 | This research focuses on developing a method for restoring the topology of digital images of paper documents captured by a camera, using algorithms for detection, segmentation, geometry restoration, and dewarping. Our methodology employs deep learning (DL) for document outline detection, followed by computer vision (CV) to create a topological 2D grid using cubic polynomial interpolation and correct nonlinear distortions by remapping the image. Using classical CV methods makes the document topology restoration process more efficient and faster, as it requires significantly fewer computational resources and memory. We developed a new pipeline for automatic document dewarping and reconstruction, along with a framework and annotated dataset to demonstrate its efficiency. Our experiments confirm the promise of our methodology and its superiority over existing benchmarks (including mobile apps and popular DL solutions, such as RectiNet, DocGeoNet, and DocTr++) both visually and in terms of document readability via Optical Character Recognition (OCR) and geometry restoration metrics. This paves the way for creating high-quality digital copies of paper documents and enhancing the efficiency of OCR systems. Project page: https://github.com/HorizonParadox/DRCCBI	 | 本研究致力于开发一种方法，用于恢复由相机捕捉的纸文档数字图像的拓扑结构，该方法利用检测、分割、几何恢复和去翘曲等算法。我们的方法采用深度学习（DL）进行文档轮廓检测，随后使用计算机视觉（CV）通过三次多项式插值创建一个拓扑二维网格，并通过重新映射图像来纠正非线性失真。通过使用传统的CV方法，使文档拓扑结构恢复过程更加高效和快速，因为它所需的计算资源和内存显著减少。我们开发了一个新的自动文档去翘曲和重建的流水线，并提供了一个框架和标注数据集以展示其效率。我们的实验确认了该方法的潜力，并在视觉效果和文档可读性（OCR）以及几何恢复指标方面优于现有的基准（包括移动应用和流行的DL解决方案，如RectiNet、DocGeoNet和DocTr++）。这为进一步创建高质量的纸文档数字副本并提高OCR系统的效率铺平了道路。项目页面：https://github.com/HorizonParadox/DRCCBI
2501.03122	 | Normalizing Batch Normalization for Long-Tailed Recognition	 | Yuxiang Bao,Guoliang Kang,Linlin Yang,Xiaoyue Duan,Bo Zhao,Baochang Zhang	 | 本文提出了一种新的方法，通过标准化Batch Normalization层的参数来纠正特征偏差，从而改善网络在处理稀有类别时的性能。实验结果表明，该方法在多个长尾识别基准测试中显著优于现有方法。	 | In real-world scenarios, the number of training samples across classes usually subjects to a long-tailed distribution. The conventionally trained network may achieve unexpected inferior performance on the rare class compared to the frequent class. Most previous works attempt to rectify the network bias from the data-level or from the classifier-level. Differently, in this paper, we identify that the bias towards the frequent class may be encoded into features, i.e., the rare-specific features which play a key role in discriminating the rare class are much weaker than the frequent-specific features. Based on such an observation, we introduce a simple yet effective approach, normalizing the parameters of Batch Normalization (BN) layer to explicitly rectify the feature bias. To achieve this end, we represent the Weight/Bias parameters of a BN layer as a vector, normalize it into a unit one and multiply the unit vector by a scalar learnable parameter. Through decoupling the direction and magnitude of parameters in BN layer to learn, the Weight/Bias exhibits a more balanced distribution and thus the strength of features becomes more even. Extensive experiments on various long-tailed recognition benchmarks (i.e., CIFAR-10/100-LT, ImageNet-LT and iNaturalist 2018) show that our method outperforms previous state-of-the-arts remarkably. The code and checkpoints are available at https://github.com/yuxiangbao/NBN.	 | 在现实世界场景中，不同类别的训练样本数量通常服从长尾分布。传统训练的网络在处理稀有类别的性能可能远逊于常见类别。大多数之前的工作尝试从数据级别或分类器级别纠正网络偏差。不同的是，在本文中，我们发现对常见类别的偏见可能被编码到特征中，即用于区分稀有类别的稀有特异性特征相较于常见特异性特征要弱得多。基于这一观察，我们提出了一种简单而有效的方法，即将Batch Normalization（BN）层的参数标准化，以明确纠正特征偏差。为此，我们将BN层的权重/偏置参数表示为向量，将其归一化为单位向量，并将其乘以一个可学习的标量参数。通过解耦BN层参数的学习方向和幅度，权重/偏置表现出更均衡的分布，从而使得特征的强度更加均衡。在各种长尾识别基准测试（即CIFAR-10/100-LT、ImageNet-LT和iNaturalist 2018）上进行的广泛实验表明，我们的方法显著优于之前的最先进方法。代码和检查点可在https://github.com/yuxiangbao/NBN获取。
2501.03120	 | CAT: Content-Adaptive Image Tokenization	 | Junhong Shen,Kushal Tirumala,Michihiro Yasunaga,Ishan Misra,Luke Zettlemoyer,Lili Yu,Chunting Zhou	 | 内容自适应分词器（CAT）根据图像内容动态调整表示能力，使用较少的词元编码简单图像，并在重构性能和推理吞吐量方面优于固定比率基线。CAT结合描述性评估系统和可变长度潜在表示，提升了图像生成的扩散变换器（DiTs）的性能。	 | Most existing image tokenizers encode images into a fixed number of tokens or patches, overlooking the inherent variability in image complexity. To address this, we introduce Content-Adaptive Tokenizer (CAT), which dynamically adjusts representation capacity based on the image content and encodes simpler images into fewer tokens. We design a caption-based evaluation system that leverages large language models (LLMs) to predict content complexity and determine the optimal compression ratio for a given image, taking into account factors critical to human perception. Trained on images with diverse compression ratios, CAT demonstrates robust performance in image reconstruction. We also utilize its variable-length latent representations to train Diffusion Transformers (DiTs) for ImageNet generation. By optimizing token allocation, CAT improves the FID score over fixed-ratio baselines trained with the same flops and boosts the inference throughput by 18.5%.	 | 现有的大多数图像分词器将图像编码为固定数量的词元或块，忽视了图像复杂度的固有变异性。为了解决这个问题，我们引入了内容自适应分词器（Content-Adaptive Tokenizer, CAT），它根据图像内容动态调整表示能力，并用较少的词元编码较为简单的图像。我们设计了一个基于描述的评估系统，利用大型语言模型（LLMs）预测内容复杂度，并确定给定图像的最佳压缩比，同时考虑到影响人类感知的关键因素。CAT在具有不同压缩比的图像上进行训练，展示了在图像重构方面的稳健性能。我们还利用其可变长度的潜在表示来训练图像生成的扩散变换器（Diffusion Transformers, DiTs）。通过优化词元分配，CAT在相同的算术操作次数（flops）下比固定比率基线提高了FID分数，并提高了18.5%的推理吞吐量。
2501.03103	 | MVP: Multimodal Emotion Recognition based on Video and Physiological Signals	 | Valeriya Strizhkova,Hadi Kachmar,Hava Chaptoukaev,Raphael Kalandadze,Natia Kukhilava,Tatia Tsmindashvili,Nibras Abo-Alzahab,Maria A. Zuluaga,Michal Balazia,Antitza Dantcheva,François Brémond,Laura Ferrari	 | 本文提出了一种多模态视频与生理信号融合（MVP）架构，用于融合长输入序列的视频和生理信号以识别情绪，并在面部视频、EDA和ECG/PPG的情绪识别任务中优于现有方法。	 | Human emotions entail a complex set of behavioral, physiological and cognitive changes. Current state-of-the-art models fuse the behavioral and physiological components using classic machine learning, rather than recent deep learning techniques. We propose to fill this gap, designing the Multimodal for Video and Physio (MVP) architecture, streamlined to fuse video and physiological signals. Differently then others approaches, MVP exploits the benefits of attention to enable the use of long input sequences (1-2 minutes). We have studied video and physiological backbones for inputting long sequences and evaluated our method with respect to the state-of-the-art. Our results show that MVP outperforms former methods for emotion recognition based on facial videos, EDA, and ECG/PPG.	 | 人类情绪涉及一系列复杂的行為、生理和认知变化。当前最先进的模型使用经典机器学习来融合行为和生理成分，而不是最近的深度学习技术。我们提出填补这一空白，设计了多模态视频与生理信号融合（MVP）架构，旨在融合视频和生理信号。与其他方法不同，MVP利用注意力的优势，能够使用长输入序列（1-2分钟）。我们研究了用于输入长序列的视频和生理信号基础模型，并将我们的方法与最先进的技术进行了比较。结果显示，MVP在基于面部视频、EDA（电导反应）和ECG/PPG（心电图/光电脉搏波图）的情绪识别方面优于以前的方法。
2501.03095	 | A Novel Structure-Agnostic Multi-Objective Approach for Weight-Sharing Compression in Deep Neural Networks	 | Rasa Khosrowshahli,Shahryar Rahnamayan,Beatrice Ombuki-Berman	 | 本文提出了一种基于多目标演化算法的深度神经网络压缩框架，通过将网络权重量化为统一大小的箱子并优化性能和压缩率的两个目标，实现模型和层独立的权重共享，从而显著减少内存占用，实验结果表明该方法在CIFAR和ImageNet数据集上可分别减少13-15倍和7-9倍的内存使用。	 | Deep neural networks suffer from storing millions and billions of weights in memory post-training, making challenging memory-intensive models to deploy on embedded devices. The weight-sharing technique is one of the popular compression approaches that use fewer weight values and share across specific connections in the network. In this paper, we propose a multi-objective evolutionary algorithm (MOEA) based compression framework independent of neural network architecture, dimension, task, and dataset. We use uniformly sized bins to quantize network weights into a single codebook (lookup table) for efficient weight representation. Using MOEA, we search for Pareto optimal $k$ bins by optimizing two objectives. Then, we apply the iterative merge technique to non-dominated Pareto frontier solutions by combining neighboring bins without degrading performance to decrease the number of bins and increase the compression ratio. Our approach is model- and layer-independent, meaning the weights are mixed in the clusters from any layer, and the uniform quantization method used in this work has $O(N)$ complexity instead of non-uniform quantization methods such as k-means with $O(Nkt)$ complexity. In addition, we use the center of clusters as the shared weight values instead of retraining shared weights, which is computationally expensive. The advantage of using evolutionary multi-objective optimization is that it can obtain non-dominated Pareto frontier solutions with respect to performance and shared weights. The experimental results show that we can reduce the neural network memory by $13.72 \sim14.98 \times$ on CIFAR-10, $11.61 \sim 12.99\times$ on CIFAR-100, and $7.44 \sim 8.58\times$ on ImageNet showcasing the effectiveness of the proposed deep neural network compression framework.	 | 深度神经网络在训练后需要存储数百万乃至数十亿的权重，这使得内存密集型模型难以部署在嵌入式设备上。权重共享是一种流行的数据压缩方法，它通过使用较少的权重值并将其共享在特定连接上来减少权重的数量。本文提出了一种独立于神经网络架构、维度、任务和数据集的多目标演化算法（MOEA）基于的压缩框架。我们使用统一大小的箱子将网络权重量化为单一的代码本（查找表），以便高效地表示权重。利用MOEA，我们通过优化两个目标来搜索Pareto最优的$k$个箱子。然后，我们应用迭代合并技术，将非支配Pareto前沿解中的相邻箱子合并，以减少箱子的数量并提高压缩率，同时不降低性能。我们的方法是模型和层独立的，意味着权重可以混合在来自任何层的簇中，而本文采用的统一量化方法具有$O(N)$的复杂度，相比之下，k-means等非统一量化方法的复杂度为$O(Nkt)$。此外，我们使用簇的中心作为共享权重值，而不是重新训练共享权重，这在计算上更为昂贵。使用演化多目标优化的优势在于，它可以同时在性能和共享权重方面获得非支配Pareto前沿解。实验结果表明，我们的方法可以将CIFAR-10上的神经网络内存减少$13.72 \sim 14.98 \times$，CIFAR-100上的内存减少$11.61 \sim 12.99 \times$，ImageNet上的内存减少$7.44 \sim 8.58 \times$，从而展示了所提出深度神经网络压缩框架的有效性。
2501.03059	 | Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation	 | Guy Yariv,Yuval Kirstain,Amit Zohar,Shelly Sheynin,Yaniv Taigman,Yossi Adi,Sagie Benaim,Adam Polyak	 | 该研究提出了一种两阶段框架来解决从图像到视频的生成问题，通过引入基于掩码的运动轨迹作为中间表示，并使用对象级注意力机制来生成连贯且逼真的视频，尤其适用于多对象场景。该方法在多个挑战性基准上取得了最优结果，并引入了一个新的基准用于评估单对象和多对象的I2V生成任务。	 | We consider the task of Image-to-Video (I2V) generation, which involves transforming static images into realistic video sequences based on a textual description. While recent advancements produce photorealistic outputs, they frequently struggle to create videos with accurate and consistent object motion, especially in multi-object scenarios. To address these limitations, we propose a two-stage compositional framework that decomposes I2V generation into: (i) An explicit intermediate representation generation stage, followed by (ii) A video generation stage that is conditioned on this representation. Our key innovation is the introduction of a mask-based motion trajectory as an intermediate representation, that captures both semantic object information and motion, enabling an expressive but compact representation of motion and semantics. To incorporate the learned representation in the second stage, we utilize object-level attention objectives. Specifically, we consider a spatial, per-object, masked-cross attention objective, integrating object-specific prompts into corresponding latent space regions and a masked spatio-temporal self-attention objective, ensuring frame-to-frame consistency for each object. We evaluate our method on challenging benchmarks with multi-object and high-motion scenarios and empirically demonstrate that the proposed method achieves state-of-the-art results in temporal coherence, motion realism, and text-prompt faithfulness. Additionally, we introduce \benchmark, a new challenging benchmark for single-object and multi-object I2V generation, and demonstrate our method's superiority on this benchmark. Project page is available at https://guyyariv.github.io/TTM/.	 | 我们考虑了从图像到视频（I2V）生成的任务，该任务涉及根据文本描述将静态图像转换为真实的视频序列。尽管最近的进展产生了高度逼真的输出，但在多对象场景中，它们在创建准确且一致的对象运动的视频方面经常遇到困难。为了应对这些局限性，我们提出了一种两阶段的组合框架，将I2V生成分解为：（i）一个显式的中间表示生成阶段，接着是（ii）一个基于此表示的视频生成阶段。我们的主要创新之处在于引入了一种基于掩码的运动轨迹作为中间表示，这种表示既捕捉了语义对象信息又捕捉了运动，从而能够以表达性但紧凑的方式表示运动和语义。为了在第二阶段中利用所学的表示，我们使用了对象级注意力目标。具体而言，我们考虑了一个空间上的、针对每个对象的、掩码交叉注意力目标，将对象特定的提示整合到相应的潜在空间区域中，并且引入了一个掩码时空自注意力目标，确保每个对象在帧与帧之间的一致性。我们在具有多对象和高运动场景的具有挑战性的基准上评估了我们的方法，并实验证明，所提出的方法在时间连贯性、运动逼真度和文本提示忠实度方面达到了最先进的结果。此外，我们还引入了一个新的基准 \benchmark，用于单对象和多对象的I2V生成，并在该基准上展示了我们方法的优越性。项目页面可在 https://guyyariv.github.io/TTM/ 中找到。
2501.03006	 | TransPixar: Advancing Text-to-Video Generation with Transparency	 | Luozhou Wang,Yijun Li,Zhifei Chen,Jui-Hsien Wang,Zhifei Zhang,He Zhang,Zhe Lin,Yingcong Chen	 | TransPixar方法通过利用扩散变换器架构和基于LoRA的微调，扩展了预训练的视频模型以生成包含透明度通道（RGBA）的视频，从而实现了视觉效果（VFX）中烟雾和反射等元素的无缝融合，进一步推动了VFX和互动内容创作的发展。	 | Text-to-video generative models have made significant strides, enabling diverse applications in entertainment, advertising, and education. However, generating RGBA video, which includes alpha channels for transparency, remains a challenge due to limited datasets and the difficulty of adapting existing models. Alpha channels are crucial for visual effects (VFX), allowing transparent elements like smoke and reflections to blend seamlessly into scenes. We introduce TransPixar, a method to extend pretrained video models for RGBA generation while retaining the original RGB capabilities. TransPixar leverages a diffusion transformer (DiT) architecture, incorporating alpha-specific tokens and using LoRA-based fine-tuning to jointly generate RGB and alpha channels with high consistency. By optimizing attention mechanisms, TransPixar preserves the strengths of the original RGB model and achieves strong alignment between RGB and alpha channels despite limited training data. Our approach effectively generates diverse and consistent RGBA videos, advancing the possibilities for VFX and interactive content creation.	 | 文本到视频生成模型已经取得了显著的进步，使其在娱乐、广告和教育等多个领域得到了广泛应用。然而，生成包含透明度通道（RGBA）的视频仍然是一个挑战，主要受限于数据集的有限性和现有模型难以适应的问题。透明度通道对于视觉效果（VFX）至关重要，它允许像烟雾和反射等透明元素在场景中无缝融合。我们提出了TransPixar方法，该方法能够在保留原始RGB能力的同时，扩展预训练的视频模型以生成RGBA视频。TransPixar利用了扩散变换器（DiT）架构，通过引入专有的透明度通道标记，并采用基于LoRA的微调方法，同时生成高一致性的RGB和透明度通道。通过优化注意力机制，TransPixar保持了原始RGB模型的优点，并在有限的训练数据下实现了RGB和透明度通道的强对齐。我们的方法能够有效生成多样化且一致的RGBA视频，进一步推动了VFX和互动内容创作的可能性。
2501.03005	 | PiLaMIM: Toward Richer Visual Representations by Integrating Pixel and Latent Masked Image Modeling	 | Junmyeong Lee,Eui Jun Hwang,Sukmin Cho,Jong C. Park	 | PiLaMIM是一种结合像素掩码图像建模和隐空间掩码图像建模优势的统一框架，通过单一编码器和两个不同解码器同时捕获低级和高级视觉特征，从而在多种视觉任务中表现出色。	 | In Masked Image Modeling (MIM), two primary methods exist: Pixel MIM and Latent MIM, each utilizing different reconstruction targets, raw pixels and latent representations, respectively. Pixel MIM tends to capture low-level visual details such as color and texture, while Latent MIM focuses on high-level semantics of an object. However, these distinct strengths of each method can lead to suboptimal performance in tasks that rely on a particular level of visual features. To address this limitation, we propose PiLaMIM, a unified framework that combines Pixel MIM and Latent MIM to integrate their complementary strengths. Our method uses a single encoder along with two distinct decoders: one for predicting pixel values and another for latent representations, ensuring the capture of both high-level and low-level visual features. We further integrate the CLS token into the reconstruction process to aggregate global context, enabling the model to capture more semantic information. Extensive experiments demonstrate that PiLaMIM outperforms key baselines such as MAE, I-JEPA and BootMAE in most cases, proving its effectiveness in extracting richer visual representations.	 | 在掩码图像建模（MIM）中，存在两种主要方法：像素MIM和隐空间MIM，分别使用不同的重建目标——原始像素和隐空间表示。像素MIM倾向于捕获低级视觉细节，如颜色和纹理，而隐空间MIM则专注于物体的高级语义。然而，每种方法的这些独特优势可能导致在依赖特定层级视觉特征的任务中表现不佳。为了解决这一局限性，我们提出了一种名为PiLaMIM的统一框架，它结合了像素MIM和隐空间MIM，以整合它们互补的优势。我们的方法使用一个单一的编码器，搭配两个不同的解码器：一个用于预测像素值，另一个用于隐空间表示，确保同时捕获高级和低级视觉特征。我们进一步将CLS标记整合到重建过程中，以聚集全局上下文，从而使模型能够捕获更多语义信息。广泛的实验证明，PiLaMIM在大多数情况下优于MAE、I-JEPA和BootMAE等关键基准模型，证明了其在提取更丰富视觉表示方面的有效性。
2501.02990	 | SurgRIPE challenge: Benchmark of Surgical Robot Instrument Pose Estimation	 | Haozheng Xu,Alistair Weld,Chi Xu,Alfie Roddan,Joao Cartucho,Mert Asim Karaoglu,Alexander Ladikos,Yangke Li,Yiping Li,Daiyun Shen,Shoujie Yang,Geonhee Lee,Seyeon Park,Jongho Shin,Young-Gon Kim,Lucy Fothergill,Dominic Jones,Pietro Valdastri,Duygu Sarikaya,Stamatia Giannarou	 | SurgRIPE挑战赛在2023年MICCAI会议上旨在提供真实手术视频数据并建立无标记姿态估计方法的评估标准，促进了先进算法的发展和集成，提高了手术器械姿态估计的准确性和鲁棒性。	 | Accurate instrument pose estimation is a crucial step towards the future of robotic surgery, enabling applications such as autonomous surgical task execution. Vision-based methods for surgical instrument pose estimation provide a practical approach to tool tracking, but they often require markers to be attached to the instruments. Recently, more research has focused on the development of marker-less methods based on deep learning. However, acquiring realistic surgical data, with ground truth instrument poses, required for deep learning training, is challenging. To address the issues in surgical instrument pose estimation, we introduce the Surgical Robot Instrument Pose Estimation (SurgRIPE) challenge, hosted at the 26th International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) in 2023. The objectives of this challenge are: (1) to provide the surgical vision community with realistic surgical video data paired with ground truth instrument poses, and (2) to establish a benchmark for evaluating markerless pose estimation methods. The challenge led to the development of several novel algorithms that showcased improved accuracy and robustness over existing methods. The performance evaluation study on the SurgRIPE dataset highlights the potential of these advanced algorithms to be integrated into robotic surgery systems, paving the way for more precise and autonomous surgical procedures. The SurgRIPE challenge has successfully established a new benchmark for the field, encouraging further research and development in surgical robot instrument pose estimation.	 | 精确的器械姿态估计是未来机器人手术的关键步骤，能够使自主手术任务执行成为可能。基于视觉的方法在手术器械姿态估计中提供了实用的工具跟踪手段，但通常需要在器械上附加标记。近年来，越来越多的研究集中在基于深度学习的无标记方法的开发上。然而，获取用于深度学习训练的真实手术数据及其对应的器械姿态真实值是非常具有挑战性的。为解决手术器械姿态估计中的问题，我们于2023年26届国际医学图像计算和计算机辅助介入会议（MICCAI）上提出了手术机器人器械姿态估计（SurgRIPE）挑战赛。该挑战赛的目标包括：（1）为手术视觉社区提供带有真实器械姿态标注的真实手术视频数据；（2）建立一种评估无标记姿态估计方法的标准。挑战赛促进了多种新型算法的发展，这些算法在准确性和鲁棒性方面超过了现有方法。 SurgRIPE 数据集上的性能评估研究突显了这些先进算法被集成到机器人手术系统中的潜力，为更精确和自主的手术程序铺平了道路。SurgRIPE 挑战赛成功地为该领域建立了一个新基准，促进了手术机器人器械姿态估计领域的进一步研究和发展。
2501.02976	 | STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution	 | Rui Xie,Yinhong Liu,Penghao Zhou,Chen Zhao,Jun Zhou,Kai Zhang,Zhenyu Zhang,Jian Yang,Zhenheng Yang,Ying Tai	 | 时空增强模型（\textbf{\name}）通过集成文本到视频（T2V）模型和局部信息增强模块（LIEM）来改善视频超分辨率，同时引入动态频率（DF）损失以增强保真度，从而在合成和实际数据集上优于现有方法。	 | Image diffusion models have been adapted for real-world video super-resolution to tackle over-smoothing issues in GAN-based methods. However, these models struggle to maintain temporal consistency, as they are trained on static images, limiting their ability to capture temporal dynamics effectively. Integrating text-to-video (T2V) models into video super-resolution for improved temporal modeling is straightforward. However, two key challenges remain: artifacts introduced by complex degradations in real-world scenarios, and compromised fidelity due to the strong generative capacity of powerful T2V models (\textit{e.g.}, CogVideoX-5B). To enhance the spatio-temporal quality of restored videos, we introduce\textbf{~\name} (\textbf{S}patial-\textbf{T}emporal \textbf{A}ugmentation with T2V models for \textbf{R}eal-world video super-resolution), a novel approach that leverages T2V models for real-world video super-resolution, achieving realistic spatial details and robust temporal consistency. Specifically, we introduce a Local Information Enhancement Module (LIEM) before the global attention block to enrich local details and mitigate degradation artifacts. Moreover, we propose a Dynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focus on different frequency components across diffusion steps. Extensive experiments demonstrate\textbf{~\name}~outperforms state-of-the-art methods on both synthetic and real-world datasets.	 | 图像扩散模型已被应用于实际视频超分辨率，以解决基于GAN的方法中过度平滑的问题。然而，这些模型在保持时间一致性方面存在问题，因为它们是基于静态图像进行训练的，这限制了它们有效捕捉时间动态的能力。将文本到视频（T2V）模型集成到视频超分辨率中以改进时间建模是直接的。然而，仍存在两个关键挑战：真实场景中复杂退化引入的伪影，以及强大的T2V模型（例如，CogVideoX-5B）的强大生成能力导致的保真度下降。为提高恢复视频的空间-时间质量，我们引入了\textbf{\name}（时空增强模型，利用T2V模型进行实际视频超分辨率），这是一种新颖的方法，利用T2V模型实现现实的空间细节和稳健的时间一致性。具体来说，在全局注意力块之前引入了一个局部信息增强模块（LIEM），以丰富局部细节并减轻退化伪影。此外，我们提出了一种动态频率（DF）损失来增强保真度，引导模型在扩散步中关注不同频率成分。广泛实验表明，\textbf{\name}在合成数据集和实际数据集上均优于现有最先进的方法。
2501.02973	 | HaWoR: World-Space Hand Motion Reconstruction from Egocentric Videos	 | Jinglei Zhang,Jiankang Deng,Chao Ma,Rolandos Alexandros Potamias	 | HaWoR是一种从第一人称视频中在世界坐标系中进行高保真手部运动重建的方法，通过在相机空间中重建手部运动和在世界坐标系中估计相机轨迹，并使用自适应的第一人称SLAM框架和新颖的运动填充网络来实现这一目标，从而在不同基准数据集中取得了最先进的性能。	 | Despite the advent in 3D hand pose estimation, current methods predominantly focus on single-image 3D hand reconstruction in the camera frame, overlooking the world-space motion of the hands. Such limitation prohibits their direct use in egocentric video settings, where hands and camera are continuously in motion. In this work, we propose HaWoR, a high-fidelity method for hand motion reconstruction in world coordinates from egocentric videos. We propose to decouple the task by reconstructing the hand motion in the camera space and estimating the camera trajectory in the world coordinate system. To achieve precise camera trajectory estimation, we propose an adaptive egocentric SLAM framework that addresses the shortcomings of traditional SLAM methods, providing robust performance under challenging camera dynamics. To ensure robust hand motion trajectories, even when the hands move out of view frustum, we devise a novel motion infiller network that effectively completes the missing frames of the sequence. Through extensive quantitative and qualitative evaluations, we demonstrate that HaWoR achieves state-of-the-art performance on both hand motion reconstruction and world-frame camera trajectory estimation under different egocentric benchmark datasets. Code and models are available on https://hawor-project.github.io/ .	 | 尽管在3D手部姿态估计方面取得了进展，当前的方法大多集中在单张图像的相机坐标系中的3D手部重建，而忽视了手部在世界空间的运动。这种局限性阻止了它们在第一人称视频设置中的直接使用，在这种设置中，手和相机持续处于运动状态。在这项工作中，我们提出了一种名为HaWoR的方法，该方法能够从第一人称视频中在世界坐标系中进行高保真的手部运动重建。我们通过将任务分解为在相机空间中重建手部运动和在世界坐标系中估计相机轨迹来实现这一目标。为了精确估计相机轨迹，我们提出了一种自适应的第一人称SLAM框架，该框架解决了传统SLAM方法的不足，能够在复杂相机运动下提供稳健的表现。为了确保即使手部不在视锥内移动时手部运动轨迹的稳健性，我们设计了一种新颖的运动填充网络，能够有效完成序列中的缺失帧。通过广泛的定量和定性评估，我们证明了HaWoR在不同第一人称基准数据集中，在手部运动重建和世界坐标系中相机轨迹估计方面均达到了最先进的性能。代码和模型可在https://hawor-project.github.io/获取。
2501.02966	 | Human Gaze Boosts Object-Centered Representation Learning	 | Timothy Schaumlöffel,Arthur Aubret,Gemma Roig,Jochen Triesch	 | 这项研究通过使用Ego4D数据集和模拟的自我为中心视觉体验，探讨了集中于中心视觉信息是否能增强以物体为中心的视觉表示，并表明关注中心视觉可产生更好的物体表示，同时SSL模型利用注视运动的时间动态来构建更强的视觉表示，标志着向生物启发的视觉表示学习迈出了一大步。	 | Recent self-supervised learning (SSL) models trained on human-like egocentric visual inputs substantially underperform on image recognition tasks compared to humans. These models train on raw, uniform visual inputs collected from head-mounted cameras. This is different from humans, as the anatomical structure of the retina and visual cortex relatively amplifies the central visual information, i.e. around humans' gaze location. This selective amplification in humans likely aids in forming object-centered visual representations. Here, we investigate whether focusing on central visual information boosts egocentric visual object learning. We simulate 5-months of egocentric visual experience using the large-scale Ego4D dataset and generate gaze locations with a human gaze prediction model. To account for the importance of central vision in humans, we crop the visual area around the gaze location. Finally, we train a time-based SSL model on these modified inputs. Our experiments demonstrate that focusing on central vision leads to better object-centered representations. Our analysis shows that the SSL model leverages the temporal dynamics of the gaze movements to build stronger visual representations. Overall, our work marks a significant step toward bio-inspired learning of visual representations.	 | 最近，基于人类中心视角输入训练的自监督学习（SSL）模型在图像识别任务上的表现远不如人类。这些模型是在来自头戴式摄像头收集的原始、均匀视觉输入上进行训练的。这与人类的情况不同，因为视网膜和视觉皮层的解剖结构相对增强了中心视觉信息，即主要是围绕人类注视点的信息。这种在人类中的选择性放大可能有助于形成以物体为中心的视觉表示。在此，我们探讨了集中于中心视觉信息是否可以增强以自我为中心的视觉物体学习。我们使用大规模的Ego4D数据集模拟了5个月的自我为中心的视觉体验，并使用人类注视预测模型生成注视点。为了强调人类中心视觉的重要性，我们截取了围绕注视点的视觉区域。最后，我们在这些修改后的输入上训练了一个基于时间的SSL模型。我们的实验表明，关注中心视觉可以产生更好的以物体为中心的表示。我们的分析显示，SSL模型利用注视运动的时间动态来构建更强的视觉表示。总体而言，我们的工作标志着向基于生物启发的视觉表示学习迈出了一大步。
2501.02964	 | Socratic Questioning: Learn to Self-guide Multimodal Reasoning in the Wild	 | Wanpeng Hu,Haodi Liu,Lin Chen,Feng Zhou,Changming Xiao,Qi Yang,Changshui Zhang	 | 本文提出了一种名为苏格拉底式提问（Socratic Questioning, SQ）的多轮训练和推理框架，结合Chain of Thought（CoT）和视觉指令调优方法，以提高轻量级多模态大型语言模型在复杂视觉推理任务中的性能，并显著降低了幻觉现象的发生，同时公开了一个新的多模态数据集CapQA用于评估。	 | Complex visual reasoning remains a key challenge today. Typically, the challenge is tackled using methodologies such as Chain of Thought (COT) and visual instruction tuning. However, how to organically combine these two methodologies for greater success remains unexplored. Also, issues like hallucinations and high training cost still need to be addressed. In this work, we devise an innovative multi-round training and reasoning framework suitable for lightweight Multimodal Large Language Models (MLLMs). Our self-questioning approach heuristically guides MLLMs to focus on visual clues relevant to the target problem, reducing hallucinations and enhancing the model's ability to describe fine-grained image details. This ultimately enables the model to perform well in complex visual reasoning and question-answering tasks. We have named this framework Socratic Questioning(SQ). To facilitate future research, we create a multimodal mini-dataset named CapQA, which includes 1k images of fine-grained activities, for visual instruction tuning and evaluation, our proposed SQ method leads to a 31.2% improvement in the hallucination score. Our extensive experiments on various benchmarks demonstrate SQ's remarkable capabilities in heuristic self-questioning, zero-shot visual reasoning and hallucination mitigation. Our model and code will be publicly available.	 | 复杂视觉推理仍然是当今的一个主要挑战。通常，这一挑战是通过使用诸如Chain of Thought（CoT）和视觉指令调优等方法来应对的。然而，如何将这两种方法有机地结合起来以取得更大的成功尚未被探索。此外，幻觉和高训练成本等问题仍然需要解决。在本工作中，我们设计了一个适用于轻量级多模态大型语言模型（MLLMs）的创新多轮训练和推理框架。我们的自提问方法启发式地指导MLLMs专注于与目标问题相关的视觉线索，从而减少幻觉并增强模型描述细粒度图像细节的能力。最终，这使得模型在复杂视觉推理和问答任务中表现出色。我们将这种框架命名为苏格拉底式提问（Socratic Questioning, SQ）。为了促进未来的研究，我们创建了一个包含1000张细粒度活动图像的多模态小型数据集CapQA，用于视觉指令调优和评估，我们提出的SQ方法在幻觉分数上提高了31.2%。我们对各种基准的广泛实验展示了SQ在启发式自提问、零样本视觉推理和幻觉缓解方面的卓越能力。我们的模型和代码将公开提供。
2501.02962	 | SceneVTG++: Controllable Multilingual Visual Text Generation in the Wild	 | Jiawei Liu,Yuanzhi Zhu,Feiyu Gao,Zhibo Yang,Peng Wang,Junyang Lin,Xinggang Wang,Wenyu Liu	 | 本文提出了一种两阶段方法SceneVTG++，旨在生成符合真实性、合理性、实用性和可控性的自然场景文本，并通过实验验证了其在生成高质量文本方面的优越性能及其在OCR任务中的实用价值。	 | Generating visual text in natural scene images is a challenging task with many unsolved problems. Different from generating text on artificially designed images (such as posters, covers, cartoons, etc.), the text in natural scene images needs to meet the following four key criteria: (1) Fidelity: the generated text should appear as realistic as a photograph and be completely accurate, with no errors in any of the strokes. (2) Reasonability: the text should be generated on reasonable carrier areas (such as boards, signs, walls, etc.), and the generated text content should also be relevant to the scene. (3) Utility: the generated text can facilitate to the training of natural scene OCR (Optical Character Recognition) tasks. (4) Controllability: The attribute of the text (such as font and color) should be controllable as needed.In this paper, we propose a two stage method, SceneVTG++, which simultaneously satisfies the four aspects mentioned above. SceneVTG++ consists of a Text Layout and Content Generator (TLCG) and a Controllable Local Text Diffusion (CLTD). The former utilizes the world knowledge of multi modal large language models to find reasonable text areas and recommend text content according to the nature scene background images, while the latter generates controllable multilingual text based on the diffusion model. Through extensive experiments, we respectively verified the effectiveness of TLCG and CLTD, and demonstrated the state-of-the-art text generation performance of SceneVTG++. In addition, the generated images have superior utility in OCR tasks like text detection and text recognition. Codes and datasets will be available.	 | 在自然场景图像中生成视觉文本是一项具有许多未解决问题的挑战性任务。与在人工设计的图像（如海报、封面、卡通等）上生成文本不同，自然场景图像中的文本需要满足以下四个关键标准：   (1) 真实性：生成的文本应看起来如同照片一般真实，没有任何笔画错误。   (2) 合理性：文本应生成在合理的载体区域（如板子、标志、墙壁等），并且生成的文本内容也应与场景相关。   (3) 实用性：生成的文本可以促进自然场景OCR（光学字符识别）任务的训练。   (4) 可控性：文本的属性（如字体和颜色）应可根据需要进行控制。   在本文中，我们提出了一种两阶段方法，SceneVTG++，该方法同时满足上述四个方面的要求。SceneVTG++由文本布局和内容生成器（TLCG）和可控局部文本扩散（CLTD）组成。TLCG 利用多模态大语言模型的世界知识，根据自然场景背景图像找到合理的文本区域，并推荐相应的文本内容，而CLTD则基于扩散模型生成可控制的多语言文本。通过广泛的实验，我们分别验证了TLCG和CLTD的有效性，并展示了SceneVTG++在文本生成方面的先进性能。此外，生成的图像在文本检测和文本识别等OCR任务中具有优越的实用性。代码和数据集将可供获取。
2501.02955	 | MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models	 | Wenyi Hong,Yean Cheng,Zhuoyi Yang,Weihan Wang,Lefan Wang,Xiaotao Gu,Shiyu Huang,Yuxiao Dong,Jie Tang	 | 本文介绍了MotionBench，这是一个全面的评估基准，旨在评估视频理解模型在细粒度运动理解方面的性能，并通过多种类型的运动导向问题来测试模型的能力。研究发现现有视觉语言模型在这方面的表现不佳，为此提出了一种新的Through-Encoder（TE）融合方法来改进模型的运动理解能力。	 | In recent years, vision language models (VLMs) have made significant advancements in video understanding. However, a crucial capability - fine-grained motion comprehension - remains under-explored in current benchmarks. To address this gap, we propose MotionBench, a comprehensive evaluation benchmark designed to assess the fine-grained motion comprehension of video understanding models. MotionBench evaluates models' motion-level perception through six primary categories of motion-oriented question types and includes data collected from diverse sources, ensuring a broad representation of real-world video content. Experimental results reveal that existing VLMs perform poorly in understanding fine-grained motions. To enhance VLM's ability to perceive fine-grained motion within a limited sequence length of LLM, we conduct extensive experiments reviewing VLM architectures optimized for video feature compression and propose a novel and efficient Through-Encoder (TE) Fusion method. Experiments show that higher frame rate inputs and TE Fusion yield improvements in motion understanding, yet there is still substantial room for enhancement. Our benchmark aims to guide and motivate the development of more capable video understanding models, emphasizing the importance of fine-grained motion comprehension. Project page: https://motion-bench.github.io .	 | 近年来，视觉语言模型（VLMs）在视频理解方面取得了显著进步。然而，一种关键能力——细粒度运动理解——在当前的基准测试中仍然被忽视。为了解决这一问题，我们提出了MotionBench，这是一个全面的评估基准，旨在评估视频理解模型的细粒度运动理解能力。MotionBench 通过六大类以运动为导向的问题类型来评估模型的运动级感知，并包含了来自多种来源的数据，确保了对真实世界视频内容的广泛代表。实验结果表明，现有的VLMs在理解细粒度运动方面表现不佳。为了增强VLM在有限序列长度LLM中感知细粒度运动的能力，我们进行了广泛的实验，审查了优化视频特征压缩的VLM架构，并提出了一种新颖且高效的Through-Encoder（TE）融合方法。实验显示，更高帧率的输入和TE融合方法在运动理解方面有所改进，但仍存在很大的提升空间。我们的基准旨在引导和激励开发更强大的视频理解模型，强调细粒度运动理解的重要性。项目页面：https://motion-bench.github.io 。
2501.02937	 | 4D-CS: Exploiting Cluster Prior for 4D Spatio-Temporal LiDAR Semantic Segmentation	 | Jiexi Zhong,Zhiheng Li,Yubo Cui,Zheng Fang	 | 本文提出了一种名为4D-CS的双分支网络，通过生成跨多帧的簇标签来增强点云的时空一致性，从而实现更一致的语义分割，特别是在处理移动对象时效果显著，并在SemanticKITTI和nuScenes数据集上取得了最先进的成果。	 | Semantic segmentation of LiDAR points has significant value for autonomous driving and mobile robot systems. Most approaches explore spatio-temporal information of multi-scan to identify the semantic classes and motion states for each point. However, these methods often overlook the segmentation consistency in space and time, which may result in point clouds within the same object being predicted as different categories. To handle this issue, our core idea is to generate cluster labels across multiple frames that can reflect the complete spatial structure and temporal information of objects. These labels serve as explicit guidance for our dual-branch network, 4D-CS, which integrates point-based and cluster-based branches to enable more consistent segmentation. Specifically, in the point-based branch, we leverage historical knowledge to enrich the current feature through temporal fusion on multiple views. In the cluster-based branch, we propose a new strategy to produce cluster labels of foreground objects and apply them to gather point-wise information to derive cluster features. We then merge neighboring clusters across multiple scans to restore missing features due to occlusion. Finally, in the point-cluster fusion stage, we adaptively fuse the information from the two branches to optimize segmentation results. Extensive experiments confirm the effectiveness of the proposed method, and we achieve state-of-the-art results on the multi-scan semantic and moving object segmentation on SemanticKITTI and nuScenes datasets. The code will be available at https://github.com/NEU-REAL/4D-CS.git.	 | 基于LiDAR点的语义分割对于自动驾驶和移动机器人系统具有重要价值。大多数方法通过探索多扫描的时空信息来识别每个点的语义类别和运动状态。然而，这些方法往往忽视了时空中的分割一致性，可能会导致相同对象内的点云被预测为不同的类别。为了解决这一问题，我们的核心理念是生成跨多帧的簇标签，这些标签能够反映物体的完整空间结构和时间信息。这些标签作为双分支网络4D-CS的显式指导，该网络集成了基于点和基于簇的分支，以实现更一致的分割。具体而言，在基于点的分支中，我们通过多视角的时序融合利用历史知识来丰富当前特征。在基于簇的分支中，我们提出了一种新的策略来生成前景物体的簇标签，并将其应用于收集点级信息以推导簇特征。然后，我们合并多扫描中的相邻簇以恢复由于遮挡而缺失的特征。最后，在点-簇融合阶段，我们适应性地融合两个分支的信息以优化分割结果。广泛的实验验证了所提出方法的有效性，我们在SemanticKITTI和nuScenes数据集上的多扫描语义分割和移动物体分割上取得了最先进的成果。源代码可在https://github.com/NEU-REAL/4D-CS.git获取。
2501.02922	 | Label-free Concept Based Multiple Instance Learning for Gigapixel Histopathology	 | Susu Sun,Leslie Tessier,Frédérique Meeuwsen,Clément Grisi,Dominique van Midden,Geert Litjens,Christian F. Baumgartner	 | 该研究提出了一种新颖的固有可解释的多实例学习（MIL）方法——Concept MIL，它利用视觉-语言模型直接从图像特征中预测病理概念，并通过线性组合WSI的前K个片段来生成解释，从而在两个广泛使用的病理学数据集上达到了与最先进的模型相当的性能，并获得了用户的研究验证。	 | Multiple Instance Learning (MIL) methods allow for gigapixel Whole-Slide Image (WSI) analysis with only slide-level annotations. Interpretability is crucial for safely deploying such algorithms in high-stakes medical domains. Traditional MIL methods offer explanations by highlighting salient regions. However, such spatial heatmaps provide limited insights for end users. To address this, we propose a novel inherently interpretable WSI-classification approach that uses human-understandable pathology concepts to generate explanations. Our proposed Concept MIL model leverages recent advances in vision-language models to directly predict pathology concepts based on image features. The model's predictions are obtained through a linear combination of the concepts identified on the top-K patches of a WSI, enabling inherent explanations by tracing each concept's influence on the prediction. In contrast to traditional concept-based interpretable models, our approach eliminates the need for costly human annotations by leveraging the vision-language model. We validate our method on two widely used pathology datasets: Camelyon16 and PANDA. On both datasets, Concept MIL achieves AUC and accuracy scores over 0.9, putting it on par with state-of-the-art models. We further find that 87.1\% (Camelyon16) and 85.3\% (PANDA) of the top 20 patches fall within the tumor region. A user study shows that the concepts identified by our model align with the concepts used by pathologists, making it a promising strategy for human-interpretable WSI classification.	 | 多实例学习（MIL）方法允许使用仅在幻灯片级别标注的超像素全切片图像（WSI）进行分析。可解释性对于在高风险医疗领域安全部署此类算法至关重要。传统的MIL方法通过突出显示显着区域来提供解释，但这样的空间热图对最终用户提供的见解有限。为了解决这一问题，我们提出了一种新颖的固有可解释的WSI分类方法，该方法使用人类可理解的病理概念生成解释。我们提出的Concept MIL模型利用最近在视觉-语言模型方面的进展直接根据图像特征来预测病理概念。模型的预测是通过将WSI的前K个片段中识别的概念的线性组合获得的，从而可以通过追踪每个概念对预测的影响来提供固有的解释。与传统的基于概念的可解释模型不同，我们的方法通过利用视觉-语言模型消除了对昂贵的人工标注的需求。我们在两个广泛使用的病理学数据集Camelyon16和PANDA上验证了我们的方法。在两个数据集上，Concept MIL的AUC和精度得分均超过0.9，与最先进的模型相当。进一步的研究发现，在Camelyon16数据集的前20个片段中有87.1%（Camelyon16）和在PANDA数据集中有85.3%（PANDA）位于肿瘤区域。用户研究显示，我们的模型识别的概念与病理学家使用的概念相一致，这使其成为一种有希望的人机可解释的WSI分类策略。
2501.02921	 | Unsupervised Tomato Split Anomaly Detection using Hyperspectral Imaging and Variational Autoencoders	 | Mahmoud Abdulsalam,Usman Zahidi,Bradley Hurst,Simon Pearson,Grzegorz Cielniak,James Brown	 | 在温室农业中，番茄开裂是一个重大挑战，影响产品质量。通过使用带有超高光谱输入的定制变分自编码器，研究确定了530nm至550nm的波长范围适用于检测番茄开裂，并能通过分析重构损失来识别和估计异常区域。	 | Tomato anomalies/damages pose a significant challenge in greenhouse farming. While this method of cultivation benefits from efficient resource utilization, anomalies can significantly degrade the quality of farm produce. A common anomaly associated with tomatoes is splitting, characterized by the development of cracks on the tomato skin, which degrades its quality. Detecting this type of anomaly is challenging due to dynamic variations in appearance and sizes, compounded by dataset scarcity. We address this problem in an unsupervised manner by utilizing a tailored variational autoencoder (VAE) with hyperspectral input. Preliminary analysis of the dataset enabled us to select the optimal range of wavelengths for detecting this anomaly. Our findings indicate that the 530nm - 550nm range is suitable for identifying tomato dry splits. The analysis on reconstruction loss allow us to not only detect the anomalies but also to some degree estimate the anomalous regions.	 | 番茄异常/损伤在温室农业中构成了重大挑战。虽然这种栽培方法能够高效利用资源，但异常现象会显著降低农产品的质量。番茄常见的异常之一是开裂，表现为番茄表皮出现裂纹，从而降低其质量。检测这种类型的异常具有挑战性，因为其外观和大小的变化存在动态差异，并且数据集稀缺。我们通过利用带有超高光谱输入的定制变分自编码器（VAE）以无监督的方式解决了这一问题。对数据集的初步分析使我们能够选择合适的波长范围以检测这种异常。我们的研究结果表明，530nm至550nm的波长范围适用于识别番茄干裂。通过分析重构损失，我们不仅能检测到异常，还能在一定程度上估计异常区域。
2501.02916	 | Spiking monocular event based 6D pose estimation for space application	 | Jonathan Courtois,Benoît Miramond,Alain Pegatoquet	 | 本文介绍了一种基于事件的航天器姿态估计算法，利用尖峰神经网络和事件驱动处理技术，提出了SEENIC数据集，并展示了其在位置和旋转误差上的良好 performance，为全事件驱动的嵌入式姿态估计奠定了基础。	 | With the growing interest in on On-orbit servicing (OOS) and Active Debris Removal (ADR) missions, spacecraft poses estimation algorithms are being developed using deep learning to improve the precision of this complex task and find the most efficient solution. With the advances of bio-inspired low-power solutions, such a spiking neural networks and event-based processing and cameras, and their recent work for space applications, we propose to investigate the feasibility of a fully event-based solution to improve event-based pose estimation for spacecraft. In this paper, we address the first event-based dataset SEENIC with real event frames captured by an event-based camera on a testbed. We show the methods and results of the first event-based solution for this use case, where our small spiking end-to-end network (S2E2) solution achieves interesting results over 21cm position error and 14degree rotation error, which is the first step towards fully event-based processing for embedded spacecraft pose estimation.	 | 随着对在轨服务（On-orbit Servicing, OOS）和主动碎片移除（Active Debris Removal, ADR）任务的兴趣日益增长，正在开发使用深度学习的航天器姿态估计算法，以提高这一复杂任务的精度并找到最有效的解决方案。随着生物启发的低功耗解决方案的发展，如尖峰神经网络和事件驱动处理及相机，以及其在空间应用中的最新研究工作，我们提出了一种完全基于事件的解决方案，以提高航天器姿态估计的事件驱动方法的可行性。在本文中，我们介绍了第一套基于事件的数据集SEENIC，该数据集包含在实验台上由基于事件的相机捕获的真实事件帧。我们展示了针对此用例的第一个基于事件的解决方案的方法和结果，其中我们的小型尖峰端到端网络（S2E2）解决方案在位置误差（21厘米）和旋转误差（14度）方面取得了令人感兴趣的结果，这是朝着完全基于事件的处理方向进行嵌入式航天器姿态估计的重要一步。
2501.02913	 | Pointmap-Conditioned Diffusion for Consistent Novel View Synthesis	 | Thang-Anh-Quan Nguyen,Nathan Piasco,Luis Roldão,Moussab Bennehar,Dzmitry Tsishkou,Laurent Caraffa,Jean-Philippe Tarel,Roland Brémond	 | PointmapDiffusion 是一种利用预训练的 2D 扩散模型和点图条件信号的新颖单张图像新视角合成框架，能够在保持生成质量和几何一致性的同时，减少可训练参数，并在多种真实世界数据集上优于其他基线方法。	 | In this paper, we present PointmapDiffusion, a novel framework for single-image novel view synthesis (NVS) that utilizes pre-trained 2D diffusion models. Our method is the first to leverage pointmaps (i.e. rasterized 3D scene coordinates) as a conditioning signal, capturing geometric prior from the reference images to guide the diffusion process. By embedding reference attention blocks and a ControlNet for pointmap features, our model balances between generative capability and geometric consistency, enabling accurate view synthesis across varying viewpoints. Extensive experiments on diverse real-world datasets demonstrate that PointmapDiffusion achieves high-quality, multi-view consistent results with significantly fewer trainable parameters compared to other baselines for single-image NVS tasks.	 | 在本文中，我们提出了一种新颖的单张图像新视角合成（NVS）框架——PointmapDiffusion，该框架利用了预训练的2D扩散模型。我们的方法是首个利用点图（即2D化后的3D场景坐标）作为条件信号，从参考图像中捕获几何先验以引导扩散过程的方法。通过嵌入参考注意力块和ControlNet来处理点图特征，我们的模型在生成能力和几何一致性之间取得了平衡，从而能够准确地在不同视角下进行视图合成。在多种真实世界数据集上的广泛实验表明，与单张图像NVS任务的其他基线方法相比，PointmapDiffusion可以在显著减少可训练参数的情况下获得高质量、多视角一致的结果。
2501.02909	 | Comprehensive Pathological Image Segmentation via Teacher Aggregation for Tumor Microenvironment Analysis	 | Daisuke Komura,Maki Takao,Mieko Ochi,Takumi Onoyama,Hiroto Katoh,Hiroyuki Abe,Hiroyuki Sano,Teppei Konishi,Toshio Kumasaka,Tomoyuki Yokose,Yohei Miyagi,Tetsuo Ushiku,Shumpei Ishikawa	 | PAGET（Pathological image segmentation via AGgrEgated Teachers）是一种新知识蒸馏方法，能够综合分析组织切片，识别并分类14种关键的肿瘤微环境组分，适用于多种组织类型和医疗机构，推动了肿瘤微环境的定量分析和精确临床决策。	 | The tumor microenvironment (TME) plays a crucial role in cancer progression and treatment response, yet current methods for its comprehensive analysis in H&E-stained tissue slides face significant limitations in the diversity of tissue cell types and accuracy. Here, we present PAGET (Pathological image segmentation via AGgrEgated Teachers), a new knowledge distillation approach that integrates multiple segmentation models while considering the hierarchical nature of cell types in the TME. By leveraging a unique dataset created through immunohistochemical restaining techniques and existing segmentation models, PAGET enables simultaneous identification and classification of 14 key TME components. We demonstrate PAGET's ability to perform rapid, comprehensive TME segmentation across various tissue types and medical institutions, advancing the quantitative analysis of tumor microenvironments. This method represents a significant step forward in enhancing our understanding of cancer biology and supporting precise clinical decision-making from large-scale histopathology images.	 | 肿瘤微环境（TME）在癌症进展和治疗反应中发挥着重要作用，但目前用于H&E染色组织切片综合分析的方法在组织细胞类型多样性和准确性方面存在显著局限。为此，我们提出了一种名为PAGET（Pathological image segmentation via AGgrEgated Teachers）的新知识蒸馏方法，该方法整合了多个分割模型，并考虑了TME中细胞类型的层次结构。通过利用免疫组化再染色技术创建的独特数据集和现有的分割模型，PAGET 能够同时识别和分类14种关键的TME组分。我们展示了PAGET能够在多种组织类型和医疗机构中进行快速、全面的TME分割，从而推动肿瘤微环境的定量分析。这种方法代表了在大规模病理图像分析中增强我们对癌症生物学理解的重要进展，并支持精确的临床决策。
2501.02892	 | FoundPAD: Foundation Models Reloaded for Face Presentation Attack Detection	 | Guray Ozgur,Eduarda Caldeira,Tahar Chettaoui,Fadi Boutros,Raghavendra Ramachandra,Naser Damer	 | 这项研究利用预训练的基础模型（FM）来提高展示攻击检测（PAD）的一般化能力，并减少对大量训练数据的需求，提出的FoundPAD架构在不同数据可用性场景下表现出色，并已公开发布以促进进一步研究。	 | Although face recognition systems have seen a massive performance enhancement in recent years, they are still targeted by threats such as presentation attacks, leading to the need for generalizable presentation attack detection (PAD) algorithms. Current PAD solutions suffer from two main problems: low generalization to unknown cenarios and large training data requirements. Foundation models (FM) are pre-trained on extensive datasets, achieving remarkable results when generalizing to unseen domains and allowing for efficient task-specific adaption even when little training data are available. In this work, we recognize the potential of FMs to address common PAD problems and tackle the PAD task with an adapted FM for the first time. The FM under consideration is adapted with LoRA weights while simultaneously training a classification header. The resultant architecture, FoundPAD, is highly generalizable to unseen domains, achieving competitive results in several settings under different data availability scenarios and even when using synthetic training data. To encourage reproducibility and facilitate further research in PAD, we publicly release the implementation of FoundPAD at https://github.com/gurayozgur/FoundPAD .	 | 尽管近年来面部识别系统取得了显著的性能提升，但它们仍然面临着诸如展示攻击等威胁，因此需要能够泛化到未知场景的一般化展示攻击检测（PAD）算法。目前的PAD解决方案存在两个主要问题：对未知场景的一般化能力低以及需要大量的训练数据。基础模型（FM）是在大量数据集上进行预训练的，能够在未知领域进行泛化，并且即使在少量训练数据的情况下也可以实现高效的特定任务适配。在这项工作中，我们认识到FM在解决常见PAD问题方面的潜力，并首次使用适配的FM来处理PAD任务。所考虑的FM通过LoRA权重进行适配，并同时训练一个分类头部。由此产生的架构FoundPAD在不同数据可用性场景下对未知领域具有高度的泛化能力，并在使用合成训练数据的情况下也能取得竞争力的结果。为了促进PAD领域的可再现性和进一步研究，我们已在 https://github.com/gurayozgur/FoundPAD 公开发布了FoundPAD的实现。
2501.02885	 | MDP3: A Training-free Approach for List-wise Frame Selection in Video-LLMs	 | Hui Sun,Shiyin Lu,Huanyu Wang,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Ming Li	 | 本文提出了一种基于马尔可夫决策过程的动态规划（MDP3）方法，用于有效选择视频帧，以解决视频大型语言模型在处理多帧时面临的挑战。MDP3方法通过条件高斯核和确定性点过程捕捉查询相关性和列表多样性，并通过分割视频段并建模为MDP来融入序列性，从而显著优于现有方法。	 | Video large language models (Video-LLMs) have made significant progress in understanding videos. However, processing multiple frames leads to lengthy visual token sequences, presenting challenges such as the limited context length cannot accommodate the entire video, and the inclusion of irrelevant frames hinders visual perception. Hence, effective frame selection is crucial. This paper emphasizes that frame selection should follow three key principles: query relevance, list-wise diversity, and sequentiality. Existing methods, such as uniform frame sampling and query-frame matching, do not capture all of these principles. Thus, we propose Markov decision determinantal point process with dynamic programming (MDP3) for frame selection, a training-free and model-agnostic method that can be seamlessly integrated into existing Video-LLMs. Our method first estimates frame similarities conditioned on the query using a conditional Gaussian kernel within the reproducing kernel Hilbert space~(RKHS). We then apply the determinantal point process~(DPP) to the similarity matrix to capture both query relevance and list-wise diversity. To incorporate sequentiality, we segment the video and apply DPP within each segment, conditioned on the preceding segment selection, modeled as a Markov decision process~(MDP) for allocating selection sizes across segments. Theoretically, MDP3 provides a \((1 - 1/e)\)-approximate solution to the NP-hard list-wise frame selection problem with pseudo-polynomial time complexity, demonstrating its efficiency. Empirically, MDP3 significantly outperforms existing methods, verifying its effectiveness and robustness.	 | 视频大型语言模型（Video-LLMs）在理解视频方面取得了显著进展。然而，处理多帧会导致视觉标记序列变长，带来诸如上下文长度有限无法容纳整个视频，以及包含无关帧影响视觉感知等挑战。因此，有效的帧选择至关重要。本文强调，帧选择应遵循三个关键原则：查询相关性、列表多样性以及序列性。现有的方法，如均匀帧采样和查询帧匹配，并不能捕捉到所有这些原则。因此，我们提出了基于马尔可夫决策过程的动态规划（MDP3）方法进行帧选择，这是一种无需训练且与模型无关的方法，可以无缝集成到现有的Video-LLMs中。我们的方法首先使用条件高斯核在再生核希尔伯特空间（RKHS）内估计帧间的相似性，然后应用确定性点过程（DPP）到相似性矩阵上，以捕捉查询相关性和列表多样性。为了融入序列性，我们将视频分割成多个段，对每个段应用DPP，并基于前一个段的选择，将其建模为马尔可夫决策过程（MDP），以分配选择大小到各个段。理论上，MDP3提供了NP难的列表式帧选择问题的\((1 - 1/e)\)近似解，具有伪多项式时间复杂度，证明了其效率。实验证明，MDP3显著优于现有方法，验证了其有效性和鲁棒性。
2501.02872	 | Two-Dimensional Unknown View Tomography from Unknown Angle Distributions	 | Kaishva Chintan Shah,Karthik S. Gurumoorthy,Ajit Rajwade	 | 这项研究提出了一种在未知视角分布情况下的2D层析成像技术，通过交替估计视角分布和潜在的2D结构，并使用交叉验证误差优化，该方法在半参数von Mises密度混合模型和概率质量函数模型下进行了验证。	 | This study presents a technique for 2D tomography under unknown viewing angles when the distribution of the viewing angles is also unknown. Unknown view tomography (UVT) is a problem encountered in cryo-electron microscopy and in the geometric calibration of CT systems. There exists a moderate-sized literature on the 2D UVT problem, but most existing 2D UVT algorithms assume knowledge of the angle distribution which is not available usually. Our proposed methodology formulates the problem as an optimization task based on cross-validation error, to estimate the angle distribution jointly with the underlying 2D structure in an alternating fashion. We explore the algorithm's capabilities for the case of two probability distribution models: a semi-parametric mixture of von Mises densities and a probability mass function model. We evaluate our algorithm's performance under noisy projections using a PCA-based denoising technique and Graph Laplacian Tomography (GLT) driven by order statistics of the estimated distribution, to ensure near-perfect ordering, and compare our algorithm to intuitive baselines.	 | 这项研究提出了一种在未知视角情况下进行2D层析成像的技术，同时也未知视角分布。未知视角层析成像（UVT）问题在冷冻电子显微镜和CT系统的几何校准中都会遇到。对于2D UVT问题，已经存在一定的文献，但大多数现有的2D UVT算法都假设知道视角分布，而实际情况中这一信息通常是不可用的。我们提出的方法将问题表述为基于交叉验证误差的优化任务，交替地估计视角分布和潜在的2D结构。我们探讨了该算法在两种概率分布模型下的能力：半参数von Mises密度混合模型和概率质量函数模型。我们使用基于主成分分析（PCA）的降噪技术和由估计分布的顺序统计量驱动的图拉普拉斯层析成像（GLT）来评估算法在噪声投影下的性能，并确保近乎完美的排序，然后将我们的算法与直观的基础算法进行比较。
2501.02858	 | A Novel Vision Transformer for Camera-LiDAR Fusion based Traffic Object Segmentation	 | Toomas Tahves,Junyi Gu,Mauro Bellone,Raivo Sell	 | 本文介绍了Camera-LiDAR Fusion Transformer (CLFT)模型，该模型通过结合视觉变换器和自注意力机制融合相机和LiDAR数据，实现了多种交通物体的分割，但在恶劣天气条件下仍需进一步优化以提高性能。	 | This paper presents Camera-LiDAR Fusion Transformer (CLFT) models for traffic object segmentation, which leverage the fusion of camera and LiDAR data using vision transformers. Building on the methodology of visual transformers that exploit the self-attention mechanism, we extend segmentation capabilities with additional classification options to a diverse class of objects including cyclists, traffic signs, and pedestrians across diverse weather conditions. Despite good performance, the models face challenges under adverse conditions which underscores the need for further optimization to enhance performance in darkness and rain. In summary, the CLFT models offer a compelling solution for autonomous driving perception, advancing the state-of-the-art in multimodal fusion and object segmentation, with ongoing efforts required to address existing limitations and fully harness their potential in practical deployments.	 | 本文介绍了利用视觉变换器融合相机和LiDAR数据的Camera-LiDAR Fusion Transformer (CLFT) 模型，用于交通物体分割。该模型基于视变换器利用自注意力机制的方法，通过扩展分割能力并增加额外的分类选项，能够对包括自行车手、交通标志和行人在内的多种类型物体进行分割，涵盖多种天气条件。尽管在性能上表现出色，但在恶劣条件下，模型仍面临挑战，这强调了进一步优化以增强在黑暗和雨天的性能的必要性。总之，CLFT模型为自主驾驶感知提供了一个有说服力的解决方案，推动了多模态融合和对象分割的前沿技术，但仍需持续努力以解决现有局限性，充分发挥其在实际部署中的潜力。
2501.02855	 | Synthetic Fungi Datasets: A Time-Aligned Approach	 | A. Rani,D. O. Arroyo,P. Durdevic	 | 该研究提供了一个合成的时间对齐图像数据集，模拟了真菌生长的关键阶段，旨在支持深度学习在真菌形态变化研究中的应用，并促进自动化真菌分析和疾病监测。	 | Fungi undergo dynamic morphological transformations throughout their lifecycle, forming intricate networks as they transition from spores to mature mycelium structures. To support the study of these time-dependent processes, we present a synthetic, time-aligned image dataset that models key stages of fungal growth. This dataset systematically captures phenomena such as spore size reduction, branching dynamics, and the emergence of complex mycelium networks. The controlled generation process ensures temporal consistency, scalability, and structural alignment, addressing the limitations of real-world fungal datasets. Optimized for deep learning (DL) applications, this dataset facilitates the development of models for classifying growth stages, predicting fungal development, and analyzing morphological patterns over time. With applications spanning agriculture, medicine, and industrial mycology, this resource provides a robust foundation for automating fungal analysis, enhancing disease monitoring, and advancing fungal biology research through artificial intelligence.	 | 真菌在其生命周期中会经历动态的形态变化，从孢子形成复杂的菌丝网络。为了支持对这些时间依赖性过程的研究，我们提供了一个合成的时间对齐的图像数据集，该数据集模拟了真菌生长的关键阶段。该数据集系统地捕捉了孢子大小减少、分枝动态以及复杂菌丝网络出现的现象。受控生成过程确保了时间一致性、可扩展性和结构对齐，解决了真实世界真菌数据集的局限性。该数据集针对深度学习（DL）应用进行了优化，促进了生长阶段分类、预测真菌发育和分析时间变化的形态模式模型的开发。该资源的应用范围涵盖农业、医学和工业真菌学，为自动化真菌分析、增强疾病监测并通过人工智能推进真菌生物学研究提供了坚实的基础。
2501.02850	 | Large Language Models for Video Surveillance Applications	 | Ulindu De Silva,Leon Fernando,Billy Lau Pik Lik,Zann Koh,Sam Conrad Joyce,Belinda Yuen,Chau Yuen	 | 本文提出了一种利用生成式人工智能中的视觉语言模型来生成定制化文本摘要的工具，以增强视频分析过程，该工具可以在不进行详尽手动审查的情况下，快速导航和验证CCTV视频中的重要事件，并且在时间和空间质量及一致性方面的准确率分别为80%和70%。	 | The rapid increase in video content production has resulted in enormous data volumes, creating significant challenges for efficient analysis and resource management. To address this, robust video analysis tools are essential. This paper presents an innovative proof of concept using Generative Artificial Intelligence (GenAI) in the form of Vision Language Models to enhance the downstream video analysis process. Our tool generates customized textual summaries based on user-defined queries, providing focused insights within extensive video datasets. Unlike traditional methods that offer generic summaries or limited action recognition, our approach utilizes Vision Language Models to extract relevant information, improving analysis precision and efficiency. The proposed method produces textual summaries from extensive CCTV footage, which can then be stored for an indefinite time in a very small storage space compared to videos, allowing users to quickly navigate and verify significant events without exhaustive manual review. Qualitative evaluations result in 80% and 70% accuracy in temporal and spatial quality and consistency of the pipeline respectively.	 | 随着视频内容生产的速度急剧增加，产生的数据量巨大，这为有效分析和资源管理带来了巨大挑战。为应对这一挑战，强大的视频分析工具变得至关重要。本文介绍了一种创新的概念验证，利用生成式人工智能（GenAI）中的视觉语言模型来增强下游视频分析过程。我们的工具根据用户定义的查询生成定制化的文本摘要，提供对广泛视频数据集的聚焦洞察。不同于传统的提供通用摘要或有限动作识别的方法，我们的方法利用视觉语言模型提取相关信息，从而提高分析的精确性和效率。所提出的方法可以从广泛的闭路电视（CCTV）视频片段中生成文本摘要，然后可以存储在非常小的存储空间中，与视频相比，这使得用户无需进行详尽的手动审查即可迅速导航和验证重要事件。定性评估结果显示，在时间和空间质量及一致性方面的准确率分别为80%和70%。
2501.02845	 | HOGSA: Bimanual Hand-Object Interaction Understanding with 3D Gaussian Splatting Based Data Augmentation	 | Wentian Qu,Jiahe Li,Jian Cheng,Jian Shi,Chenyu Meng,Cuixia Ma,Hongan Wang,Xiaoming Deng,Yinda Zhang	 | 本文提出了一种基于3D Gaussian Splatting的数据增强框架，用于生成大规模逼真的双臂手物交互数据，通过优化模块和超分辨率技术显著扩展了数据集的姿态分布，并提升了基准性能。	 | Understanding of bimanual hand-object interaction plays an important role in robotics and virtual reality. However, due to significant occlusions between hands and object as well as the high degree-of-freedom motions, it is challenging to collect and annotate a high-quality, large-scale dataset, which prevents further improvement of bimanual hand-object interaction-related baselines. In this work, we propose a new 3D Gaussian Splatting based data augmentation framework for bimanual hand-object interaction, which is capable of augmenting existing dataset to large-scale photorealistic data with various hand-object pose and viewpoints. First, we use mesh-based 3DGS to model objects and hands, and to deal with the rendering blur problem due to multi-resolution input images used, we design a super-resolution module. Second, we extend the single hand grasping pose optimization module for the bimanual hand object to generate various poses of bimanual hand-object interaction, which can significantly expand the pose distribution of the dataset. Third, we conduct an analysis for the impact of different aspects of the proposed data augmentation on the understanding of the bimanual hand-object interaction. We perform our data augmentation on two benchmarks, H2O and Arctic, and verify that our method can improve the performance of the baselines.	 | 理解双臂手物交互对于机器人技术和虚拟现实至关重要。然而，由于手和物体之间的显著遮挡以及高自由度的运动，收集和标注高质量、大规模的数据集非常具有挑战性，这阻碍了双臂手物交互相关基线的进一步改进。本文提出了一种新的基于3D Gaussian Splatting的数据增强框架，用于双臂手物交互，能够将现有数据集扩展为大规模的逼真数据，涵盖多种手物姿态和视角。首先，我们使用基于网格的3DGS来建模对象和手，并为处理多分辨率输入图像导致的渲染模糊问题，设计了一个超分辨率模块。其次，我们将单手抓取姿态优化模块扩展到双臂手物交互，以生成多种双臂手物交互姿态，显著扩展了数据集的姿态分布。第三，我们分析了所提数据增强的不同方面对理解双臂手物交互的影响。我们在H2O和Arctic两个基准上进行了数据增强，并验证了我们的方法可以提高基线性能。
2501.02840	 | Enhanced Rooftop Solar Panel Detection by Efficiently Aggregating Local Features	 | Kuldeep Kurte,Kedar Kulkarni	 | 本文提出了一种结合卫星图像和卷积神经网络的屋顶太阳能光伏面板检测方法，并使用局部聚合描述子向量技术提取全局特征，最终实现高精度的屋顶-PV分类，同时提出了一种三阶段方法以有效应对新城市或地区的有限标注数据问题。	 | In this paper, we present an enhanced Convolutional Neural Network (CNN)-based rooftop solar photovoltaic (PV) panel detection approach using satellite images. We propose to use pre-trained CNN-based model to extract the local convolutional features of rooftops. These local features are then combined using the Vectors of Locally Aggregated Descriptors (VLAD) technique to obtain rooftop-level global features, which are then used to train traditional Machine Learning (ML) models to identify rooftop images that do and do not contain PV panels. On the dataset used in this study, the proposed approach achieved rooftop-PV classification scores exceeding the predefined threshold of 0.9 across all three cities for each of the feature extractor networks evaluated. Moreover, we propose a 3-phase approach to enable efficient utilization of the previously trained models on a new city or region with limited labelled data. We illustrate the effectiveness of this 3-phase approach for multi-city rooftop-PV detection task.	 | 在本文中，我们提出了一种使用卫星图像增强的基于卷积神经网络（CNN）的屋顶太阳能光伏（PV）面板检测方法。我们提议使用预训练的基于CNN的模型来提取屋顶的局部卷积特征。然后，使用局部聚合描述子向量（VLAD）技术结合这些局部特征，以获得屋顶级别的全局特征，再用这些全局特征训练传统的机器学习（ML）模型来识别包含和不包含PV面板的屋顶图像。在本研究使用的数据集上，所提出的方法在评估的每个特征提取网络中均实现了超过预设阈值0.9的屋顶-PV分类分数，即在三个城市中均达到这一标准。此外，我们提出了一种三阶段方法，以便在新城市或地区有限标注数据的情况下高效利用已训练的模型。我们还展示了这种三阶段方法在多城市屋顶-PV检测任务中的有效性。
2501.02831	 | Universal Features Guided Zero-Shot Category-Level Object Pose Estimation	 | Wentian Qu,Chenyu Meng,Heng Li,Jian Cheng,Cuixia Ma,Hongan Wang,Xiao Zhou,Xiaoming Deng,Ping Tan	 | 该研究提出了一种零样本方法，用于实现未见类别的六自由度物体姿态估计，通过结合2D和3D通用特征建立语义相似性对应关系，并通过迭代优化策略和3D特征对齐解决姿态偏差和形状差异问题。该方法在REAL275和Wild6D基准测试中展示了优于现有方法的性能。	 | Object pose estimation, crucial in computer vision and robotics applications, faces challenges with the diversity of unseen categories. We propose a zero-shot method to achieve category-level 6-DOF object pose estimation, which exploits both 2D and 3D universal features of input RGB-D image to establish semantic similarity-based correspondences and can be extended to unseen categories without additional model fine-tuning. Our method begins with combining efficient 2D universal features to find sparse correspondences between intra-category objects and gets initial coarse pose. To handle the correspondence degradation of 2D universal features if the pose deviates much from the target pose, we use an iterative strategy to optimize the pose. Subsequently, to resolve pose ambiguities due to shape differences between intra-category objects, the coarse pose is refined by optimizing with dense alignment constraint of 3D universal features. Our method outperforms previous methods on the REAL275 and Wild6D benchmarks for unseen categories.	 | 在计算机视觉和机器人应用中至关重要的物体姿态估计面临未见类别的多样性挑战。我们提出了一个零样本方法，用于实现类别级别的六自由度（6-DOF）物体姿态估计，该方法利用输入RGB-D图像的2D和3D通用特征来建立基于语义相似性的对应关系，并且可以在无需额外模型微调的情况下扩展到未见类别。我们的方法首先结合高效2D通用特征，找出同一类别内物体之间的稀疏对应关系，从而获得初始粗略姿态。为了处理如果姿态与目标姿态偏差较大时2D通用特征对应关系退化的问题，我们使用迭代策略来优化姿态。随后，为了解决由于同一类别内物体形状差异导致的姿态歧义性问题，粗略姿态通过优化3D通用特征的密集对齐约束进行细化。我们的方法在REAL275和Wild6D基准测试中对未见类别表现优于之前的方法。
2501.02822	 | RDD4D: 4D Attention-Guided Road Damage Detection And Classification	 | Asma Alkalbani,Muhammad Saqib,Ahmed Salim Alrawahi,Abbas Anwar,Chandarnath Adak,Saeed Anwar	 | 该研究提出了一个名为DRDD的新数据集和一种新的模型RDD4D，旨在提高多类型道路损伤的检测精度，特别是在大尺寸损伤的检测上，AP达到了0.458，并且在CrackTinyNet数据集上实现了约0.21的性能提升。	 | Road damage detection and assessment are crucial components of infrastructure maintenance. However, current methods often struggle with detecting multiple types of road damage in a single image, particularly at varying scales. This is due to the lack of road datasets with various damage types having varying scales. To overcome this deficiency, first, we present a novel dataset called Diverse Road Damage Dataset (DRDD) for road damage detection that captures the diverse road damage types in individual images, addressing a crucial gap in existing datasets. Then, we provide our model, RDD4D, that exploits Attention4D blocks, enabling better feature refinement across multiple scales. The Attention4D module processes feature maps through an attention mechanism combining positional encoding and "Talking Head" components to capture local and global contextual information. In our comprehensive experimental analysis comparing various state-of-the-art models on our proposed, our enhanced model demonstrated superior performance in detecting large-sized road cracks with an Average Precision (AP) of 0.458 and maintained competitive performance with an overall AP of 0.445. Moreover, we also provide results on the CrackTinyNet dataset; our model achieved around a 0.21 increase in performance. The code, model weights, dataset, and our results are available on \href{https://github.com/msaqib17/Road_Damage_Detection}{https://github.com/msaqib17/Road\_Damage\_Detection}.	 | 道路损伤检测和评估是基础设施维护中的关键组成部分。然而，当前的方法在单一图像中同时检测多种类型的道路损伤时常常遇到困难，尤其是在不同尺度上。这主要是因为缺乏包含多种不同类型和不同尺度损伤的道路数据集。为了解决这一缺陷，首先，我们提出了一种名为Diverse Road Damage Dataset (DRDD) 的新数据集，用于道路损伤检测，该数据集捕捉了单个图像中的多种道路损伤类型，解决了现有数据集中的关键空白。然后，我们提供了一种名为RDD4D的模型，利用了Attention4D模块，使其能够在多个尺度上更好地进行特征提炼。Attention4D模块通过结合位置编码和“Talking Head”组件的注意机制对特征图进行处理，从而捕捉局部和全局上下文信息。在我们对多种最先进的模型进行全面实验分析并与我们提出的方法进行比较后，我们的增强模型在检测大尺寸道路裂缝方面的平均精度 (AP) 达到了0.458，并且在整体AP为0.445的情况下保持了竞争力。此外，我们还提供了在CrackTinyNet数据集上的结果；我们的模型在该数据集上的性能提升了约0.21。我们的代码、模型权重、数据集和结果可以在 \href{https://github.com/msaqib17/Road_Damage_Detection}{https://github.com/msaqib17/Road_Damage_Detection} 上获得。
2501.02816	 | InpDiffusion: Image Inpainting Localization via Conditional Diffusion Models	 | Kai Wang,Shaozhang Niu,Qixian Hao,Jiwei Zhang	 | 本文提出了一种新的图像修复定位方法InpDiffusion，该方法将IIL任务视为生成条件掩码的问题，并通过结合图像语义条件增强的去噪过程和边缘监督策略来逐步细化预测结果。实验结果表明InpDiffusion在多个数据集上显著优于现有方法，展示了优秀的泛化能力和鲁棒性。	 | As artificial intelligence advances rapidly, particularly with the advent of GANs and diffusion models, the accuracy of Image Inpainting Localization (IIL) has become increasingly challenging. Current IIL methods face two main challenges: a tendency towards overconfidence, leading to incorrect predictions; and difficulty in detecting subtle tampering boundaries in inpainted images. In response, we propose a new paradigm that treats IIL as a conditional mask generation task utilizing diffusion models. Our method, InpDiffusion, utilizes the denoising process enhanced by the integration of image semantic conditions to progressively refine predictions. During denoising, we employ edge conditions and introduce a novel edge supervision strategy to enhance the model's perception of edge details in inpainted objects. Balancing the diffusion model's stochastic sampling with edge supervision of tampered image regions mitigates the risk of incorrect predictions from overconfidence and prevents the loss of subtle boundaries that can result from overly stochastic processes. Furthermore, we propose an innovative Dual-stream Multi-scale Feature Extractor (DMFE) for extracting multi-scale features, enhancing feature representation by considering both semantic and edge conditions of the inpainted images. Extensive experiments across challenging datasets demonstrate that the InpDiffusion significantly outperforms existing state-of-the-art methods in IIL tasks, while also showcasing excellent generalization capabilities and robustness.	 | 随着人工智能的迅速发展，特别是生成对抗网络（GANs）和扩散模型的出现，图像修复定位（IIL）的准确性变得越来越具有挑战性。目前的IIL方法面临两大主要挑战：过度自信的倾向，导致错误的预测；以及在修复图像中检测微小篡改边界的困难。为应对这些挑战，我们提出了一种新的范式，即将IIL视为利用扩散模型生成条件掩码的任务。我们的方法名为InpDiffusion，通过结合图像语义条件增强的去噪过程，逐步细化预测结果。在去噪过程中，我们使用边缘条件并引入了一种新颖的边缘监督策略，以增强模型对修复对象边缘细节的感知。平衡扩散模型的随机采样与篡改图像区域的边缘监督，可以减轻过度自信导致的预测错误风险，并防止由于过度随机过程导致的微小边界丢失。此外，我们提出了一个创新的双流多尺度特征提取器（DMFE），用于提取多尺度特征，通过同时考虑语义和边缘条件，增强特征表示。在多个具有挑战性的数据集上的广泛实验表明，InpDiffusion在IIL任务中显著优于现有的最先进的方法，同时展示出出色的泛化能力和鲁棒性。
2501.02811	 | First-place Solution for Streetscape Shop Sign Recognition Competition	 | Bin Wang,Li Jing	 | 该研究提出了一种新型多阶段方法，结合多模态特征融合、自我监督训练和基于Transformer的大模型，显著提升了街道视图商铺招牌的文本识别能力，并通过BoxDQN等创新技术进一步优化了结果。	 | Text recognition technology applied to street-view storefront signs is increasingly utilized across various practical domains, including map navigation, smart city planning analysis, and business value assessments in commercial districts. This technology holds significant research and commercial potential. Nevertheless, it faces numerous challenges. Street view images often contain signboards with complex designs and diverse text styles, complicating the text recognition process. A notable advancement in this field was introduced by our team in a recent competition. We developed a novel multistage approach that integrates multimodal feature fusion, extensive self-supervised training, and a Transformer-based large model. Furthermore, innovative techniques such as BoxDQN, which relies on reinforcement learning, and text rectification methods were employed, leading to impressive outcomes. Comprehensive experiments have validated the effectiveness of these methods, showcasing our potential to enhance text recognition capabilities in complex urban environments.	 | 文本识别技术在街道视图商铺招牌中的应用日益广泛，涵盖了地图导航、智慧城市规划分析以及商业街区的商业价值评估等多个实用领域。这项技术在研究和商业领域都具有巨大的潜力。然而，它也面临着诸多挑战。街道视图图像经常包含设计复杂、字体多样的招牌，这增加了文本识别的难度。我们的团队在最近的一项比赛中引入了这一领域的显著进步。我们开发了一种新型多阶段方法，结合了多模态特征融合、广泛的自我监督训练和基于Transformer的大模型。此外，我们还采用了BoxDQN等创新技术，依赖强化学习，并使用了文本校正方法，取得了令人印象深刻的结果。全面的实验验证了这些方法的有效性，展示了我们在复杂城市环境中提升文本识别能力的潜力。
2501.02807	 | AE-NeRF: Augmenting Event-Based Neural Radiance Fields for Non-ideal Conditions and Larger Scene	 | Chaoran Feng,Wangbo Yu,Xinhua Cheng,Zhenyu Tang,Junwu Zhang,Li Yuan,Yonghong Tian	 | 本文提出了一种AE-NeRF方法，通过利用事件流的密度和结合事件驱动的NeRF框架，实现了在非理想条件下（如非均匀事件序列和噪声相机姿态）的鲁棒三维重建，并通过分层事件蒸馏和新的损失函数进一步提高了重建质量。	 | Compared to frame-based methods, computational neuromorphic imaging using event cameras offers significant advantages, such as minimal motion blur, enhanced temporal resolution, and high dynamic range. The multi-view consistency of Neural Radiance Fields combined with the unique benefits of event cameras, has spurred recent research into reconstructing NeRF from data captured by moving event cameras. While showing impressive performance, existing methods rely on ideal conditions with the availability of uniform and high-quality event sequences and accurate camera poses, and mainly focus on the object level reconstruction, thus limiting their practical applications. In this work, we propose AE-NeRF to address the challenges of learning event-based NeRF from non-ideal conditions, including non-uniform event sequences, noisy poses, and various scales of scenes. Our method exploits the density of event streams and jointly learn a pose correction module with an event-based NeRF (e-NeRF) framework for robust 3D reconstruction from inaccurate camera poses. To generalize to larger scenes, we propose hierarchical event distillation with a proposal e-NeRF network and a vanilla e-NeRF network to resample and refine the reconstruction process. We further propose an event reconstruction loss and a temporal loss to improve the view consistency of the reconstructed scene. We established a comprehensive benchmark that includes large-scale scenes to simulate practical non-ideal conditions, incorporating both synthetic and challenging real-world event datasets. The experimental results show that our method achieves a new state-of-the-art in event-based 3D reconstruction.	 | 与基于帧的方法相比，使用事件摄像头进行计算神经形态成像具有显著优势，如最小的运动模糊、增强的时间分辨率和高动态范围。结合神经辐射场的多视角一致性与事件摄像头的独特优势，近年来的研究已经开始探索从移动事件摄像头捕捉的数据中重建NeRF的技术。尽管表现出色，但现有方法依赖于理想条件，需要均匀且高质量的事件序列以及准确的相机姿态，并主要集中在对象级别的重建，从而限制了其实用性。在本文中，我们提出了一种AE-NeRF方法，以应对从非理想条件下学习事件驱动的NeRF所面临的挑战，包括非均匀事件序列、噪声姿态以及不同规模的场景。我们的方法利用事件流的密度，并结合事件驱动的NeRF（e-NeRF）框架，共同学习姿态校正模块，从而实现从不准确的相机姿态中进行鲁棒的三维重建。为了适应更大的场景，我们提出了一种分层事件蒸馏方法，利用建议的e-NeRF网络和常规e-NeRF网络进行采样和细化重建过程。我们进一步提出了一种事件重建损失和时间损失，以提高重建场景的视角一致性。我们建立了一个全面的基准测试，其中包括大型场景以模拟实际的非理想条件，结合了合成和具有挑战性的真实事件数据集。实验结果表明，我们的方法在基于事件的三维重建中达到了新的最佳性能。
2501.02800	 | COph100: A comprehensive fundus image registration dataset from infants constituting the "RIDIRP" database	 | Yan Hu,Mingdao Gong,Zhongxi Qiu,Jiabao Liu,Hongli Shen,Mingzhen Yuan,Xiaoqing Zhang,Heng Li,Hai Lu,Jiang Liu	 | COph100是一个全新的全面婴儿视网膜图像配准数据集，包含了100个眼睛的2到9次检查会话共491对图像，旨在解决现有数据集缺乏临床挑战的问题，有助于婴儿眼病的诊断和治疗研究。	 | Retinal image registration is vital for diagnostic therapeutic applications within the field of ophthalmology. Existing public datasets, focusing on adult retinal pathologies with high-quality images, have limited number of image pairs and neglect clinical challenges. To address this gap, we introduce COph100, a novel and challenging dataset known as the Comprehensive Ophthalmology Retinal Image Registration dataset for infants with a wide range of image quality issues constituting the public "RIDIRP" database. COph100 consists of 100 eyes, each with 2 to 9 examination sessions, amounting to a total of 491 image pairs carefully selected from the publicly available dataset. We manually labeled the corresponding ground truth image points and provided automatic vessel segmentation masks for each image. We have assessed COph100 in terms of image quality and registration outcomes using state-of-the-art algorithms. This resource enables a robust comparison of retinal registration methodologies and aids in the analysis of disease progression in infants, thereby deepening our understanding of pediatric ophthalmic conditions.	 | 视网膜图像配准对眼科的诊断和治疗应用至关重要。现有公开数据集主要关注成人视网膜病理学，且图像质量高，但图像对数量有限且忽略了临床挑战。为解决这一问题，我们引入了COph100，这是一个全新的具有挑战性的数据集，称为全面婴儿视网膜图像配准数据集，它涵盖了从公共“RIDIRP”数据库中选取的具有各种图像质量问题的眼底图像，包括100个眼睛，每个眼睛有2到9次检查会话，总共包含491对图像。我们手动标注了相应的地面真实图像点，并为每张图像提供了自动血管分割掩模。我们使用最新的算法评估了COph100的图像质量和配准结果。该资源使得视网膜配准方法的稳健比较成为可能，并有助于分析婴儿的眼病进展情况，从而加深我们对儿科眼科条件的理解。
2501.02785	 | Hybrid deep convolution model for lung cancer detection with transfer learning	 | Sugandha Saxena,S. N. Prasad,Ashwin M Polnaya,Shweta Agarwala	 | 研究进展显著提升了医疗健康领域的疾病理解和治疗选择，但肺癌仍因早期诊断挑战而成为全球癌症死亡的主要原因。为此，本文提出了一种名为最大灵敏度神经网络（MSNN）的混合深度卷积模型，通过优化灵敏度和特异度实现了98%的准确率和97%的灵敏度，该模型还能可视化肺癌分类的关键区域，显著降低了误诊率。	 | Advances in healthcare research have significantly enhanced our understanding of disease mechanisms, diagnostic precision, and therapeutic options. Yet, lung cancer remains one of the leading causes of cancer-related mortality worldwide due to challenges in early and accurate diagnosis. While current lung cancer detection models show promise, there is considerable potential for further improving the accuracy for timely intervention. To address this challenge, we introduce a hybrid deep convolution model leveraging transfer learning, named the Maximum Sensitivity Neural Network (MSNN). MSNN is designed to improve the precision of lung cancer detection by refining sensitivity and specificity. This model has surpassed existing deep learning approaches through experimental validation, achieving an accuracy of 98% and a sensitivity of 97%. By overlaying sensitivity maps onto lung Computed Tomography (CT) scans, it enables the visualization of regions most indicative of malignant or benign classifications. This innovative method demonstrates exceptional performance in distinguishing lung cancer with minimal false positives, thereby enhancing the accuracy of medical diagnoses.	 | 医疗健康领域的研究进展显著提升了我们对疾病机制的理解、诊断精准度以及治疗选择。然而，由于早期和准确诊断的挑战，肺癌仍然是全球癌症相关死亡的主要原因之一。尽管现有的肺癌检测模型显示出潜力，但仍然有很大的空间进一步提高其准确性以实现及时干预。为应对这一挑战，我们引入了一种结合迁移学习的混合深度卷积模型，名为最大灵敏度神经网络（MSNN）。MSNN旨在通过优化灵敏度和特异度来提高肺癌检测的精确度。该模型通过实验验证超越了现有的深度学习方法，准确率达到98%，灵敏度达到97%。通过在肺部计算机断层扫描（CT）图像上叠加灵敏度图，该模型能够可视化最能指示恶性或良性分类的区域。这一创新方法在区分肺癌方面表现出色，且误诊率极低，从而提高了医学诊断的准确性。
2501.02773	 | Unsupervised Domain Adaptation for Occlusion Resilient Human Pose Estimation	 | Arindam Dutta,Sarosij Bose,Saketh Bachu,Calvin-Khang Ta,Konstantinos Karydis,Amit K. Roy-Chowdhury	 | OR-POSE是一种无监督领域自适应算法，通过迭代伪标签精炼和利用人体姿态先验来提高在遮挡情况下的姿态估计准确性，从而在具有挑战性的遮挡数据集上实现了约7%的性能提升。	 | Occlusions are a significant challenge to human pose estimation algorithms, often resulting in inaccurate and anatomically implausible poses. Although current occlusion-robust human pose estimation algorithms exhibit impressive performance on existing datasets, their success is largely attributed to supervised training and the availability of additional information, such as multiple views or temporal continuity. Furthermore, these algorithms typically suffer from performance degradation under distribution shifts. While existing domain adaptive human pose estimation algorithms address this bottleneck, they tend to perform suboptimally when the target domain images are occluded, a common occurrence in real-life scenarios. To address these challenges, we propose OR-POSE: Unsupervised Domain Adaptation for Occlusion Resilient Human POSE Estimation. OR-POSE is an innovative unsupervised domain adaptation algorithm which effectively mitigates domain shifts and overcomes occlusion challenges by employing the mean teacher framework for iterative pseudo-label refinement. Additionally, OR-POSE reinforces realistic pose prediction by leveraging a learned human pose prior which incorporates the anatomical constraints of humans in the adaptation process. Lastly, OR-POSE avoids overfitting to inaccurate pseudo labels generated from heavily occluded images by employing a novel visibility-based curriculum learning approach. This enables the model to gradually transition from training samples with relatively less occlusion to more challenging, heavily occluded samples. Extensive experiments show that OR-POSE outperforms existing analogous state-of-the-art algorithms by $\sim$ 7% on challenging occluded human pose estimation datasets.	 | 遮挡是人体姿态估计算法的一个重大挑战，常常导致姿态不准确且不符合解剖学的姿势。虽然当前的抗遮挡人体姿态估计算法在现有数据集上表现出色，但它们的成功很大程度上归功于监督训练和额外信息（如多视角或时间连续性）的可用性。此外，这些算法通常在分布变化下性能下降。尽管现有的领域自适应人体姿态估计算法解决了这一瓶颈问题，但在目标域图像被遮挡的情况下，它们的表现通常不佳，而在现实生活中这种情况很常见。为了解决这些挑战，我们提出了一种名为OR-POSE的方法：无监督领域自适应算法，用于提高对遮挡的鲁棒性人体姿态估计。OR-POSE是一种创新的无监督领域自适应算法，通过采用均值教师框架进行迭代伪标签精炼来有效缓解领域变化并克服遮挡挑战。此外，OR-POSE通过利用一个学习到的人体姿态先验来增强真实的姿态预测，该先验在适应过程中整合了人类的解剖约束。最后，OR-POSE通过采用一种新颖的基于可见性的递增课程学习方法，避免了对来自严重遮挡图像的不准确伪标签的过度拟合。这使得模型能够逐渐从相对较少遮挡的训练样本过渡到更具挑战性的、严重遮挡的样本。广泛的实验表明，与现有的同类最佳算法相比，OR-POSE在具有挑战性的遮挡人体姿态估计数据集上的性能提高了约7%。
2501.02771	 | WorldPose: A World Cup Dataset for Global 3D Human Pose Estimation	 | Tianjian Jiang,Johsan Billingham,Sebastian Müksch,Juan Zarate,Nicolas Evans,Martin R. Oswald,Marc Polleyfeys,Otmar Hilliges,Manuel Kaufmann,Jie Song	 | WorldPose是一个基于2022年FIFA世界杯视频的新数据集，旨在推动野外多人全局姿态估计的研究，该数据集通过高清多视角设置在多个足球场上捕捉超过250万个3D姿态，展示了前所未有的精度，支持新研究并挑战现有技术。	 | We present WorldPose, a novel dataset for advancing research in multi-person global pose estimation in the wild, featuring footage from the 2022 FIFA World Cup. While previous datasets have primarily focused on local poses, often limited to a single person or in constrained, indoor settings, the infrastructure deployed for this sporting event allows access to multiple fixed and moving cameras in different stadiums. We exploit the static multi-view setup of HD cameras to recover the 3D player poses and motions with unprecedented accuracy given capture areas of more than 1.75 acres. We then leverage the captured players' motions and field markings to calibrate a moving broadcasting camera. The resulting dataset comprises more than 80 sequences with approx 2.5 million 3D poses and a total traveling distance of over 120 km. Subsequently, we conduct an in-depth analysis of the SOTA methods for global pose estimation. Our experiments demonstrate that WorldPose challenges existing multi-person techniques, supporting the potential for new research in this area and others, such as sports analysis. All pose annotations (in SMPL format), broadcasting camera parameters and footage will be released for academic research purposes.	 | 我们介绍了WorldPose，这是一个新颖的数据集，旨在推动在野外进行多人全球姿态估计的研究，该数据集采用了2022年FIFA世界杯的视频素材。尽管以前的数据集主要关注局部姿态，通常限于单一人物或在受控的室内环境中，但此次体育赛事部署的基础设施允许我们在不同的体育场中访问多个固定的和移动的摄像头。我们利用高清摄像头的多视图静态设置，通过超过1.75英亩的捕捉区域，以前所未有的精度恢复了球员的3D姿态和动作。然后，我们利用捕捉到的球员运动和场地标记来校准移动的广播摄像头。最终数据集包括超过80个序列，约250万3D姿态，总行进距离超过120公里。随后，我们对全球姿态估计的最新技术进行了深入分析。我们的实验表明，WorldPose能够挑战现有的多人技术，支持该领域及其他领域（如运动分析）的新研究的潜力。所有姿态注释（SMPL格式）、广播摄像头参数和视频资料将用于学术研究目的，供学术界使用。
2501.02765	 | Visual Large Language Models for Generalized and Specialized Applications	 | Yifan Li,Zhixin Lai,Wentao Bao,Zhen Tan,Anh Dao,Kewei Sui,Jiayi Shen,Dong Liu,Huan Liu,Yu Kong	 | 本文综述了视觉大型语言模型（VLLMs）在多种模态下的多样化应用，并探讨了其应用场景、伦理考量和未来发展方向。通过全面分析VLLMs的通用性和专门应用，论文旨在为未来的研究创新和更广泛的实际应用提供指导。	 | Visual-language models (VLM) have emerged as a powerful tool for learning a unified embedding space for vision and language. Inspired by large language models, which have demonstrated strong reasoning and multi-task capabilities, visual large language models (VLLMs) are gaining increasing attention for building general-purpose VLMs. Despite the significant progress made in VLLMs, the related literature remains limited, particularly from a comprehensive application perspective, encompassing generalized and specialized applications across vision (image, video, depth), action, and language modalities. In this survey, we focus on the diverse applications of VLLMs, examining their using scenarios, identifying ethics consideration and challenges, and discussing future directions for their development. By synthesizing these contents, we aim to provide a comprehensive guide that will pave the way for future innovations and broader applications of VLLMs. The paper list repository is available: https://github.com/JackYFL/awesome-VLLMs.	 | 视觉语言模型（VLM）已成为学习视觉和语言统一嵌入空间的强大工具。受到大型语言模型的启发，这些模型在推理和多任务能力方面表现出色，视觉大型语言模型（VLLMs）正逐渐受到关注，用于构建通用目的的VLM。尽管在VLLMs方面取得了显著进展，但相关文献仍然有限，尤其是在全面应用视角方面，涵盖视觉（图像、视频、深度）、动作和语言等多种模态的一般性和专门应用。  在本文综述中，我们重点关注VLLMs的多样化应用，探讨其应用场景，识别伦理考量和挑战，并讨论其未来发展方向。通过综合这些内容，我们旨在提供一个全面的指南，为未来的创新和更广泛的应用铺平道路。该论文列表仓库已公开：https://github.com/JackYFL/awesome-VLLMs。
2501.02763	 | LDMapNet-U: An End-to-End System for City-Scale Lane-Level Map Updating	 | Deguo Xia,Weiming Zhang,Xiyan Liu,Wei Zhang,Chenting Gong,Xiao Tan,Jizhou Huang,Mengmeng Yang,Diange Yang	 | LDMapNet-U 是一种端到端的城市规模车道级地图更新方法，通过引入先验地图编码和实例变化预测模块，实现自动更新并生成矢量化地图和变化信息，显著提高了更新效率和准确性。该方法已在百度地图成功部署，支持超过360个城市每周更新，并服务数亿用户。	 | An up-to-date city-scale lane-level map is an indispensable infrastructure and a key enabling technology for ensuring the safety and user experience of autonomous driving systems. In industrial scenarios, reliance on manual annotation for map updates creates a critical bottleneck. Lane-level updates require precise change information and must ensure consistency with adjacent data while adhering to strict standards. Traditional methods utilize a three-stage approach-construction, change detection, and updating-which often necessitates manual verification due to accuracy limitations. This results in labor-intensive processes and hampers timely updates. To address these challenges, we propose LDMapNet-U, which implements a new end-to-end paradigm for city-scale lane-level map updating. By reconceptualizing the update task as an end-to-end map generation process grounded in historical map data, we introduce a paradigm shift in map updating that simultaneously generates vectorized maps and change information. To achieve this, a Prior-Map Encoding (PME) module is introduced to effectively encode historical maps, serving as a critical reference for detecting changes. Additionally, we incorporate a novel Instance Change Prediction (ICP) module that learns to predict associations with historical maps. Consequently, LDMapNet-U simultaneously achieves vectorized map element generation and change detection. To demonstrate the superiority and effectiveness of LDMapNet-U, extensive experiments are conducted using large-scale real-world datasets. In addition, LDMapNet-U has been successfully deployed in production at Baidu Maps since April 2024, supporting map updating for over 360 cities and significantly shortening the update cycle from quarterly to weekly. The updated maps serve hundreds of millions of users and are integrated into the autonomous driving systems of several leading vehicle companies.	 | 城市规模车道级地图是一种不可或缺的基础设施和关键技术，对于确保自动驾驶系统的安全性和用户体验至关重要。在工业应用场景中，依赖人工标注进行地图更新成为一个关键瓶颈。车道级更新需要精确的变化信息，并且必须确保与相邻数据的一致性，同时遵守严格的规范。传统方法采用三阶段的方法——建设、变化检测和更新——由于准确性限制，常常需要人工验证，导致过程繁琐且难以及时更新。为了解决这些挑战，我们提出了LDMapNet-U，它实现了城市规模车道级地图更新的全新端到端范式。通过将更新任务重新构想为基于历史地图数据的端到端地图生成过程，我们引入了地图更新范式的转变，同时生成矢量化地图和变化信息。为此，我们引入了一个先验地图编码（PME）模块，能够有效编码历史地图，作为检测变化的关键参考。此外，我们还引入了一个新颖的实例变化预测（ICP）模块，能够学习预测与历史地图的关联性。因此，LDMapNet-U 同时实现了矢量化地图元素生成和变化检测。为了证明LDMapNet-U 的优越性和有效性，我们使用大规模的真实世界数据集进行了广泛实验。此外，LDMapNet-U 自2024年4月起在百度地图成功部署生产环境中，支持超过360个城市的地图更新，并显著缩短了更新周期，从季度缩短到每周。更新后的地图服务了数亿用户，并被多家领先的汽车公司集成到其自动驾驶系统中。
2501.02741	 | Brick-Diffusion: Generating Long Videos with Brick-to-Wall Denoising	 | Yunlong Yuan,Yuanfan Guo,Chunwei Wang,Hang Xu,Li Zhang	 | 本文提出了一种全新的无需训练的方法——砖墙扩散（Brick-Diffusion），该方法能够生成任意长度的长视频，并通过砖到墙的去噪策略提高视频质量。实验结果表明，砖墙扩散在生成高保真视频方面优于现有基线方法。	 | Recent advances in diffusion models have greatly improved text-driven video generation. However, training models for long video generation demands significant computational power and extensive data, leading most video diffusion models to be limited to a small number of frames. Existing training-free methods that attempt to generate long videos using pre-trained short video diffusion models often struggle with issues such as insufficient motion dynamics and degraded video fidelity. In this paper, we present Brick-Diffusion, a novel, training-free approach capable of generating long videos of arbitrary length. Our method introduces a brick-to-wall denoising strategy, where the latent is denoised in segments, with a stride applied in subsequent iterations. This process mimics the construction of a staggered brick wall, where each brick represents a denoised segment, enabling communication between frames and improving overall video quality. Through quantitative and qualitative evaluations, we demonstrate that Brick-Diffusion outperforms existing baseline methods in generating high-fidelity videos.	 | 近年来，扩散模型的进步极大地提高了由文本驱动的视频生成能力。然而，训练用于长视频生成的模型需要大量的计算能力和数据，导致大多数视频扩散模型仅限于生成少量帧。目前，一些无需训练的方法尝试使用预训练的短视频扩散模型生成长视频，但这些方法往往面临诸如运动动态不足和视频保真度下降等问题。在本文中，我们提出了砖墙扩散（Brick-Diffusion），这是一种全新的无需训练的方法，能够生成任意长度的长视频。我们的方法引入了一种砖到墙的去噪策略，即在多次迭代中对潜空间进行分段去噪，并在后续迭代中应用步长。这一过程模拟了砖墙的错缝叠砌，每个砖块代表一个去噪段，使帧之间能够进行通信，从而提高整体视频质量。通过定量和定性的评估，我们证明砖墙扩散在生成高保真视频方面优于现有的基线方法。
2501.02740	 | Interpretable Recognition of Fused Magnesium Furnace Working Conditions with Deep Convolutional Stochastic Configuration Networks	 | Li Weitao,Zhang Xinru,Wang Dianhui,Tong Qianqian,Chai Tianyou	 | 本文提出了一种基于深度卷积随机配置网络（DCSCNs）的可解释工作条件识别方法，通过监督学习生成物理上有意义的卷积核，并利用强化学习自适应修剪卷积核，从而提高识别准确性和可解释性。	 | To address the issues of a weak generalization capability and interpretability in working condition recognition model of a fused magnesium furnace, this paper proposes an interpretable working condition recognition method based on deep convolutional stochastic configuration networks (DCSCNs). Firstly, a supervised learning mechanism is employed to generate physically meaningful Gaussian differential convolution kernels. An incremental method is utilized to construct a DCSCNs model, ensuring the convergence of recognition errors in a hierarchical manner and avoiding the iterative optimization process of convolutional kernel parameters using the widely used backpropagation algorithm. The independent coefficient of channel feature maps is defined to obtain the visualization results of feature class activation maps for the fused magnesium furnace. A joint reward function is constructed based on the recognition accuracy, the interpretable trustworthiness evaluation metrics, and the model parameter quantity. Reinforcement learning (RL) is applied to adaptively prune the convolutional kernels of the DCSCNs model, aiming to build a compact, highly performed and interpretable network. The experimental results demonstrate that the proposed method outperforms the other deep learning approaches in terms of recognition accuracy and interpretability.	 | 为了应对融合镁炉工作条件识别模型中泛化能力弱和可解释性差的问题，本文提出了一种基于深度卷积随机配置网络（DCSCNs）的可解释工作条件识别方法。首先，采用监督学习机制生成物理上有意义的高斯微分卷积核。利用增量方法构建DCSCNs模型，确保识别误差在层次化方式下收敛，避免使用广泛使用的反向传播算法进行卷积核参数的迭代优化过程。定义了通道特征图的独立系数，以获得融合镁炉特征类激活图的可视化结果。基于识别准确率、可解释的信任度评估指标以及模型参数数量构建联合奖励函数。采用强化学习（RL）自适应修剪DCSCNs模型的卷积核，旨在构建一个紧凑、高性能且可解释的网络。实验结果表明，所提出的方法在识别准确性和可解释性方面优于其他深度学习方法。
2501.02737	 | Holistic Semantic Representation for Navigational Trajectory Generation	 | Ji Cao,Tongya Zheng,Qinghong Guo,Yu Wang,Junshu Dai,Shunyu Liu,Jie Yang,Jie Song,Mingli Song	 | 本文提出了一种全局语义表示（HOSER）框架，用于提高导航轨迹生成的质量，该框架能够在不同尺度上整合时空语义，并在多个真实世界数据集上显著优于现有方法。	 | Trajectory generation has garnered significant attention from researchers in the field of spatio-temporal analysis, as it can generate substantial synthesized human mobility trajectories that enhance user privacy and alleviate data scarcity. However, existing trajectory generation methods often focus on improving trajectory generation quality from a singular perspective, lacking a comprehensive semantic understanding across various scales. Consequently, we are inspired to develop a HOlistic SEmantic Representation (HOSER) framework for navigational trajectory generation. Given an origin-and-destination (OD) pair and the starting time point of a latent trajectory, we first propose a Road Network Encoder to expand the receptive field of road- and zone-level semantics. Second, we design a Multi-Granularity Trajectory Encoder to integrate the spatio-temporal semantics of the generated trajectory at both the point and trajectory levels. Finally, we employ a Destination-Oriented Navigator to seamlessly integrate destination-oriented guidance. Extensive experiments on three real-world datasets demonstrate that HOSER outperforms state-of-the-art baselines by a significant margin. Moreover, the model's performance in few-shot learning and zero-shot learning scenarios further verifies the effectiveness of our holistic semantic representation.	 | 轨迹生成在时空分析领域的研究者中引起了广泛关注，因为它能够生成大量合成的人类移动轨迹，从而增强用户隐私并缓解数据稀缺问题。然而，现有的轨迹生成方法往往侧重于从单一视角提高轨迹生成质量，缺乏在不同尺度上的全面语义理解。因此，我们灵感源于此，开发了一个全局语义表示（HOlistic SEmantic Representation, HOSER）框架，用于导航轨迹生成。给定一个起讫点（origin-and-destination, OD）对和潜在轨迹的起始时间点，我们首先提出了一种道路网络编码器，以扩展道路和区域级语义的接受域。其次，我们设计了多尺度轨迹编码器，以在点和轨迹两个级别上整合生成轨迹的时空语义。最后，我们采用目标导向导航器无缝集成目标导向的指引。在三个真实世界数据集上的广泛实验表明，HOSER 在性能上显著优于最先进的基线方法。此外，在少量学习和零样本学习场景中，模型的表现进一步验证了我们整体语义表示的有效性。
2501.02706	 | Multilevel Semantic-Aware Model for AI-Generated Video Quality Assessment	 | Jiaze Li,Haoran Xu,Shiding Zhu,Junwei He,Haozhao Wang	 | 本文提出了一种多层次语义感知模型MSA-VQA，利用CLIP基的语义监督和跨注意力机制，在帧、段和视频三个层次上分析AI生成视频的质量，并通过提示语义监督模块和语义变异感知模块提升评估效果，实验结果表明其达到了最先进的性能。	 | The rapid development of diffusion models has greatly advanced AI-generated videos in terms of length and consistency recently, yet assessing AI-generated videos still remains challenging. Previous approaches have often focused on User-Generated Content(UGC), but few have targeted AI-Generated Video Quality Assessment methods. In this work, we introduce MSA-VQA, a Multilevel Semantic-Aware Model for AI-Generated Video Quality Assessment, which leverages CLIP-based semantic supervision and cross-attention mechanisms. Our hierarchical framework analyzes video content at three levels: frame, segment, and video. We propose a Prompt Semantic Supervision Module using text encoder of CLIP to ensure semantic consistency between videos and conditional prompts. Additionally, we propose the Semantic Mutation-aware Module to capture subtle variations between frames. Extensive experiments demonstrate our method achieves state-of-the-art results.	 | 近年来，扩散模型的快速发展极大地推进了AI生成视频的长度和一致性，然而对AI生成视频的评估仍然具有挑战性。之前的方法通常关注用户生成内容(UGC)，但很少专注于AI生成视频质量评估方法。在这项工作中，我们引入了MSA-VQA，一种多层次语义感知模型，该模型利用CLIP基的语义监督和跨注意力机制。我们的分层框架在帧、段和视频三个层次上分析视频内容。我们提出了一种提示语义监督模块，使用CLIP的文本编码器来确保视频之间的语义一致性并与条件提示相关联。此外，我们提出了语义变异感知模块来捕捉帧之间的细微差异。广泛的实验表明，我们的方法达到了最先进的性能。
2501.02701	 | Underwater Image Restoration Through a Prior Guided Hybrid Sense Approach and Extensive Benchmark Analysis	 | Xiaojiao Guo,Xuhang Chen,Shuqiang Wang,Chi-Man Pun	 | 该研究提出了一种新颖的彩色平衡先验引导混合传感水下图像恢复框架（GuidedHybSensUIR），该框架在多尺度上运行，利用细节恢复模块和特征上下文化模块来恢复水下图像的色彩和细节，实验结果表明该方法在多个基准数据集上整体上优于其他37种最先进的方法。	 | Underwater imaging grapples with challenges from light-water interactions, leading to color distortions and reduced clarity. In response to these challenges, we propose a novel Color Balance Prior \textbf{Guided} \textbf{Hyb}rid \textbf{Sens}e \textbf{U}nderwater \textbf{I}mage \textbf{R}estoration framework (\textbf{GuidedHybSensUIR}). This framework operates on multiple scales, employing the proposed \textbf{Detail Restorer} module to restore low-level detailed features at finer scales and utilizing the proposed \textbf{Feature Contextualizer} module to capture long-range contextual relations of high-level general features at a broader scale. The hybridization of these different scales of sensing results effectively addresses color casts and restores blurry details. In order to effectively point out the evolutionary direction for the model, we propose a novel \textbf{Color Balance Prior} as a strong guide in the feature contextualization step and as a weak guide in the final decoding phase. We construct a comprehensive benchmark using paired training data from three real-world underwater datasets and evaluate on six test sets, including three paired and three unpaired, sourced from four real-world underwater datasets. Subsequently, we tested 14 traditional and retrained 23 deep learning existing underwater image restoration methods on this benchmark, obtaining metric results for each approach. This effort aims to furnish a valuable benchmarking dataset for standard basis for comparison. The extensive experiment results demonstrate that our method outperforms 37 other state-of-the-art methods overall on various benchmark datasets and metrics, despite not achieving the best results in certain individual cases. The code and dataset are available at \href{https://github.com/CXH-Research/GuidedHybSensUIR}{https://github.com/CXH-Research/GuidedHybSensUIR}.	 | 水下成像面临着来自光水相互作用的挑战，导致色彩失真和清晰度降低。针对这些挑战，我们提出了一种新颖的彩色平衡先验引导混合传感水下图像恢复框架（GuidedHybSensUIR）。该框架在多尺度上运行，采用提出的细节恢复模块来在细粒度尺度上恢复低级别详细的特征，同时利用提出的特征上下文化模块来在较大尺度上捕捉高级别通用特征的长距离上下文关系。这些不同尺度的传感结果的混合有效地解决了色彩偏移并恢复了模糊的细节。为了有效指导模型的演化方向，我们提出了一种新颖的彩色平衡先验，作为特征上下文化步骤中的强向导，并在最终解码阶段作为弱向导。我们使用来自三个真实世界水下数据集的配对训练数据构建了一个综合基准，并在六组测试集上进行评估，包括三组配对和三组未配对，这些测试集均来自四个真实世界水下数据集。随后，我们在该基准上测试了14种传统方法，并重新训练了23种现有的水下图像恢复深度学习方法，获得了每种方法的度量结果。此次努力旨在为标准比较提供有价值的基准数据集。广泛的实验结果表明，尽管在某些个别情况下未能达到最佳效果，我们的方法在各种基准数据集和度量标准上整体上优于37种其他最先进的方法。代码和数据集可在 \href{https://github.com/CXH-Research/GuidedHybSensUIR}{https://github.com/CXH-Research/GuidedHybSensUIR} 获取。
2501.02699	 | EAGLE: Enhanced Visual Grounding Minimizes Hallucinations in Instructional Multimodal Models	 | Andrés Villa,Juan León Alcázar,Motasem Alfarra,Vladimir Araujo,Alvaro Soto,Bernard Ghanem	 | 本文提出了一种名为EAGLE的方法，通过直接增强视觉组件的能力来缓解多模态架构中的幻觉问题，无需依赖大型语言模型或融合模块，仅通过对原始对比预训练任务进行简单修改即可提高视觉编码器的定位和语言对齐，从而显著减少幻觉的发生。	 | Large language models and vision transformers have demonstrated impressive zero-shot capabilities, enabling significant transferability in downstream tasks. The fusion of these models has resulted in multi-modal architectures with enhanced instructional capabilities. Despite incorporating vast image and language pre-training, these multi-modal architectures often generate responses that deviate from the ground truth in the image data. These failure cases are known as hallucinations. Current methods for mitigating hallucinations generally focus on regularizing the language component, improving the fusion module, or ensembling multiple visual encoders to improve visual representation. In this paper, we address the hallucination issue by directly enhancing the capabilities of the visual component. Our approach, named EAGLE, is fully agnostic to the LLM or fusion module and works as a post-pretraining approach that improves the grounding and language alignment of the visual encoder. We show that a straightforward reformulation of the original contrastive pre-training task results in an improved visual encoder that can be incorporated into the instructional multi-modal architecture without additional instructional training. As a result, EAGLE achieves a significant reduction in hallucinations across multiple challenging benchmarks and tasks.	 | 大型语言模型和视觉变换器展示了惊人的零样本能力，使其在下游任务中具备了显著的迁移性能。将这两种模型结合起来，形成了具有增强指令能力的多模态架构。尽管这些多模态架构在图像和语言预训练方面包含了大量信息，但它们通常会生成与图像数据 ground truth 相差甚远的响应，这些错误情况被称为幻觉。目前缓解幻觉的方法主要集中在正则化语言部分、改进融合模块或ensemble多个视觉编码器以提高视觉表示方面。在本文中，我们通过直接增强视觉组件的能力来解决幻觉问题。我们的方法名为 EAGLE，它完全不依赖于 LLM 或融合模块，并且作为后预训练方法，它能够改进视觉编码器的定位和语言对齐。我们表明，对原始对比预训练任务进行简单的重新表述可以得到一个改进的视觉编码器，该编码器可以直接集成到指令性的多模态架构中，而不需要额外的指令训练。因此，EAGLE 在多个具有挑战性的基准和任务中显著减少了幻觉的发生。
2501.02690	 | GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking	 | Weikang Bian,Zhaoyang Huang,Xiaoyu Shi,Yijin Li,Fu-Yun Wang,Hongsheng Li	 | 该研究提出了一种名为GS-DiT的新框架，通过引入伪4D高斯场和高效的密集3D点跟踪方法，实现了在生成视频时对4D内容（包括相机姿态和对象运动）的可控编辑，从而支持复杂的镜头技巧。	 | 4D video control is essential in video generation as it enables the use of sophisticated lens techniques, such as multi-camera shooting and dolly zoom, which are currently unsupported by existing methods. Training a video Diffusion Transformer (DiT) directly to control 4D content requires expensive multi-view videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that optimizes a 4D representation and renders videos according to different 4D elements, such as camera pose and object motion editing, we bring pseudo 4D Gaussian fields to video generation. Specifically, we propose a novel framework that constructs a pseudo 4D Gaussian field with dense 3D point tracking and renders the Gaussian field for all video frames. Then we finetune a pretrained DiT to generate videos following the guidance of the rendered video, dubbed as GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense 3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art sparse 3D point tracking method, in accuracy and accelerates the inference speed by two orders of magnitude. During the inference stage, GS-DiT can generate videos with the same dynamic content while adhering to different camera parameters, addressing a significant limitation of current video generation models. GS-DiT demonstrates strong generalization capabilities and extends the 4D controllability of Gaussian splatting to video generation beyond just camera poses. It supports advanced cinematic effects through the manipulation of the Gaussian field and camera intrinsics, making it a powerful tool for creative video production. Demos are available at https://wkbian.github.io/Projects/GS-DiT/.	 | 4D 视频控制是视频生成中的关键要素，因为它使得能够使用复杂的镜头技巧，如多机位拍摄和推拉变焦，而现有的方法尚不支持这些技巧。直接训练视频扩散变换器（DiT）来控制4D内容需要昂贵的多视角视频。受单目动态新型视图合成（MDVS）的启发，该方法优化4D表示并根据不同的4D元素（如相机姿态和对象运动编辑）渲染视频，我们引入了伪4D 高斯场到视频生成中。具体来说，我们提出了一种新的框架，使用密集3D点跟踪构建一个伪4D 高斯场，并为所有视频帧渲染该高斯场。然后，我们对预训练的DiT进行微调，使其在生成视频时遵循渲染视频的指导，命名为GS-DiT。为了提高GS-DiT的训练效率，我们还提出了一种高效的密集3D点跟踪（D3D-PT）方法来构建伪4D 高斯场。我们的D3D-PT在准确性和推理速度上均优于现有的最先进的稀疏3D点跟踪方法SpatialTracker，加速了推理速度两个数量级。在推理阶段，GS-DiT能够在保持动态内容一致的同时，遵循不同的相机参数生成视频，从而解决了当前视频生成模型的一个重要限制。GS-DiT展示了强大的泛化能力，并将高斯散点图的4D可控性扩展到视频生成中，不仅限于相机姿态。通过操控高斯场和相机内参，它可以支持高级的电影效果，使其成为创意视频制作的强大工具。演示可在 https://wkbian.github.io/Projects/GS-DiT/ 获取。
2501.02669	 | Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?	 | Simon Park,Abhishek Panigrahi,Yun Cheng,Dingli Yu,Anirudh Goyal,Sanjeev Arora	 | 该研究引入了一个合成框架来评估视觉语言模型在算法视觉推理任务中的能力，涵盖表格读取、网格导航和视觉类比三个任务，并通过消融实验探讨了自回归训练中图像到文本转换的重要性及其对多步骤推理泛化能力的影响。	 | While Vision Language Models (VLMs) are impressive in tasks such as visual question answering (VQA) and image captioning, their ability to apply multi-step reasoning to images has lagged, giving rise to perceptions of modality imbalance or brittleness. Towards systematic study of such issues, we introduce a synthetic framework for assessing the ability of VLMs to perform algorithmic visual reasoning (AVR), comprising three tasks: Table Readout, Grid Navigation, and Visual Analogy. Each has two levels of difficulty, SIMPLE and HARD, and even the SIMPLE versions are difficult for frontier VLMs. We seek strategies for training on the SIMPLE version of the tasks that improve performance on the corresponding HARD task, i.e., S2H generalization. This synthetic framework, where each task also has a text-only version, allows a quantification of the modality imbalance, and how it is impacted by training strategy. Ablations highlight the importance of explicit image-to-text conversion in promoting S2H generalization when using auto-regressive training. We also report results of mechanistic study of this phenomenon, including a measure of gradient alignment that seems to identify training strategies that promote better S2H generalization.	 | 尽管视觉语言模型（VLMs）在视觉问答（VQA）和图像字幕等任务中表现出色，它们在对图像进行多步骤推理方面的能力仍然不足，这引发了模态不平衡或脆弱性的感知。为了系统地研究这些问题，我们引入了一个合成框架，用于评估VLMs进行算法视觉推理（AVR）的能力，该框架包括三个任务：表格读取、网格导航和视觉类比。每个任务都有简单和困难两种难度级别，即使是简单的版本也对最前沿的VLMs具有挑战性。我们寻求在简单版本的任务上进行训练以提高在对应困难任务上的性能，即S2H泛化的策略。该合成框架中，每个任务还包含一个纯文本版本，允许对模态不平衡进行量化，并研究训练策略对其影响。消融实验强调了在使用自回归训练时，显式的图像到文本转换在促进S2H泛化中的重要性。我们还报告了关于这种现象的机理研究结果，包括一个梯度对齐度量，似乎能够识别出促进更好S2H泛化的训练策略。
2501.02649	 | Tighnari: Multi-modal Plant Species Prediction Based on Hierarchical Cross-Attention Using Graph-Based and Vision Backbone-Extracted Features	 | Haixu Liu,Penghao Jiang,Zerui Tao,Muyan Wan,Qiuzhuang Sun	 | 研究利用88,987份欧洲植物调查记录及其相关的环境数据，通过一种基于图结构的特征构造和结果修正方法，结合Swin-Transformer和分层交叉注意力机制，预测了4,716份植物调查的结果，提高了物种组成的预测准确性。	 | Predicting plant species composition in specific spatiotemporal contexts plays an important role in biodiversity management and conservation, as well as in improving species identification tools. Our work utilizes 88,987 plant survey records conducted in specific spatiotemporal contexts across Europe. We also use the corresponding satellite images, time series data, climate time series, and other rasterized environmental data such as land cover, human footprint, bioclimatic, and soil variables as training data to train the model to predict the outcomes of 4,716 plant surveys. We propose a feature construction and result correction method based on the graph structure. Through comparative experiments, we select the best-performing backbone networks for feature extraction in both temporal and image modalities. In this process, we built a backbone network based on the Swin-Transformer Block for extracting temporal Cubes features. We then design a hierarchical cross-attention mechanism capable of robustly fusing features from multiple modalities. During training, we adopt a 10-fold cross-fusion method based on fine-tuning and use a Threshold Top-K method for post-processing. Ablation experiments demonstrate the improvements in model performance brought by our proposed solution pipeline.	 | 在特定的空间和时间背景下预测植物物种组成对于生物多样性的管理和保护以及改进物种识别工具具有重要意义。我们的研究利用了欧洲范围内88,987份植物调查记录。我们还使用了相应的卫星图像、时间序列数据、气候时间序列以及土地覆盖、人类足迹、生物气候和土壤变量等栅格化环境数据作为训练数据，以训练模型预测4,716份植物调查的结果。我们提出了一种基于图结构的特征构造和结果修正方法。通过对照实验，我们在时间维度和图像模态中选择了表现最佳的骨干网络进行特征提取。在这一过程中，我们基于Swin-Transformer Block构建了一个骨干网络以提取时间立方体特征。然后，我们设计了一种分层交叉注意力机制，能够稳健地融合多模态特征。在训练过程中，我们采用了基于微调的10折交叉融合方法，并使用了阈值Top-K方法进行后处理。消融实验表明了我们提出的方法管线在模型性能上的改进。
2501.02640	 | Multispectral Pedestrian Detection with Sparsely Annotated Label	 | Chan Lee,Seungho Shin,Gyeong-Moon Park,Jung Uk Kim	 | 本文提出了一个名为SAMPD的新框架，以解决现有稀疏标注多光谱行人检测方法中的两个问题：一是缺乏有效提升缺失标注质量伪标签的方法，二是依赖固定的真实标注导致学习的行人视觉外观有限。通过引入多光谱行人感知自适应权重模块、正向伪标签增强模块以及自适应行人检索增强模块，SAMPD能够在稀疏标注环境下显著提升多光谱行人检测的性能。	 | Although existing Sparsely Annotated Object Detection (SAOD) approches have made progress in handling sparsely annotated environments in multispectral domain, where only some pedestrians are annotated, they still have the following limitations: (i) they lack considerations for improving the quality of pseudo-labels for missing annotations, and (ii) they rely on fixed ground truth annotations, which leads to learning only a limited range of pedestrian visual appearances in the multispectral domain. To address these issues, we propose a novel framework called Sparsely Annotated Multispectral Pedestrian Detection (SAMPD). For limitation (i), we introduce Multispectral Pedestrian-aware Adaptive Weight (MPAW) and Positive Pseudo-label Enhancement (PPE) module. Utilizing multispectral knowledge, these modules ensure the generation of high-quality pseudo-labels and enable effective learning by increasing weights for high-quality pseudo-labels based on modality characteristics. To address limitation (ii), we propose an Adaptive Pedestrian Retrieval Augmentation (APRA) module, which adaptively incorporates pedestrian patches from ground-truth and dynamically integrates high-quality pseudo-labels with the ground-truth, facilitating a more diverse learning pool of pedestrians. Extensive experimental results demonstrate that our SAMPD significantly enhances performance in sparsely annotated environments within the multispectral domain.	 | 尽管现有的稀疏标注目标检测（SAOD）方法已经在多光谱领域取得了进展，能够处理只有部分行人被标注的稀疏标注环境，但它们仍然存在以下局限性：(i) 缺乏提高缺失标注质量伪标签的方法，以及(ii) 依赖固定的地面真实标注，导致只能学习多光谱领域有限的行人视觉外观。为了应对这些问题，我们提出了一种名为稀疏标注多光谱行人检测（SAMPD）的新框架。针对问题 (i)，我们引入了多光谱行人感知自适应权重（MPAW）模块和正向伪标签增强（PPE）模块。通过利用多光谱知识，这些模块确保生成高质量的伪标签，并根据模态特征增强高质量伪标签的权重，从而实现有效的学习。针对问题 (ii)，我们提出了自适应行人检索增强（APRA）模块，该模块能够自适应地融入来自地面真实标注的行人片段，并动态地将高质量伪标签与地面真实标注整合，从而促进更加多样化的行人学习池。广泛的实验结果表明，我们的SAMPD在多光谱领域的稀疏标注环境中显著提升了性能。
2501.02618	 | Identifying Surgical Instruments in Pedagogical Cataract Surgery Videos through an Optimized Aggregation Network	 | Sanya Sinha,Michal Balazia,Francois Bremond	 | 本文提出了一种基于YOLOV9架构的深度学习模型，该模型结合了可编程梯度信息机制和新颖的一般优化高效层聚合网络，用于实时识别教学性白内障手术视频中的手术器械，并在包含10种器械类别的数据集上取得了优异的性能。	 | Instructional cataract surgery videos are crucial for ophthalmologists and trainees to observe surgical details repeatedly. This paper presents a deep learning model for real-time identification of surgical instruments in these videos, using a custom dataset scraped from open-access sources. Inspired by the architecture of YOLOV9, the model employs a Programmable Gradient Information (PGI) mechanism and a novel Generally-Optimized Efficient Layer Aggregation Network (Go-ELAN) to address the information bottleneck problem, enhancing Minimum Average Precision (mAP) at higher Non-Maximum Suppression Intersection over Union (NMS IoU) scores. The Go-ELAN YOLOV9 model, evaluated against YOLO v5, v7, v8, v9 vanilla, Laptool and DETR, achieves a superior mAP of 73.74 at IoU 0.5 on a dataset of 615 images with 10 instrument classes, demonstrating the effectiveness of the proposed model.	 | 以下是该摘要的中文翻译：  教学性白内障手术视频对于眼科医生和实习生反复观察手术细节至关重要。本文提出了一种深度学习模型，用于实时识别这些视频中的手术器械，该模型利用了一个从开放访问来源抓取的自定义数据集。该模型受到YOLOV9架构的启发，采用了一种可编程梯度信息（PGI）机制和一种新颖的一般优化高效层聚合网络（Go-ELAN），以解决信息瓶颈问题，从而在较高的非极大值抑制交并比（NMS IoU）得分下提高最小平均精确度（mAP）。在包含10种器械类别的615张图像数据集上，Go-ELAN YOLOV9模型与YOLO v5、v7、v8、v9 vanilla、Laptool和DETR进行了对比评估，实现了在IoU 0.5时mAP为73.74的优异结果，证明了所提模型的有效性。
2501.02593	 | Evolving Skeletons: Motion Dynamics in Action Recognition	 | Jushang Qiu,Lei Wang	 | 本文评估了时空图卷积网络（ST-GCN）和超图模型（如Hyperformer）在传统骨架序列和通过Taylor Videos变换后的骨架序列上的性能，并分析了这两种表示方法的优缺点，特别是在运动动力学上的增强效果及其面临的挑战。	 | Skeleton-based action recognition has gained significant attention for its ability to efficiently represent spatiotemporal information in a lightweight format. Most existing approaches use graph-based models to process skeleton sequences, where each pose is represented as a skeletal graph structured around human physical connectivity. Among these, the Spatiotemporal Graph Convolutional Network (ST-GCN) has become a widely used framework. Alternatively, hypergraph-based models, such as the Hyperformer, capture higher-order correlations, offering a more expressive representation of complex joint interactions. A recent advancement, termed Taylor Videos, introduces motion-enhanced skeleton sequences by embedding motion concepts, providing a fresh perspective on interpreting human actions in skeleton-based action recognition. In this paper, we conduct a comprehensive evaluation of both traditional skeleton sequences and Taylor-transformed skeletons using ST-GCN and Hyperformer models on the NTU-60 and NTU-120 datasets. We compare skeletal graph and hypergraph representations, analyzing static poses against motion-injected poses. Our findings highlight the strengths and limitations of Taylor-transformed skeletons, demonstrating their potential to enhance motion dynamics while exposing current challenges in fully using their benefits. This study underscores the need for innovative skeletal modelling techniques to effectively handle motion-rich data and advance the field of action recognition.	 | 基于骨架的动作识别由于能够以轻量级格式高效表示时空信息而受到了广泛关注。现有的大多数方法使用图模型来处理骨架序列，其中每个姿态都以围绕人体物理连接构建的骨骼图的形式表示。在这之中，时空图卷积网络（ST-GCN）已成为广泛应用的框架。另一种方法是基于超图的模型，如Hyperformer，能够捕捉更高阶的相关性，提供对复杂关节交互的更丰富表示。最近的一项进展称为Taylor Videos，通过嵌入运动概念引入了运动增强的骨架序列，为基于骨架的动作识别提供了新的视角。在本文中，我们使用ST-GCN和Hyperformer模型在NTU-60和NTU-120数据集上对传统骨架序列和Taylor变换后的骨架序列进行了全面评估。我们比较了骨骼图和超图表示，分析了静态姿态与运动注入姿态之间的差异。我们的研究结果突显了Taylor变换后的骨架序列的优缺点，表明它们在增强运动动力学方面具有潜力，同时也揭示了充分利用其优势所面临的挑战。本研究强调了需要创新的骨骼建模技术来有效处理富含运动的数据，并推动动作识别领域的进步。
2501.02584	 | Efficient Architectures for High Resolution Vision-Language Models	 | Miguel Carvalho,Bruno Martins	 | Pheye是一种新型架构，能够高效处理高分辨率图像且参数量少于同等规模的视觉语言模型，特别是在需要精细图像理解或处理场景文本的任务中表现出色。	 | Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This work introduces Pheye, a novel architecture that efficiently processes high-resolution images while training fewer parameters than similarly sized VLMs. Notably, Pheye achieves a high efficiency while maintaining strong performance, particularly in tasks that demand fine-grained image understanding and/or the handling of scene-text.	 | 视觉语言模型（VLMs）最近取得了显著的进步。然而，准确识别高分辨率图像中的细微细节仍然面临挑战，这限制了其在多种任务中的性能。本工作提出了Pheye，一种新型架构，能够高效处理高分辨率图像，同时参数量少于同等规模的VLMs。值得注意的是，Pheye 在保持强大性能的同时，尤其在需要精细图像理解或处理场景文本的任务中表现出色。
2501.02576	 | DepthMaster: Taming Diffusion Models for Monocular Depth Estimation	 | Ziyang Song,Zerong Wang,Bo Li,Hao Zhang,Ruijie Zhu,Li Liu,Peng-Tao Jiang,Tianzhu Zhang	 | 本文提出了一种名为DepthMaster的单步扩散模型，该模型通过特征对齐模块和傅里叶增强模块改进了生成特征与判别特征之间的差距，从而在保持高效推理的同时提高了深度估计的泛化能力和细节保留能力，优于现有基于扩散的方法。	 | Monocular depth estimation within the diffusion-denoising paradigm demonstrates impressive generalization ability but suffers from low inference speed. Recent methods adopt a single-step deterministic paradigm to improve inference efficiency while maintaining comparable performance. However, they overlook the gap between generative and discriminative features, leading to suboptimal results. In this work, we propose DepthMaster, a single-step diffusion model designed to adapt generative features for the discriminative depth estimation task. First, to mitigate overfitting to texture details introduced by generative features, we propose a Feature Alignment module, which incorporates high-quality semantic features to enhance the denoising network's representation capability. Second, to address the lack of fine-grained details in the single-step deterministic framework, we propose a Fourier Enhancement module to adaptively balance low-frequency structure and high-frequency details. We adopt a two-stage training strategy to fully leverage the potential of the two modules. In the first stage, we focus on learning the global scene structure with the Feature Alignment module, while in the second stage, we exploit the Fourier Enhancement module to improve the visual quality. Through these efforts, our model achieves state-of-the-art performance in terms of generalization and detail preservation, outperforming other diffusion-based methods across various datasets. Our project page can be found at https://indu1ge.github.io/DepthMaster_page.	 | 在基于扩散去噪的单目深度估计框架下，该方法展示了令人印象深刻的泛化能力，但速度较慢。最近的方法采用单步确定性范式来提高推理效率，同时保持与先前方法相当的性能。然而，它们忽略了生成特征与判别特征之间的差距，导致结果次优。在本文中，我们提出了一种名为DepthMaster的单步扩散模型，旨在适应生成特征以用于判别深度估计任务。首先，为了缓解生成特征引入的纹理细节导致的过拟合问题，我们提出了一种特征对齐模块，该模块加入了高质量的语义特征，以增强去噪网络的表示能力。其次，为了应对单步确定性框架缺乏细粒度细节的问题，我们提出了一种傅里叶增强模块，以自适应地平衡低频结构和高频细节。我们采用两阶段训练策略以充分利用两个模块的潜力。在第一阶段，我们重点利用特征对齐模块学习全局场景结构；在第二阶段，我们利用傅里叶增强模块提升视觉质量。通过这些努力，我们的模型在泛化能力和细节保留方面达到了最先进的性能，超越了其他基于扩散的方法在多个数据集上的表现。我们的项目页面可以在 https://indu1ge.github.io/DepthMaster_page 找到。
2501.02570	 | Decoding fMRI Data into Captions using Prefix Language Modeling	 | Vyacheslav Shen,Kassymzhomart Kunanbayev,Dae-Shik Kim	 | 该研究提出了一种新的大脑解码方法，通过从fMRI信号预测DINOv2模型的图像嵌入，并将该嵌入的[CLS]令牌提供给GPT-2语言模型生成图caption，以此减少计算需求并更好地利用体素位置信息。	 | With the advancements in Large Language and Latent Diffusion models, brain decoding has achieved remarkable results in recent years. The works on the NSD dataset, with stimuli images from the COCO dataset, leverage the embeddings from the CLIP model for image reconstruction and GIT for captioning. However, the current captioning approach introduces the challenge of potential data contamination given that the GIT model was trained on the COCO dataset. In this work, we present an alternative method for decoding brain signals into image captions by predicting a DINOv2 model's embedding of an image from the corresponding fMRI signal and then providing its [CLS] token as the prefix to the GPT-2 language model which decreases computational requirements considerably. Additionally, instead of commonly used Linear Regression, we explore 3D Convolutional Neural Network mapping of fMRI signals to image embedding space for better accounting positional information of voxels.	 | 随着大型语言模型和潜在扩散模型的发展，近年来大脑解码取得了显著成果。基于NSD数据集的工作，使用COCO数据集中的刺激图像，利用CLIP模型的嵌入进行图像重构，并使用GIT进行图caption生成。然而，当前的caption生成方法引入了潜在的数据污染问题，因为GIT模型是在COCO数据集上训练的。在这项工作中，我们提出了一种替代方法，通过从对应的fMRI信号预测一个DINOv2模型的图像嵌入，然后将该嵌入的[CLS]令牌作为前缀提供给GPT-2语言模型，从而大大减少了计算需求。此外，我们探索了从fMRI信号到图像嵌入空间的3D卷积神经网络映射方法，而不是常用的线性回归，以更好地考虑体素的位置信息。
2501.02564	 | Balanced Multi-view Clustering	 | Zhenglai Li,Jun Wang,Chang Tang,Xinzhong Zhu,Wei Zhang,Xinwang Liu	 | 本文分析了多视图聚类中联合训练范式的不平衡问题，并提出了一种新颖的平衡多视图聚类（BMvC）方法，该方法通过引入视图特异对比正则化（VCR）来优化每个视图的特征提取，从而更均衡地利用多视图信息以提高聚类效果。	 | Multi-view clustering (MvC) aims to integrate information from different views to enhance the capability of the model in capturing the underlying data structures. The widely used joint training paradigm in MvC is potentially not fully leverage the multi-view information, since the imbalanced and under-optimized view-specific features caused by the uniform learning objective for all views. For instance, particular views with more discriminative information could dominate the learning process in the joint training paradigm, leading to other views being under-optimized. To alleviate this issue, we first analyze the imbalanced phenomenon in the joint-training paradigm of multi-view clustering from the perspective of gradient descent for each view-specific feature extractor. Then, we propose a novel balanced multi-view clustering (BMvC) method, which introduces a view-specific contrastive regularization (VCR) to modulate the optimization of each view. Concretely, VCR preserves the sample similarities captured from the joint features and view-specific ones into the clustering distributions corresponding to view-specific features to enhance the learning process of view-specific feature extractors. Additionally, a theoretical analysis is provided to illustrate that VCR adaptively modulates the magnitudes of gradients for updating the parameters of view-specific feature extractors to achieve a balanced multi-view learning procedure. In such a manner, BMvC achieves a better trade-off between the exploitation of view-specific patterns and the exploration of view-invariance patterns to fully learn the multi-view information for the clustering task. Finally, a set of experiments are conducted to verify the superiority of the proposed method compared with state-of-the-art approaches both on eight benchmark MvC datasets and two spatially resolved transcriptomics datasets.	 | 多视图聚类（MvC）旨在通过集成不同视图的信息来增强模型捕捉潜在数据结构的能力。在MvC中广泛使用的联合训练范式可能未能充分利用多视图信息，因为所有视图统一的学习目标导致了视图特异特征的不平衡和优化不足。例如，具有更多区分性信息的特定视图可能在联合训练范式中主导学习过程，导致其他视图的优化不足。为了解决这一问题，我们首先从每个视图特异特征提取器的梯度下降角度分析了多视图聚类联合训练范式的不平衡现象。然后，我们提出了一种新颖的平衡多视图聚类（BMvC）方法，该方法引入了视图特异对比正则化（VCR）来调节每个视图的优化。具体而言，VCR 保留了由联合特征和视图特异特征捕获的样本相似性，并将其纳入相应的聚类分布中，以增强视图特异特征提取器的学习过程。此外，我们提供了理论分析来说明VCR自适应调节更新视图特异特征提取器参数的梯度大小，以实现平衡的多视图学习过程。在这种方式下，BMvC 在充分学习多视图信息以完成聚类任务方面实现了视图特异模式利用和视图不变模式探索之间的更好权衡。最后，我们在八个基准MvC数据集和两个空间解析转录组学数据集上进行了一系列实验，以验证与最新方法相比，所提出方法的优越性。
2501.02539	 | AHMSA-Net: Adaptive Hierarchical Multi-Scale Attention Network for Micro-Expression Recognition	 | Lijun Zhang,Yifan Zhang,Weicheng Tang,Xinzhi Sun,Xiaomeng Wang,Zhanshan Li	 | 本文提出了一种适应性分层多尺度注意力网络（AHMSA-Net）用于情绪微表达识别（MER），通过动态调整光流特征图的大小并融合不同尺度的特征来提高细微变化的捕捉能力，实验结果表明AHMSA-Net在主要的微表情数据库上达到了78.21%到77.08%的识别准确率。	 | Micro-expression recognition (MER) presents a significant challenge due to the transient and subtle nature of the motion changes involved. In recent years, deep learning methods based on attention mechanisms have made some breakthroughs in MER. However, these methods still suffer from the limitations of insufficient feature capture and poor dynamic adaptation when coping with the instantaneous subtle movement changes of micro-expressions. Therefore, in this paper, we design an Adaptive Hierarchical Multi-Scale Attention Network (AHMSA-Net) for MER. Specifically, we first utilize the onset and apex frames of the micro-expression sequence to extract three-dimensional (3D) optical flow maps, including horizontal optical flow, vertical optical flow, and optical flow strain. Subsequently, the optical flow feature maps are inputted into AHMSA-Net, which consists of two parts: an adaptive hierarchical framework and a multi-scale attention mechanism. Based on the adaptive downsampling hierarchical attention framework, AHMSA-Net captures the subtle changes of micro-expressions from different granularities (fine and coarse) by dynamically adjusting the size of the optical flow feature map at each layer. Based on the multi-scale attention mechanism, AHMSA-Net learns micro-expression action information by fusing features from different scales (channel and spatial). These two modules work together to comprehensively improve the accuracy of MER. Additionally, rigorous experiments demonstrate that the proposed method achieves competitive results on major micro-expression databases, with AHMSA-Net achieving recognition accuracy of up to 78.21% on composite databases (SMIC, SAMM, CASMEII) and 77.08% on the CASME^{}3 database.	 | 情绪微表达识别（MER）由于涉及的运动变化短暂且微妙，因此提出了重大挑战。近年来，基于注意力机制的深度学习方法在MER方面取得了一些突破。然而，这些方法仍受到特征捕捉不足和动态适应性差的限制，尤其是在处理微表情瞬时的细微运动变化时。因此，本文设计了适应性分层多尺度注意力网络（AHMSA-Net）用于MER。具体地，我们首先利用微表情序列的起始帧和顶点帧，提取三维（3D）光流图，包括水平光流、垂直光流和光流应变。随后，将光流特征图输入AHMSA-Net，该网络由两部分组成：适应性分层框架和多尺度注意力机制。基于自适应下采样分层注意力框架，AHMSA-Net通过动态调整每一层光流特征图的大小，从不同粒度（精细和粗略）捕获微表情的细微变化。基于多尺度注意力机制，AHMSA-Net通过融合不同尺度（通道和空间）的特征来学习微表情动作信息。这两个模块共同工作，全面提高了情绪微表达识别的准确性。此外，严格的实验表明，所提出的方法在主要的微表情数据库中取得了竞争力的结果，AHMSA-Net在综合数据库（SMIC，SAMM，CASMEII）上的识别准确率达到78.21%，在CASME^{}3数据库上的识别准确率达到77.08%。
2501.02534	 | Pixel-Wise Feature Selection for Perceptual Edge Detection without post-processing	 | Hao Shu	 | 尽管深度卷积神经网络在图像边缘检测方面取得了显著进展，但现有模型仍高度依赖于非最大抑制等后处理技术，且在高精度要求下表现不佳。本文提出了一种新的深度网络特征选择范式，能够促进特征的差异化选择，并无缝集成到现有边缘检测模型中，从而在无需后处理的情况下显著提升性能和感知质量。	 | Although deep convolutional neutral networks (CNNs) have significantly enhanced performance in image edge detection (ED), current models remain highly dependent on post-processing techniques such as non-maximum suppression (NMS), and often fail to deliver satisfactory perceptual results, while the performance will deteriorate significantly if the allowed error toleration distance decreases. These limitations arise from the uniform fusion of features across all pixels, regardless of their specific characteristics, such as the distinction between textural and edge areas. If the features extracted by the ED models are selected more meticulously and encompass greater diversity, the resulting predictions are expected to be more accurate and perceptually meaningful. Motivated by this observation, this paper proposes a novel feature selection paradigm for deep networks that facilitates the differential selection of features and can be seamlessly integrated into existing ED models. By incorporating this additional structure, the performance of conventional ED models is substantially enhanced without post-processing, while simultaneously enhancing the perceptual quality of the predictions. Extensive experimental evaluations validate the effectiveness of the proposed model.	 | 尽管深度卷积神经网络（CNNs）在图像边缘检测（ED）方面显著提升了性能，但当前的模型依然高度依赖于非最大抑制（NMS）等后处理技术，经常无法提供满意的感知结果，尤其是在允许的误差容忍距离减小时，性能会大幅下降。这些限制源于所有像素特征的统一融合，而不考虑它们的具体特征，如纹理区域和边缘区域的区别。如果边缘检测模型提取的特征更加精心选择且更为多样化，预计最终的预测会更加准确和具有感知意义。基于这一观察，本文提出了一种新的深度网络特征选择范式，该范式可以促进特征的差异化选择，并且能够无缝集成到现有的边缘检测模型中。通过引入这种额外的结构，传统的边缘检测模型在无需后处理的情况下性能得到了显著提升，同时提高了预测的感知质量。大量的实验评估验证了所提出模型的有效性。
2501.02527	 | Vision-Driven Prompt Optimization for Large Language Models in Multimodal Generative Tasks	 | Leo Franklin,Apiradee Boonmee,Kritsada Wongsuwan	 | 本文提出了一种名为视觉驱动的提示优化（VDPO）的新框架，利用大型语言模型动态生成文本提示以指导高保真图像合成，并在多个基准测试中显著优于现有方法。VDPO展示了出色的可扩展性、鲁棒性和泛化能力，适用于多种视觉生成任务，并得到了人类评估的验证。	 | Vision generation remains a challenging frontier in artificial intelligence, requiring seamless integration of visual understanding and generative capabilities. In this paper, we propose a novel framework, Vision-Driven Prompt Optimization (VDPO), that leverages Large Language Models (LLMs) to dynamically generate textual prompts from visual inputs, guiding high-fidelity image synthesis. VDPO combines a visual embedding prompt tuner, a textual instruction generator, and a vision generation module to achieve state-of-the-art performance in diverse vision generation tasks. Extensive experiments on benchmarks such as COCO and Sketchy demonstrate that VDPO consistently outperforms existing methods, achieving significant improvements in FID, LPIPS, and BLEU/CIDEr scores. Additional analyses reveal the scalability, robustness, and generalization capabilities of VDPO, making it a versatile solution for in-domain and out-of-domain tasks. Human evaluations further validate the practical superiority of VDPO in generating visually appealing and semantically coherent outputs.	 | 视觉生成仍然是人工智能领域的技术前沿，需要无缝集成视觉理解与生成能力。在这篇论文中，我们提出了一个新颖的框架——视觉驱动的提示优化（VDPO），该框架利用大型语言模型（LLMs）从视觉输入动态生成文本提示，引导高保真图像合成。VDPO结合了视觉嵌入提示调谐器、文本指令生成器和视觉生成模块，以在各种视觉生成任务中达到最先进的性能。在COCO和Sketchy等基准上的大量实验表明，VDPO始终优于现有方法，在FID、LPIPS和BLEU/CIDEr得分上取得了显著改进。进一步的分析显示了VDPO的可扩展性、鲁棒性和泛化能力，使其成为领域内和领域外任务的多功能解决方案。人类评估进一步验证了VDPO在生成视觉上吸引人且语义上一致的输出方面的实际优势。
2501.02523	 | Face-MakeUp: Multimodal Facial Prompts for Text-to-Image Generation	 | Dawei Dai,Mingming Jia,Yinxiu Zhou,Hang Xing,Chenghang Li	 | 本文提出了一个名为Face-MakeUp的模型，该模型基于一个包含400万高质量面部图像-文本对的新数据集FaceCaptionHQ-4M进行训练，旨在通过图像提示生成所需的面部图像。通过整合多尺度内容特征和姿态特征，Face-MakeUp在保持面部一致性方面表现出色，验证结果表明其在多个面部图像生成任务中达到最佳性能。	 | Facial images have extensive practical applications. Although the current large-scale text-image diffusion models exhibit strong generation capabilities, it is challenging to generate the desired facial images using only text prompt. Image prompts are a logical choice. However, current methods of this type generally focus on general domain. In this paper, we aim to optimize image makeup techniques to generate the desired facial images. Specifically, (1) we built a dataset of 4 million high-quality face image-text pairs (FaceCaptionHQ-4M) based on LAION-Face to train our Face-MakeUp model; (2) to maintain consistency with the reference facial image, we extract/learn multi-scale content features and pose features for the facial image, integrating these into the diffusion model to enhance the preservation of facial identity features for diffusion models. Validation on two face-related test datasets demonstrates that our Face-MakeUp can achieve the best comprehensive performance.All codes are available at:https://github.com/ddw2AIGROUP2CQUPT/Face-MakeUp	 | 面部图像具有广泛的实际应用。尽管当前大规模的文本-图像扩散模型具有很强的生成能力，但仅通过文本提示生成所需面部图像仍然是一个挑战。图像提示是一个合理的选择。然而，目前这类方法通常集中在通用领域。本文旨在优化面部化妆技术以生成所需的面部图像。具体而言，（1）基于LAION-Face构建了一个包含400万高质量面部图像-文本对的数据集（FaceCaptionHQ-4M）来训练我们的Face-MakeUp模型；（2）为了保持与参考面部图像的一致性，我们提取/学习面部图像的多尺度内容特征和姿态特征，并将这些特征整合到扩散模型中，以增强扩散模型对面部身份特征的保留。在两个面部相关测试数据集上的验证表明，我们的Face-MakeUp可以实现最佳的整体性能。所有代码可在以下链接获取：https://github.com/ddw2AIGROUP2CQUPT/Face-MakeUp
2501.02519	 | Layout2Scene: 3D Semantic Layout Guided Scene Generation via Geometry and Appearance Diffusion Priors	 | Minglin Chen,Longguang Wang,Sheng Ao,Ye Zhang,Kai Xu,Yulan Guo	 | 该研究提出了一种名为Layout2Scene的方法，通过使用附加的语义布局作为提示，来生成合理的3D场景，该方法利用两阶段方案优化初始化场景的几何结构和外观，并通过语义引导的扩散模型提高生成质量。实验结果表明，该方法能生成更合理和真实的场景，并支持灵活编辑，促进多种下游应用。	 | 3D scene generation conditioned on text prompts has significantly progressed due to the development of 2D diffusion generation models. However, the textual description of 3D scenes is inherently inaccurate and lacks fine-grained control during training, leading to implausible scene generation. As an intuitive and feasible solution, the 3D layout allows for precise specification of object locations within the scene. To this end, we present a text-to-scene generation method (namely, Layout2Scene) using additional semantic layout as the prompt to inject precise control of 3D object positions. Specifically, we first introduce a scene hybrid representation to decouple objects and backgrounds, which is initialized via a pre-trained text-to-3D model. Then, we propose a two-stage scheme to optimize the geometry and appearance of the initialized scene separately. To fully leverage 2D diffusion priors in geometry and appearance generation, we introduce a semantic-guided geometry diffusion model and a semantic-geometry guided diffusion model which are finetuned on a scene dataset. Extensive experiments demonstrate that our method can generate more plausible and realistic scenes as compared to state-of-the-art approaches. Furthermore, the generated scene allows for flexible yet precise editing, thereby facilitating multiple downstream applications.	 | 基于2D扩散生成模型的发展，条件于文本提示的3D场景生成取得了显著进展。然而，3D场景的文本描述在训练过程中固有地不准确且缺乏精细的控制，导致生成的场景不合理。为此，3D布局提供了一种直观且可行的解决方案，允许精确指定场景中物体的位置。为此，我们提出了一种基于文本生成场景的方法（名为Layout2Scene），使用附加的语义布局作为提示，以注入对3D物体位置的精确控制。具体而言，我们首先引入了一种场景混合表示，通过预训练的文本到3D模型初始化。然后，我们提出了一种两阶段方案分别优化初始化场景的几何结构和外观。为了充分利用2D扩散先验在几何结构和外观生成中的作用，我们引入了语义引导的几何扩散模型和语义-几何引导的扩散模型，并在场景数据集上进行微调。大量实验表明，与最先进的方法相比，我们的方法可以生成更合理和真实的场景。此外，生成的场景允许灵活而精确的编辑，从而促进多种下游应用。
2501.02509	 | Facial Attractiveness Prediction in Live Streaming: A New Benchmark and Multi-modal Method	 | Hui Li,Xiaoyu Ren,Hongjiu Yu,Huiyu Duan,Kai Li,Ying Chen,Libo Wang,Xiongkuo Min,Guangtao Zhai,Xu Liu	 | 本文提出了LiveBeauty，首个针对直播场景的大规模面部吸引力预测数据集，并通过多模态FAP方法在直播中的面部吸引力评估上取得了最先进的性能。该数据集包含10,000张面部图像及200,000个吸引力注释，适用于直播场景下的面部修复和内容推荐等应用。	 | Facial attractiveness prediction (FAP) has long been an important computer vision task, which could be widely applied in live streaming for facial retouching, content recommendation, etc. However, previous FAP datasets are either small, closed-source, or lack diversity. Moreover, the corresponding FAP models exhibit limited generalization and adaptation ability. To overcome these limitations, in this paper we present LiveBeauty, the first large-scale live-specific FAP dataset, in a more challenging application scenario, i.e., live streaming. 10,000 face images are collected from a live streaming platform directly, with 200,000 corresponding attractiveness annotations obtained from a well-devised subjective experiment, making LiveBeauty the largest open-access FAP dataset in the challenging live scenario. Furthermore, a multi-modal FAP method is proposed to measure the facial attractiveness in live streaming. Specifically, we first extract holistic facial prior knowledge and multi-modal aesthetic semantic features via a Personalized Attractiveness Prior Module (PAPM) and a Multi-modal Attractiveness Encoder Module (MAEM), respectively, then integrate the extracted features through a Cross-Modal Fusion Module (CMFM). Extensive experiments conducted on both LiveBeauty and other open-source FAP datasets demonstrate that our proposed method achieves state-of-the-art performance. Dataset will be available soon.	 | 面部吸引力预测（FAP）一直是重要的计算机视觉任务，可以广泛应用在直播中的面部修复、内容推荐等领域。然而，之前的FAP数据集要么规模小，要么是闭源的，要么缺乏多样性。此外，相应的FAP模型的泛化能力和适应能力也有限。为克服这些限制，在本文中，我们提出了LiveBeauty，这是首个针对直播特定的大规模FAP数据集，应用于更具挑战性的应用场景，即直播。从直播平台直接收集了10,000张面部图像，并通过精心设计的主观实验获得了200,000个对应吸引力注释，使得LiveBeauty成为在挑战性直播场景下最大的开放访问FAP数据集。此外，我们提出了一种多模态FAP方法，以测量直播中的面部吸引力。具体而言，我们首先通过个性化吸引力先验模块（PAPM）和多模态吸引力编码模块（MAEM）分别提取整体面部先验知识和多模态审美语义特征，然后通过跨模态融合模块（CMFM）整合提取的特征。在LiveBeauty和其他开源FAP数据集上的广泛实验表明，我们提出的方法达到了最先进的性能。数据集即将发布。
2501.02504	 | Watch Video, Catch Keyword: Context-aware Keyword Attention for Moment Retrieval and Highlight Detection	 | Sung Jin Um,Dongjin Kim,Sangmin Lee,Jung Uk Kim	 | 本文提出了一种新的视频上下文感知关键词注意力模块，通过结合视频上下文聚类和关键词感知对比学习，以提高片段检索和高光检测任务的性能。实验结果表明，该方法显著优于现有方法，在多个基准测试上取得了更好的表现。	 | The goal of video moment retrieval and highlight detection is to identify specific segments and highlights based on a given text query. With the rapid growth of video content and the overlap between these tasks, recent works have addressed both simultaneously. However, they still struggle to fully capture the overall video context, making it challenging to determine which words are most relevant. In this paper, we present a novel Video Context-aware Keyword Attention module that overcomes this limitation by capturing keyword variation within the context of the entire video. To achieve this, we introduce a video context clustering module that provides concise representations of the overall video context, thereby enhancing the understanding of keyword dynamics. Furthermore, we propose a keyword weight detection module with keyword-aware contrastive learning that incorporates keyword information to enhance fine-grained alignment between visual and textual features. Extensive experiments on the QVHighlights, TVSum, and Charades-STA benchmarks demonstrate that our proposed method significantly improves performance in moment retrieval and highlight detection tasks compared to existing approaches. Our code is available at: https://github.com/VisualAIKHU/Keyword-DETR	 | 视频片段检索和高光检测的目标是根据给定的文本查询识别特定的片段和高光。随着视频内容的迅速增长以及这些任务之间的重叠，近期的研究工作开始同时处理这两个任务。然而，它们仍然难以全面捕捉视频的整体上下文，因此难以确定哪些词语最为相关。本文中，我们提出了一种新的视频上下文感知关键词注意力模块，通过在整个视频上下文中的关键词变化来克服这一限制。为此，我们引入了一个视频上下文聚类模块，提供整体视频上下文的简洁表示，从而增强关键词动态的理解。此外，我们提出了一个关键词感知对比学习的关键词权重检测模块，将关键词信息纳入其中，以增强视觉和文本特征的细粒度对齐。我们在QVHighlights、TVSum和Charades-STA基准测试上的广泛实验表明，与现有方法相比，我们提出的算法在片段检索和高光检测任务上的性能显著提高。我们的代码可在以下链接获取：https://github.com/VisualAIKHU/Keyword-DETR
2501.02487	 | ACE++: Instruction-Based Image Creation and Editing via Context-Aware Content Filling	 | Chaojie Mao,Jingfeng Zhang,Yulin Pan,Zeyinzi Jiang,Zhen Han,Yu Liu,Jingren Zhou	 | 该研究提出了一种名为ACE++的基于指令的扩散框架，改进了输入格式并采用了两阶段训练方案来处理多种图像生成和编辑任务，显示出在图像质量和提示遵循能力上的优势。	 | We report ACE++, an instruction-based diffusion framework that tackles various image generation and editing tasks. Inspired by the input format for the inpainting task proposed by FLUX.1-Fill-dev, we improve the Long-context Condition Unit (LCU) introduced in ACE and extend this input paradigm to any editing and generation tasks. To take full advantage of image generative priors, we develop a two-stage training scheme to minimize the efforts of finetuning powerful text-to-image diffusion models like FLUX.1-dev. In the first stage, we pre-train the model using task data with the 0-ref tasks from the text-to-image model. There are many models in the community based on the post-training of text-to-image foundational models that meet this training paradigm of the first stage. For example, FLUX.1-Fill-dev deals primarily with painting tasks and can be used as an initialization to accelerate the training process. In the second stage, we finetune the above model to support the general instructions using all tasks defined in ACE. To promote the widespread application of ACE++ in different scenarios, we provide a comprehensive set of models that cover both full finetuning and lightweight finetuning, while considering general applicability and applicability in vertical scenarios. The qualitative analysis showcases the superiority of ACE++ in terms of generating image quality and prompt following ability.	 | 我们报告了一种基于指令的扩散框架ACE++，它解决了多种图像生成和编辑任务。受FLUX.1-Fill-dev提出用于修复任务的输入格式启发，我们改进了ACE中引入的长上下文条件单元（LCU），并将这一输入范式扩展到任何编辑和生成任务中。为了充分利用图像生成先验，我们开发了一种两阶段训练方案，以减少调优强大的文本到图像扩散模型（如FLUX.1-dev）所需的精力。在第一阶段，我们使用带有文本到图像模型0-ref任务的数据来预训练模型。社区中有许多基于文本到图像基础模型后训练的模型符合第一阶段的训练范式，例如FLUX.1-Fill-dev主要处理绘画任务，并可作为初始化以加速训练过程。在第二阶段，我们进一步调优上述模型，以支持使用ACE中定义的所有任务的通用指令。为了促进ACE++在不同场景中的广泛应用，我们提供了一套全面的模型，涵盖了完全调优和轻量级调优，同时考虑了通用适用性和垂直场景的适用性。定性的分析展示了ACE++在图像质量和提示遵循能力方面优于其他方法。
2501.02476	 | Noise-Tolerant Hybrid Prototypical Learning with Noisy Web Data	 | Chao Liang,Linchao Zhu,Zongxin Yang,Wei Chen,Yi Yang	 | 该研究解决了在少量干净标注图像和大量噪声标注网络图像的情况下学习无偏分类器的问题，提出了一种名为SimNoiPro的相似性最大化损失方法，通过生成耐噪声混合原型并优化噪声图像的多样性，从而改善了类原型的质量和分类性能。	 | We focus on the challenging problem of learning an unbiased classifier from a large number of potentially relevant but noisily labeled web images given only a few clean labeled images. This problem is particularly practical because it reduces the expensive annotation costs by utilizing freely accessible web images with noisy labels. Typically, prototypes are representative images or features used to classify or identify other images. However, in the few clean and many noisy scenarios, the class prototype can be severely biased due to the presence of irrelevant noisy images. The resulting prototypes are less compact and discriminative, as previous methods do not take into account the diverse range of images in the noisy web image collections. On the other hand, the relation modeling between noisy and clean images is not learned for the class prototype generation in an end-to-end manner, which results in a suboptimal class prototype. In this article, we introduce a similarity maximization loss named SimNoiPro. Our SimNoiPro first generates noise-tolerant hybrid prototypes composed of clean and noise-tolerant prototypes and then pulls them closer to each other. Our approach considers the diversity of noisy images by explicit division and overcomes the optimization discrepancy issue. This enables better relation modeling between clean and noisy images and helps extract judicious information from the noisy image set. The evaluation results on two extended few-shot classification benchmarks confirm that our SimNoiPro outperforms prior methods in measuring image relations and cleaning noisy data.	 | 我们专注于从大量潜在相关但标注不准确的网络图像中学习无偏分类器的问题，仅需少量干净标注的图像。这个问题特别具有实践意义，因为它通过利用免费可访问的网络图像（这些图像带有噪声标签）来减少昂贵的标注成本。通常，原型是用于分类或识别其他图像的代表性图像或特征。然而，在少量干净标注和大量噪声标注的情况下，由于存在无关的噪声图像，类原型可能会受到严重偏差的影响。因此，生成的原型较少紧凑且区分度较低，因为以前的方法没有考虑到网络图像集合中图像的多样性范围。另一方面，噪声和干净图像之间的关系在类原型生成中没有以端到端的方式学习，这导致生成的类原型次优。在这篇文章中，我们引入了一种相似性最大化损失，称为SimNoiPro。SimNoiPro首先生成耐噪声混合原型，这些原型由干净原型和耐噪声原型组成，然后将它们拉近。我们的方法通过显式划分来考虑噪声图像的多样性，并克服了优化偏差问题。这使得更好地建模干净和噪声图像之间的关系，并有助于从噪声图像集中提取有价值的信息。在两个扩展的少量样本分类基准上的评估结果表明，我们的SimNoiPro在衡量图像关系和清理噪声数据方面优于先前的方法。
2501.02474	 | Generalization-Enhanced Few-Shot Object Detection in Remote Sensing	 | Hui Lin,Nan Li,Pengjuan Yao,Kexin Dong,Yuhan Guo,Danfeng Hong,Ying Zhang,Congcong Wen	 | 本文提出了一种增强泛化能力的小样本目标检测（GE-FSOD）模型，旨在解决遥感环境中目标检测的挑战。GE-FSOD通过引入交叉层次融合 Pyramid 注意网络、多阶段细化区域建议网络和广义分类损失等关键技术，显著提高了遥感小样本目标检测任务中的性能。	 | Remote sensing object detection is particularly challenging due to the high resolution, multi-scale features, and diverse ground object characteristics inherent in satellite and UAV imagery. These challenges necessitate more advanced approaches for effective object detection in such environments. While deep learning methods have achieved remarkable success in remote sensing object detection, they typically rely on large amounts of labeled data. Acquiring sufficient labeled data, particularly for novel or rare objects, is both challenging and time-consuming in remote sensing scenarios, limiting the generalization capabilities of existing models. To address these challenges, few-shot learning (FSL) has emerged as a promising approach, aiming to enable models to learn new classes from limited labeled examples. Building on this concept, few-shot object detection (FSOD) specifically targets object detection challenges in data-limited conditions. However, the generalization capability of FSOD models, particularly in remote sensing, is often constrained by the complex and diverse characteristics of the objects present in such environments. In this paper, we propose the Generalization-Enhanced Few-Shot Object Detection (GE-FSOD) model to improve the generalization capability in remote sensing FSOD tasks. Our model introduces three key innovations: the Cross-Level Fusion Pyramid Attention Network (CFPAN) for enhanced multi-scale feature representation, the Multi-Stage Refinement Region Proposal Network (MRRPN) for more accurate region proposals, and the Generalized Classification Loss (GCL) for improved classification performance in few-shot scenarios. Extensive experiments on the DIOR and NWPU VHR-10 datasets show that our model achieves state-of-the-art performance for few-shot object detection in remote sensing.	 | 遥感目标检测由于卫星和无人机图像中固有的高分辨率、多尺度特征和多样的地物特性而尤其具有挑战性。这些挑战需要更先进的方法来有效地在这些环境中进行目标检测。尽管深度学习方法在遥感目标检测方面取得了显著的成果，但它们通常依赖大量标记数据。在遥感场景中获取足够的标记数据，尤其是对于新型或罕见目标，既具有挑战性又耗时，这限制了现有模型的一般化能力。为了解决这些挑战，小样本学习（Few-Shot Learning，FSL）作为一种有前景的方法逐渐兴起，旨在使模型能够从少量标记示例中学习新类别。基于这一概念，小样本目标检测（Few-Shot Object Detection，FSOD）专门针对数据受限条件下的目标检测挑战。然而，特别是在遥感领域，FSOD模型的一般化能力通常受限于这些环境中物体的复杂多样特性。在本文中，我们提出了一种增强泛化能力的小样本目标检测（Generalization-Enhanced Few-Shot Object Detection，GE-FSOD）模型，以提高遥感FSOD任务中的泛化能力。我们的模型引入了三项关键技术创新：用于增强多尺度特征表示的交叉层次融合 Pyramid 注意网络（CFPAN）、用于更准确区域建议的多阶段细化区域建议网络（MRRPN），以及用于改进小样本场景分类性能的广义分类损失（GCL）。在DIOR和NWPU VHR-10数据集上的广泛实验表明，我们的模型在遥感小样本目标检测任务中达到了最先进的性能。
2501.02467	 | DeTrack: In-model Latent Denoising Learning for Visual Object Tracking	 | Xinyu Zhou,Jinglun Li,Lingyi Hong,Kaixun Jiang,Pinxue Guo,Weifeng Ge,Wenqiang Zhang	 | 本文提出了一种新的视觉对象跟踪方法，通过引入噪声来生成训练用的边界框，并将跟踪问题重新表述为去噪学习过程，从而增强模型对未见过数据的鲁棒性。具体地，该方法基于去噪视觉变换器（DVIT）设计，将去噪过程分解为多个去噪块，有效提高了跟踪精度和实时性。	 | Previous visual object tracking methods employ image-feature regression models or coordinate autoregression models for bounding box prediction. Image-feature regression methods heavily depend on matching results and do not utilize positional prior, while the autoregressive approach can only be trained using bounding boxes available in the training set, potentially resulting in suboptimal performance during testing with unseen data. Inspired by the diffusion model, denoising learning enhances the model's robustness to unseen data. Therefore, We introduce noise to bounding boxes, generating noisy boxes for training, thus enhancing model robustness on testing data. We propose a new paradigm to formulate the visual object tracking problem as a denoising learning process. However, tracking algorithms are usually asked to run in real-time, directly applying the diffusion model to object tracking would severely impair tracking speed. Therefore, we decompose the denoising learning process into every denoising block within a model, not by running the model multiple times, and thus we summarize the proposed paradigm as an in-model latent denoising learning process. Specifically, we propose a denoising Vision Transformer (ViT), which is composed of multiple denoising blocks. In the denoising block, template and search embeddings are projected into every denoising block as conditions. A denoising block is responsible for removing the noise in a predicted bounding box, and multiple stacked denoising blocks cooperate to accomplish the whole denoising process. Subsequently, we utilize image features and trajectory information to refine the denoised bounding box. Besides, we also utilize trajectory memory and visual memory to improve tracking stability. Experimental results validate the effectiveness of our approach, achieving competitive performance on several challenging datasets.	 | 上一代视觉对象跟踪方法使用图像特征回归模型或坐标自回归模型来进行边界框预测。图像特征回归方法高度依赖匹配结果，并未利用位置先验信息，而自回归方法只能利用训练集中可用的边界框进行训练，这可能在测试中面对未见过的数据时导致性能不佳。受扩散模型的启发，去噪学习能够增强模型对未见过数据的鲁棒性。因此，我们向边界框引入噪声，生成训练用的噪声边界框，从而增强模型在测试数据上的鲁棒性。我们提出了一种新的范式，将视觉对象跟踪问题重新表述为去噪学习过程。然而，跟踪算法通常需要实时运行，直接将扩散模型应用于对象跟踪会导致跟踪速度严重下降。因此，我们将去噪学习过程分解为模型内的每个去噪块，而不是多次运行模型，并将此范式总结为一种模型内的潜在去噪学习过程。具体来说，我们提出了一种去噪视觉变换器（Denoising Vision Transformer, DVIT），它由多个去噪块组成。在去噪块中，模板嵌入和搜索嵌入作为条件被投影到每个去噪块中。一个去噪块负责去除预测边界框中的噪声，多个堆叠的去噪块协作完成整个去噪过程。之后，我们利用图像特征和轨迹信息来细化去噪后的边界框，同时还利用轨迹记忆和视觉记忆来提高跟踪稳定性。实验结果验证了我们方法的有效性，在几个具有挑战性的数据集上取得了竞争性的性能。
2501.02464	 | Depth Any Camera: Zero-Shot Metric Depth Estimation from Any Camera	 | Yuliang Guo,Sparsh Garg,S. Mahdi H. Miangoleh,Xinyu Huang,Liu Ren	 | Depth Any Camera (DAC) 是一种强大的零样本度量深度估计框架，能够将视角训练模型扩展到鱼眼和360度相机等不同视场相机类型中，仅通过视角图像训练就能实现优异的泛化性能，大幅提高δ1准确度，特别是在鱼眼和360度数据集上。	 | While recent depth estimation methods exhibit strong zero-shot generalization, achieving accurate metric depth across diverse camera types-particularly those with large fields of view (FoV) such as fisheye and 360-degree cameras-remains a significant challenge. This paper presents Depth Any Camera (DAC), a powerful zero-shot metric depth estimation framework that extends a perspective-trained model to effectively handle cameras with varying FoVs. The framework is designed to ensure that all existing 3D data can be leveraged, regardless of the specific camera types used in new applications. Remarkably, DAC is trained exclusively on perspective images but generalizes seamlessly to fisheye and 360-degree cameras without the need for specialized training data. DAC employs Equi-Rectangular Projection (ERP) as a unified image representation, enabling consistent processing of images with diverse FoVs. Its key components include a pitch-aware Image-to-ERP conversion for efficient online augmentation in ERP space, a FoV alignment operation to support effective training across a wide range of FoVs, and multi-resolution data augmentation to address resolution disparities between training and testing. DAC achieves state-of-the-art zero-shot metric depth estimation, improving delta-1 ($δ_1$) accuracy by up to 50% on multiple fisheye and 360-degree datasets compared to prior metric depth foundation models, demonstrating robust generalization across camera types.	 | 尽管最近的深度估计方法在零样本泛化方面表现出色，并能够在多种相机类型之间实现准确的度量深度估计，特别是对于具有大视场（FoV）的鱼眼和360度相机等相机类型，这一任务仍然极具挑战性。本文介绍了Depth Any Camera (DAC)，一种强大的零样本度量深度估计框架，能够将视角训练模型扩展到不同的FoV相机类型的处理中。该框架设计旨在确保所有现有的3D数据可以被利用，无论新应用中使用的是哪种特定类型的相机。令人惊讶的是，DAC仅在视角图像上进行训练，但能够无缝地泛化到鱼眼和360度相机，无需特殊的训练数据。DAC采用球面矩形投影（ERP）作为统一的图像表示，使得具有不同FoV的图像能够一致地处理。其关键组件包括带有俯仰感知的图像到ERP的转换，以实现高效的在线增强，FoV对齐操作以支持不同FoV范围内的有效训练，以及多分辨率数据增强以解决训练和测试之间的分辨率差异。DAC实现了最先进的零样本度量深度估计，与之前的度量深度基础模型相比，在多个鱼眼和360度数据集上将delta-1（δ1）准确度提高了高达50%，展示了其在不同相机类型之间的稳健泛化能力。
2501.02461	 | FedRSClip: Federated Learning for Remote Sensing Scene Classification Using Vision-Language Models	 | Hui Lin,Chao Zhang,Danfeng Hong,Kexin Dong,Congcong Wen	 | 本文提出了一种名为FedRSCLIP的联邦学习框架，用于遥感图像分类，该框架通过引入提示学习机制和双提示对齐约束解决了大规模模型传输和数据异质性问题。实验结果表明，FedRSCLIP在多个遥感图像分类数据集上实现了优异的性能。	 | Remote sensing data is often distributed across multiple institutions, and due to privacy concerns and data-sharing restrictions, leveraging large-scale datasets in a centralized training framework is challenging. Federated learning offers a promising solution by enabling collaborative model training across distributed data sources without requiring data centralization. However, current Vision-Language Models (VLMs), which typically contain billions of parameters, pose significant communication challenges for traditional federated learning approaches based on model parameter updates, as they would incur substantial communication costs. In this paper, we propose FedRSCLIP, the first federated learning framework designed for remote sensing image classification based on a VLM, specifically CLIP. FedRSCLIP addresses the challenges of data heterogeneity and large-scale model transmission in federated environments by introducing Prompt Learning, which optimizes only a small set of tunable parameters. The framework introduces a dual-prompt mechanism, comprising Shared Prompts for global knowledge sharing and Private Prompts for client-specific adaptation. To maintain semantic coherence between shared and private prompts, we propose the Dual Prompt Alignment Constraint to balance global consistency and local adaptability across diverse client distributions. Additionally, to enhance cross-modal representation learning, we introduce the Cross-Modal Feature Alignment Constraint to align multimodal features between text and image prompts. To validate the effectiveness of our proposed model, we construct a Fed-RSIC dataset based on three existing remote sensing image classification datasets, specifically designed to simulate various federated learning configurations. Experimental results demonstrate the effectiveness and superiority of FedRSCLIP in remote sensing image classification.	 | 遥感数据通常分布在多个机构中，由于隐私问题和数据共享限制，将大规模数据集集中在单一训练框架中进行利用是具有挑战性的。联邦学习提供了一种有前景的解决方案，它能够在不集中数据的情况下，通过分布式数据源进行协作模型训练。然而，当前的视觉-语言模型（VLMs）通常包含数十亿个参数，这为基于模型参数更新的传统联邦学习方法带来了显著的通信挑战，因为这将导致巨大的通信成本。在本文中，我们提出了FedRSCLIP，这是首个基于VLM（特别是CLIP）的遥感图像分类的联邦学习框架。FedRSCLIP通过引入提示学习机制来解决联邦环境中数据异质性和大规模模型传输的挑战，该机制仅优化一个小型的可调参数集。该框架引入了双提示机制，包括共享提示用于全球知识共享和私人提示用于客户端特定的适应。为了在共享和私人提示之间保持语义一致性，我们提出了双提示对齐约束，以平衡不同客户端分布中的全局一致性和局部适应性。此外，为了增强跨模态表示学习，我们引入了跨模态特征对齐约束，以在文本和图像提示之间对齐多模态特征。为了验证我们所提模型的有效性，我们基于三个现有的遥感图像分类数据集构建了一个Fed-RSIC数据集，专门设计用于模拟各种联邦学习配置。实验结果证明了FedRSCLIP在遥感图像分类中的有效性和优越性。
2501.02458	 | Neural Reflectance Fields for Radio-Frequency Ray Tracing	 | Haifeng Jia,Xinyi Chen,Yichen Wei,Yifei Sun,Yibo Pi	 | 该研究提出了一个通过学习无线电频率信号路径损耗来有效估计材料反射率的方法，利用神经反射场模型和可微RF光线跟踪框架优化信号强度，能够在减少训练数据的情况下更准确地预测接收器功率。	 | Ray tracing is widely employed to model the propagation of radio-frequency (RF) signal in complex environment. The modelling performance greatly depends on how accurately the target scene can be depicted, including the scene geometry and surface material properties. The advances in computer vision and LiDAR make scene geometry estimation increasingly accurate, but there still lacks scalable and efficient approaches to estimate the material reflectivity in real-world environment. In this work, we tackle this problem by learning the material reflectivity efficiently from the path loss of the RF signal from the transmitters to receivers. Specifically, we want the learned material reflection coefficients to minimize the gap between the predicted and measured powers of the receivers. We achieve this by translating the neural reflectance field from optics to RF domain by modelling both the amplitude and phase of RF signals to account for the multipath effects. We further propose a differentiable RF ray tracing framework that optimizes the neural reflectance field to match the signal strength measurements. We simulate a complex real-world environment for experiments and our simulation results show that the neural reflectance field can successfully learn the reflection coefficients for all incident angles. As a result, our approach achieves better accuracy in predicting the powers of receivers with significantly less training data compared to existing approaches.	 | 光线跟踪广泛用于模拟复杂数字环境中的无线电频率（RF）信号传播。模型性能取决于目标场景的准确表示，包括场景几何结构和表面材料属性。计算机视觉和激光雷达（LiDAR）的进步使得场景几何结构的估计越来越准确，但在现实环境中估计材料的反射率方面仍然缺乏可扩展和高效的方法。在本工作中，我们通过学习从发射器到接收器的RF信号路径损耗来有效估计材料反射率来解决这一问题。具体来说，我们希望所学习的材料反射系数能最小化预测接收器功率与实际测量功率之间的差距。我们通过建模RF信号的幅度和相位来将神经反射场从光学领域转换到RF领域，以考虑多路径效应。我们进一步提出了一种可微RF光线跟踪框架，该框架优化了神经反射场以匹配信号强度测量值。我们为实验模拟了一个复杂的现实环境，并且我们的仿真结果表明，神经反射场可以成功学习所有入射角的反射系数。因此，与现有的方法相比，我们的方法在显著减少训练数据的情况下，能更好地预测接收器的功率。
2501.02451	 | Enhancing Contrastive Learning for Retinal Imaging via Adjusted Augmentation Scales	 | Zijie Cheng,Boxuan Li,André Altmann,Pearse A Keane,Yukun Zhou	 | 该研究探讨了对比学习在医学成像领域的局限性，发现密集的医学图像分布模式对预训练任务构成挑战，尤其是正负样本对的构建。研究显示，使用弱增强预训练的模型在多个数据集上性能优于强增强预训练的模型，提升显著，表明优化增强策略对提高对比学习在医学成像中的效用至关重要。	 | Contrastive learning, a prominent approach within self-supervised learning, has demonstrated significant effectiveness in developing generalizable models for various applications involving natural images. However, recent research indicates that these successes do not necessarily extend to the medical imaging domain. In this paper, we investigate the reasons for this suboptimal performance and hypothesize that the dense distribution of medical images poses challenges to the pretext tasks in contrastive learning, particularly in constructing positive and negative pairs. We explore model performance under different augmentation strategies and compare the results to those achieved with strong augmentations. Our study includes six publicly available datasets covering multiple clinically relevant tasks. We further assess the model's generalizability through external evaluations. The model pre-trained with weak augmentation outperforms those with strong augmentation, improving AUROC from 0.838 to 0.848 and AUPR from 0.523 to 0.597 on MESSIDOR2, and showing similar enhancements across other datasets. Our findings suggest that optimizing the scale of augmentation is critical for enhancing the efficacy of contrastive learning in medical imaging.	 | 对比学习作为一种重要的自监督学习方法，在自然图像应用的各种领域中开发可泛化的模型方面已经显示出显著的有效性。然而，近期的研究表明，这些成功并不一定适用于医学成像领域。在本文中，我们探讨了这种次优性能的原因，并假设医学图像密集的分布模式对对比学习中的预训练任务构成了挑战，尤其是在构建正样本和负样本对方面。我们研究了不同增强策略下的模型性能，并将结果与强增强方法进行了比较。我们的研究包括六个公开可用的数据集，涵盖了多个临床相关任务。我们进一步通过外部评估来评估模型的泛化能力。使用弱增强预训练的模型在性能上优于使用强增强预训练的模型，在MESSIDOR2数据集上，AUROC从0.838提高到0.848，AUPR从0.523提高到0.597，其他数据集上也显示出类似的提升。我们的研究结果表明，在医学成像中优化增强的规模对于提高对比学习的效用至关重要。
2501.02450	 | GCP: Guarded Collaborative Perception with Spatial-Temporal Aware Malicious Agent Detection	 | Yihang Tao,Senkang Hu,Yue Hu,Haonan An,Hangcheng Cao,Yuguang Fang	 | 协作感知通过车辆间的消息共享扩展了感知范围，提高了自动驾驶的安全性，但易受恶意代理的攻击。本文提出了一种新的盲区混淆（BAC）攻击，并设计了Guarded Collaborative Perception（GCP）框架，通过时空感知方法有效防御该攻击，实验结果显示GCP在各种攻击场景下比现有防御策略更具优越性。	 | Collaborative perception significantly enhances autonomous driving safety by extending each vehicle's perception range through message sharing among connected and autonomous vehicles. Unfortunately, it is also vulnerable to adversarial message attacks from malicious agents, resulting in severe performance degradation. While existing defenses employ hypothesis-and-verification frameworks to detect malicious agents based on single-shot outliers, they overlook temporal message correlations, which can be circumvented by subtle yet harmful perturbations in model input and output spaces. This paper reveals a novel blind area confusion (BAC) attack that compromises existing single-shot outlier-based detection methods. As a countermeasure, we propose GCP, a Guarded Collaborative Perception framework based on spatial-temporal aware malicious agent detection, which maintains single-shot spatial consistency through a confidence-scaled spatial concordance loss, while simultaneously examining temporal anomalies by reconstructing historical bird's eye view motion flows in low-confidence regions. We also employ a joint spatial-temporal Benjamini-Hochberg test to synthesize dual-domain anomaly results for reliable malicious agent detection. Extensive experiments demonstrate GCP's superior performance under diverse attack scenarios, achieving up to 34.69% improvements in AP@0.5 compared to the state-of-the-art CP defense strategies under BAC attacks, while maintaining consistent 5-8% improvements under other typical attacks. Code will be released at https://github.com/CP-Security/GCP.git.	 | 协作感知显著通过连接和自主车辆之间的消息共享来扩展每辆车的感知范围，从而增强自动驾驶的安全性。不幸的是，它也容易受到恶意代理的对抗消息攻击，导致性能严重下降。虽然现有的防御措施使用假设和验证框架基于单次离群值来检测恶意代理，但它们忽略了时间消息的关联性，这些关联性可以通过模型输入和输出空间中的微妙但有害的扰动来规避。本文揭示了一种新颖的盲区混淆（BAC）攻击，该攻击破坏了现有的基于单次离群值检测方法。作为应对措施，我们提出了一种基于时空感知恶意代理检测的Guarded Collaborative Perception（GCP）框架，通过使用可信度缩放的空间一致性损失来保持单次的空间一致性，同时通过重构低可信度区域的历史鸟瞰视图运动流来检查时间异常。我们还采用联合时空Benjamini-Hochberg测试来综合双域异常结果，以实现可靠的恶意代理检测。广泛的实验表明，在各种攻击场景下，GCP在AP@0.5上的性能优于最先进的CP防御策略，尤其是在BAC攻击下，GCP的性能提升了高达34.69%，同时在其他典型攻击下也保持了5-8%的一致性改进。代码将在https://github.com/CP-Security/GCP.git上发布。
2501.02447	 | MedSegDiffNCA: Diffusion Models With Neural Cellular Automata for Skin Lesion Segmentation	 | Avni Mittal,John Kalkhof,Anirban Mukhopadhyay,Arnav Bhavsar	 | 本文提出了三种基于NCA的方法以改进基于扩散的医学图像分割，分别是Multi-MedSegDiffNCA、CBAM-MedSegDiffNCA和MultiCBAM-MedSegDiffNCA，其中MultiCBAM-MedSegDiffNCA在Dice分数为87.84%的情况下，仅使用60-110倍更少的参数，为资源有限的医疗环境提供了更高效的解决方案。	 | Denoising Diffusion Models (DDMs) are widely used for high-quality image generation and medical image segmentation but often rely on Unet-based architectures, leading to high computational overhead, especially with high-resolution images. This work proposes three NCA-based improvements for diffusion-based medical image segmentation. First, Multi-MedSegDiffNCA uses a multilevel NCA framework to refine rough noise estimates generated by lower level NCA models. Second, CBAM-MedSegDiffNCA incorporates channel and spatial attention for improved segmentation. Third, MultiCBAM-MedSegDiffNCA combines these methods with a new RGB channel loss for semantic guidance. Evaluations on Lesion segmentation show that MultiCBAM-MedSegDiffNCA matches Unet-based model performance with dice score of 87.84% while using 60-110 times fewer parameters, offering a more efficient solution for low resource medical settings.	 | 去噪扩散模型（DDMs）广泛应用于高质量图像生成和医学图像分割，但通常依赖于基于Unet的架构，导致在高分辨率图像处理时计算开销较高。本文提出了三种基于NCA的方法以改进基于扩散的医学图像分割。首先，Multi-MedSegDiffNCA 使用多级NCA框架来细化由下级NCA模型生成的粗糙噪声估计。其次，CBAM-MedSegDiffNCA 结合了通道和空间注意力机制以提高分割性能。第三，MultiCBAM-MedSegDiffNCA 将这些方法与新的RGB通道损失结合，以提供语义引导。在病变分割上的评估表明，MultiCBAM-MedSegDiffNCA 在Dice分数为87.84%的情况下，仅使用60-110倍更少的参数，为资源有限的医疗环境提供了更高效的解决方案。
2501.02430	 | FOLDER: Accelerating Multi-modal Large Language Models with Enhanced Performance	 | Haicheng Wang,Zhemeng Yu,Gabriele Spadaro,Chen Ju,Victor Quétu,Enzo Tartaglione	 | FOLDER是一种简单有效的即插即用模块，能够缩短视觉标记序列长度，减轻多模态大规模语言模型在训练和推理过程中的计算和内存需求，同时保持模型性能，甚至在去除高达70%的视觉标记后仍能达到或超过原模型的表现。	 | Recently, Multi-modal Large Language Models (MLLMs) have shown remarkable effectiveness for multi-modal tasks due to their abilities to generate and understand cross-modal data. However, processing long sequences of visual tokens extracted from visual backbones poses a challenge for deployment in real-time applications. To address this issue, we introduce FOLDER, a simple yet effective plug-and-play module designed to reduce the length of the visual token sequence, mitigating both computational and memory demands during training and inference. Through a comprehensive analysis of the token reduction process, we analyze the information loss introduced by different reduction strategies and develop FOLDER to preserve key information while removing visual redundancy. We showcase the effectiveness of FOLDER by integrating it into the visual backbone of several MLLMs, significantly accelerating the inference phase. Furthermore, we evaluate its utility as a training accelerator or even performance booster for MLLMs. In both contexts, FOLDER achieves comparable or even better performance than the original models, while dramatically reducing complexity by removing up to 70% of visual tokens.	 | 近年来，多模态大规模语言模型（MLLMs）因其生成和理解跨模态数据的能力，在多模态任务中表现出显著的有效性。然而，在实时应用中处理由视觉骨干提取的长序列视觉标记仍旧存在挑战。为解决这一问题，我们提出了FOLDER，这是一种简单而有效的即插即用模块，旨在缩短视觉标记序列的长度，减轻训练和推理过程中的计算和内存需求。通过对标记减少过程的全面分析，我们分析了不同减少策略引入的信息损失，并开发了FOLDER以保留关键信息同时去除视觉冗余。我们通过将FOLDER整合到几种MLLM的视觉骨干中，展示了其在加速推理阶段的有效性。此外，我们还评估了FOLDER作为MLLM训练加速器或性能增强器的实用性。在两种情况下，FOLDER都达到了与原模型相当甚至更好的性能，同时通过去除多达70%的视觉标记大幅降低了复杂性。
2501.02427	 | MetaNeRV: Meta Neural Representations for Videos with Spatial-Temporal Guidance	 | Jialong Guo,Ke liu,Jiangchao Yao,Zhihua Wang,Jiajun Bu,Haishuai Wang	 | MetaNeRV 是一种新颖的框架，利用元学习来快速初始化神经表示方法（NeRV），从而提高未见过视频的表示效率；通过时空引导技术，MetaNeRV 能够更有效地捕捉视频的空间和时间特性，提升视频表示和压缩性能。	 | Neural Representations for Videos (NeRV) has emerged as a promising implicit neural representation (INR) approach for video analysis, which represents videos as neural networks with frame indexes as inputs. However, NeRV-based methods are time-consuming when adapting to a large number of diverse videos, as each video requires a separate NeRV model to be trained from scratch. In addition, NeRV-based methods spatially require generating a high-dimension signal (i.e., an entire image) from the input of a low-dimension timestamp, and a video typically consists of tens of frames temporally that have a minor change between adjacent frames. To improve the efficiency of video representation, we propose Meta Neural Representations for Videos, named MetaNeRV, a novel framework for fast NeRV representation for unseen videos. MetaNeRV leverages a meta-learning framework to learn an optimal parameter initialization, which serves as a good starting point for adapting to new videos. To address the unique spatial and temporal characteristics of video modality, we further introduce spatial-temporal guidance to improve the representation capabilities of MetaNeRV. Specifically, the spatial guidance with a multi-resolution loss aims to capture the information from different resolution stages, and the temporal guidance with an effective progressive learning strategy could gradually refine the number of fitted frames during the meta-learning process. Extensive experiments conducted on multiple datasets demonstrate the superiority of MetaNeRV for video representations and video compression.	 | 以下是该摘要的中文翻译：  神经表示方法（NeRV）已逐渐成为视频分析的一种有前途的隐式神经表示（INR）方法，它通过将帧索引作为输入来表示视频作为神经网络。然而，基于NeRV的方法在适应大量多样化的视频时效率较低，因为每一段视频都需要从零开始训练一个独立的NeRV模型。此外，基于NeRV的方法在空间上需要从低维度的时间戳输入生成高维信号（即整幅图像），而视频通常由几十帧组成，相邻帧之间的变化相对较小。为了提高视频表示的效率，我们提出了一种名为MetaNeRV的新颖框架，用于快速表示未见过的视频的NeRV表示。MetaNeRV利用元学习框架学习一个最优参数初始化，这可以作为适应新视频的良好起点。为了解决视频模态的独特空间和时间特性，我们进一步引入了时空引导，以提高MetaNeRV的表示能力。具体而言，多分辨率损失的空间引导旨在从不同分辨率阶段捕获信息，而有效的渐进学习策略的时间引导则可以在元学习过程中逐步细化拟合帧的数量。在多个数据集上进行的大量实验表明，MetaNeRV在视频表示和视频压缩方面具有优越性。
2501.02414	 | Journey into Automation: Image-Derived Pavement Texture Extraction and Evaluation	 | Bingjie Lu(1),Han-Cheng Dan(1),Yichen Zhang(1),Zhetao Huang(1) ((1) Central South University)	 | 该研究开发了一种自动系统，用于基于路面图像提取纹理特征并评估均质纹理深度（MTD），通过改进3D图像处理技术并建立多变量预测模型，特别是使用梯度提升树模型，实现了高精度的MTD预测（R² = 0.9858），且现场测试表明其性能优于现有技术。	 | Mean texture depth (MTD) is pivotal in assessing the skid resistance of asphalt pavements and ensuring road safety. This study focuses on developing an automated system for extracting texture features and evaluating MTD based on pavement images. The contributions of this work are threefold: firstly, it proposes an economical method to acquire three-dimensional (3D) pavement texture data; secondly, it enhances 3D image processing techniques and formulates features that represent various aspects of texture; thirdly, it establishes multivariate prediction models that link these features with MTD values. Validation results demonstrate that the Gradient Boosting Tree (GBT) model achieves remarkable prediction stability and accuracy (R2 = 0.9858), and field tests indicate the superiority of the proposed method over other techniques, with relative errors below 10%. This method offers a comprehensive end-to-end solution for pavement quality evaluation, from images input to MTD predictions output.	 | 均质纹理深度（MTD）在评估沥青路面的抗滑性能和确保道路安全方面至关重要。本研究旨在基于路面图像开发一种自动提取纹理特征和评估MTD的系统。本文的主要贡献有三个方面：首先，提出了一种经济有效的方法来获取三维（3D）路面纹理数据；其次，改进了3D图像处理技术，并形成了能够代表纹理各个方面特征的方法；最后，建立了多变量预测模型，将这些特征与MTD值联系起来。验证结果表明，梯度提升树（GBT）模型在预测稳定性和准确性方面表现出色（R² = 0.9858），现场测试表明，所提出的方法优于其他技术，相对误差低于10%。该方法提供了从输入图像到输出MTD预测的端到端全面解决方案。
2501.02376	 | Generalizable Origin Identification for Text-Guided Image-to-Image Diffusion Models	 | Wenhao Wang,Yifan Sun,Zongxin Yang,Zhentao Tan,Zhengdong Hu,Yi Yang	 | 该研究介绍了一个新的任务——基于文本指导的图像到图像扩散模型的源图像识别（ID\(^2\)），并提出了一种理论上保证的方法和数据集，该方法能够有效识别不同扩散模型生成图像的原始图像，显著优于基于相似性的方法。	 | Text-guided image-to-image diffusion models excel in translating images based on textual prompts, allowing for precise and creative visual modifications. However, such a powerful technique can be misused for spreading misinformation, infringing on copyrights, and evading content tracing. This motivates us to introduce the task of origin IDentification for text-guided Image-to-image Diffusion models (ID$^2$), aiming to retrieve the original image of a given translated query. A straightforward solution to ID$^2$ involves training a specialized deep embedding model to extract and compare features from both query and reference images. However, due to visual discrepancy across generations produced by different diffusion models, this similarity-based approach fails when training on images from one model and testing on those from another, limiting its effectiveness in real-world applications. To solve this challenge of the proposed ID$^2$ task, we contribute the first dataset and a theoretically guaranteed method, both emphasizing generalizability. The curated dataset, OriPID, contains abundant Origins and guided Prompts, which can be used to train and test potential IDentification models across various diffusion models. In the method section, we first prove the existence of a linear transformation that minimizes the distance between the pre-trained Variational Autoencoder (VAE) embeddings of generated samples and their origins. Subsequently, it is demonstrated that such a simple linear transformation can be generalized across different diffusion models. Experimental results show that the proposed method achieves satisfying generalization performance, significantly surpassing similarity-based methods ($+31.6\%$ mAP), even those with generalization designs.	 | 基于文本指导的图像到图像扩散模型在根据文本提示转换图像方面表现出色，允许进行精确和创意的视觉修改。然而，这种强大的技术也可能被滥用，用于传播虚假信息、侵犯版权和逃避内容追踪。这促使我们引入基于文本指导的图像到图像扩散模型的源图像识别任务（ID\(^2\)），旨在检索给定翻译查询的原始图像。一个简单的ID\(^2\)解决方案是训练一个专门的深度嵌入模型，从查询图像和参考图像中提取和比较特征。然而，由于不同扩散模型生成的图像之间存在视觉差异，基于相似性的方法在使用一个模型的图像进行训练并在另一个模型的图像上进行测试时会失效，这限制了其在实际应用中的有效性。为了解决提出的ID\(^2\)任务中的这一挑战，我们贡献了第一个数据集和一种理论上保证的方法，都强调了泛化能力。精心策划的数据集OriPID包含丰富的起源和指导提示，可以用于训练和测试各种扩散模型中的识别模型。在方法部分，我们首先证明了存在一种线性变换，可以最小化生成样本的先验变分自编码器（VAE）嵌入与其起源之间的距离。随后，我们展示了这样的简单线性变换在不同的扩散模型间具有泛化能力。实验结果表明，所提出的方法实现了令人满意的泛化性能，显著超越了基于相似性的方法（mAP提高31.6%），即使那些设计了泛化能力的方法也是如此。
2501.02363	 | V2X-DGPE: Addressing Domain Gaps and Pose Errors for Robust Collaborative 3D Object Detection	 | Sichao Wang,Chuang Zhang,Ming Yuan,Qing Xu,Lei He,Jianqiang Wang	 | V2X-DGPE 是一种用于解决V2X协作感知中异构节点间领域差异和姿态误差的高精度、鲁棒特征级协作感知框架，通过知识蒸馏、历史信息利用及可变形注意力机制有效减少了特征分布差异并提升了检测性能，实验结果表明其在DAIR-V2X数据集上优于现有方法。	 | In V2X collaborative perception, the domain gaps between heterogeneous nodes pose a significant challenge for effective information fusion. Pose errors arising from latency and GPS localization noise further exacerbate the issue by leading to feature misalignment. To overcome these challenges, we propose V2X-DGPE, a high-accuracy and robust V2X feature-level collaborative perception framework. V2X-DGPE employs a Knowledge Distillation Framework and a Feature Compensation Module to learn domain-invariant representations from multi-source data, effectively reducing the feature distribution gap between vehicles and roadside infrastructure. Historical information is utilized to provide the model with a more comprehensive understanding of the current scene. Furthermore, a Collaborative Fusion Module leverages a heterogeneous self-attention mechanism to extract and integrate heterogeneous representations from vehicles and infrastructure. To address pose errors, V2X-DGPE introduces a deformable attention mechanism, enabling the model to adaptively focus on critical parts of the input features by dynamically offsetting sampling points. Extensive experiments on the real-world DAIR-V2X dataset demonstrate that the proposed method outperforms existing approaches, achieving state-of-the-art detection performance. The code is available at https://github.com/wangsch10/V2X-DGPE.	 | 在V2X协作感知中，异构节点之间的领域差异构成了有效信息融合的重大挑战。由于延迟和GPS定位噪声导致的姿态误差进一步加剧了这一问题，使得特征对齐变得困难。为了解决这些挑战，我们提出了V2X-DGPE，这是一种高精度和鲁棒的V2X特征级协作感知框架。V2X-DGPE利用知识蒸馏框架和特征补偿模块从多源数据中学习领域不变的表示，从而有效地减少了车辆与路边基础设施之间的特征分布差异。利用历史信息使模型对当前场景有更全面的理解。此外，协作融合模块通过异构自注意力机制来提取和整合来自车辆和基础设施的异构表示。为了应对姿态误差，V2X-DGPE引入了一个可变形注意力机制，使模型能够通过动态偏移采样点来适应性地关注输入特征的关键部分。在实际世界的DAIR-V2X数据集上的 extensive 实验表明，所提出的方法在检测性能上超越了现有方法，达到了最先进的水平。源代码可在 https://github.com/wangsch10/V2X-DGPE 获取。
2501.02355	 | CorrFill: Enhancing Faithfulness in Reference-based Inpainting with Correspondence Guidance in Diffusion Models	 | Kuan-Hung Liu,Cheng-Kun Yang,Min-Hung Chen,Yu-Lun Liu,Yen-Yu Lin	 | 本文提出了一种名为CorrFill的无需训练模块，通过增强参考图像和目标图像之间的几何相关性，提升扩散模型在图像修复任务中的表现，尤其是对参考图像的忠实度。实验表明，CorrFill显著提高了多个基线方法的效果，包括当前最先进的方法。	 | In the task of reference-based image inpainting, an additional reference image is provided to restore a damaged target image to its original state. The advancement of diffusion models, particularly Stable Diffusion, allows for simple formulations in this task. However, existing diffusion-based methods often lack explicit constraints on the correlation between the reference and damaged images, resulting in lower faithfulness to the reference images in the inpainting results. In this work, we propose CorrFill, a training-free module designed to enhance the awareness of geometric correlations between the reference and target images. This enhancement is achieved by guiding the inpainting process with correspondence constraints estimated during inpainting, utilizing attention masking in self-attention layers and an objective function to update the input tensor according to the constraints. Experimental results demonstrate that CorrFill significantly enhances the performance of multiple baseline diffusion-based methods, including state-of-the-art approaches, by emphasizing faithfulness to the reference images.	 | 在基于参考的图像修复任务中，提供了一个额外的参考图像以恢复受损的目标图像至其原始状态。扩散模型的发展，尤其是Stable Diffusion，使得在这一任务中有了更简单的实现方式。然而，现有的基于扩散的方法往往缺乏对参考图像和受损图像之间相关性的显式约束，导致修复结果对参考图像的忠实度较低。本文中，我们提出了一种名为CorrFill的无需训练的模块，旨在增强参考图像和目标图像之间几何相关性的意识。这种增强通过在修复过程中利用对应约束，利用自注意力层中的注意力掩码，并结合目标函数更新输入张量来实现。实验结果表明，CorrFill显著提升了多个基线的扩散模型方法的表现，包括最先进的方法，通过强调对参考图像的忠实度。
2501.02344	 | Accurate Crop Yield Estimation of Blueberries using Deep Learning and Smart Drones	 | Hieu D. Nguyen,Brandon McHenry,Thanh Nguyen,Harper Zappone,Anthony Thompson,Chau Tran,Anthony Segrest,Luke Tonon	 | 该研究提出了一种基于智能无人机和计算机视觉的AI流水线，用于蓝莓田中蓝莓数量和产量的精确估计，通过结合灌木检测模型和浆果检测模型，安全地获取灌木侧面图像；研究展示了模型在精度和召回率上的良好表现，并探讨了在不同采样策略下绘制蓝莓田地图的挑战。	 | We present an AI pipeline that involves using smart drones equipped with computer vision to obtain a more accurate fruit count and yield estimation of the number of blueberries in a field. The core components are two object-detection models based on the YOLO deep learning architecture: a Bush Model that is able to detect blueberry bushes from images captured at low altitudes and at different angles, and a Berry Model that can detect individual berries that are visible on a bush. Together, both models allow for more accurate crop yield estimation by allowing intelligent control of the drone's position and camera to safely capture side-view images of bushes up close. In addition to providing experimental results for our models, which show good accuracy in terms of precision and recall when captured images are cropped around the foreground center bush, we also describe how to deploy our models to map out blueberry fields using different sampling strategies, and discuss the challenges of annotating very small objects (blueberries) and difficulties in evaluating the effectiveness of our models.	 | 我们提出了一种基于智能无人机和计算机视觉的AI流水线，用于更准确地统计蓝莓田中蓝莓的数量和产量估计。核心组件包括基于YOLO深度学习架构的两个目标检测模型：一个灌木模型，可以从低空拍摄、不同角度的图像中检测蓝莓灌木；另一个是浆果模型，可以检测灌木上可见的单个浆果。这两种模型结合使用，可以更准确地进行作物产量估算，通过智能控制无人机的位置和摄像头，安全地近距离拍摄灌木的侧面图像。除了提供我们模型的实验结果，这些结果表明，在捕获的图像中以前景中心灌木为中心裁剪时，模型在精确度和召回率方面表现良好，我们还描述了如何部署我们的模型来使用不同的采样策略绘制蓝莓田地图，并讨论了标注非常小的对象（蓝莓）以及评估我们模型效果的挑战。
2501.02314	 | RadarNeXt: Real-Time and Reliable 3D Object Detector Based On 4D mmWave Imaging Radar	 | Liye Jia,Runwei Guan,Haocheng Zhao,Qiuchi Zhao,Ka Lok Man,Jeremy Smith,Limin Yu,Yutao Yue	 | RadarNeXt是一种基于4D毫米波雷达点云的实时三维物体检测器，通过可重参数化神经网络和多路径可变形前景增强网络（MDFEN）提高检测准确性和效率，实现了在不同硬件上的高帧率推理。	 | 3D object detection is crucial for Autonomous Driving (AD) and Advanced Driver Assistance Systems (ADAS). However, most 3D detectors prioritize detection accuracy, often overlooking network inference speed in practical applications. In this paper, we propose RadarNeXt, a real-time and reliable 3D object detector based on the 4D mmWave radar point clouds. It leverages the re-parameterizable neural networks to catch multi-scale features, reduce memory cost and accelerate the inference. Moreover, to highlight the irregular foreground features of radar point clouds and suppress background clutter, we propose a Multi-path Deformable Foreground Enhancement Network (MDFEN), ensuring detection accuracy while minimizing the sacrifice of speed and excessive number of parameters. Experimental results on View-of-Delft and TJ4DRadSet datasets validate the exceptional performance and efficiency of RadarNeXt, achieving 50.48 and 32.30 mAPs with the variant using our proposed MDFEN. Notably, our RadarNeXt variants achieve inference speeds of over 67.10 FPS on the RTX A4000 GPU and 28.40 FPS on the Jetson AGX Orin. This research demonstrates that RadarNeXt brings a novel and effective paradigm for 3D perception based on 4D mmWave radar.	 | 三维物体检测对于自动驾驶（AD）和高级驾驶辅助系统（ADAS）至关重要。然而，大多数三维检测器更侧重于检测准确性，往往忽视了实际应用中的网络推理速度。在本文中，我们提出了RadarNeXt，这是一种基于4D毫米波雷达点云的实时可靠三维物体检测器。它利用可重参数化的神经网络捕捉多尺度特征，减少内存消耗并加速推理。此外，为了突出毫米波雷达点云的不规则前景特征并抑制背景杂波，我们提出了多路径可变形前景增强网络（MDFEN），以确保检测准确性的同时尽量减少速度和参数数量的牺牲。在View-of-Delft和TJ4DRadSet数据集上的实验结果验证了RadarNeXt的出色性能和效率，使用我们提出MDFEN的变种分别实现了50.48和32.30的mAP值。值得注意的是，我们的RadarNeXt变种在RTX A4000 GPU上的推理速度超过67.10 FPS，在Jetson AGX Orin上的推理速度为28.40 FPS。这项研究展示了RadarNeXt为基于4D毫米波雷达的三维感知提供了新颖且有效的范式。
2501.02285	 | Hyperbolic Contrastive Learning for Hierarchical 3D Point Cloud Embedding	 | Yingjie Liu,Pengyu Zhang,Ziyao He,Mingsong Chen,Xuan Tang,Xian Wei	 | 该研究扩展了双曲多模态对比预训练方法，引入了3D点云模态，并通过蕴含、模态差异和对齐正则化器学习分层嵌入，从而提高下游任务性能。	 | Hyperbolic spaces allow for more efficient modeling of complex, hierarchical structures, which is particularly beneficial in tasks involving multi-modal data. Although hyperbolic geometries have been proven effective for language-image pre-training, their capabilities to unify language, image, and 3D Point Cloud modalities are under-explored. We extend the 3D Point Cloud modality in hyperbolic multi-modal contrastive pre-training. Additionally, we explore the entailment, modality gap, and alignment regularizers for learning hierarchical 3D embeddings and facilitating the transfer of knowledge from both Text and Image modalities. These regularizers enable the learning of intra-modal hierarchy within each modality and inter-modal hierarchy across text, 2D images, and 3D Point Clouds.Experimental results demonstrate that our proposed training strategy yields an outstanding 3D Point Cloud encoder, and the obtained 3D Point Cloud hierarchical embeddings significantly improve performance on various downstream tasks.	 | 双曲空间允许更高效地建模复杂、分层结构，这在涉及多模态数据的任务中尤其有益。尽管已证明双曲几何在语言-图像预训练中非常有效，但它们在统一语言、图像和3D点云模态方面的能力尚未得到充分探索。我们扩展了在双曲多模态对比预训练中的3D点云模态。此外，我们探讨了蕴含、模态差异和对齐正则化器，以学习分层的3D嵌入并促进从文本和图像模态的知识迁移。这些正则化器使每个模态内的模态内分层和跨文本、2D图像和3D点云的模态间分层的学习成为可能。实验结果表明，我们提出的学习策略产生了出色的3D点云编码器，而获得的3D点云分层嵌入显著提高了各种下游任务的性能。
2501.02270	 | Efficient Video-Based ALPR System Using YOLO and Visual Rhythm	 | Victor Nascimento Ribeiro,Nina S. T. Hirata	 | 该研究提出了一种自动车牌识别（ALPR）系统，可以从视频的单帧中提取并识别车牌信息，这种方法利用光学字符识别（OCR）模型，旨在提高识别的精确性和效率。	 | Automatic License Plate Recognition (ALPR) involves extracting vehicle license plate information from image or a video capture. These systems have gained popularity due to the wide availability of low-cost surveillance cameras and advances in Deep Learning. Typically, video-based ALPR systems rely on multiple frames to detect the vehicle and recognize the license plates. Therefore, we propose a system capable of extracting exactly one frame per vehicle and recognizing its license plate characters from this singular image using an Optical Character Recognition (OCR) model. Early experiments show that this methodology is viable.	 | 自动车牌识别（ALPR）涉及从图像或视频捕获中提取车辆车牌信息。由于低价格监控摄像头的广泛可用性和深度学习的进步，这些系统已经变得非常受欢迎。通常，基于视频的ALPR系统依赖于多帧来检测车辆并识别车牌。因此，我们提出了一种系统，该系统能够为每个车辆提取精确的一帧，并从该单一图像中使用光学字符识别（OCR）模型识别车牌字符。早期的实验表明，这种方法是可行的。
2501.02269	 | TDM: Temporally-Consistent Diffusion Model for All-in-One Real-World Video Restoration	 | Yizhou Li,Zihua Liu,Yusuke Monno,Masatoshi Okutomi	 | 本文提出了一种基于扩散的全能视频恢复方法，利用预训练的Stable Diffusion和ControlNet，并结合任务提示引导训练策略、滑动窗口跨帧注意机制和高效推理策略，能够有效恢复多种类型的视频退化，展现出优秀的泛化能力和时间一致性。这种方法克服了传统方法需要为每个任务单独训练模型的限制，为视频恢复任务提供了一种统一且高效的解决方案。	 | In this paper, we propose the first diffusion-based all-in-one video restoration method that utilizes the power of a pre-trained Stable Diffusion and a fine-tuned ControlNet. Our method can restore various types of video degradation with a single unified model, overcoming the limitation of standard methods that require specific models for each restoration task. Our contributions include an efficient training strategy with Task Prompt Guidance (TPG) for diverse restoration tasks, an inference strategy that combines Denoising Diffusion Implicit Models~(DDIM) inversion with a novel Sliding Window Cross-Frame Attention (SW-CFA) mechanism for enhanced content preservation and temporal consistency, and a scalable pipeline that makes our method all-in-one to adapt to different video restoration tasks. Through extensive experiments on five video restoration tasks, we demonstrate the superiority of our method in generalization capability to real-world videos and temporal consistency preservation over existing state-of-the-art methods. Our method advances the video restoration task by providing a unified solution that enhances video quality across multiple applications.	 | 在本文中，我们提出了首个利用预训练的 Stable Diffusion 和微调的 ControlNet 功能的基于扩散的全能视频恢复方法。我们的方法可以使用单一统一模型恢复各种类型的视频退化，克服了标准方法需要为每个恢复任务专门模型的限制。我们的贡献包括一种高效的任务提示引导（Task Prompt Guidance, TPG）训练策略，适用于多种恢复任务；一种结合了降噪扩散隐模型（DDIM）逆变换与新型滑动窗口跨帧注意机制（SW-CFA）的推理策略，以增强内容保留和时间一致性；以及一个可扩展的流水线，使我们的方法能够适应不同的视频恢复任务。通过对五种视频恢复任务的广泛实验，我们展示了我们的方法在泛化能力和时间一致性保留方面优于现有最先进的方法。我们的方法推动了视频恢复任务的发展，提供了一种统一的解决方案，提高了多个应用中视频质量。
2501.02268	 | What Kind of Visual Tokens Do We Need? Training-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph	 | Yutao Jiang,Qiong Wu,Wenhao Lin,Wei Yu,Yiyi Zhou	 | 本文研究了多模态大型语言模型所需的视觉标记，并提出了一种基于视觉标记语义相似性的无监督剪枝方法G-Prune，以减少计算开销同时保持性能。实验表明，G-Prune能显著降低LLaVA-NeXT的FLOPs，同时在VQA2.0和TextVQA上的准确率仅轻微下降。	 | Recent Multimodal Large Language Models(MLLMs) often use a large number of visual tokens to compensate their visual shortcoming, leading to excessive computation and obvious visual redundancy. In this paper, we investigate what kind of visual tokens are needed for MLLMs, and reveal that both foreground and background tokens are critical for MLLMs given the varying difficulties of examples. Based on this observation, we propose a graph-based method towards training-free visual token pruning, termed G-Prune.In particular, G-Prune regards visual tokens as nodes, and construct their connections based on their semantic similarities. Afterwards, the information flow is propagated via weighted links, and the most important tokens after iterations are kept for MLLMs, which can be front or background.To validate G-Prune, we apply it to a recent MLLM called LLaVA-NeXT, and conduct extensive experiments on a set of benchmarks.The experiment results show that G-Prune can greatly reduce computation overhead while retaining high performance on both coarse- and fine-grained tasks. For instance, G-Prune can reduce 63.57\% FLOPs of LLaVA-NeXT on VQA2.0 and TextVQA with only 0.95\% and 2.34\% accuracy drops, respectively.	 | 以下是该摘要的中文翻译：  近年来，多模态大型语言模型（MLLMs）经常使用大量的视觉标记来弥补其视觉方面的不足，导致计算量过大，并且存在明显的视觉冗余。本文研究了MLLMs所需哪些视觉标记，并揭示了对于不同难度的示例，前景和背景标记对于MLLMs都是至关重要的。基于这一观察，我们提出了一种训练无监督的视觉标记剪枝方法，称为G-Prune。具体而言，G-Prune将视觉标记视为节点，并基于它们的语义相似性构建它们之间的连接。然后，信息通过加权链接传播，在迭代后保留最重要的标记供MLLMs使用，这些标记可能是前景或背景。  为验证G-Prune的有效性，我们将其应用于最近的MLLM之一——LLaVA-NeXT，并在一系列基准测试上进行了广泛的实验。实验结果表明，G-Prune可以显著减少计算开销，同时在粗粒度和细粒度任务上保持高性能。例如，在VQA2.0和TextVQA上，G-Prune可以使LLaVA-NeXT的FLOPs降低63.57%，同时准确率分别下降0.95%和2.34%。
2501.02264	 | Unsupervised Class Generation to Expand Semantic Segmentation Datasets	 | Javier Montalvo,Álvaro García-Martín,Pablo Carballeira,Juan C. SanMiguel	 | 本文提出了一种无监督的管道，利用Stable Diffusion和Segment Anything Module生成带有分割掩码的类别示例，并将生成的新类别集成到语义分割数据集中，从而提高模型对新类别的分割性能，平均交并比达到51%，同时减少其他类别的错误。	 | Semantic segmentation is a computer vision task where classification is performed at a pixel level. Due to this, the process of labeling images for semantic segmentation is time-consuming and expensive. To mitigate this cost there has been a surge in the use of synthetically generated data -- usually created using simulators or videogames -- which, in combination with domain adaptation methods, can effectively learn how to segment real data. Still, these datasets have a particular limitation: due to their closed-set nature, it is not possible to include novel classes without modifying the tool used to generate them, which is often not public. Concurrently, generative models have made remarkable progress, particularly with the introduction of diffusion models, enabling the creation of high-quality images from text prompts without additional supervision.   In this work, we propose an unsupervised pipeline that leverages Stable Diffusion and Segment Anything Module to generate class examples with an associated segmentation mask, and a method to integrate generated cutouts for novel classes in semantic segmentation datasets, all with minimal user input. Our approach aims to improve the performance of unsupervised domain adaptation methods by introducing novel samples into the training data without modifications to the underlying algorithms. With our methods, we show how models can not only effectively learn how to segment novel classes, with an average performance of 51% IoU, but also reduce errors for other, already existing classes, reaching a higher performance level overall.	 | 语义分割是计算机视觉任务之一，其中分类是在像素级别进行的。由于这一点，语义分割图像标签的过程既耗时又昂贵。为了降低这种成本，使用合成数据的潮流逐渐兴起——这些数据通常使用模拟器或电子游戏创建，与领域适应方法结合使用，可以有效地学习如何分割真实数据。然而，这些数据集有一个特定的限制：由于它们的封闭集性质，无法在不修改用于生成它们的工具的情况下包含新的类别，而这些工具通常不是公开的。同时，生成模型取得了显著的进步，尤其是在扩散模型的引入之后，能够从文本提示生成高质量的图像，无需额外的监督。  在本文中，我们提出了一种无监督管道，该管道利用了Stable Diffusion和Segment Anything Module来生成带有相关分割掩码的类别示例，并提出了一种方法，用于在语义分割数据集中集成生成的新类别的剪裁图，所有这些都需要最少的用户输入。我们提出的方法旨在通过在训练数据中引入新样本来改进无监督领域适应方法，而无需修改底层算法。通过我们的方法，我们展示了模型不仅能够有效地学习如何分割新类别，平均交并比达到51%，而且还能减少其他已存在类别的错误，整体性能更高。
2501.02260	 | MagicFace: High-Fidelity Facial Expression Editing with Action-Unit Control	 | Mengting Wei,Tuomas Varanka,Xingxun Jiang,Huai-Qian Khor,Guoying Zhao	 | MagicFace是一种通过控制面部动作单元的相对变化来进行精细、连续和可解释的表情编辑的方法，能够在保持身份、姿态、背景和面部细节一致性的前提下，对特定人的表情进行编辑。该方法利用条件扩散模型和ID编码器来实现面部细节的一致性，并通过引入属性控制器来保持背景和姿态的一致性。	 | We address the problem of facial expression editing by controling the relative variation of facial action-unit (AU) from the same person. This enables us to edit this specific person's expression in a fine-grained, continuous and interpretable manner, while preserving their identity, pose, background and detailed facial attributes. Key to our model, which we dub MagicFace, is a diffusion model conditioned on AU variations and an ID encoder to preserve facial details of high consistency. Specifically, to preserve the facial details with the input identity, we leverage the power of pretrained Stable-Diffusion models and design an ID encoder to merge appearance features through self-attention. To keep background and pose consistency, we introduce an efficient Attribute Controller by explicitly informing the model of current background and pose of the target. By injecting AU variations into a denoising UNet, our model can animate arbitrary identities with various AU combinations, yielding superior results in high-fidelity expression editing compared to other facial expression editing works. Code is publicly available at https://github.com/weimengting/MagicFace.	 | 我们通过控制同一人的面部动作单元（AU）的相对变化来解决面部表情编辑的问题。这使我们能够在保持其身份、姿态、背景和面部详细特征的情况下，对特定人的表情进行精细、连续和可解释的编辑。我们提出的方法称为MagicFace，核心在于一个条件于AU变化的扩散模型以及一个ID编码器用于保持面部细节的一致性。具体来说，为了保留输入身份的面部细节，我们利用预训练的Stable-Diffusion模型的力量，并设计一个ID编码器通过自注意力机制融合外观特征。为了保持背景和姿态的一致性，我们引入了一个高效的属性控制器，通过明确告知模型目标当前的背景和姿态。通过将AU变化注入去噪UNet中，我们的模型能够使用各种AU组合对任意身份进行动画处理，与其它面部表情编辑方法相比，在高保真表情编辑方面表现出更优的结果。代码可以在https://github.com/weimengting/MagicFace公开获取。
2501.02232	 | Distillation-Enhanced Physical Adversarial Attacks	 | Wei Liu,Yonglin Wu,Chaoqun Li,Zhuodong Liu,Huanqian Yan	 | 本文提出了一种新的物理对抗攻击方法，通过利用知识蒸馏原理优化隐蔽性和攻击性能之间的平衡，实验结果显示该方法在提升攻击效果的同时保持了较高的隐蔽性，展现了其实用价值。	 | The study of physical adversarial patches is crucial for identifying vulnerabilities in AI-based recognition systems and developing more robust deep learning models. While recent research has focused on improving patch stealthiness for greater practical applicability, achieving an effective balance between stealth and attack performance remains a significant challenge. To address this issue, we propose a novel physical adversarial attack method that leverages knowledge distillation. Specifically, we first define a stealthy color space tailored to the target environment to ensure smooth blending. Then, we optimize an adversarial patch in an unconstrained color space, which serves as the 'teacher' patch. Finally, we use an adversarial knowledge distillation module to transfer the teacher patch's knowledge to the 'student' patch, guiding the optimization of the stealthy patch. Experimental results show that our approach improves attack performance by 20%, while maintaining stealth, highlighting its practical value.	 | 物理对抗遮盖物的研究对于识别基于AI的识别系统中的漏洞以及开发更稳健的深度学习模型至关重要。虽然最近的研究主要集中在提高遮盖物的隐蔽性以增强其实用性，但实现隐蔽性和攻击性能的有效平衡仍然是一个重大挑战。为了解决这一问题，我们提出了一种新的物理对抗攻击方法，该方法利用了知识蒸馏的原理。具体而言，我们首先定义一个针对目标环境的隐蔽色彩空间，以确保平滑融合。然后，在未受约束的色彩空间中优化一个对抗遮盖物，充当“教师”遮盖物。最后，使用对抗知识蒸馏模块将“教师”遮盖物的知识转移到“学生”遮盖物上，指导隐蔽遮盖物的优化。实验结果显示，我们的方法在提高攻击性能20%的同时保持了隐蔽性，突显了其实用价值。
2501.02211	 | Examining the Robustness of Homogeneity Bias to Hyperparameter Adjustments in GPT-4	 | Messi H.J. Lee	 | 研究发现，视觉-语言模型在训练中存在同质性偏差，尤其是在描绘不同种族和性别群体时，即使调整GPT-4的超参数如采样温度和top p，这种偏差仍然普遍存在且具有非线性关系，表明超参数调整在减轻这些偏差方面有限效用。	 | Vision-Language Models trained on massive collections of human-generated data often reproduce and amplify societal stereotypes. One critical form of stereotyping reproduced by these models is homogeneity bias-the tendency to represent certain groups as more homogeneous than others. We investigate how this bias responds to hyperparameter adjustments in GPT-4, specifically examining sampling temperature and top p which control the randomness of model outputs. By generating stories about individuals from different racial and gender groups and comparing their similarities using vector representations, we assess both bias robustness and its relationship with hyperparameter values. We find that (1) homogeneity bias persists across most hyperparameter configurations, with Black Americans and women being represented more homogeneously than White Americans and men, (2) the relationship between hyperparameters and group representations shows unexpected non-linear patterns, particularly at extreme values, and (3) hyperparameter adjustments affect racial and gender homogeneity bias differently-while increasing temperature or decreasing top p can reduce racial homogeneity bias, these changes show different effects on gender homogeneity bias. Our findings suggest that while hyperparameter tuning may mitigate certain biases to some extent, it cannot serve as a universal solution for addressing homogeneity bias across different social group dimensions.	 | 视觉-语言模型在大规模的人类生成数据上进行训练时，往往会复制和放大社会上的刻板印象。这些模型中，一个关键形式的刻板印象是同质性偏差——即倾向于将某些群体描绘得比其他群体更为同质化。我们研究了这种偏差如何响应GPT-4中的超参数调整，特别是检查了控制模型输出随机性的采样温度和top p参数。通过生成来自不同种族和性别群体的个人故事，并使用向量表示法比较它们的相似性，我们评估了偏差的鲁棒性及其与超参数值的关系。我们发现： 1. 同质性偏差在大多数超参数配置中仍然存在，黑人美国人和女性比白人美国人和男性被描绘得更同质化； 2. 超参数与群体表示之间的关系显示出非线性的模式，特别是在极端值上； 3. 超参数调整对种族和性别同质性偏差的影响不同——增加温度或减少top p可以减少种族同质性偏差，但这些变化对性别同质性偏差的影响不同。 我们的研究结果表明，虽然超参数调整可能在某种程度上减轻某些偏差，但无法成为应对不同社会群体维度中的同质性偏差的普遍解决方案。
2501.02207	 | Self-Supervised Learning for Detecting AI-Generated Faces as Anomalies	 | Mian Zou,Baosheng Yu,Yibing Zhan,Kede Ma	 | 本文提出了一种异常检测方法，通过利用照片面部图像学习摄像机固有特征和面部特定特征，来识别由人工智能生成的面部。该方法通过预训练任务对EXIF元数据标签进行排序并分类人工处理的面部图像，随后使用高斯混合模型建模学习到的特征分布，标记低概率的面部为人工智能生成的。	 | The detection of AI-generated faces is commonly approached as a binary classification task. Nevertheless, the resulting detectors frequently struggle to adapt to novel AI face generators, which evolve rapidly. In this paper, we describe an anomaly detection method for AI-generated faces by leveraging self-supervised learning of camera-intrinsic and face-specific features purely from photographic face images. The success of our method lies in designing a pretext task that trains a feature extractor to rank four ordinal exchangeable image file format (EXIF) tags and classify artificially manipulated face images. Subsequently, we model the learned feature distribution of photographic face images using a Gaussian mixture model. Faces with low likelihoods are flagged as AI-generated. Both quantitative and qualitative experiments validate the effectiveness of our method. Our code is available at \url{https://github.com/MZMMSEC/AIGFD_EXIF.git}.	 | 人工智能生成的面部检测通常被处理为二分类任务。然而，这些检测器往往难以适应快速演进的新的人工智能面部生成器。在本文中，我们通过仅利用照片面部图像来学习摄像机固有特征和面部特定特征，描述了一种异常检测方法，用于人工智能生成的面部。我们方法的成功之处在于设计了一个预训练任务，该任务训练特征提取器对四个序数可互换的元数据标签（EXIF）进行排名，并分类人工处理的面部图像。随后，我们使用高斯混合模型对学习到的特征分布进行建模。具有低概率的面部被标记为由人工智能生成的。定量和定性的实验都验证了我们方法的有效性。我们的代码可从 \url{https://github.com/MZMMSEC/AIGFD_EXIF.git} 获取。
2501.02201	 | Accounting for Focus Ambiguity in Visual Questions	 | Chongyan Chen,Yu-Yun Tseng,Zhuoheng Li,Anush Venkatesh,Danna Gurari	 | 该研究提出了VQ-FocusAmbiguity，首个旨在解决视觉定位问题中描述区域位置歧义的VQA数据集，并通过基准测试展示了现代模型面临的挑战；该数据集及其评估服务器已公开分享，以促进相关研究进展。	 | No existing work on visual question answering explicitly accounts for ambiguity regarding where the content described in the question is located in the image. To fill this gap, we introduce VQ-FocusAmbiguity, the first VQA dataset that visually grounds each region described in the question that is necessary to arrive at the answer. We then provide an analysis showing how our dataset for visually grounding `questions' is distinct from visually grounding `answers', and characterize the properties of the questions and segmentations provided in our dataset. Finally, we benchmark modern models for two novel tasks: recognizing whether a visual question has focus ambiguity and localizing all plausible focus regions within the image. Results show that the dataset is challenging for modern models. To facilitate future progress on these tasks, we publicly share the dataset with an evaluation server at https://focusambiguity.github.io/.	 | 现有视觉问答研究中没有明确考虑问题中描述的内容在图像中的位置存在歧义。为弥补这一空白，我们引入了VQ-FocusAmbiguity，这是首个在每个问题中视觉定位所必需描述区域的VQA数据集。随后，我们提供了一种分析，说明我们的数据集在视觉定位问题与视觉定位答案方面有何不同，并描述了数据集中提供的问题和分割的特性。最后，我们对两项新任务进行了基准测试：识别视觉问题是否存在焦点歧义以及在图像中定位所有可能的焦点区域。结果显示，该数据集对现代模型来说具有挑战性。为了促进这些任务的未来进展，我们公开分享了该数据集及其评估服务器，网址为https://focusambiguity.github.io/。
2501.02189	 | Benchmark Evaluations, Applications, and Challenges of Large Vision Language Models: A Survey	 | Zongxia Li,Xiyang Wu,Hongyang Du,Huy Nghiem,Guangyao Shi	 | 多模态视觉语言模型（VLMs）能够通过视觉和文本两种模态感知和推理世界，近年来在零样本分类任务上表现出色，但缺乏全面综述；本文提供了一个系统性的VLMs概览，涵盖主要模型、架构、训练方法、基准测试、应用及挑战。	 | Multimodal Vision Language Models (VLMs) have emerged as a transformative technology at the intersection of computer vision and natural language processing, enabling machines to perceive and reason about the world through both visual and textual modalities. For example, models such as CLIP, Claude, and GPT-4V demonstrate strong reasoning and understanding abilities on visual and textual data and beat classical single modality vision models on zero-shot classification. Despite their rapid advancements in research and growing popularity in applications, a comprehensive survey of existing studies on VLMs is notably lacking, particularly for researchers aiming to leverage VLMs in their specific domains. To this end, we provide a systematic overview of VLMs in the following aspects: model information of the major VLMs developed over the past five years (2019-2024); the main architectures and training methods of these VLMs; summary and categorization of the popular benchmarks and evaluation metrics of VLMs; the applications of VLMs including embodied agents, robotics, and video generation; the challenges and issues faced by current VLMs such as hallucination, fairness, and safety. Detailed collections including papers and model repository links are listed in https://github.com/zli12321/Awesome-VLM-Papers-And-Models.git.	 | 多模态视觉语言模型（Multimodal Vision-Language Models, VLMs）在计算机视觉和自然语言处理的交汇处涌现出来，成为了变革性的技术。这些模型使机器能够通过视觉和文本两种模态来感知和推理世界。例如，如CLIP、Claude和GPT-4V等模型在视觉和文本数据上的推理和理解能力表现出色，并在零样本分类任务上超越了传统的单一模态视觉模型。尽管这些模型在研究上的快速发展和在应用中的日益流行，但对于VLMs的全面综述仍然相对缺乏，特别是对于希望在特定领域利用VLMs的研究人员来说。为了解决这一问题，我们提供了一个系统性的VLMs概览，涵盖以下几个方面：过去五年（2019-2024）开发的主要VLM模型的信息；这些VLM模型的主要架构和训练方法；VLMs流行的基准测试和评估指标的总结和分类；VLMs的应用，包括体态代理、机器人技术和视频生成；当前VLMs面临的挑战和问题，如幻觉、公平性和安全性。详细收集包括论文和模型存储库链接的列表可以在https://github.com/zli12321/Awesome-VLM-Papers-And-Models.git中找到。
2501.02180	 | Phase Retrieval by Quaternionic Reweighted Amplitude Flow on Image Reconstruction	 | Ren Hu,Pan Lian	 | 本文提出了一系列基于四元数幅度模型的新型算法，包括四元数重加权幅度流动（QRAF）及其变体算法和四元数扰动幅度流动（QPAF）算法，用于解决四元数相位检索问题，并通过实验验证了其在恢复性能和计算效率上的优势。	 | Quaternionic signal processing provides powerful tools for efficiently managing color signals by preserving the intrinsic correlations among signal dimensions through quaternion algebra. In this paper, we address the quaternionic phase retrieval problem by systematically developing novel algorithms based on an amplitude-based model. Specifically, we propose the Quaternionic Reweighted Amplitude Flow (QRAF) algorithm, which is further enhanced by three of its variants: incremental, accelerated, and adapted QRAF algorithms. In addition, we introduce the Quaternionic Perturbed Amplitude Flow (QPAF) algorithm, which has linear convergence. Extensive numerical experiments on both synthetic data and real images, demonstrate that our proposed methods significantly improve recovery performance and computational efficiency compared to state-of-the-art approaches.	 | 四元数信号处理通过四元数代数保留信号各维度之间的内在关联，提供了有效管理色彩信号的强大工具。本文通过系统地基于幅度模型开发新型算法来解决四元数相位检索问题。具体来说，我们提出了四元数重加权幅度流动（QRAF）算法，并进一步开发了其三种变体：增量QRAF、加速QRAF和自适应QRAF算法。此外，我们还引入了四元数扰动幅度流动（QPAF）算法，该算法具有线性收敛性。在合成数据和真实图像上的广泛数值实验表明，我们提出的方法在恢复性能和计算效率方面显著优于现有方法。
2501.02167	 | Generating Multimodal Images with GAN: Integrating Text, Image, and Style	 | Chaoyi Tan,Wenqing Zhang,Zhen Qi,Kowei Shih,Xinshi Li,Ao Xiang	 | 本文提出了一种基于生成对抗网络的多模态图像生成方法，能够有效结合文本描述、参考图像和风格信息，生成高质量的图像，并在多个公开数据集上表现优于现有方法；该方法通过设计文本编码器、图像特征提取器和风格整合模块，并引入多种损失函数优化生成过程。	 | In the field of computer vision, multimodal image generation has become a research hotspot, especially the task of integrating text, image, and style. In this study, we propose a multimodal image generation method based on Generative Adversarial Networks (GAN), capable of effectively combining text descriptions, reference images, and style information to generate images that meet multimodal requirements. This method involves the design of a text encoder, an image feature extractor, and a style integration module, ensuring that the generated images maintain high quality in terms of visual content and style consistency. We also introduce multiple loss functions, including adversarial loss, text-image consistency loss, and style matching loss, to optimize the generation process. Experimental results show that our method produces images with high clarity and consistency across multiple public datasets, demonstrating significant performance improvements compared to existing methods. The outcomes of this study provide new insights into multimodal image generation and present broad application prospects.	 | 在计算机视觉领域，多模态图像生成已成为研究热点，特别是在将文本、图像和风格结合的任务中。本文提出了一种基于生成对抗网络（GAN）的多模态图像生成方法，能够有效结合文本描述、参考图像和风格信息，生成满足多模态要求的图像。该方法包括文本编码器、图像特征提取器和风格整合模块的设计，确保生成的图像在视觉内容和风格一致性方面保持高质量。此外，我们引入了多个损失函数，包括对抗损失、文本-图像一致性损失和风格匹配损失，以优化生成过程。实验结果表明，我们的方法在多个公开数据集上生成的图像具有高清晰度和一致性，性能显著优于现有方法。本研究的结果为多模态图像生成提供了新的见解，并展示了广泛的应用前景。
2501.02158	 | Joint Optimization for 4D Human-Scene Reconstruction in the Wild	 | Zhizheng Liu,Joe Lin,Wayne Wu,Bolei Zhou	 | JOSH 是一种新颖的基于优化的方法，能够从单目视频中在野外进行4D人类-场景重构，通过联合优化场景几何和人类动作，提高了全局动作估计和场景重建的准确性；其高效版本JOSH3R利用网络视频的伪标签进行训练，进一步验证了其准确性和泛化能力。	 | Reconstructing human motion and its surrounding environment is crucial for understanding human-scene interaction and predicting human movements in the scene. While much progress has been made in capturing human-scene interaction in constrained environments, those prior methods can hardly reconstruct the natural and diverse human motion and scene context from web videos. In this work, we propose JOSH, a novel optimization-based method for 4D human-scene reconstruction in the wild from monocular videos. JOSH uses techniques in both dense scene reconstruction and human mesh recovery as initialization, and then it leverages the human-scene contact constraints to jointly optimize the scene, the camera poses, and the human motion. Experiment results show JOSH achieves better results on both global human motion estimation and dense scene reconstruction by joint optimization of scene geometry and human motion. We further design a more efficient model, JOSH3R, and directly train it with pseudo-labels from web videos. JOSH3R outperforms other optimization-free methods by only training with labels predicted from JOSH, further demonstrating its accuracy and generalization ability.	 | 理解人类与环境的互动以及预测场景中的人类动作对于重构人类运动及其周围的环境至关重要。虽然在受限环境中捕捉人类与环境的互动方面取得了许多进展，但先前的方法很难从网络视频中重构出自然且多样的人类运动和场景背景。在这项工作中，我们提出了JOSH，这是一种新颖的基于优化的方法，用于从单目视频中在野外进行4D人类-场景重构。JOSH 使用密集场景重建和人体网格恢复技术进行初始化，然后利用人类-场景接触约束来联合优化场景、相机姿态和人类动作。实验结果显示，JOSH 通过场景几何和人类动作的联合优化，在全局人类动作估计和密集场景重建方面都能取得更好的结果。此外，我们还设计了一个更高效的模型JOSH3R，并直接用来自网络视频的伪标签对其进行训练。JOSH3R仅通过使用从JOSH预测的标签进行训练，就优于其他非优化方法，进一步证明了其准确性和泛化能力。
2501.02151	 | From Images to Detection: Machine Learning for Blood Pattern Classification	 | Yilin Li,Weining Shen	 | 该研究通过分析血液溅洒模式（BPA）来区分枪击和冲击引起的血液溅洒，提出了一种高效准确的分类模型，并讨论了BPA面临的挑战及其未来方向。	 | Bloodstain Pattern Analysis (BPA) helps us understand how bloodstains form, with a focus on their size, shape, and distribution. This aids in crime scene reconstruction and provides insight into victim positions and crime investigation. One challenge in BPA is distinguishing between different types of bloodstains, such as those from firearms, impacts, or other mechanisms. Our study focuses on differentiating impact spatter bloodstain patterns from gunshot bloodstain patterns. We distinguish patterns by extracting well-designed individual stain features, applying effective data consolidation methods, and selecting boosting classifiers. As a result, we have developed a model that excels in both accuracy and efficiency. In addition, we use outside data sources from previous studies to discuss the challenges and future directions for BPA.	 | 血液溅洒模式分析（BPA）有助于我们理解血液溅洒的形成过程，重点关注其大小、形状和分布。这有助于犯罪现场的重建，并为受害者的位置和犯罪调查提供见解。BPA 中的一个挑战是区分不同类型的血液溅洒，例如来自枪击、冲击或其他机制的血液溅洒。我们的研究重点是区分枪击血液溅洒模式和冲击血液溅洒模式。我们通过提取精心设计的单个斑点特征、应用有效的数据整合方法以及选择增强分类器来区分这些模式。结果，我们建立了一个在准确性和效率方面都表现出色的模型。此外，我们还利用之前研究中的外部数据源来讨论BPA面临的挑战及其未来方向。
2501.02146	 | Plasma-CycleGAN: Plasma Biomarker-Guided MRI to PET Cross-modality Translation Using Conditional CycleGAN	 | Yanxi Chen,Yi Su,Celine Dumitrascu,Kewei Chen,David Weidman,Richard J Caselli,Nicholas Ashton,Eric M Reiman,Yalin Wang	 | 该研究探讨了将血液生物标志物（BBBM）整合到深度生成模型中，以提高MRI和PET成像之间的跨模态翻译质量，并提出了一种新的基于CycleGAN的生成模型——Plasma-CycleGAN，该模型使用BBBM作为条件从MRI合成PET图像，首次实现了利用BBBM进行条件化跨模态翻译的方法。	 | Cross-modality translation between MRI and PET imaging is challenging due to the distinct mechanisms underlying these modalities. Blood-based biomarkers (BBBMs) are revolutionizing Alzheimer's disease (AD) detection by identifying patients and quantifying brain amyloid levels. However, the potential of BBBMs to enhance PET image synthesis remains unexplored. In this paper, we performed a thorough study on the effect of incorporating BBBM into deep generative models. By evaluating three widely used cross-modality translation models, we found that BBBMs integration consistently enhances the generative quality across all models. By visual inspection of the generated results, we observed that PET images generated by CycleGAN exhibit the best visual fidelity. Based on these findings, we propose Plasma-CycleGAN, a novel generative model based on CycleGAN, to synthesize PET images from MRI using BBBMs as conditions. This is the first approach to integrate BBBMs in conditional cross-modality translation between MRI and PET.	 | MRI和PET成像之间的跨模态翻译由于这两种成像模态背后的机制不同而具有挑战性。基于血液的生物标志物（BBBM）正在通过识别患者并量化大脑淀粉样蛋白水平来革新阿尔茨海默病（AD）的检测。然而，BBBM在增强PET图像合成方面的潜力尚未得到探索。在本文中，我们对将BBBM整合到深度生成模型中的效果进行了全面研究。通过评估三种广泛使用的跨模态翻译模型，我们发现BBBM的整合在所有模型中都一致地提高了生成质量。通过视觉检查生成的结果，我们观察到由CycleGAN生成的PET图像具有最佳的视觉保真度。基于这些发现，我们提出了一种基于CycleGAN的新生成模型——Plasma-CycleGAN，该模型使用BBBM作为条件从MRI合成PET图像。这是首次将BBBM应用于MRI和PET之间条件化跨模态翻译的方法。
2501.02143	 | SafeAug: Safety-Critical Driving Data Augmentation from Naturalistic Datasets	 | Zhaobin Mo,Yunlong Li,Xuan Di	 | 该研究提出了一种新的框架，通过使用YOLOv5检测、深度估计和3D变换来增强自然场景中的安全关键型驾驶数据，以改善自动驾驶算法的安全性和可靠性。实验结果表明，基于增强数据集训练的算法性能优于现有方法，包括SMOGN和重要性采样。	 | Safety-critical driving data is crucial for developing safe and trustworthy self-driving algorithms. Due to the scarcity of safety-critical data in naturalistic datasets, current approaches primarily utilize simulated or artificially generated images. However, there remains a gap in authenticity between these generated images and naturalistic ones. We propose a novel framework to augment the safety-critical driving data from the naturalistic dataset to address this issue. In this framework, we first detect vehicles using YOLOv5, followed by depth estimation and 3D transformation to simulate vehicle proximity and critical driving scenarios better. This allows for targeted modification of vehicle dynamics data to reflect potentially hazardous situations. Compared to the simulated or artificially generated data, our augmentation methods can generate safety-critical driving data with minimal compromise on image authenticity. Experiments using KITTI datasets demonstrate that a downstream self-driving algorithm trained on this augmented dataset performs superiorly compared to the baselines, which include SMOGN and importance sampling.	 | 安全关键型驾驶数据对于开发安全可靠的自动驾驶算法至关重要。由于自然场景数据集中安全关键型数据的稀缺性，当前方法主要使用模拟或人工生成的图像。然而，这些生成的图像与自然场景图像之间仍存在真实性差距。为此，我们提出了一种新的框架，以增强自然场景数据集中的安全关键型驾驶数据，以解决这一问题。在该框架中，我们首先使用YOLOv5检测车辆，然后进行深度估计和3D变换，以更好地模拟车辆接近和关键驾驶场景，从而实现对车辆动力学数据的有针对性修改，以反映潜在的危险情况。与模拟或人工生成的数据相比，我们的增强方法可以在最大限度保留图像真实性的情况下生成安全关键型驾驶数据。使用KITTI数据集的实验表明，基于此增强数据集训练的下游自动驾驶算法在性能上优于基线方法，包括SMOGN和重要性采样。
2501.02135	 | AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs	 | Sanjoy Chowdhury,Sayan Nag,Subhrajyoti Dasgupta,Yaoting Wang,Mohamed Elhoseiny,Ruohan Gao,Dinesh Manocha	 | 该研究引入了AVTrustBench，一个包含60万个样本的视听可信度评估基准，用于评估AVLLMs在对抗攻击、组合推理和模态特定依赖方面的性能，并提出了一种新的训练策略CAVPref来提高模型的鲁棒性。	 | With the rapid advancement of Multi-modal Large Language Models (MLLMs), several diagnostic benchmarks have recently been developed to assess these models' multi-modal reasoning proficiency. However, these benchmarks are restricted to assessing primarily the visual aspect and do not examine the holistic audio-visual (AV) understanding. Moreover, currently, there are no benchmarks that investigate the capabilities of AVLLMs to calibrate their responses when presented with perturbed inputs. To this end, we introduce Audio-Visual Trustworthiness assessment Benchmark (AVTrustBench), comprising 600K samples spanning over 9 meticulously crafted tasks, evaluating the capabilities of AVLLMs across three distinct dimensions: Adversarial attack, Compositional reasoning, and Modality-specific dependency. Using our benchmark we extensively evaluate 13 state-of-the-art AVLLMs. The findings reveal that the majority of existing models fall significantly short of achieving human-like comprehension, offering valuable insights for future research directions. To alleviate the limitations in the existing approaches, we further propose a robust, model-agnostic calibrated audio-visual preference optimization based training strategy CAVPref, obtaining a gain up to 30.19% across all 9 tasks. We will publicly release our code and benchmark to facilitate future research in this direction.	 | 随着多模态大语言模型（MLLMs）的迅猛发展，最近已经开发出了一些诊断基准，以评估这些模型的多模态推理能力。然而，这些基准主要关注视觉方面的评估，而不考察整体的视听（AV）理解。此外，目前没有基准研究视听大语言模型（AVLLMs）在面对干扰输入时调整其响应的能力。为此，我们引入了视听可信度评估基准（AVTrustBench），包含60万个样本，覆盖9个精心设计的任务，评估AVLLMs在三个不同维度上的能力：对抗攻击、组合推理和模态特定依赖。我们使用该基准对13个最先进的AVLLMs进行了广泛评估。研究发现，现有的大多数模型在实现类人理解方面存在显著差距，为未来的研究方向提供了有价值的见解。为了解决现有方法的局限性，我们进一步提出了一种鲁棒的、模型无关的校准视听偏好优化训练策略（CAVPref），在所有9个任务中获得了高达30.19%的提升。我们将公开发布我们的代码和基准，以促进该领域的未来研究。
2501.02112	 | Siamese Networks for Cat Re-Identification: Exploring Neural Models for Cat Instance Recognition	 | Tobias Trein,Luan Fonseca Garcia	 | 中国城市出行公司Hello Inc.推出的“Hello 街头猫”项目通过部署智能喂食站和推广TNR方法，结合深度学习技术实现了对家猫的自动识别与管理，显著提升了城市家猫种群控制和福利管理的效率与可靠性。	 | Street cats in urban areas often rely on human intervention for survival, leading to challenges in population control and welfare management. In April 2023, Hello Inc., a Chinese urban mobility company, launched the Hello Street Cat initiative to address these issues. The project deployed over 21,000 smart feeding stations across 14 cities in China, integrating livestreaming cameras and treat dispensers activated through user donations. It also promotes the Trap-Neuter-Return (TNR) method, supported by a community-driven platform, HelloStreetCatWiki, where volunteers catalog and identify cats. However, manual identification is inefficient and unsustainable, creating a need for automated solutions. This study explores Deep Learning-based models for re-identifying street cats in the Hello Street Cat initiative. A dataset of 2,796 images of 69 cats was used to train Siamese Networks with EfficientNetB0, MobileNet and VGG16 as base models, evaluated under contrastive and triplet loss functions. VGG16 paired with contrastive loss emerged as the most effective configuration, achieving up to 97% accuracy and an F1 score of 0.9344 during testing. The approach leverages image augmentation and dataset refinement to overcome challenges posed by limited data and diverse visual variations. These findings underscore the potential of automated cat re-identification to streamline population monitoring and welfare efforts. By reducing reliance on manual processes, the method offers a scalable and reliable solution for communitydriven initiatives. Future research will focus on expanding datasets and developing real-time implementations to enhance practicality in large-scale deployments.	 | 城市街道上的家猫常常依赖于人类的干预来生存，这给种群控制和福利管理带来了挑战。2023年4月，中国一家城市出行公司——Hello Inc. 推出了“Hello 街头猫”项目，以应对这些问题。该项目在中国14个城市部署了超过21,000个智能喂食站，集成了直播摄像头和通过用户捐赠激活的投食器。该项目还推广了“捕捉-绝育-释放”（TNR）方法，并由一个由 HelloStreetCatWiki 构建的社区驱动平台支持，志愿者在这里记录和识别猫咪。然而，手动识别效率低下且不可持续，因此存在需要自动解决方案的需求。本研究探讨了基于深度学习的模型在“Hello 街头猫”项目中重新识别街头猫咪的可能性。研究使用了2,796张69只猫咪的照片数据集，训练了基于 EfficientNetB0、MobileNet 和 VGG16 的Siamese 网络，并在对比损失函数和三重损失函数下进行评估。使用 VGG16 与对比损失函数的配置表现最佳，在测试中准确率达到97%，F1得分为0.9344。该方法利用图像增强和数据集精炼来克服数据有限和视觉变化多样带来的挑战。这些发现突显了自动猫重新识别在简化种群监测和福利努力方面的潜力。通过减少对手动过程的依赖，该方法为社区驱动项目提供了可扩展且可靠的方法。未来的研究将专注于扩展数据集和开发实时实现，以增强大规模部署中的实用性。
2501.02080	 | AI-Powered Cow Detection in Complex Farm Environments	 | Voncarlos,Ines,Thomas,Sebastien,Elsa,Marjorie,Abdoulaye	 | 本研究通过使用YOLOv8和CBAM模块开发了一种新的牛检测模型，该模型在复杂农场环境中显著提高了动物福利监测的准确性，相比基线模型提高了2.3%的mAP值，并达到95.2%的精度。这项研究不仅解决了现有检测算法在多种环境条件下的局限性，还推进了AI在畜牧监测中的应用，提升了智能农业水平。	 | Animal welfare has become a critical issue in contemporary society, emphasizing our ethical responsibilities toward animals, particularly within livestock farming. The advent of Artificial Intelligence (AI) technologies, specifically computer vision, offers an innovative approach to monitoring and enhancing animal welfare. Cows, as essential contributors to sustainable agriculture, are central to this effort. However, existing cow detection algorithms face challenges in real-world farming environments, such as complex lighting, occlusions, pose variations, and background interference, hindering detection. Model generalization is crucial for adaptation across contexts beyond the training dataset. This study addresses these challenges using a diverse cow dataset from six environments, including indoor and outdoor scenarios. We propose a detection model combining YOLOv8 with the CBAM (Convolutional Block Attention Module) and assess its performance against baseline models, including Mask R-CNN, YOLOv5, and YOLOv8. Our findings show baseline models degrade in complex conditions, while our approach improves using CBAM. YOLOv8-CBAM outperformed YOLOv8 by 2.3% in mAP, achieving 95.2% precision and an mAP@0.5:0.95 of 82.6%, demonstrating superior accuracy. Contributions include (1) analyzing detection limitations, (2) proposing a robust model, and (3) benchmarking state-of-the-art algorithms. Applications include health monitoring, behavioral analysis, and tracking in smart farms, enabling precise detection in challenging settings. This study advances AI-driven livestock monitoring, improving animal welfare and smart agriculture.	 | 动物福利已成为当代社会的关键问题，强调我们在对待动物，特别是在畜牧生产中的道德责任。人工智能（AI）技术，尤其是计算机视觉，为监测和提高动物福利提供了创新的方法。作为可持续农业的重要贡献者，牛在这一努力中占据了中心地位。然而，现有的牛检测算法在现实农场环境中面临挑战，如复杂的光照条件、遮挡、姿态变化和背景干扰，这些都阻碍了检测效果。模型泛化对于在训练数据集之外的情景中适应至关重要。本研究通过从六个环境中收集的多样化的牛数据集（包括室内和室外场景）来应对这些挑战。我们提出了一种结合YOLOv8和CBAM（卷积块注意力模块）的检测模型，并对其性能与基线模型（包括Mask R-CNN、YOLOv5和YOLOv8）进行了比较。研究结果显示，在复杂条件下，基线模型表现下降，而我们的方法通过使用CBAM取得了改进。YOLOv8-CBAM在mAP上的表现提高了2.3%，达到了95.2%的精度和mAP@0.5:0.95为82.6%，显示出更高的准确性。主要贡献包括：（1）分析检测的局限性，（2）提出一种稳健的模型，（3）基准测试最先进的算法。应用包括健康监测、行为分析和智能农场中的追踪，在复杂环境中实现精确的检测。本研究推进了AI驱动的畜牧监测，提高了动物福利和智能农业水平。
2501.02066	 | RadHop-Net: A Lightweight Radiomics-to-Error Regression for False Positive Reduction In MRI Prostate Cancer Detection	 | Vasileios Magoulianitis,Jiaxin Yang,Catherine A. Alexander,C.-C. Jay Kuo	 | 本文提出了一种名为RadHop-Net的新颖轻量级CNN，用于减少双参数磁共振成像中临床显著前列腺癌筛查的假阳性率，通过两阶段管道提高病变检测精度，同时保持较小的模型大小，并优于现有方法。	 | Clinically significant prostate cancer (csPCa) is a leading cause of cancer death in men, yet it has a high survival rate if diagnosed early. Bi-parametric MRI (bpMRI) reading has become a prominent screening test for csPCa. However, this process has a high false positive (FP) rate, incurring higher diagnostic costs and patient discomfort. This paper introduces RadHop-Net, a novel and lightweight CNN for FP reduction. The pipeline consists of two stages: Stage 1 employs data driven radiomics to extract candidate ROIs. In contrast, Stage 2 expands the receptive field about each ROI using RadHop-Net to compensate for the predicted error from Stage 1. Moreover, a novel loss function for regression problems is introduced to balance the influence between FPs and true positives (TPs). RadHop-Net is trained in a radiomics-to-error manner, thus decoupling from the common voxel-to-label approach. The proposed Stage 2 improves the average precision (AP) in lesion detection from 0.407 to 0.468 in the publicly available pi-cai dataset, also maintaining a significantly smaller model size than the state-of-the-art.	 | 临床显著前列腺癌（csPCa）是男性癌症死亡的主要原因之一，但如果及早诊断，其生存率很高。双参数磁共振成像（bpMRI）阅读已成为csPCa筛查的重要测试方法。然而，这一过程的假阳性（FP）率很高，导致更高的诊断成本和患者不适。本文介绍了RadHop-Net，这是一种新颖且轻量级的CNN，用于减少FP。该管道由两个阶段组成：第一阶段使用数据驱动的影像组学提取候选ROI。相比之下，第二阶段利用RadHop-Net扩展每个ROI的接收场，以弥补第一阶段预测的误差。此外，还提出了一种新的回归问题损失函数，以平衡FP和真阳性（TP）的影响。RadHop-Net以影像组学到误差的方式进行训练，从而解耦了与常见的体素到标签方法。所提出的第二阶段在公开可用的pi-cai数据集中将病变检测的平均精度（AP）从0.407提高到0.468，同时保持了显著更小的模型大小，优于现有最先进的方法。
2501.02064	 | ArtCrafter: Text-Image Aligning Style Transfer via Embedding Reframing	 | Nisha Huang,Kaer Huang,Yifan Pu,Jiangshan Wang,Jie Guo,Yiqiang Yan,Xiu Li	 | ArtCrafter 是一个新颖的文本到图像的风格迁移框架，它通过引入基于注意力的风格提取模块和文本-图像对齐增强组件，有效解决了风格捕捉和多样性平衡的问题，显著提升了视觉风格化的效果和多样性。	 | Recent years have witnessed significant advancements in text-guided style transfer, primarily attributed to innovations in diffusion models. These models excel in conditional guidance, utilizing text or images to direct the sampling process. However, despite their capabilities, direct conditional guidance approaches often face challenges in balancing the expressiveness of textual semantics with the diversity of output results while capturing stylistic features. To address these challenges, we introduce ArtCrafter, a novel framework for text-to-image style transfer. Specifically, we introduce an attention-based style extraction module, meticulously engineered to capture the subtle stylistic elements within an image. This module features a multi-layer architecture that leverages the capabilities of perceiver attention mechanisms to integrate fine-grained information. Additionally, we present a novel text-image aligning augmentation component that adeptly balances control over both modalities, enabling the model to efficiently map image and text embeddings into a shared feature space. We achieve this through attention operations that enable smooth information flow between modalities. Lastly, we incorporate an explicit modulation that seamlessly blends multimodal enhanced embeddings with original embeddings through an embedding reframing design, empowering the model to generate diverse outputs. Extensive experiments demonstrate that ArtCrafter yields impressive results in visual stylization, exhibiting exceptional levels of stylistic intensity, controllability, and diversity.	 | 近年来，文本引导的风格迁移取得了显著进展，这主要归功于扩散模型的创新。这些模型在条件引导方面表现出色，能够利用文本或图像来指导采样过程。然而，尽管它们具有强大的能力，直接的条件引导方法在平衡文本语义的表达性和输出结果的多样性之间往往难以兼顾，同时捕捉到风格特征也存在挑战。为了解决这些问题，我们提出了ArtCrafter，一个新颖的从文本到图像的风格迁移框架。具体来说，我们引入了一种基于注意力的风格提取模块，该模块专门设计用于捕获图像中的细微风格元素。该模块采用多层架构，利用感知器注意力机制的优势来整合细粒度信息。此外，我们还提出了一种新颖的文本-图像对齐增强组件，能够巧妙地平衡两种模态的控制，使模型能够高效地将图像和文本嵌入映射到共享特征空间。我们通过注意力操作实现模态间信息流的平滑传递来实现这一点。最后，我们通过嵌入重构设计引入了一种显式的调制，无缝地将多模态增强嵌入与原始嵌入融合，从而使模型能够生成多样化的输出。广泛的实验表明，ArtCrafter在视觉风格化方面取得了令人印象深刻的结果，显示出极高的风格强度、可控性和多样性。
2501.02048	 | DreamMask: Boosting Open-vocabulary Panoptic Segmentation with Synthetic Data	 | Yuanpeng Tu,Xi Chen,Ser-Nam Lim,Hengshuang Zhao	 | 本文探讨了如何提升开放词汇全景分割模型在新类别上的泛化能力，提出了DreamMask方法，该方法通过自动数据生成管道和合成-真实对齐损失，显著提高了模型在多个基准上的性能，特别是在COCO数据集训练后应用于ADE20K测试时，mIoU指标提高了2.1%。	 | Open-vocabulary panoptic segmentation has received significant attention due to its applicability in the real world. Despite claims of robust generalization, we find that the advancements of previous works are attributed mainly on trained categories, exposing a lack of generalization to novel classes. In this paper, we explore boosting existing models from a data-centric perspective. We propose DreamMask, which systematically explores how to generate training data in the open-vocabulary setting, and how to train the model with both real and synthetic data. For the first part, we propose an automatic data generation pipeline with off-the-shelf models. We propose crucial designs for vocabulary expansion, layout arrangement, data filtering, etc. Equipped with these techniques, our generated data could significantly outperform the manually collected web data. To train the model with generated data, a synthetic-real alignment loss is designed to bridge the representation gap, bringing noticeable improvements across multiple benchmarks. In general, DreamMask significantly simplifies the collection of large-scale training data, serving as a plug-and-play enhancement for existing methods. For instance, when trained on COCO and tested on ADE20K, the model equipped with DreamMask outperforms the previous state-of-the-art by a substantial margin of 2.1% mIoU.	 | 开放词汇全景分割由于其实用性受到了广泛关注。尽管已有研究声称具有稳健的泛化能力，但我们的研究发现，先前工作的进步主要集中在训练过的类别上，暴露了其在新类别上的泛化能力存在不足。在本文中，我们从数据为中心的角度探讨了如何提升现有模型。我们提出了 DreamMask，系统地研究了如何在开放词汇设置中生成训练数据，以及如何使用真实和合成数据训练模型。对于第一部分，我们提出了一种使用现成模型的自动数据生成管道。我们提出了词汇扩展、布局排列、数据过滤等关键设计。凭借这些技术，我们生成的数据显著优于手动收集的网页数据。为了使用生成的数据训练模型，我们设计了一种合成-真实对齐损失，以弥合表示差距，从而在多个基准上带来了显著的改进。总体而言，DreamMask 显著简化了大规模训练数据的收集过程，作为现有方法的即插即用增强。例如，当在 COCO 数据集上训练并在 ADE20K 上测试时，配备了 DreamMask 的模型在 mIoU 指标上比之前的最佳方法高出 2.1%。
2501.02041	 | MRG: A Multi-Robot Manufacturing Digital Scene Generation Method Using Multi-Instance Point Cloud Registration	 | Songjie Han,Yinhua Liu,Yanzheng Li,Hua Chen,Dongmei Yang	 | 本文提出了一种基于多实例点云注册的新型多机器人制造数字场景生成方法，通过实例中心的变压器模块和假设生成模块，以及高效的筛选和优化算法，显著提高了仿真环境的精确性和可靠性。实验结果显示，该方法在多个数据集上优于现有技术，特别是在制造场景中表现出色。	 | A high-fidelity digital simulation environment is crucial for accurately replicating physical operational processes. However, inconsistencies between simulation and physical environments result in low confidence in simulation outcomes, limiting their effectiveness in guiding real-world production. Unlike the traditional step-by-step point cloud "segmentation-registration" generation method, this paper introduces, for the first time, a novel Multi-Robot Manufacturing Digital Scene Generation (MRG) method that leverages multi-instance point cloud registration, specifically within manufacturing scenes. Tailored to the characteristics of industrial robots and manufacturing settings, an instance-focused transformer module is developed to delineate instance boundaries and capture correlations between local regions. Additionally, a hypothesis generation module is proposed to extract target instances while preserving key features. Finally, an efficient screening and optimization algorithm is designed to refine the final registration results. Experimental evaluations on the Scan2CAD and Welding-Station datasets demonstrate that: (1) the proposed method outperforms existing multi-instance point cloud registration techniques; (2) compared to state-of-the-art methods, the Scan2CAD dataset achieves improvements in MR and MP by 12.15% and 17.79%, respectively; and (3) on the Welding-Station dataset, MR and MP are enhanced by 16.95% and 24.15%, respectively. This work marks the first application of multi-instance point cloud registration in manufacturing scenes, significantly advancing the precision and reliability of digital simulation environments for industrial applications.	 | 高保真数字仿真环境对于准确复制物理操作过程至关重要。然而，仿真与物理环境之间的不一致性导致对仿真结果缺乏信心，限制了其在指导实际生产中的有效性。不同于传统的逐点云分割和注册方法，本文首次引入了一种基于多实例点云注册的新型多机器人制造数字场景生成（MRG）方法，特别适用于制造场景。该方法针对工业机器人的特性和制造环境的特点，开发了一种以实例为中心的变压器模块，用于划分实例边界并捕捉局部区域之间的关联性。此外，还提出了假设生成模块，以提取目标实例并保留关键特征。最后，设计了一种高效的筛选和优化算法，以细化最终的注册结果。在Scan2CAD和Welding-Station数据集上的实验评估表明：（1）所提出的方法优于现有的多实例点云注册技术；（2）与最先进的方法相比，Scan2CAD数据集分别在MR和MP上提高了12.15%和17.79%；（3）在Welding-Station数据集上，MR和MP分别提高了16.95%和24.15%。这项工作标志着首次将多实例点云注册应用于制造场景，显著提升了数字仿真环境在工业应用中的精确性和可靠性。
2501.02040	 | A Separable Self-attention Inspired by the State Space Model for Computer Vision	 | Juntao Zhang,Shaogeng Liu,Kun Bian,You Zhou,Pei Zhang,Jianning Liu,Jun Zhou,Bingyan Liu	 | Mamba是一种高效的线性状态空间模型，本文首次将其设计理念应用于可分离自注意力机制中，并提出VMINet架构进行公平比较，结果显示VMINet在图像分类和高分辨率密集预测任务中取得了竞争力的结果。	 | Mamba is an efficient State Space Model (SSM) with linear computational complexity. Although SSMs are not suitable for handling non-causal data, Vision Mamba (ViM) methods still demonstrate good performance in tasks such as image classification and object detection. Recent studies have shown that there is a rich theoretical connection between state space models and attention variants. We propose a novel separable self attention method, for the first time introducing some excellent design concepts of Mamba into separable self-attention. To ensure a fair comparison with ViMs, we introduce VMINet, a simple yet powerful prototype architecture, constructed solely by stacking our novel attention modules with the most basic down-sampling layers. Notably, VMINet differs significantly from the conventional Transformer architecture. Our experiments demonstrate that VMINet has achieved competitive results on image classification and high-resolution dense prediction tasks.Code is available at: \url{https://github.com/yws-wxs/VMINet}.	 | Mamba 是一种高效的狀態空間模型（SSM），其計算複雜度為線性。雖然狀態空間模型不適合處理非因襲數據，但視覺 Mamba（ViM）方法仍然在圖像分類和目標檢測等任務中展示了良好的性能。最近的研究表明，狀態空間模型和注意機制的變種之間存在豐富的理論聯系。我們提出了一種新的可分離自注意方法，首次將 Mamba 的一些優秀設計理念引入到可分離自注意中。為了確保與 ViMs 的公平比較，我們引入了 VMINet，一個簡單且強大的原型架構，僅由堆疊我們的新注意模塊和最基本的下採樣層構成。值得注意的是，VMINet 與傳統的Transformer架構有顯著不同。我們的實驗表明，VMINet 在圖像分類和高分辨率密集預測任務中取得了競爭力的結果。相關代碼可在以下連結獲取：\url{https://github.com/yws-wxs/VMINet}。
2501.02035	 | 3D Cloud reconstruction through geospatially-aware Masked Autoencoders	 | Stella Girtsou,Emiliano Diaz Salas-Porras,Lilli Freischem,Joppe Massant,Kyriaki-Margarita Bintsi,Guiseppe Castiglione,William Jones,Michael Eisinger,Emmanuel Johnson,Anna Jungbluth	 | 该研究利用MSG/SEVIRI图像和CloudSat/CPR雷达数据，通过自我监督学习方法（包括遮蔽自编码器和地理感知SatMAE）重建三维云结构，显著改进了现有技术并展示了SSL在云重建中的潜力。	 | Clouds play a key role in Earth's radiation balance with complex effects that introduce large uncertainties into climate models. Real-time 3D cloud data is essential for improving climate predictions. This study leverages geostationary imagery from MSG/SEVIRI and radar reflectivity measurements of cloud profiles from CloudSat/CPR to reconstruct 3D cloud structures. We first apply self-supervised learning (SSL) methods-Masked Autoencoders (MAE) and geospatially-aware SatMAE on unlabelled MSG images, and then fine-tune our models on matched image-profile pairs. Our approach outperforms state-of-the-art methods like U-Nets, and our geospatial encoding further improves prediction results, demonstrating the potential of SSL for cloud reconstruction.	 | 云层在地球辐射平衡中发挥着关键作用，其复杂的影响引入了气候模型中的大量不确定性。实时三维云数据对于改进气候预测至关重要。本研究利用静止轨道上的MSG/SEVIRI图像以及CloudSat/CPR的雷达反射率测量的云层剖面数据，重建三维云结构。我们首先应用自我监督学习（SSL）方法——遮蔽自编码器（MAE）和地理感知SatMAE对未标记的MSG图像进行处理，然后在匹配的图像-剖面对上微调我们的模型。我们的方法超越了现有的先进技术，如U-Nets，并且我们的地理编码进一步提高了预测结果，展示了SSL在云重建中的潜力。
2501.02017	 | Rephotography in the Digital Era: Mass Rephotography and re.photos, the Web Portal for Rephotography	 | Axel Schaffland	 | 自19世纪中叶以来，数字重新摄影在注册、保存、展示和分享方面取得了显著进展，本文介绍了一种集成了协作重新摄影、交互式图像注册等功能的平台re.photos，并讨论了实现大规模重新摄影所需满足的额外要求，包括自动化图像注册、直观的智能手机应用支持以及长期存储等多项功能。	 | Since the beginning of rephotography in the middle of the 19th century, techniques in registration, conservation, presentation, and sharing of rephotographs have come a long way. Here, we will present existing digital approaches to rephotography and discuss future approaches and requirements for digital mass rephotography. We present re.photos, an existing web portal for rephotography, featuring methods for collaborative rephotography, interactive image registration, as well as retrieval, organization, and sharing of rephotographs. For mass rephotography additional requirements must be met. Batches of template images and rephotographs must be handled simultaneously, image registration must be automated, and intuitive smartphone apps for rephotography must be available. Long--term storage with persistent identifiers, automatic or mass georeferencing, as well as gamification and social media integration are further requirements we will discuss in this paper.	 | 自19世纪中叶重新摄影开始以来，重新摄影在注册、保存、展示和分享方面已经取得了长足的进步。在这里，我们将介绍现有的数字重新摄影方法，并讨论未来数字大规模重新摄影的方法和要求。我们介绍了一个现有的重新摄影网络平台re.photos，该平台集成了协作重新摄影、交互式图像注册，以及重新摄影图片的检索、组织和分享功能。对于大规模重新摄影，还需要满足一些额外的要求。需要同时处理多个模板图像和重新摄影图像，图像注册必须实现自动化，并且需要提供直观的智能手机应用程序进行重新摄影。长期存储并赋予持久标识符、自动或批量地理参考、以及游戏化和社交媒体集成，也是我们在本文中将讨论的进一步要求。
2501.01999	 | On the Utility of Equivariance and Symmetry Breaking in Deep Learning Architectures on Point Clouds	 | Sharvaree Vadgama,Mohammad Mohaiminul Islam,Domas Buracus,Christian Shewmake,Erik Bekkers	 | 本文研究了对称层在点云模型中的作用，发现对称性在提高模型性能方面具有积极影响，尤其是在任务复杂度增加时更为显著。	 | This paper explores the key factors that influence the performance of models working with point clouds, across different tasks of varying geometric complexity. In this work, we explore the trade-offs between flexibility and weight-sharing introduced by equivariant layers, assessing when equivariance boosts or detracts from performance. It is often argued that providing more information as input improves a model's performance. However, if this additional information breaks certain properties, such as $\SE(3)$ equivariance, does it remain beneficial? We identify the key aspects of equivariant and non-equivariant architectures that drive success in different tasks by benchmarking them on segmentation, regression, and generation tasks across multiple datasets with increasing complexity. We observe a positive impact of equivariance, which becomes more pronounced with increasing task complexity, even when strict equivariance is not required.	 | 本文探讨了影响点云模型性能的关键因素，这些模型在不同几何复杂度的任务中工作。在本文中，我们研究了由对称层引入的灵活性与权重共享之间的权衡关系，并评估了对称性在提升性能与削弱性能之间的效果。通常认为，提供更多输入信息可以改善模型的性能。然而，如果这种额外信息破坏了某些特性，比如$\SE(3)$对称性，它是否仍然有益？我们通过在多个复杂度递增的数据集上对分割、回归和生成任务进行基准测试，来识别驱动不同任务成功的关键方面，从而区分对称性和非对称性架构。我们观察到，对称性具有积极影响，这种影响在任务复杂度增加时更为显著，即使不需要严格的对称性。
2501.01998	 | SmartSpatial: Enhancing the 3D Spatial Arrangement Capabilities of Stable Diffusion Models and Introducing a Novel 3D Spatial Evaluation Framework	 | Mao Xun Huang,Hen-Hsen Huang	 | SmartSpatial 是一种通过 3D 感知条件和注意力机制增强稳定扩散模型的空间布局能力的方法，显著提高了复杂空间布局的准确性；结合 SmartSpatial 和 SmartSpatialEval 综合评估框架，该方法在 COCO 和 SpatialPrompts 数据集上取得了优于现有方法的性能，设定了新的基准。	 | Stable Diffusion models have made remarkable strides in generating photorealistic images from text prompts but often falter when tasked with accurately representing complex spatial arrangements, particularly involving intricate 3D relationships. To address this limitation, we introduce SmartSpatial, an innovative approach that enhances the spatial arrangement capabilities of Stable Diffusion models through 3D-aware conditioning and attention-guided mechanisms. SmartSpatial incorporates depth information and employs cross-attention control to ensure precise object placement, delivering notable improvements in spatial accuracy metrics. In conjunction with SmartSpatial, we present SmartSpatialEval, a comprehensive evaluation framework designed to assess spatial relationships. This framework utilizes vision-language models and graph-based dependency parsing for performance analysis. Experimental results on the COCO and SpatialPrompts datasets show that SmartSpatial significantly outperforms existing methods, setting new benchmarks for spatial arrangement accuracy in image generation.	 | 稳定扩散模型在从文本提示生成逼真图像方面取得了显著的进展，但在准确表示复杂的空间布局，尤其是涉及复杂的3D关系时，经常表现不佳。为解决这一局限性，我们引入了SmartSpatial，一种创新的方法，通过3D感知条件和注意力导向机制增强了稳定扩散模型的空间布局能力。SmartSpatial整合了深度信息，并使用交叉注意力控制以确保精确的对象放置，从而在空间准确性指标上取得显著改进。结合SmartSpatial，我们还提出了SmartSpatialEval，一种综合的评估框架，旨在评估空间关系。该框架利用视觉-语言模型和基于图的依赖解析进行性能分析。在COCO和SpatialPrompts数据集上的实验结果表明，SmartSpatial 显著优于现有方法，为图像生成中的空间布局准确性设定了新的基准。
2501.01993	 | A Novel Convolution and Attention Mechanism-based Model for 6D Object Pose Estimation	 | Alexander Du,Yingwu Zhu	 | 本文提出了一种基于图的表示方法，直接从RGB图像中估计6D物体姿态，通过节点连接和空间互动捕捉特征间的复杂依赖关系，并结合空间注意力、自我注意力和Legendre卷积层，显著提高了姿态估计的准确性，在多个基准数据集上优于现有方法。	 | Estimating 6D object poses from RGB images is challenging because the lack of depth information requires inferring a three dimensional structure from 2D projections. Traditional methods often rely on deep learning with grid based data structures but struggle to capture complex dependencies among extracted features. To overcome this, we introduce a graph based representation derived directly from images, where spatial temporal features of each pixel serve as nodes, and relationships between them are defined through node connectivity and spatial interactions. We also employ feature selection mechanisms that use spatial attention and self attention distillation, along with a Legendre convolution layer leveraging the orthogonality of Legendre polynomials for numerical stability. Experiments on the LINEMOD, Occluded LINEMOD, and YCB Video datasets demonstrate that our method outperforms nine existing approaches and achieves state of the art benchmark in object pose estimation.	 | 从RGB图像估计6D物体姿态极具挑战性，因为缺乏深度信息要求从二维投影中推断三维结构。传统的方法往往依赖于基于网格的数据结构进行深度学习，但难以捕捉提取特征之间的复杂依赖关系。为了解决这个问题，我们提出了一种直接从图像中获得的基于图的表示方法，其中每个像素的时空特征作为节点，节点之间的关系通过节点连接和空间互动来定义。我们还采用了利用空间注意力和自我注意力提炼的特征选择机制，并使用了Legendre卷积层，通过Legendre多项式的正交性实现数值稳定性。在LINEMOD、遮挡LINEMOD和YCB Video数据集上的实验表明，我们的方法优于现有的九种方法，并在物体姿态估计中达到了最先进的基准。
2501.01991	 | A Hybrid Deep Learning and Model-Checking Framework for Accurate Brain Tumor Detection and Validation	 | Lahcen El Fatimi,Elhoucine Elfatimi,Hanifa Bouchaneb	 | 本文提出了一种将模型检查与深度学习结合的混合框架，用于医学影像中的脑肿瘤检测和验证，通过结合CNN特征提取和K-FCM聚类实现了高准确率和召回率，证明了其在医学图像分析中的有效性和潜力。	 | Model checking, a formal verification technique, ensures systems meet predefined requirements, playing a crucial role in minimizing errors and enhancing quality during development. This paper introduces a novel hybrid framework integrating model checking with deep learning for brain tumor detection and validation in medical imaging. By combining model-checking principles with CNN-based feature extraction and K-FCM clustering for segmentation, the proposed approach enhances the reliability of tumor detection and segmentation. Experimental results highlight the framework's effectiveness, achieving 98\% accuracy, 96.15\% precision, and 100\% recall, demonstrating its potential as a robust tool for advanced medical image analysis.	 | 模型检查是一种形式验证技术，确保系统满足预定义的要求，对于减少开发中的错误和提高质量起着关键作用。本文介绍了一种新的混合框架，将模型检查与深度学习相结合，用于医学影像中的脑肿瘤检测和验证。通过结合模型检查原理与基于CNN的特征提取和K-FCM聚类进行分割，所提出的方法增强了肿瘤检测和分割的可靠性。实验结果表明该框架的有效性，准确率达到98%，精确率达到96.15%，召回率达到100%，证明了其作为先进医学图像分析工具的潜力。
2501.01989	 | CRRG-CLIP: Automatic Generation of Chest Radiology Reports and Classification of Chest Radiographs	 | Jianfei Xu,Thanet Markchom,Huizhi Liang	 | 该研究提出了CRRG-CLIP模型，一种端到端的自动化胸部放射学报告生成和影像分类模型，通过结合Faster R-CNN、GPT-2和无监督的CLIP模型，显著提升了报告的准确性和影像分类的性能。	 | The complexity of stacked imaging and the massive number of radiographs make writing radiology reports complex and inefficient. Even highly experienced radiologists struggle to maintain accuracy and consistency in interpreting radiographs under prolonged high-intensity work. To address these issues, this work proposes the CRRG-CLIP Model (Chest Radiology Report Generation and Radiograph Classification Model), an end-to-end model for automated report generation and radiograph classification. The model consists of two modules: the radiology report generation module and the radiograph classification module. The generation module uses Faster R-CNN to identify anatomical regions in radiographs, a binary classifier to select key regions, and GPT-2 to generate semantically coherent reports. The classification module uses the unsupervised Contrastive Language Image Pretraining (CLIP) model, addressing the challenges of high-cost labelled datasets and insufficient features. The results show that the generation module performs comparably to high-performance baseline models on BLEU, METEOR, and ROUGE-L metrics, and outperformed the GPT-4o model on BLEU-2, BLEU-3, BLEU-4, and ROUGE-L metrics. The classification module significantly surpasses the state-of-the-art model in AUC and Accuracy. This demonstrates that the proposed model achieves high accuracy, readability, and fluency in report generation, while multimodal contrastive training with unlabelled radiograph-report pairs enhances classification performance.	 | 成像层次的复杂性和大量放射影像使得编写放射学报告变得复杂且效率低下。即使经验最丰富的放射科医生，在长时间高强度的工作下也难以保持诊断的准确性和一致性。为了解决这些问题，本研究提出了CRRG-CLIP模型（胸部放射学报告生成和影像分类模型），这是一种端到端的自动化报告生成和影像分类模型。该模型由两个模块组成：放射学报告生成模块和影像分类模块。生成模块使用了Faster R-CNN来识别放射影像中的解剖区域，二元分类器来选择关键区域，以及GPT-2来生成语义连贯的报告。分类模块使用了无监督语言图像预训练（CLIP）模型，解决了高成本标注数据集和特征不足的问题。实验结果表明，生成模块在BLEU、METEOR和ROUGE-L指标上与高性能基线模型表现相当，且在BLEU-2, BLEU-3, BLEU-4和ROUGE-L指标上优于GPT-4o模型。分类模块在AUC和准确率指标上显著超越了现有最佳模型。这表明，提出的模型在报告生成方面实现了高准确性、可读性和流畅性，同时，使用未标注的放射影像-报告配对进行多模态对比训练提高了分类性能。
2501.01987	 | Gender Bias in Text-to-Video Generation Models: A case study of Sora	 | Mohammad Nadeem,Shahab Saquib Sohail,Erik Cambria,Björn W. Schuller,Amir Hussain	 | 研究表明，OpenAI的Sora文本到视频生成模型存在性别偏见，模型将特定性别不公正地与刻板印象行为和职业联系在一起，反映出训练数据中的社会偏见。	 | The advent of text-to-video generation models has revolutionized content creation as it produces high-quality videos from textual prompts. However, concerns regarding inherent biases in such models have prompted scrutiny, particularly regarding gender representation. Our study investigates the presence of gender bias in OpenAI's Sora, a state-of-the-art text-to-video generation model. We uncover significant evidence of bias by analyzing the generated videos from a diverse set of gender-neutral and stereotypical prompts. The results indicate that Sora disproportionately associates specific genders with stereotypical behaviors and professions, which reflects societal prejudices embedded in its training data.	 | 文本到视频生成模型的出现彻底改变了内容创作方式，因为它能够从文本提示中生成高质量的视频。然而，对于这些模型内在偏见的担忧引发了广泛关注，尤其是在性别代表性方面。我们的研究旨在调查OpenAI的Sora（一种最新的文本到视频生成模型）中是否存在性别偏见。通过分析来自多样性别中立和刻板印象提示生成的视频，我们发现了显著的偏见证据。研究结果显示，Sora 不公正地将特定性别与刻板印象行为和职业联系在一起，这反映了在其训练数据中嵌入的社会偏见。
2501.01986	 | FrameFusion: Combining Similarity and Importance for Video Token Reduction on Large Visual Language Models	 | Tianyu Fu,Tengxuan Liu,Qinghao Han,Guohao Dai,Shengen Yan,Huazhong Yang,Xuefei Ning,Yu Wang	 | 本文提出了一种名为FrameFusion的新方法，该方法结合基于相似性的标记合并和基于重要性的剪枝，以减少大型视觉-语言模型中的视觉标记数量，从而提高模型效率。实验结果显示，FrameFusion能够显著减少标记数量并加速模型运行，同时对性能影响较小。	 | The increasing demand to process long and high-resolution videos significantly burdens Large Vision-Language Models (LVLMs) due to the enormous number of visual tokens. Existing token reduction methods primarily focus on importance-based token pruning, which overlooks the redundancy caused by frame resemblance and repetitive visual elements. In this paper, we analyze the high vision token similarities in LVLMs. We reveal that token similarity distribution condenses as layers deepen while maintaining ranking consistency. Leveraging the unique properties of similarity over importance, we introduce FrameFusion, a novel approach that combines similarity-based merging with importance-based pruning for better token reduction in LVLMs. FrameFusion identifies and merges similar tokens before pruning, opening up a new perspective for token reduction. We evaluate FrameFusion on diverse LVLMs, including Llava-Video-{7B,32B,72B}, and MiniCPM-V-8B, on video understanding, question-answering, and retrieval benchmarks. Experiments show that FrameFusion reduces vision tokens by 70$\%$, achieving 3.4-4.4x LLM speedups and 1.6-1.9x end-to-end speedups, with an average performance impact of less than 3$\%$. Our code is available at https://github.com/thu-nics/FrameFusion.	 | 随着对处理长且高分辨率视频的需求不断增长，这给大型视觉-语言模型（LVLMs）带来了巨大负担，因为视觉标记的数量非常庞大。现有的标记减少方法主要集中在基于重要性的标记剪枝上，而忽视了帧相似性和重复视觉元素引起的冗余。在本文中，我们分析了LVLM中的高视觉标记相似性。我们发现，随着层数加深，标记相似性的分布会变得更为集中，同时保持排名的一致性。利用相似性和重要性之间的独特属性，我们引入了FrameFusion，这是一种新颖的方法，结合基于相似性的合并与基于重要性的剪枝，以更好地减少LVLM中的标记。FrameFusion 在剪枝前识别并合并相似的标记，为标记减少提供了一个新的视角。我们在包括Llava-Video-{7B,32B,72B}和MiniCPM-V-8B在内的多种LVLM上评估了FrameFusion。实验表明，FrameFusion 可以将视觉标记减少70%，实现3.4-4.4倍的LLM加速和1.6-1.9倍的端到端加速，平均性能影响不到3%。我们的代码可以在https://github.com/thu-nics/FrameFusion 获取。
2501.01985	 | Fall Detection in Passenger Elevators using Intelligent Surveillance Camera Systems: An Application with YoloV8 Nano Model	 | Pinar Yozgatli,Yavuz Acar,Mehmet Tulumen,Selman Minga,Salih Selamet,Beytullah Nalbant,Mustafa Talha Toru,Berna Koca,Tevfik Keles,Mehmet Selcok	 | 本研究利用YoloV8 Nano模型在乘客电梯环境中进行跌倒事件检测，通过训练包含超过10,000张图像的多样化数据集，实现了较高的检测精度（85%）和召回率（82%），展示了其在电梯安全系统中的应用潜力。	 | Computer vision technology, which involves analyzing images and videos captured by cameras through deep learning algorithms, has significantly advanced the field of human fall detection. This study focuses on the application of the YoloV8 Nano model in identifying fall incidents within passenger elevators, a context that presents unique challenges due to the enclosed environment and varying lighting conditions. By training the model on a robust dataset comprising over 10,000 images across diverse elevator types, we aim to enhance the detection precision and recall rates. The model's performance, with an 85% precision and 82% recall in fall detection, underscores its potential for integration into existing elevator safety systems to enable rapid intervention.	 | 计算机视觉技术通过深度学习算法分析由摄像头捕获的图像和视频，极大地推动了人类跌倒检测领域的进步。本研究重点探讨了使用 YoloV8 Nano 模型在乘客电梯中识别跌倒事件的应用，这一环境由于其封闭性及光线条件的多样性而带来了独特的挑战。通过对包含超过10,000张图像的多样化的电梯类型数据集进行训练，我们旨在提高跌倒事件检测的精度和召回率。该模型在跌倒检测方面的表现（精度85%，召回率82%）证明了它在现有电梯安全系统中集成的潜力，从而能够实现快速干预。
2501.01983	 | ECG-guided individual identification via PPG	 | Riling Wei,Hanjie Chen,Kelu Yao,Chuanguang Yang,Jun Wang,Chao Li	 | 一种新的跨模态知识蒸馏框架通过引入电生理图（ECG）信号来提高基于光电容积描记法（PPG）的个体识别技术的效果，实验结果显示该框架在识别准确率上显著提升，尤其是在识别未见过的个体时。	 | Photoplethsmography (PPG)-based individual identification aiming at recognizing humans via intrinsic cardiovascular activities has raised extensive attention due to its high security and resistance to mimicry. However, this kind of technology witnesses unpromising results due to the limitation of low information density. To this end, electrocardiogram (ECG) signals have been introduced as a novel modality to enhance the density of input information. Specifically, a novel cross-modal knowledge distillation framework is implemented to propagate discriminate knowledge from ECG modality to PPG modality without incurring additional computational demands at the inference phase. Furthermore, to ensure efficient knowledge propagation, Contrastive Language-Image Pre-training (CLIP)-based knowledge alignment and cross-knowledge assessment modules are proposed respectively. Comprehensive experiments are conducted and results show our framework outperforms the baseline model with the improvement of 2.8% and 3.0% in terms of overall accuracy on seen- and unseen individual recognitions.	 | 基于光电容积描记法（PPG）的个体识别技术旨在通过内在的心血管活动来识别人类，由于其高度的安全性和对抗伪装的能力，引起了广泛的关注。然而，这类技术由于信息密度低的原因，结果并不理想。为了解决这一问题，电生理图（ECG）信号被引入作为新的模态来提高输入信息的密度。具体而言，实现了一种新的跨模态知识蒸馏框架，能够在推理阶段不增加额外计算开销的情况下，从ECG模态传递有区别的知识到PPG模态。此外，为了确保高效的知识传递，提出了基于CLIP的知識对齐模块和跨知识评估模块。进行了全面的实验，结果显示，我们的框架在整体准确率方面，对于已见过和未见过个体的识别，分别比基线模型提高了2.8%和3.0%。
2501.01982	 | Is Your Image a Good Storyteller?	 | Xiujie Song,Xiaoyi Pang,Haifeng Tang,Mengyue Wu,Kenny Q. Zhu	 | 该研究关注图像的语义复杂性评估，指出虽然图像的视觉复杂性容易量化，但其语义复杂性长期以来被忽视，提出图像语义评估（ISA）任务，并构建了首个ISA数据集，利用语言模型来解决这一视觉问题，实验结果表明了其有效性和可行性。	 | Quantifying image complexity at the entity level is straightforward, but the assessment of semantic complexity has been largely overlooked. In fact, there are differences in semantic complexity across images. Images with richer semantics can tell vivid and engaging stories and offer a wide range of application scenarios. For example, the Cookie Theft picture is such a kind of image and is widely used to assess human language and cognitive abilities due to its higher semantic complexity. Additionally, semantically rich images can benefit the development of vision models, as images with limited semantics are becoming less challenging for them. However, such images are scarce, highlighting the need for a greater number of them. For instance, there is a need for more images like Cookie Theft to cater to people from different cultural backgrounds and eras. Assessing semantic complexity requires human experts and empirical evidence. Automatic evaluation of how semantically rich an image will be the first step of mining or generating more images with rich semantics, and benefit human cognitive assessment, Artificial Intelligence, and various other applications. In response, we propose the Image Semantic Assessment (ISA) task to address this problem. We introduce the first ISA dataset and a novel method that leverages language to solve this vision problem. Experiments on our dataset demonstrate the effectiveness of our approach. Our data and code are available at: https://github.com/xiujiesong/ISA.	 | 图像在实体层面上的复杂性量化相对简单，但语义复杂性的评估却长期被忽视。实际上，不同图像之间存在语义复杂度的差异。富含语义的图像能够讲述生动有趣的故事，并提供广泛的应用场景。例如，《饼干偷窃》这幅图像就是一种具有较高语义复杂度的图像，常被用来评估人类语言和认知能力。此外，富含语义的图像有助于视觉模型的发展，因为那些语义有限的图像对模型来说已经不再构成挑战。然而，这种图像稀缺，因此需要更多的此类图像。例如，需要更多像《饼干偷窃》这样的图像以满足不同文化背景和时代的用户需求。评估语义复杂性需要依靠人类专家和实证证据。自动评估图像语义丰富度是挖掘或生成更多富含语义图像的第一步，这将有助于人类认知评估、人工智能以及各种其他应用。为此，我们提出了图像语义评估（ISA）任务来应对这一问题。我们介绍了第一个ISA数据集，并提出了一种利用语言解决这一视觉问题的新方法。在我们数据集上的实验表明了我们方法的有效性。我们的数据和代码可在以下链接获取：https://github.com/xiujiesong/ISA。
2501.01981	 | Optical Character Recognition using Convolutional Neural Networks for Ashokan Brahmi Inscriptions	 | Yash Agrawal,Srinidhi Balasubramanian,Rahul Meena,Rohail Alam,Himanshu Malviya,Rohini P	 | 本文研究了一个基于卷积神经网络的光学字符识别系统，用于识别阿育王梵文字母，并通过迁移学习将LeNet、VGG-16和MobileNet适应该任务，最终发现MobileNet在验证集上达到了95.94%的准确率。研究表明，MobileNet在识别阿育王梵文字母方面表现出色，具有重要的应用价值。	 | This research paper delves into the development of an Optical Character Recognition (OCR) system for the recognition of Ashokan Brahmi characters using Convolutional Neural Networks. It utilizes a comprehensive dataset of character images to train the models, along with data augmentation techniques to optimize the training process. Furthermore, the paper incorporates image preprocessing to remove noise, as well as image segmentation to facilitate line and character segmentation. The study mainly focuses on three pre-trained CNNs, namely LeNet, VGG-16, and MobileNet and compares their accuracy. Transfer learning was employed to adapt the pre-trained models to the Ashokan Brahmi character dataset. The findings reveal that MobileNet outperforms the other two models in terms of accuracy, achieving a validation accuracy of 95.94% and validation loss of 0.129. The paper provides an in-depth analysis of the implementation process using MobileNet and discusses the implications of the findings.   The use of OCR for character recognition is of significant importance in the field of epigraphy, specifically for the preservation and digitization of ancient scripts. The results of this research paper demonstrate the effectiveness of using pre-trained CNNs for the recognition of Ashokan Brahmi characters.	 | 本文研究了一个基于卷积神经网络（Convolutional Neural Networks, CNN）的光学字符识别（Optical Character Recognition, OCR）系统，用于识别阿育王梵文字母。该研究使用了一套全面的字符图像数据集进行模型训练，并结合数据增强技术以优化训练过程。此外，论文还引入了图像预处理以去除噪声，以及图像分割以帮助进行行和字符分割。研究主要集中在三种预训练的CNN模型，即LeNet、VGG-16和MobileNet，并对比了它们的准确性。通过迁移学习，将预训练模型适应阿育王梵文字母数据集。研究结果表明，MobileNet在准确性方面优于其他两种模型，在验证集上达到了95.94%的准确率和0.129的验证损失。论文详细分析了使用MobileNet的实现过程，并讨论了研究结果的意义。OCR在铭文研究领域，特别是在古代文字的保存和数字化方面，具有重要意义。本文的研究结果证明了使用预训练CNN模型识别阿育王梵文字母的有效性。
2501.01980	 | Polarimetric BSSRDF Acquisition of Dynamic Faces	 | Hyunho Ha,Inseung Hwang,Nestor Monzon,Jaemin Cho,Donggun Kim,Seung-Hwan Baek,Adolfo Muñoz,Diego Gutierrez,Min H. Kim	 | 研究人员提出了一种新的极化光获取方法，专门用于动态人类面部，以捕捉广泛肤色和面部表情下的空间变化外观和精确几何形状，同时揭示生物物理组成部分及其浓度，这是首次实现同时获取动态人类面部的极化和光谱反射信息以及基于生物物理参数和几何形状的方法。	 | Acquisition and modeling of polarized light reflection and scattering help reveal the shape, structure, and physical characteristics of an object, which is increasingly important in computer graphics. However, current polarimetric acquisition systems are limited to static and opaque objects. Human faces, on the other hand, present a particularly difficult challenge, given their complex structure and reflectance properties, the strong presence of spatially-varying subsurface scattering, and their dynamic nature. We present a new polarimetric acquisition method for dynamic human faces, which focuses on capturing spatially varying appearance and precise geometry, across a wide spectrum of skin tones and facial expressions. It includes both single and heterogeneous subsurface scattering, index of refraction, and specular roughness and intensity, among other parameters, while revealing biophysically-based components such as inner- and outer-layer hemoglobin, eumelanin and pheomelanin. Our method leverages such components' unique multispectral absorption profiles to quantify their concentrations, which in turn inform our model about the complex interactions occurring within the skin layers. To our knowledge, our work is the first to simultaneously acquire polarimetric and spectral reflectance information alongside biophysically-based skin parameters and geometry of dynamic human faces. Moreover, our polarimetric skin model integrates seamlessly into various rendering pipelines.	 | 极化光反射和散射的获取与建模有助于揭示物体的形状、结构和物理特性，在计算机图形学中变得越来越重要。然而，目前的极化光获取系统仅适用于静态和不透明的物体。相比之下，人类面部由于其复杂的结构和反射特性、强烈的空间变化性皮下散射以及动态性质，构成了一个特别具有挑战性的难题。我们提出了一种新的极化光获取方法，专门用于动态人类面部，旨在捕捉广泛肤色和面部表情下空间变化的外观和精确几何形状。该方法包括单层和异质层皮下散射、折射率以及镜面粗糙度和强度等参数，同时揭示了基于生物物理的组成部分，如深层和表层血红蛋白、真黑素和pheomelanin。我们的方法利用这些组成部分的独特多光谱吸收特性来定量测定其浓度，从而帮助我们的模型了解发生在皮肤各层内的复杂相互作用。据我们所知，我们的工作首次实现了同时获取动态人类面部的极化和光谱反射信息以及基于生物物理参数和几何形状。此外，我们的极化光皮肤模型可以无缝集成到各种渲染管道中。
2501.01973	 | INFELM: In-depth Fairness Evaluation of Large Text-To-Image Models	 | Di Jin,Xing Liu,Yu Liu,Jia Qing Yap,Andrea Wong,Adriana Crespo,Qi Lin,Zhiyuan Yin,Qiang Yan,Ryan Ye	 | 该研究提出了INFELM，一种深入的公平性评估方法，用于广泛使用的话语到图像模型，通过结合先进的肤色分类器、偏见敏感内容对齐度量和多样化表示偏见评估，解决现有评估框架的局限性，并揭示当前模型在多个社会偏见敏感领域的不足。	 | The rapid development of large language models (LLMs) and large vision models (LVMs) have propelled the evolution of multi-modal AI systems, which have demonstrated the remarkable potential for industrial applications by emulating human-like cognition. However, they also pose significant ethical challenges, including amplifying harmful content and reinforcing societal biases. For instance, biases in some industrial image generation models highlighted the urgent need for robust fairness assessments. Most existing evaluation frameworks focus on the comprehensiveness of various aspects of the models, but they exhibit critical limitations, including insufficient attention to content generation alignment and social bias-sensitive domains. More importantly, their reliance on pixel-detection techniques is prone to inaccuracies.   To address these issues, this paper presents INFELM, an in-depth fairness evaluation on widely-used text-to-image models. Our key contributions are: (1) an advanced skintone classifier incorporating facial topology and refined skin pixel representation to enhance classification precision by at least 16.04%, (2) a bias-sensitive content alignment measurement for understanding societal impacts, (3) a generalizable representation bias evaluation for diverse demographic groups, and (4) extensive experiments analyzing large-scale text-to-image model outputs across six social-bias-sensitive domains. We find that existing models in the study generally do not meet the empirical fairness criteria, and representation bias is generally more pronounced than alignment errors. INFELM establishes a robust benchmark for fairness assessment, supporting the development of multi-modal AI systems that align with ethical and human-centric principles.	 | 大型语言模型（LLMs）和大型视觉模型（LVMs）的快速发展推动了多模态人工智能系统的演进，这些系统展示了在工业应用中模拟人类认知的强大潜力。然而，它们也带来了重大的伦理挑战，包括放大有害内容和强化社会偏见。例如，某些工业级图像生成模型中的偏见凸显了进行robust公平评估的迫切需要。现有的大多数评估框架主要关注模型各方面内容的全面性，但它们存在关键局限性，包括对内容生成对齐和社交偏见敏感领域关注不足。更重要的是，它们依赖于像素检测技术，容易出现不准确的情况。  为了解决这些问题，本文提出了INFELM，这是一种深入的公平性评估，针对广泛使用的话语到图像模型。我们的主要贡献包括：（1）结合面部拓扑和精炼的皮肤像素表示的先进肤色分类器，以提升分类精度至少16.04%，（2）一种针对社会影响的偏见敏感内容对齐度量，（3）一种适用于多样人口群体的一般化表示偏见评估，以及（4）运用广泛的实验分析大规模话语到图像模型输出在六个社会偏见敏感领域的表现。我们发现，研究中的现有模型通常未能满足实证公平性标准，表示偏见通常比对齐错误更为显著。INFELM建立了公平性评估的坚实基准，支持开发符合伦理和以人为本原则的多模态人工智能系统。
2501.01960	 | GAF-FusionNet: Multimodal ECG Analysis via Gramian Angular Fields and Split Attention	 | Jiahao Qin,Feng Liu	 | 本文提出了一种名为GAF-FusionNet的多模态框架，结合时间序列分析和Gramian Angular Fields（GAF）来改善ECG分类，该方法在ECG200、ECG5000和MIT-BIH心律失常数据库上分别达到了94.5%、96.9%和99.6%的准确率，优于现有最佳方法。	 | Electrocardiogram (ECG) analysis plays a crucial role in diagnosing cardiovascular diseases, but accurate interpretation of these complex signals remains challenging. This paper introduces a novel multimodal framework(GAF-FusionNet) for ECG classification that integrates time-series analysis with image-based representation using Gramian Angular Fields (GAF). Our approach employs a dual-layer cross-channel split attention module to adaptively fuse temporal and spatial features, enabling nuanced integration of complementary information. We evaluate GAF-FusionNet on three diverse ECG datasets: ECG200, ECG5000, and the MIT-BIH Arrhythmia Database. Results demonstrate significant improvements over state-of-the-art methods, with our model achieving 94.5\%, 96.9\%, and 99.6\% accuracy on the respective datasets. Our code will soon be available at https://github.com/Cross-Innovation-Lab/GAF-FusionNet.git.	 | 心电图（ECG）分析在诊断心血管疾病中发挥着重要作用，但准确解析这些复杂信号仍然具有挑战性。本文介绍了一种新颖的多模态框架（GAF-FusionNet），该框架将时间序列分析与基于图像的表示（Gramian Angular Fields，GAF）相结合，用于ECG分类。我们的方法采用双层跨通道分裂注意力模块，以自适应地融合时序和空间特征，实现互补信息的精细化整合。我们将在ECG200、ECG5000和MIT-BIH心律失常数据库这三种不同数据集上评估GAF-FusionNet。结果表明，与现有最佳方法相比，我们的模型分别在上述数据集上达到了94.5%、96.9%和99.6%的准确率。我们的代码将在不久的将来在https://github.com/Cross-Innovation-Lab/GAF-FusionNet.git公开。
2501.01959	 | STEAM-EEG: Spatiotemporal EEG Analysis with Markov Transfer Fields and Attentive CNNs	 | Jiahao Qin,Feng Liu	 | 本文提出了一种名为STEAM-EEG的方法，结合马尔可夫转移场和计算机图形技术，将复杂脑电图信号转化为可视化图像，以促进数据探索和模式识别。	 | Electroencephalogram (EEG) signals play a pivotal role in biomedical research and clinical applications, including epilepsy diagnosis, sleep disorder analysis, and brain-computer interfaces. However, the effective analysis and interpretation of these complex signals often present significant challenges. This paper presents a novel approach that integrates computer graphics techniques with biological signal pattern recognition, specifically using Markov Transfer Fields (MTFs) for EEG time series imaging. The proposed framework (STEAM-EEG) employs the capabilities of MTFs to capture the spatiotemporal dynamics of EEG signals, transforming them into visually informative images. These images are then rendered, visualised, and modelled using state-of-the-art computer graphics techniques, thereby facilitating enhanced data exploration, pattern recognition, and decision-making. The code could be accessed from GitHub.	 | 脑电图（EEG）信号在生物医学研究和临床应用中发挥着关键作用，包括癫痫诊断、睡眠障碍分析以及脑-机接口等方面。然而，有效分析和解读这些复杂信号常常带来显著挑战。本文提出了一种创新方法，将计算机图形技术与生物信号模式识别相结合，特别是利用马尔可夫转移场（MTFs）对EEG时间序列进行成像。所提出的方法（STEAM-EEG）利用MTFs的能力捕捉EEG信号的空间-时间动态，并将其转化为具有视觉信息的图像。这些图像随后使用最先进的计算机图形技术进行渲染、可视化和建模，从而促进数据探索、模式识别和决策的增强。相关代码可以从GitHub获取。
2501.03151	 | Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches	 | Alhassan Mumuni,Fuseini Mumuni	 | 大型预训练基础模型已经在视觉-语言、语言、扩散和视觉-语言-行动等多种模型中展示了解决复杂AI问题的能力，尤其是多模态大型语言模型能够提供丰富的表示能力和广泛的功能。然而，这些模型的认知能力仍然有限，为了达到人类水平的通用智能，必须解决实体化、符号接地、因果关系和记忆等基础问题，并在大型语言模型中实现这些概念。	 | Generative artificial intelligence (AI) systems based on large-scale pretrained foundation models (PFMs) such as vision-language models, large language models (LLMs), diffusion models and vision-language-action (VLA) models have demonstrated the ability to solve complex and truly non-trivial AI problems in a wide variety of domains and contexts. Multimodal large language models (MLLMs), in particular, learn from vast and diverse data sources, allowing rich and nuanced representations of the world and, thereby, providing extensive capabilities, including the ability to reason, engage in meaningful dialog; collaborate with humans and other agents to jointly solve complex problems; and understand social and emotional aspects of humans. Despite this impressive feat, the cognitive abilities of state-of-the-art LLMs trained on large-scale datasets are still superficial and brittle. Consequently, generic LLMs are severely limited in their generalist capabilities. A number of foundational problems -- embodiment, symbol grounding, causality and memory -- are required to be addressed for LLMs to attain human-level general intelligence. These concepts are more aligned with human cognition and provide LLMs with inherent human-like cognitive properties that support the realization of physically-plausible, semantically meaningful, flexible and more generalizable knowledge and intelligence. In this work, we discuss the aforementioned foundational issues and survey state-of-the art approaches for implementing these concepts in LLMs. Specifically, we discuss how the principles of embodiment, symbol grounding, causality and memory can be leveraged toward the attainment of artificial general intelligence (AGI) in an organic manner.	 | 基于大规模预训练基础模型（PFMs）的生成型人工智能（AI）系统，如视觉-语言模型、大型语言模型（LLMs）、扩散模型和视觉-语言-行动（VLA）模型，已经在各种领域和情境下展示了解决复杂且真正非平凡AI问题的能力。特别是多模态大型语言模型（MLLMs），它们从大量的多样化数据源中学习，能够对世界进行丰富且细腻的表示，从而提供广泛的能力，包括推理、进行有意义的对话、与人类和其他代理共同解决复杂问题、以及理解人类的社会和情感方面。尽管取得了这一令人印象深刻的成就，但基于大规模数据集训练的最新LLMs的认知能力仍然肤浅且脆弱。因此，通用型LLMs在专家领域的能力仍然受到严重限制。为了使LLMs达到人类水平的通用智能，必须解决一系列基础问题——实体化、符号接地、因果关系和记忆。这些概念更加接近人类认知，赋予LLMs类似人类的认知属性，支持实现物理上可信、语义上有意义、灵活且更具普适性的知识和智能。本文讨论了上述基础问题，并综述了在LLMs中实现这些概念的最新方法。具体来说，我们探讨了如何通过有机利用实体化、符号接地、因果关系和记忆的原则来实现人工通用智能（AGI）。
2501.03053	 | Dr. Tongue: Sign-Oriented Multi-label Detection for Remote Tongue Diagnosis	 | Yiliang Chen,Steven SC Ho,Cheng Xu,Yao Jie Xie,Wing-Fai Yeung,Shengfeng He,Jing Qin	 | 本文提出了一种基于符号的多标签属性检测框架，用于通过远程医疗准确识别舌诊特征，从而实现全面的健康评估。该方法通过标准化舌象图像并使用自适应特征提取模块及符号网络（SignNet）来模拟医师诊断过程，并通过一个专门设计的广泛舌象图像数据集进行了验证，提高了舌象特征检测的准确性。	 | Tongue diagnosis is a vital tool in Western and Traditional Chinese Medicine, providing key insights into a patient's health by analyzing tongue attributes. The COVID-19 pandemic has heightened the need for accurate remote medical assessments, emphasizing the importance of precise tongue attribute recognition via telehealth. To address this, we propose a Sign-Oriented multi-label Attributes Detection framework. Our approach begins with an adaptive tongue feature extraction module that standardizes tongue images and mitigates environmental factors. This is followed by a Sign-oriented Network (SignNet) that identifies specific tongue attributes, emulating the diagnostic process of experienced practitioners and enabling comprehensive health evaluations. To validate our methodology, we developed an extensive tongue image dataset specifically designed for telemedicine. Unlike existing datasets, ours is tailored for remote diagnosis, with a comprehensive set of attribute labels. This dataset will be openly available, providing a valuable resource for research. Initial tests have shown improved accuracy in detecting various tongue attributes, highlighting our framework's potential as an essential tool for remote medical assessments.	 | 舌诊是西方医学和传统中医中的一项重要工具，通过分析舌象特征来提供患者健康的关键见解。COVID-19 疫情进一步凸显了准确远程医疗评估的需求，强调了通过远程医疗准确识别舌象特征的重要性。为此，我们提出了一种基于符号的多标签属性检测框架。我们的方法首先包含一个自适应舌象特征提取模块，该模块标准化舌象图像并减轻环境因素的影响。随后是一个基于符号的网络（SignNet），用于识别特定的舌象特征，模拟经验丰富医师的诊断过程，从而实现全面的健康评估。为了验证我们的方法，我们开发了一个针对远程医疗设计的广泛舌象图像数据集。与现有数据集不同，我们的数据集专门适用于远程诊断，并包含一套全面的属性标签。该数据集将公开提供，为研究工作提供宝贵资源。初步测试表明，该方法在检测多种舌象特征方面提高了准确性，突显了我们的框架作为远程医疗评估工具的潜在重要性。
2501.03030	 | DDRM-PR: Fourier Phase Retrieval using Denoising Diffusion Restoration Models	 | Mehmet Onurcan Kaya,Figen S. Oktem	 | 本文利用去噪扩散恢复模型（DDRM）解决非线性相位检索问题，结合预训练的无条件扩散先验和基于模型的交替投影方法，从噪声强度测量中重构图像，并通过模拟和实验数据验证了其性能与局限性。	 | Diffusion models have demonstrated their utility as learned priors for solving various inverse problems. However, most existing approaches are limited to linear inverse problems. This paper exploits the efficient and unsupervised posterior sampling framework of Denoising Diffusion Restoration Models (DDRM) for the solution of nonlinear phase retrieval problem, which requires reconstructing an image from its noisy intensity-only measurements such as Fourier intensity. The approach combines the model-based alternating-projection methods with the DDRM to utilize pretrained unconditional diffusion priors for phase retrieval. The performance is demonstrated through both simulations and experimental data. Results demonstrate the potential of this approach for improving the alternating-projection methods as well as its limitations.	 | 扩散模型已经证明了它们在解决各种逆问题方面作为学习先验的实用性。然而，大多数现有方法仅限于线性逆问题。本文利用去噪扩散恢复模型（DDRM）的高效且无监督的后验采样框架，解决非线性相位检索问题，该问题需要从其噪声强度测量（如傅里叶强度）中重构图像。该方法结合了基于模型的交替投影方法与DDRM，利用预训练的无条件扩散先验进行相位检索。性能通过模拟和实验数据进行了验证。结果表明了这种方法提高交替投影方法的潜力以及其局限性。
2501.03021	 | A Trust-Guided Approach to MR Image Reconstruction with Side Information	 | Arda Atalık,Sumit Chopra,Daniel K. Sodickson	 | 该研究提出了一种新的端到端深度学习框架TGVN，能够有效整合上下文辅助信息以解决线性反问题，从而在保持数据忠实性的同时改善多对比度MRI图像重建的质量和速度。	 | Reducing MRI scan times can improve patient care and lower healthcare costs. Many acceleration methods are designed to reconstruct diagnostic-quality images from limited sets of acquired $\textit{k}$-space data. This task can be framed as a linear inverse problem (LIP), where, as a result of undersampling, the forward operator may become rank-deficient or exhibit small singular values. This results in ambiguities in reconstruction, in which multiple generally incorrect or non-diagnostic images can map to the same acquired data. To address such ambiguities, it is crucial to incorporate prior knowledge, for example in the form of regularization. Another form of prior knowledge less commonly used in medical imaging is contextual side information garnered from other sources than the current acquisition. Here, we propose the $\textbf{T}$rust-$\textbf{G}$uided $\textbf{V}$ariational $\textbf{N}$etwork $\textbf{(TGVN)}$, a novel end-to-end deep learning framework that effectively integrates side information into LIPs. TGVN eliminates undesirable solutions from the ambiguous space of the forward operator while remaining faithful to the acquired data. We demonstrate its effectiveness in multi-coil, multi-contrast MR image reconstruction, where incomplete or low-quality measurements from one contrast are used as side information to reconstruct high-quality images of another contrast from heavily under-sampled data. Our method is robust across different contrasts, anatomies, and field strengths. Compared to baselines that also utilize side information, TGVN achieves superior image quality at challenging under-sampling levels, drastically speeding up acquisition while minimizing hallucinations. Our approach is also versatile enough to incorporate many different types of side information (including previous scans or even text) into any LIP.	 | 减少MRI扫描时间可以提高患者护理质量并降低医疗成本。许多加速方法旨在从有限的采集k空间数据集中重建诊断质量的图像。这个问题可以被表述为线性反问题（LIP），由于欠采样，前向算子可能会出现秩不足或者表现出较小的奇异值。这会导致重建中的歧义性，在这种情况下，多张通常不正确或非诊断性的图像可以对应于相同的采集数据。为了应对这种歧义性，有必要融入先前知识，例如以正则化的形式。另一种较少用于医学成像的先前知识形式是从当前采集之外的其他来源获得的上下文辅助信息。在此，我们提出了**T**rust-**G**uided **V**ariational **N**etwork **(TGVN)**，这是一种新颖的端到端深度学习框架，能够有效地将辅助信息整合到线性反问题中。TGVN可以在前向算子的模糊空间中消除不希望的解决方案，同时保持对采集数据的忠实性。我们展示了它在多线圈、多对比度MR图像重建中的有效性，其中来自一种对比度的不完整或低质量测量可以作为辅助信息用于从严重欠采样的数据重建另一种对比度的高质量图像。该方法在不同的对比度、解剖结构和磁场强度下具有鲁棒性。与也利用辅助信息的基线方法相比，TGVN在挑战性的欠采样水平下实现了更高质量的图像效果，大幅加速了数据采集的同时减少了幻觉。我们的方法还足够灵活，可以将多种类型的辅助信息（包括之前的扫描甚至文字）整合到任何线性反问题中。
2501.03012	 | Analyzing Fine-tuning Representation Shift for Multimodal LLMs Steering alignment	 | Pegah Khayatan,Mustafa Shukor,Jayneel Parekh,Matthieu Cord	 | 本研究系统地分析了多模态LLM在微调过程中隐藏状态表示的演变，揭示了模型如何专门化于新的多模态任务，并通过转移向量展示了如何控制模型的行为。通过将隐藏状态映射到可解释的概念，该研究提供了新的视角来解释多模态任务中的模型适应过程。	 | Multimodal LLMs have reached remarkable levels of proficiency in understanding multimodal inputs, driving extensive research to develop increasingly powerful models. However, much less attention has been paid to understanding and explaining the underlying mechanisms of these models. Most existing explainability research examines these models only in their final states, overlooking the dynamic representational shifts that occur during training. In this work, we systematically analyze the evolution of hidden state representations to reveal how fine-tuning alters the internal structure of a model to specialize in new multimodal tasks. Using a concept-based approach, we map hidden states to interpretable visual and textual concepts, enabling us to trace changes in encoded concepts across modalities as training progresses. We also demonstrate the use of shift vectors to capture these concepts changes. These shift vectors allow us to recover fine-tuned concepts by shifting those in the original model. Finally, we explore the practical impact of our findings on model steering, showing that we can adjust multimodal LLMs behaviors without any training, such as modifying answer types, captions style, or biasing the model toward specific responses. Our work sheds light on how multimodal representations evolve through fine-tuning and offers a new perspective for interpreting model adaptation in multimodal tasks. The code for this project is publicly available at https://github.com/mshukor/xl-vlms.	 | 多模态LLM在理解多模态输入方面已经达到了令人瞩目的水平，推动了大量研究以开发越来越强大的模型。然而，对这些模型的底层机制的理解和解释却相对较少。目前大多数现有的解释性研究只关注模型的最终状态，忽视了训练过程中发生的动态表示变化。在本工作中，我们系统地分析隐藏状态表示的演变，揭示了微调如何改变模型的内部结构以专门化于新的多模态任务。我们采用了基于概念的方法，将隐藏状态映射到可解释的视觉和文本概念，使我们能够随着训练的进展跟踪不同模态中编码概念的变化。我们还展示了使用转移向量来捕捉这些概念变化的方法。这些转移向量允许我们通过在原始模型中转移这些向量来恢复微调的概念。最后，我们探讨了这些发现对模型控制的实践影响，展示了我们可以在没有任何训练的情况下调整多模态LLM的行为，例如修改答案类型、图注风格或偏向特定的响应。我们的工作揭示了多模态表示在微调过程中如何演变，并为解释多模态任务中的模型适应提供了新的视角。该项目的代码可在https://github.com/mshukor/xl-vlms 公开获取。
2501.02992	 | GLFC: Unified Global-Local Feature and Contrast Learning with Mamba-Enhanced UNet for Synthetic CT Generation from CBCT	 | Xianhao Zhou,Jianghao Wu,Huangxuan Zhao,Lei Chen,Shaoting Zhang,Guotai Wang,Guotai Wang	 | 本文提出了一种名为GLFC的框架，通过结合Mamba块增强的UNet和多重对比损失，有效提升了从锥形束CT生成合成CT图像的质量，实验结果显著优于现有方法，SSIM从77.91%提高到91.50%。	 | Generating synthetic Computed Tomography (CT) images from Cone Beam Computed Tomography (CBCT) is desirable for improving the image quality of CBCT. Existing synthetic CT (sCT) generation methods using Convolutional Neural Networks (CNN) and Transformers often face difficulties in effectively capturing both global and local features and contrasts for high-quality sCT generation. In this work, we propose a Global-Local Feature and Contrast learning (GLFC) framework for sCT generation. First, a Mamba-Enhanced UNet (MEUNet) is introduced by integrating Mamba blocks into the skip connections of a high-resolution UNet for effective global and local feature learning. Second, we propose a Multiple Contrast Loss (MCL) that calculates synthetic loss at different intensity windows to improve quality for both soft tissues and bone regions. Experiments on the SynthRAD2023 dataset demonstrate that GLFC improved the SSIM of sCT from 77.91% to 91.50% compared with the original CBCT, and significantly outperformed several existing methods for sCT generation. The code is available at https://github.com/intelland/GLFC	 | 从锥形束计算机断层扫描（CBCT）生成合成计算机断层扫描（sCT）图像对于提高CBCT图像质量是很有价值的。现有的使用卷积神经网络（CNN）和Transformer的sCT生成方法往往难以有效捕捉全局和局部特征及对比度，从而影响高质量sCT的生成。在这项工作中，我们提出了一种全局-局部特征和对比度学习（GLFC）框架，用于sCT生成。首先，我们通过将Mamba块整合到高分辨率UNet的跳过连接中，引入了一种Mamba-增强的UNet（MEUNet），以实现有效的全局和局部特征学习。其次，我们提出了一种多重对比损失（MCL），该损失在不同的强度窗口中计算合成损失，从而提高软组织和骨区域的质量。在SynthRAD2023数据集上的实验表明，GLFC将sCT的SSIM从77.91%提高到91.50%，相较于原始CBCT显著提升了sCT生成的质量，并且在多个现有方法中表现出了显著的优势。相关代码可在https://github.com/intelland/GLFC 获取。
2501.02867	 | Diff-Lung: Diffusion-Based Texture Synthesis for Enhanced Pathological Tissue Segmentation in Lung CT Scans	 | Rezkellah Noureddine Khiati,Pierre-Yves Brillet,Radu Ispas,Catalin Fetita	 | 本文提出了一种利用扩散模型进行数据增强的方法，以解决肺病理模式分割中的类别不平衡问题，从而提高分割准确性，特别是对罕见模式的分割，进而提升肺CT扫描的自动化分析能力。	 | Accurate quantification of the extent of lung pathological patterns (fibrosis, ground-glass opacity, emphysema, consolidation) is prerequisite for diagnosis and follow-up of interstitial lung diseases. However, segmentation is challenging due to the significant class imbalance between healthy and pathological tissues. This paper addresses this issue by leveraging a diffusion model for data augmentation applied during training an AI model. Our approach generates synthetic pathological tissue patches while preserving essential shape characteristics and intricate details specific to each tissue type. This method enhances the segmentation process by increasing the occurence of underrepresented classes in the training data. We demonstrate that our diffusion-based augmentation technique improves segmentation accuracy across all pathological tissue types, particularly for the less common patterns. This advancement contributes to more reliable automated analysis of lung CT scans, potentially improving clinical decision-making and patient outcomes	 | 准确量化肺病理模式（纤维化、磨玻璃影、肺气肿、实变）的范围是诊断和监测间质性肺疾病的先决条件。然而，由于健康组织和病理组织之间的显著类别不平衡，分割过程极具挑战性。本文通过在训练AI模型时利用扩散模型进行数据增强来解决这一问题。我们的方法生成合成病理组织斑块，同时保留每种组织类型特有的重要形状特征和细微细节。这种方法通过增加训练数据中代表性不足类别的出现频率，增强了分割过程。我们证明，基于扩散的增强技术可以提高所有病理组织类型的分割准确性，尤其是对于较为罕见的模式。这一进展有助于更可靠的肺CT扫描自动化分析，可能改善临床决策和患者预后。
2501.02860	 | Seeing the Whole in the Parts in Self-Supervised Representation Learning	 | Arthur Aubret,Céline Teulière,Jochen Triesch	 | 本文提出了一种新的自我监督学习方法CO-SSL，通过在池化前将局部表示与全局图像表示对齐来建模空间共现，该方法在多个数据集上表现出色，并且对噪声和对抗攻击具有更高的鲁棒性。	 | Recent successes in self-supervised learning (SSL) model spatial co-occurrences of visual features either by masking portions of an image or by aggressively cropping it. Here, we propose a new way to model spatial co-occurrences by aligning local representations (before pooling) with a global image representation. We present CO-SSL, a family of instance discrimination methods and show that it outperforms previous methods on several datasets, including ImageNet-1K where it achieves 71.5% of Top-1 accuracy with 100 pre-training epochs. CO-SSL is also more robust to noise corruption, internal corruption, small adversarial attacks, and large training crop sizes. Our analysis further indicates that CO-SSL learns highly redundant local representations, which offers an explanation for its robustness. Overall, our work suggests that aligning local and global representations may be a powerful principle of unsupervised category learning.	 | 近年来，自我监督学习（SSL）模型通过遮蔽图像的部分或猛烈裁剪图像来建模视觉特征的空间共现。在此基础上，我们提出了一种新的方法来建模空间共现，即将局部表示（在池化之前）与全局图像表示对齐。我们介绍了CO-SSL这一家族的实例鉴别方法，并且在多个数据集上展示了其优越性，包括在ImageNet-1K上达到了71.5%的Top-1精度，使用了100个预训练周期。CO-SSL在噪声破坏、内部破坏、小的对抗性攻击以及大的训练裁剪尺寸方面也表现得更为稳健。我们的分析进一步表明，CO-SSL学习了高度冗余的局部表示，这为其稳健性提供了一种解释。总体而言，我们的工作表明将局部和全局表示对齐可能是无监督类别学习的一种强大原则。
2501.02795	 | InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via LLM Fusion	 | Zhaoyi Yan,Zhijie Sang,Yiming Zhang,Yuhao Fu,Baoyi He,Qi Zhou,Yining Di,Chunlin Ji,Shengyu Zhang,Fei Wu,Hongxia Yang	 | 本文提出了一种将多个领域特化的大型语言模型集成到高效枢纽模型中的策略，通过逐步两两融合和统一融合两种方法，特别是在引入了速率-偏斜自适应融合（RSAF）技术后，显著提升了模型在几个关键任务上的性能，分别在GSM8K、MATH和HumanEval任务上实现了高达9.27%、8.80%和8.89%的准确率提升。	 | Large Language Models (LLMs) have demonstrated strong performance across various reasoning tasks, yet building a single model that consistently excels across all domains remains challenging. This paper addresses this problem by exploring strategies to integrate multiple domain-specialized models into an efficient pivot model.We propose two fusion strategies to combine the strengths of multiple LLMs: (1) a pairwise, multi-step fusion approach that sequentially distills each source model into the pivot model, followed by a weight merging step to integrate the distilled models into the final model. This method achieves strong performance but requires substantial training effort; and (2) a unified fusion approach that aggregates all source models' outputs simultaneously.To improve the fusion process, we introduce a novel Rate-Skewness Adaptive Fusion (RSAF) technique, which dynamically adjusts top-K ratios during parameter merging for enhanced flexibility and stability.Furthermore, we propose an uncertainty-based weighting method for the unified approach, which dynamically balances the contributions of source models and outperforms other logits/distribution ensemble methods.We achieved accuracy improvements of 9.27%, 8.80%, and 8.89% on the GSM8K, MATH, and HumanEval tasks, respectively.	 | 大型语言模型（LLMs）在各种推理任务中表现出强大的性能，但构建一个能够在所有领域都持续表现出色的单一模型仍然颇具挑战性。本文通过探讨将多个领域特化的模型集成到高效枢纽模型中的策略来解决这一问题。我们提出了两种融合策略来结合多个LLM的优势：（1）一种逐步两两融合的方法，该方法首先逐个将每个源模型提炼到枢纽模型中，然后进行权重合并步骤以将提炼后的模型整合到最终模型中。这种方法在性能上表现出色，但需要大量的训练努力；（2）一种统一融合方法，该方法同时聚合所有源模型的输出。为了改进融合过程，我们引入了一种新的速率-偏斜自适应融合（RSAF）技术，在参数合并过程中动态调整Top-K比率，以增强灵活性和稳定性。此外，我们为统一方法提出了基于不确定性的权重方法，这种方法能够动态平衡源模型的贡献，并优于其他概率分布集成方法。我们在GSM8K、MATH和HumanEval任务上分别取得了9.27%、8.80%和8.89%的准确率提升。
2501.02786	 | CCStereo: Audio-Visual Contextual and Contrastive Learning for Binaural Audio Generation	 | Yuanhong Chen,Kazuki Shimada,Christian Simon,Yukara Ikemiya,Takashi Shibuya,Yuki Mitsufuji	 | 本文提出了一种新的音频-视觉双耳生成模型，该模型结合了音频-视觉条件归一化层和一种新的对比学习方法，以提高空间细节的保留并减少过度拟合，该方法在FAIR-Play和MUSIC-Stereo基准测试中表现出最先进的生成准确性。	 | Binaural audio generation (BAG) aims to convert monaural audio to stereo audio using visual prompts, requiring a deep understanding of spatial and semantic information. However, current models risk overfitting to room environments and lose fine-grained spatial details. In this paper, we propose a new audio-visual binaural generation model incorporating an audio-visual conditional normalisation layer that dynamically aligns the mean and variance of the target difference audio features using visual context, along with a new contrastive learning method to enhance spatial sensitivity by mining negative samples from shuffled visual features. We also introduce a cost-efficient way to utilise test-time augmentation in video data to enhance performance. Our approach achieves state-of-the-art generation accuracy on the FAIR-Play and MUSIC-Stereo benchmarks.	 | 双耳音频生成（BAG）旨在使用视觉提示将单声道音频转换为立体声音频，这需要对空间和语义信息有深刻的理解。然而，当前的模型容易过度拟合到房间环境，并且会丢失细微的空间细节。在本文中，我们提出了一种新的音频-视觉双耳生成模型，该模型结合了音频-视觉条件归一化层，该层利用视觉上下文动态对齐目标差异音频特征的均值和方差，并提出了一种新的对比学习方法，通过从打乱的视觉特征中挖掘负样本来增强空间敏感性。我们还介绍了一种在视频数据中高效利用测试时数据增强的方法以提高性能。我们的方法在FAIR-Play和MUSIC-Stereo基准测试中实现了最先进的生成准确性。
2501.02778	 | ICFNet: Integrated Cross-modal Fusion Network for Survival Prediction	 | Binyu Zhang,Zhu Meng,Junhao Dong,Fei Su,Zhicheng Zhao	 | 本文提出了一种集成跨模态融合网络（ICFNet），该网络整合了组织病理学图像、基因表达谱、人口统计信息和治疗方案，以提高生存预测的准确性，并在多个公开数据集上优于现有方法，展示了其在临床决策支持和精准医疗中的潜力。	 | Survival prediction is a crucial task in the medical field and is essential for optimizing treatment options and resource allocation. However, current methods often rely on limited data modalities, resulting in suboptimal performance. In this paper, we propose an Integrated Cross-modal Fusion Network (ICFNet) that integrates histopathology whole slide images, genomic expression profiles, patient demographics, and treatment protocols. Specifically, three types of encoders, a residual orthogonal decomposition module and a unification fusion module are employed to merge multi-modal features to enhance prediction accuracy. Additionally, a balanced negative log-likelihood loss function is designed to ensure fair training across different patients. Extensive experiments demonstrate that our ICFNet outperforms state-of-the-art algorithms on five public TCGA datasets, including BLCA, BRCA, GBMLGG, LUAD, and UCEC, and shows its potential to support clinical decision-making and advance precision medicine. The codes are available at: https://github.com/binging512/ICFNet.	 | 生存预测是医疗领域的一项关键任务，对于优化治疗方案和资源配置至关重要。然而，当前的方法往往依赖于有限的数据模态，导致性能不佳。本文提出了一种集成跨模态融合网络（ICFNet），该网络整合了组织病理学全切片图像、基因表达谱、患者人口统计信息和治疗方案。具体来说，ICFNet 使用了三种类型的编码器、一个残差正交分解模块和一个统一融合模块，将多模态特征整合以提高预测准确性。此外，还设计了一种平衡的负对数似然损失函数，以确保不同患者的公平训练。广泛实验表明，我们的 ICFNet 在包括 BLCA、BRCA、GBMLGG、LUAD 和 UCEC 在内的五个公开的 TCGA 数据集上优于最先进的算法，并展示了其支持临床决策和推动精准医疗的潜力。相关代码可在以下链接获取：https://github.com/binging512/ICFNet。
2501.02751	 | Ultrasound-QBench: Can LLMs Aid in Quality Assessment of Ultrasound Imaging?	 | Hongyi Miao,Jun Jia,Yankun Cao,Yingjie Zhou,Yanwei Jiang,Zhi Liu,Guangtao Zhai	 | 为了应对超声图像质量下降带来的诊断准确性问题，研究引入了Ultrasound-QBench基准，该基准包含两个数据集，用于全面评估多模态大型语言模型在超声图像质量评估任务中的表现，并展示了这些模型在低级视觉任务中的初步能力。	 | With the dramatic upsurge in the volume of ultrasound examinations, low-quality ultrasound imaging has gradually increased due to variations in operator proficiency and imaging circumstances, imposing a severe burden on diagnosis accuracy and even entailing the risk of restarting the diagnosis in critical cases. To assist clinicians in selecting high-quality ultrasound images and ensuring accurate diagnoses, we introduce Ultrasound-QBench, a comprehensive benchmark that systematically evaluates multimodal large language models (MLLMs) on quality assessment tasks of ultrasound images. Ultrasound-QBench establishes two datasets collected from diverse sources: IVUSQA, consisting of 7,709 images, and CardiacUltraQA, containing 3,863 images. These images encompassing common ultrasound imaging artifacts are annotated by professional ultrasound experts and classified into three quality levels: high, medium, and low. To better evaluate MLLMs, we decompose the quality assessment task into three dimensionalities: qualitative classification, quantitative scoring, and comparative assessment. The evaluation of 7 open-source MLLMs as well as 1 proprietary MLLMs demonstrates that MLLMs possess preliminary capabilities for low-level visual tasks in ultrasound image quality classification. We hope this benchmark will inspire the research community to delve deeper into uncovering and enhancing the untapped potential of MLLMs for medical imaging tasks.	 | 随着超声检查的数量急剧增加，由于操作者技能和成像条件的不同，低质量的超声图像逐渐增多，这给诊断准确性带来了严重负担，甚至在严重情况下可能需要重新进行诊断。为了帮助临床医生选择高质量的超声图像并确保准确的诊断，我们引入了Ultrasound-QBench，这是一个全面的基准，系统地评估多模态大型语言模型（MLLMs）在超声图像质量评估任务上的表现。Ultrasound-QBench 建立了两个从不同来源收集的数据集：IVUSQA，包含7,709张图像，和CardiacUltraQA，包含3,863张图像。这些图像中包含常见的超声图像伪影，并由专业超声专家进行标注并分类为三个质量水平：高、中、低。为了更好地评估MLLMs，我们将质量评估任务分解为三个方面：定性分类、定量评分和比较评估。我们对7个开源的MLLMs和1个专有MLLMs进行了评估，结果显示MLLMs具备初步能力在超声图像质量分类的低级视觉任务中发挥作用。我们希望这个基准能激励研究社区深入探索和提升MLLMs在医学影像任务中的未开发潜力。
2501.02616	 | Multi-layer Radial Basis Function Networks for Out-of-distribution Detection	 | Amol Khanna,Chenyi Ling,Derek Everett,Edward Raff,Nathan Inkawhich	 | 本文提出了一种可轻松训练的多层径向基函数网络（MLRBFN），并通过开发新的压力机制确保其适用于离分布（OOD）检测，展示了该架构在OOD检测中的竞争力，为OOD检测方法提供了一个新的研究方向。	 | Existing methods for out-of-distribution (OOD) detection use various techniques to produce a score, separate from classification, that determines how ``OOD'' an input is. Our insight is that OOD detection can be simplified by using a neural network architecture which can effectively merge classification and OOD detection into a single step. Radial basis function networks (RBFNs) inherently link classification confidence and OOD detection; however, these networks have lost popularity due to the difficult of training them in a multi-layer fashion. In this work, we develop a multi-layer radial basis function network (MLRBFN) which can be easily trained. To ensure that these networks are also effective for OOD detection, we develop a novel depression mechanism. We apply MLRBFNs as standalone classifiers and as heads on top of pretrained feature extractors, and find that they are competitive with commonly used methods for OOD detection. Our MLRBFN architecture demonstrates a promising new direction for OOD detection methods.	 | 现有的用于离分布（OOD）检测的方法通过各种技术生成一个与分类分离的分数，以确定输入的“OOD”程度。我们的洞察是，可以通过使用能够有效合并分类和OOD检测的神经网络架构来简化OOD检测。径向基函数网络（RBFNs）内在地将分类置信度和OOD检测联系在一起；然而，由于它们难以以多层方式进行训练，这些网络已经失去了受欢迎程度。在本文中，我们开发了一种可以轻松训练的多层径向基函数网络（MLRBFN）。为了确保这些网络也适用于OOD检测，我们开发了一种新的减压机制。我们将MLRBFN作为独立分类器应用，并将其作为预训练特征提取器顶部的头部，发现它们在常用的OOD检测方法中具有竞争力。我们的MLRBFN架构展示了OOD检测方法的一个有前景的新方向。
2501.02598	 | GIT-CXR: End-to-End Transformer for Chest X-Ray Report Generation	 | Iustin Sîrbu,Iulia-Renata Sîrbu,Jasmina Bogojeska,Traian Rebedea	 | 该研究设计并评估了一种端到端的变压器方法，用于自动生成准确且完整的放射学报告，显著减轻了临床工作者的负担，并在多个评估指标上达到了最新的临床准确性。此外，研究还首次在医学影像中引入了课程学习方法，进一步提升了报告生成的性能。	 | Medical imaging is crucial for diagnosing, monitoring, and treating medical conditions. The medical reports of radiology images are the primary medium through which medical professionals attest their findings, but their writing is time consuming and requires specialized clinical expertise. The automated generation of radiography reports has thus the potential to improve and standardize patient care and significantly reduce clinicians workload. Through our work, we have designed and evaluated an end-to-end transformer-based method to generate accurate and factually complete radiology reports for X-ray images. Additionally, we are the first to introduce curriculum learning for end-to-end transformers in medical imaging and demonstrate its impact in obtaining improved performance. The experiments have been conducted using the MIMIC-CXR-JPG database, the largest available chest X-ray dataset. The results obtained are comparable with the current state-of-the-art on the natural language generation (NLG) metrics BLEU and ROUGE-L, while setting new state-of-the-art results on F1 examples-averaged, F1-macro and F1-micro metrics for clinical accuracy and on the METEOR metric widely used for NLG.	 | 医学影像对于诊断、监测和治疗医疗状况至关重要。放射影像报告是医疗专业人员证实其发现的主要媒介，但其撰写耗时且需要专门的临床专业知识。因此，自动化生成放射学报告有潜力改善和标准化患者的护理，并显著减轻临床工作者的工作负担。通过我们的工作，我们设计并评估了一种端到端变压器方法，以生成准确且事实完整的X射线影像放射学报告。此外，我们首次在医学影像中引入了课程学习方法，并展示了其在获得改进性能方面的影响力。实验是在MIMIC-CXR-JPG数据库上进行的，这是目前可用的最大规模的胸部X射线数据集。所获得的结果与当前自然语言生成（NLG）指标BLEU和ROUGE-L的最新水平相当，同时在临床准确性指标F1平均值、F1宏平均值和F1微平均值上取得了最新水平的结果，并且在广泛用于NLG的METEOR指标上也取得了新的最好结果。
2501.02583	 | Gaze Behavior During a Long-Term, In-Home, Social Robot Intervention for Children with ASD	 | Rebecca Ramnauth,Frederick Shic,Brian Scassellati	 | 一项为期一个月的家庭干预措施成功促进了自闭症谱系障碍（ASD）儿童与其社交机器人及照顾者之间的三方互动，增加了儿童的眼神接触和共同注意的次数与持续时间，并且ASD诊断指标能够预测照顾者和儿童的注视模式。	 | Atypical gaze behavior is a diagnostic hallmark of Autism Spectrum Disorder (ASD), playing a substantial role in the social and communicative challenges that individuals with ASD face. This study explores the impacts of a month-long, in-home intervention designed to promote triadic interactions between a social robot, a child with ASD, and their caregiver. Our results indicate that the intervention successfully promoted appropriate gaze behavior, encouraging children with ASD to follow the robot's gaze, resulting in more frequent and prolonged instances of spontaneous eye contact and joint attention with their caregivers. Additionally, we observed specific timelines for behavioral variability and novelty effects among users. Furthermore, diagnostic measures for ASD emerged as strong predictors of gaze patterns for both caregivers and children. These results deepen our understanding of ASD gaze patterns and highlight the potential for clinical relevance of robot-assisted interventions.	 | 异常注视行为是自闭症谱系障碍（ASD）的一个诊断特征，在ASD患者面临的社会和沟通挑战中发挥着重要作用。本研究探讨了一项为期一个月的家庭干预措施对促进社交机器人、ASD儿童及其照顾者之间的三方互动的影响。我们的结果表明，该干预措施成功地促进了适当的注视行为，鼓励ASD儿童跟随机器人的注视，从而增加了与其照顾者自发的眼神接触和共同注意的次数和持续时间。此外，我们还观察到用户在行为变异性与新颖性效应方面的特定时间线。更重要的是，ASD的诊断指标成为预测照顾者和儿童注视模式的强大指标。这些结果加深了我们对ASD注视模式的理解，并突显了机器人辅助干预的临床相关性。
2501.02558	 | Neural Error Covariance Estimation for Precise LiDAR Localization	 | Minoo Dolatabadi,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi	 | 该研究提出了一种基于神经网络的框架，用于预测LiDAR地图匹配中的定位误差协方差，从而提高自主驾驶车辆的定位精度，实验结果显示定位精度提升了2厘米。	 | Autonomous vehicles have gained significant attention due to technological advancements and their potential to transform transportation. A critical challenge in this domain is precise localization, particularly in LiDAR-based map matching, which is prone to errors due to degeneracy in the data. Most sensor fusion techniques, such as the Kalman filter, rely on accurate error covariance estimates for each sensor to improve localization accuracy. However, obtaining reliable covariance values for map matching remains a complex task. To address this challenge, we propose a neural network-based framework for predicting localization error covariance in LiDAR map matching. To achieve this, we introduce a novel dataset generation method specifically designed for error covariance estimation. In our evaluation using a Kalman filter, we achieved a 2 cm improvement in localization accuracy, a significant enhancement in this domain.	 | 自主驾驶车辆由于技术进步和其在交通领域潜在的变革作用，受到了广泛关注。在这个领域中，精准定位是一个关键挑战，特别是在基于LiDAR的地图匹配中，由于数据的退化性极易出错。大多数传感器融合技术，如卡尔曼滤波器，依赖于每个传感器准确的误差协方差估计来提高定位精度。然而，获得可靠的协方差值用于地图匹配仍然是一个复杂的任务。为了解决这一挑战，我们提出了一种基于神经网络的框架，用于预测LiDAR地图匹配中的定位误差协方差。为此，我们引入了一种新颖的数据集生成方法，专门用于误差协方差估计。在使用卡尔曼滤波器的评估中，我们实现了2厘米的定位精度提升，这是该领域的一个明显改进。
2501.02552	 | Multi-LLM Collaborative Caption Generation in Scientific Documents	 | Jaeyoung Kim,Jongho Lee,Hong-Jun Choi,Ting-Yao Hsu,Chieh-Yang Huang,Sungchul Kim,Ryan Rossi,Tong Yu,Clyde Lee Giles,Ting-Hao 'Kenneth' Huang,Sungchul Choi	 | 本文提出了一种名为MLBCAP的方法，通过利用多个专门针对不同子任务的语言模型来生成高质量的科研图表描述，解决了现有方法因信息不完整而受限的问题；该方法包括质量评估、多样化的描述生成和判断三个关键模块，并通过实验表明其生成的描述比人工撰写的更高质量。	 | Scientific figure captioning is a complex task that requires generating contextually appropriate descriptions of visual content. However, existing methods often fall short by utilizing incomplete information, treating the task solely as either an image-to-text or text summarization problem. This limitation hinders the generation of high-quality captions that fully capture the necessary details. Moreover, existing data sourced from arXiv papers contain low-quality captions, posing significant challenges for training large language models (LLMs). In this paper, we introduce a framework called Multi-LLM Collaborative Figure Caption Generation (MLBCAP) to address these challenges by leveraging specialized LLMs for distinct sub-tasks. Our approach unfolds in three key modules: (Quality Assessment) We utilize multimodal LLMs to assess the quality of training data, enabling the filtration of low-quality captions. (Diverse Caption Generation) We then employ a strategy of fine-tuning/prompting multiple LLMs on the captioning task to generate candidate captions. (Judgment) Lastly, we prompt a prominent LLM to select the highest quality caption from the candidates, followed by refining any remaining inaccuracies. Human evaluations demonstrate that informative captions produced by our approach rank better than human-written captions, highlighting its effectiveness. Our code is available at https://github.com/teamreboott/MLBCAP	 | 科学研究图表描述是一项复杂的工作，需要生成与上下文相适应的视觉内容描述。然而，现有的方法往往因使用不完整的信息而受限，将任务简单地视为图像到文本或文本总结问题。这种局限性阻碍了高质量描述的生成，这些描述能够充分捕捉必要的细节。此外，来自arXiv论文的数据包含质量较低的描述，这给训练大型语言模型（LLMs）带来了巨大的挑战。在本文中，我们提出了一种名为Multi-LLM Collaborative Figure Caption Generation（MLBCAP）的方法，通过利用专门针对不同子任务的LLM来解决这些挑战。我们的方法包含三个关键模块：（质量评估）我们使用多模态LLM来评估训练数据的质量，从而筛选出低质量的描述。 （多样化的描述生成）然后，我们采用对多个LLM进行微调/提示的方法，针对描述任务生成候选描述。 （判断）最后，我们提示一个突出的LLM从候选描述中选择最高质量的描述，然后再进一步修正任何剩余的不准确性。人类评估表明，我们方法生成的信息性描述优于人工撰写的描述，突显了其有效性。我们的代码可在https://github.com/teamreboott/MLBCAP找到。
2501.02511	 | Can Impressions of Music be Extracted from Thumbnail Images?	 | Takashi Harada,Takehiro Motomitsu,Katsuhiko Hayashi,Yusuke Sakai,Hidetaka Kamigaito	 | 该研究针对音乐检索和生成系统中缺乏包含非音乐信息（如播放场景和引发的情感）的大规模数据集的问题，提出了一种方法结合音乐缩略图图像中的非音乐特征，并通过人类评估验证其有效性，最终构建了一个包含约36万条非音乐描述的数据库，用于训练和评估音乐检索模型。	 | In recent years, there has been a notable increase in research on machine learning models for music retrieval and generation systems that are capable of taking natural language sentences as inputs. However, there is a scarcity of large-scale publicly available datasets, consisting of music data and their corresponding natural language descriptions known as music captions. In particular, non-musical information such as suitable situations for listening to a track and the emotions elicited upon listening is crucial for describing music. This type of information is underrepresented in existing music caption datasets due to the challenges associated with extracting it directly from music data. To address this issue, we propose a method for generating music caption data that incorporates non-musical aspects inferred from music thumbnail images, and validated the effectiveness of our approach through human evaluations. Additionally, we created a dataset with approximately 360,000 captions containing non-musical aspects. Leveraging this dataset, we trained a music retrieval model and demonstrated its effectiveness in music retrieval tasks through evaluation.	 | 近年来，关于能够接受自然语言句子作为输入的音乐检索和生成系统的机器学习模型的研究有了显著增加。然而，公开可用的大规模数据集仍然稀缺，这些数据集包括音乐数据及其相应的自然语言描述，即音乐描述。特别是，适合播放的场景以及听音乐时引发的情感等非音乐信息对于描述音乐至关重要。这类信息由于从音乐数据中直接提取的挑战，在现有音乐描述数据集中被严重低估。为了应对这一问题，我们提出了一种方法，该方法结合了从音乐缩略图图像中推断出的非音乐方面，并通过人类评估验证了该方法的有效性。此外，我们创建了一个包含约36万条非音乐方面描述的数据库。利用该数据库，我们训练了一个音乐检索模型，并通过评估证明了其在音乐检索任务中的有效性。
2501.02508	 | PTEENet: Post-Trained Early-Exit Neural Networks Augmentation for Inference Cost Optimization	 | Assaf Lahiany,Yehudit Aperstein	 | 本文提出了一种通过在深度神经网络推理中引入“捷径”来降低计算成本的方法，该方法基于BranchyNet和EEnet架构，并结合了预训练模型和新的分支构建块，同时使用置信度头部来调整每个分支的数据流出量，从而控制模型的速度和准确性之间的权衡。	 | For many practical applications, a high computational cost of inference over deep network architectures might be unacceptable. A small degradation in the overall inference accuracy might be a reasonable price to pay for a significant reduction in the required computational resources. In this work, we describe a method for introducing "shortcuts" into the DNN feedforward inference process by skipping costly feedforward computations whenever possible. The proposed method is based on the previously described BranchyNet (Teerapittayanon et al., 2016) and the EEnet (Demir, 2019) architectures that jointly train the main network and early exit branches. We extend those methods by attaching branches to pre-trained models and, thus, eliminating the need to alter the original weights of the network. We also suggest a new branch architecture based on convolutional building blocks to allow enough training capacity when applied on large DNNs. The proposed architecture includes confidence heads that are used for predicting the confidence level in the corresponding early exits. By defining adjusted thresholds on these confidence extensions, we can control in real-time the amount of data exiting from each branch and the overall tradeoff between speed and accuracy of our model. In our experiments, we evaluate our method using image datasets (SVHN and CIFAR10) and several DNN architectures (ResNet, DenseNet, VGG) with varied depth. Our results demonstrate that the proposed method enables us to reduce the average inference computational cost and further controlling the tradeoff between the model accuracy and the computation cost.	 | 对于许多实际应用来说，深度网络架构上进行推理的高计算成本可能是不可接受的。整体推理准确度略有下降可能是一个合理的代价，以换取显著降低所需的计算资源。在本工作中，我们描述了一种通过在可能的情况下跳过昂贵的前向传播计算引入“捷径”的方法，以简化DNN的前向推理过程。该方法基于先前描述的BranchyNet（Teerapittayanon等人，2016）和EEnet（Demir, 2019）架构，联合训练主网络和早期退出分支。我们在此基础上扩展了这些方法，将分支连接到预训练模型上，从而无需改变网络的原始权重。我们还提出了一种基于卷积构建块的新分支架构，以便在应用于大型DNN时能够提供足够的训练能力。所提出的架构包含置信度头部，用于预测相应早期退出的置信水平。通过在这些置信度扩展上定义调整后的阈值，我们可以在实时控制每个分支的数据流出量以及模型速度和准确性的总体权衡。在我们的实验中，我们使用图像数据集（SVHN和CIFAR10）和几种具有不同深度的DNN架构（ResNet、DenseNet、VGG）来评估该方法。实验结果表明，所提出的方法使我们能够降低平均推理计算成本，并进一步控制模型准确性和计算成本之间的权衡。
2501.02477	 | A Deep Positive-Negative Prototype Approach to Integrated Prototypical Discriminative Learning	 | Ramin Zarei-Sabzevar,Ahad Harati	 | 本文提出了一种深度正负原型（DPNP）模型，结合了原型学习和判别方法的优势，通过在潜在空间中形成深度正原型并引入负原型的斥力作用，提高了类内紧凑性和类间间隔。实验结果表明，DPNP 在多种数据集上表现优于最先进的模型，且使用较小的网络就能实现较高的分类准确性。	 | This paper proposes a novel Deep Positive-Negative Prototype (DPNP) model that combines prototype-based learning (PbL) with discriminative methods to improve class compactness and separability in deep neural networks. While PbL traditionally emphasizes interpretability by classifying samples based on their similarity to representative prototypes, it struggles with creating optimal decision boundaries in complex scenarios. Conversely, discriminative methods effectively separate classes but often lack intuitive interpretability. Toward exploiting advantages of these two approaches, the suggested DPNP model bridges between them by unifying class prototypes with weight vectors, thereby establishing a structured latent space that enables accurate classification using interpretable prototypes alongside a properly learned feature representation. Based on this central idea of unified prototype-weight representation, Deep Positive Prototype (DPP) is formed in the latent space as a representative for each class using off-the-shelf deep networks as feature extractors. Then, rival neighboring class DPPs are treated as implicit negative prototypes with repulsive force in DPNP, which push away DPPs from each other. This helps to enhance inter-class separation without the need for any extra parameters. Hence, through a novel loss function that integrates cross-entropy, prototype alignment, and separation terms, DPNP achieves well-organized feature space geometry, maximizing intra-class compactness and inter-class margins. We show that DPNP can organize prototypes in nearly regular positions within feature space, such that it is possible to achieve competitive classification accuracy even in much lower-dimensional feature spaces. Experimental results on several datasets demonstrate that DPNP outperforms state-of-the-art models, while using smaller networks.	 | 本文提出了一种新颖的深度正负原型（DPNP）模型，该模型结合了原型学习（PbL）和判别方法，以提高深度神经网络中的类聚类性和可分性。传统上，PbL 强调可解释性，通过将样本分类为代表性原型的相似性来进行分类，但在复杂场景下难以创建最优决策边界。相反，判别方法能够有效地区分类别，但往往缺乏直观的解释性。为了利用这两种方法的优势，提出的 DPNP 模型通过将类原型与权重向量统一起来，建立了一个结构化的潜在空间，从而能够在使用可解释原型的同时，通过适当的特征表示进行准确分类。基于这一核心思想，即统一原型-权重表示，使用现成的深度网络作为特征提取器，在潜在空间中形成了每个类别的深度正原型（DPP）。然后，在 DPNP 中，竞争相邻类别的 DPP 被视为具有斥力作用的隐性负原型，迫使 DPP 之间相互远离。这有助于增强类间的分离性，而无需额外的参数。因此，通过一个新颖的损失函数，该函数结合了交叉熵、原型对齐和分离项，DPNP 实现了有序的特征空间几何结构，最大化了类内紧凑性和类间间隔。我们展示了，DPNP 可以在特征空间内将原型几乎布置在规则位置，使得在较低维度的特征空间中也能够实现具有竞争力的分类准确性。多种数据集上的实验结果表明，与最先进的模型相比，使用较小的网络，DPNP 表现更优。
2501.02428	 | Framework for lung CT image segmentation based on UNet++	 | Hao Ziang,Jingsi Zhang,Lixian Li	 | 本文提出了一种全流程网络，结合了UNet++模型和数据增强、优化的神经网络以及参数微调，以解决医学图像分割中的过拟合和小数据集问题，实现了98.03%的最高准确率，并且过拟合风险较低。这是首个专门针对肺切片CT图像分割的全流程网络。	 | Recently, the state-of-art models for medical image segmentation is U-Net and their variants. These networks, though succeeding in deriving notable results, ignore the practical problem hanging over the medical segmentation field: overfitting and small dataset. The over-complicated deep neural networks unnecessarily extract meaningless information, and a majority of them are not suitable for lung slice CT image segmentation task. To overcome the two limitations, we proposed a new whole-process network merging advanced UNet++ model. The network comprises three main modules: data augmentation, optimized neural network, parameter fine-tuning. By incorporating diverse methods, the training results demonstrate a significant advantage over similar works, achieving leading accuracy of 98.03% with the lowest overfitting. potential. Our network is remarkable as one of the first to target on lung slice CT images.	 | 最近，医学图像分割领域的最新模型是U-Net及其变体。虽然这些网络在提取显著结果方面取得了成功，但它们忽略了医学分割领域悬而未决的实践问题：过拟合和小数据集。过于复杂的深度神经网络无必要地提取无意义的信息，其中大多数不适合用于肺切片CT图像分割任务。为克服这两个限制，我们提出了一种全新的全流程网络，结合了先进的UNet++模型。该网络包括三个主要模块：数据增强、优化的神经网络、参数微调。通过结合多种方法，训练结果在与其他类似工作相比中显示出显著优势，实现了98.03%的最高准确率，且过拟合潜力最低。我们的网络令人瞩目，因为它是在众多研究中首次针对肺切片CT图像的网络之一。
2501.02364	 | Understanding How Nonlinear Layers Create Linearly Separable Features for Low-Dimensional Data	 | Alec S. Xu,Can Yaras,Peng Wang,Qing Qu	 | 本文通过研究浅层非线性网络的线性分离能力，证明了单个非线性层可以将低维子空间的并集转换为线性可分的集合，并且当网络宽度按数据固有维度的多项式阶数扩展时，这种转换以高概率发生，从而建立了经验观察与理论理解之间的桥梁。	 | Deep neural networks have attained remarkable success across diverse classification tasks. Recent empirical studies have shown that deep networks learn features that are linearly separable across classes. However, these findings often lack rigorous justifications, even under relatively simple settings. In this work, we address this gap by examining the linear separation capabilities of shallow nonlinear networks. Specifically, inspired by the low intrinsic dimensionality of image data, we model inputs as a union of low-dimensional subspaces (UoS) and demonstrate that a single nonlinear layer can transform such data into linearly separable sets. Theoretically, we show that this transformation occurs with high probability when using random weights and quadratic activations. Notably, we prove this can be achieved when the network width scales polynomially with the intrinsic dimension of the data rather than the ambient dimension. Experimental results corroborate these theoretical findings and demonstrate that similar linear separation properties hold in practical scenarios beyond our analytical scope. This work bridges the gap between empirical observations and theoretical understanding of the separation capacity of nonlinear networks, offering deeper insights into model interpretability and generalization.	 | 深度神经网络在各种分类任务中取得了显著的成功。最近的实证研究表明，深度网络能够学习在类间线性可分的特征。然而，这些发现通常缺乏严格的理论依据，即使在相对简单的设置中也是如此。在本文中，我们通过研究浅层非线性网络的线性分离能力来填补这一空白。特别地，受图像数据固有低维性的启发，我们将输入表示为低维子空间的并集（UoS），并证明单个非线性层可以将此类数据转换为线性可分的集合。理论上，我们证明当使用随机权重和二次激活时，这种转换以高概率发生。值得注意的是，我们证明了当网络宽度按数据固有维度的多项式阶数而非环境维度的阶数扩展时，可以实现这一目标。实验结果验证了这些理论发现，并展示了在超越我们分析范围的实际场景中，类似的线性分离性质同样成立。本研究建立了经验观察与非线性网络分离能力理论理解之间的桥梁，为模型可解释性和泛化提供了更深入的洞见。
2501.02352	 | GNSS/GPS Spoofing and Jamming Identification Using Machine Learning and Deep Learning	 | Ali Ghanbarzade,Hossein Soleimani	 | 本文旨在利用机器学习和深度学习技术检测和缓解全球导航卫星系统（GNSS）中的欺骗和干扰攻击，通过实验表明，所提出的方法在欺骗和干扰检测方面达到了99%的高准确率，显著提高了系统的安全性。	 | The increasing reliance on Global Navigation Satellite Systems (GNSS), particularly the Global Positioning System (GPS), underscores the urgent need to safeguard these technologies against malicious threats such as spoofing and jamming. As the backbone for positioning, navigation, and timing (PNT) across various applications including transportation, telecommunications, and emergency services GNSS is vulnerable to deliberate interference that poses significant risks. Spoofing attacks, which involve transmitting counterfeit GNSS signals to mislead receivers into calculating incorrect positions, can result in serious consequences, from navigational errors in civilian aviation to security breaches in military operations. Furthermore, the lack of inherent security measures within GNSS systems makes them attractive targets for adversaries. While GNSS/GPS jamming and spoofing systems consist of numerous components, the ability to distinguish authentic signals from malicious ones is essential for maintaining system integrity. Recent advancements in machine learning and deep learning provide promising avenues for enhancing detection and mitigation strategies against these threats. This paper addresses both spoofing and jamming by tackling real-world challenges through machine learning, deep learning, and computer vision techniques. Through extensive experiments on two real-world datasets related to spoofing and jamming detection using advanced algorithms, we achieved state of the art results. In the GNSS/GPS jamming detection task, we attained approximately 99% accuracy, improving performance by around 5% compared to previous studies. Additionally, we addressed a challenging tasks related to spoofing detection, yielding results that underscore the potential of machine learning and deep learning in this domain.	 | 随着全球导航卫星系统（GNSS），特别是全球定位系统（GPS）的依赖性不断增加，亟需保护这些技术免受诸如欺骗和干扰等恶意威胁。作为定位、导航和授时（PNT）在各种应用中的基础，包括交通、电信和应急服务等，GNSS容易受到故意干扰，这带来了重大风险。欺骗攻击通过传输伪造的GNSS信号以误导接收器计算出错误的位置，可能导致严重的后果，从民用航空中的导航错误到军事操作中的安全漏洞。此外，GNSS系统缺乏内置的安全措施，使其成为对手的诱人目标。虽然GNSS/GPS欺骗和干扰系统包含多个组件，但区分真实信号与恶意信号的能力对于保持系统的完整性至关重要。最近机器学习和深度学习的进展为增强对抗这些威胁的检测和缓解策略提供了前景。本文通过利用机器学习、深度学习和计算机视觉技术应对实际挑战，旨在解决欺骗和干扰问题。通过在欺骗和干扰检测任务中使用先进算法进行广泛的实验证明，我们在欺骗和干扰检测方面取得了最先进的成果。在GNSS/GPS干扰检测任务中，我们达到了约99%的准确率，相比之前的研究所提高了约5%的性能。此外，我们还解决了欺骗检测中的一个具有挑战性的任务，结果表明了机器学习和深度学习在这一领域的潜力。
2501.02349	 | Revelio: A Real-World Screen-Camera Communication System with Visually Imperceptible Data Embedding	 | Abbaas Alif Mohamed Nishar,Shrinivas Kudekar,Bernard Kintzing,Ashwin Ashok	 | Revelio是一种利用时间闪烁融合原理在现实世界屏幕-相机通信系统中嵌入不可见数据的方法，通过空间自适应闪烁和像素区域形状编码信息，并使用两阶段神经网络确保解码的准确性和鲁棒性。	 | We present `Revelio', a real-world screen-camera communication system leveraging temporal flicker fusion in the OKLAB color space. Using spatially-adaptive flickering and encoding information in pixel region shapes, Revelio achieves visually imperceptible data embedding while remaining robust against noise, asynchronicity, and distortions in screen-camera channels, ensuring reliable decoding by standard smartphone cameras. The decoder, driven by a two-stage neural network, uses a weighted differential accumulator for precise frame detection and symbol recognition. Initial experiments demonstrate Revelio's effectiveness in interactive television, offering an unobtrusive method for meta-information transmission.	 | 我们提出了`Revelio'，这是一种利用OKLAB色彩空间中的时间闪烁融合原理的现实世界屏幕-相机通信系统。通过空间自适应的闪烁和在像素区域形状中编码信息，Revelio实现了视觉上不可感知的数据嵌入，同时保持对屏幕-相机通道中的噪声、异步性和失真具有鲁棒性，确保标准智能手机相机可靠解码。解码器由两阶段神经网络驱动，使用加权差分累加器进行精确的帧检测和符号识别。初步实验表明，Revelio在交互式电视中有效，提供了一种不显眼的方法来传输元信息。
2501.02342	 | Optimizing Small Language Models for In-Vehicle Function-Calling	 | Yahya Sowti Khiabani,Farris Atif,Chieh Hsu,Sven Stahlmann,Tobias Michels,Sebastian Kramer,Benedikt Heidrich,M. Saquib Sarfraz,Julian Merten,Faezeh Tafazzoli	 | 本文提出了一种将小型语言模型（SLMs）部署在车辆中作为边缘设备功能调用代理的方法，以提高车辆控制机制的灵活性和性能，并通过模型压缩技术确保其在资源限制下的高效运行。经过压缩和微调的Phi-3 mini模型在嵌入式环境中实现了每秒11个令牌的生成速度，保持了对复杂车内任务的准确处理能力，显著提升了驾驶体验。	 | We propose a holistic approach for deploying Small Language Models (SLMs) as function-calling agents within vehicles as edge devices, offering a more flexible and robust alternative to traditional rule-based systems. By leveraging SLMs, we simplify vehicle control mechanisms and enhance the user experience. Given the in-vehicle hardware constraints, we apply state-of-the-art model compression techniques, including structured pruning, healing, and quantization, ensuring that the model fits within the resource limitations while maintaining acceptable performance. Our work focuses on optimizing a representative SLM, Microsoft's Phi-3 mini, and outlines best practices for enabling embedded models, including compression, task-specific fine-tuning, and vehicle integration. We demonstrate that, despite significant reduction in model size which removes up to 2 billion parameters from the original model, our approach preserves the model's ability to handle complex in-vehicle tasks accurately and efficiently. Furthermore, by executing the model in a lightweight runtime environment, we achieve a generation speed of 11 tokens per second, making real-time, on-device inference feasible without hardware acceleration. Our results demonstrate the potential of SLMs to transform vehicle control systems, enabling more intuitive interactions between users and their vehicles for an enhanced driving experience.	 | 我们提出了一种整体方法，将小型语言模型（SLMs）部署为车辆中的边缘设备功能调用代理，作为传统基于规则系统的更灵活和稳健的替代方案。通过利用SLMs，我们简化了车辆控制机制并提升了用户体验。鉴于车载硬件的限制，我们应用了最先进的模型压缩技术，包括结构化剪枝、修复和量化，确保模型在资源限制内仍能保持可接受的性能。我们的工作集中在优化一个代表性的SLM——Microsoft的Phi-3 mini，并概述了使嵌入式模型可用的最佳实践，包括压缩、任务特定微调以及车辆集成。我们证明，尽管在模型大小上做了显著减少，从原始模型中移除了多达2亿个参数，但我们的方法仍能保持模型对复杂车内任务准确且高效的处理能力。此外，通过在轻量级运行时环境中执行模型，我们实现了每秒11个令牌的生成速度，使在设备上的实时推理成为可能，无需硬件加速。我们的结果展示了SLMs在转变车辆控制系统方面的潜力，使用户与车辆之间的互动更加直观，从而提升驾驶体验。
2501.02325	 | Revisiting Compactness for District Plans	 | Kristopher Tapp	 | 本文引入了带人口权重的基于形状的评分，并展示了其如何在基于形状和离散评分之间进行插值；同时，还提出了改进的ReCom抽样方法，以生成具有更好形状紧凑性的地图集合。	 | Modern sampling methods create ensembles of district maps that score well on discrete compactness scores, whereas the Polsby-Popper and other shape-based scores remain highly relevant for building fair maps and litigating unfair ones. The aim of this paper is twofold. First, we introduce population-weighted versions of shape-based scores and show a precise sense in which this interpolates between shape-based and discrete scores. Second, we introduce a modification of the ReCom sampling method that produces ensembles of maps with improved shape-based compactness scores.	 | 现代抽样方法创建了在离散紧凑性评分中表现良好的选区地图集合，而 Polsby-Popper 及其他基于形状的评分在构建公平地图和诉讼不公平地图方面依然非常重要。本文的目标有两个方面。首先，我们引入了基于形状的评分的带人口权重版本，并展示了这种评分如何在基于形状的评分和离散评分之间进行精确的插值。其次，我们引入了对 ReCom 抽样方法的一种修改，以生成具有改进的基于形状的紧凑性评分的地图集合。
2501.02300	 | Diabetic Retinopathy Detection Using CNN with Residual Block with DCGAN	 | Debjany Ghosh Aronno,Sumaiya Saeha	 | 该研究提出了一种利用具有残差块结构的卷积神经网络和深度卷积生成对抗网络生成多样化视网膜图像的自动化系统，以提高糖尿病视网膜病变的早期检测和分类性能，支持资源受限环境下的大规模筛查。	 | Diabetic Retinopathy (DR) is a major cause of blindness worldwide, caused by damage to the blood vessels in the retina due to diabetes. Early detection and classification of DR are crucial for timely intervention and preventing vision loss. This work proposes an automated system for DR detection using Convolutional Neural Networks (CNNs) with a residual block architecture, which enhances feature extraction and model performance. To further improve the model's robustness, we incorporate advanced data augmentation techniques, specifically leveraging a Deep Convolutional Generative Adversarial Network (DCGAN) for generating diverse retinal images. This approach increases the variability of training data, making the model more generalizable and capable of handling real-world variations in retinal images. The system is designed to classify retinal images into five distinct categories, from No DR to Proliferative DR, providing an efficient and scalable solution for early diagnosis and monitoring of DR progression. The proposed model aims to support healthcare professionals in large-scale DR screening, especially in resource-constrained settings.	 | 糖尿病视网膜病变（DR）是全球导致失明的主要原因之一，是由糖尿病引起的视网膜血管损伤所致。早期检测和分类DR对于及时干预和防止视力丧失至关重要。本研究提出了一种利用具有残差块结构的卷积神经网络（CNNs）的自动化系统，以增强特征提取和模型性能。为了进一步提高模型的鲁棒性，我们引入了高级数据增强技术，特别是利用深度卷积生成对抗网络（DCGAN）生成多样化的视网膜图像。这种方法增加了训练数据的变异性，使模型更具泛化能力，并能处理实际视网膜图像中的各种变化。该系统旨在将视网膜图像分类为五个不同的类别，从无DR到增殖性DR，提供了一种高效且可扩展的解决方案，用于早期诊断和监控DR的进展。所提出的模型旨在支持在资源受限环境中进行大规模DR筛查的医护人员。
2501.02287	 | Deep Learning-Driven Segmentation of Ischemic Stroke Lesions Using Multi-Channel MRI	 | Ashiqur Rahman,Muhammad E. H. Chowdhury,Md Sharjis Ibne Wadud,Rusab Sarmun,Adam Mushtak,Sohaib Bassam Zoghoul,Israa Al-Hashimi	 | 本研究提出了一种基于深度学习的新方法，利用多模态MRI数据（包括DWI、ADC和eDWI）分割缺血性中风病灶，通过改进的DenseNet121和自组织操作神经网络等技术，显著提高了病灶的分割精度，最高达到87.49%的Dice相似系数，优于现有方法。	 | Ischemic stroke, caused by cerebral vessel occlusion, presents substantial challenges in medical imaging due to the variability and subtlety of stroke lesions. Magnetic Resonance Imaging (MRI) plays a crucial role in diagnosing and managing ischemic stroke, yet existing segmentation techniques often fail to accurately delineate lesions. This study introduces a novel deep learning-based method for segmenting ischemic stroke lesions using multi-channel MRI modalities, including Diffusion Weighted Imaging (DWI), Apparent Diffusion Coefficient (ADC), and enhanced Diffusion Weighted Imaging (eDWI). The proposed architecture integrates DenseNet121 as the encoder with Self-Organized Operational Neural Networks (SelfONN) in the decoder, enhanced by Channel and Space Compound Attention (CSCA) and Double Squeeze-and-Excitation (DSE) blocks. Additionally, a custom loss function combining Dice Loss and Jaccard Loss with weighted averages is introduced to improve model performance. Trained and evaluated on the ISLES 2022 dataset, the model achieved Dice Similarity Coefficients (DSC) of 83.88% using DWI alone, 85.86% with DWI and ADC, and 87.49% with the integration of DWI, ADC, and eDWI. This approach not only outperforms existing methods but also addresses key limitations in current segmentation practices. These advancements significantly enhance diagnostic precision and treatment planning for ischemic stroke, providing valuable support for clinical decision-making.	 | 缺血性中风是由脑血管阻塞引起的，因其病灶的多样性和细微性而在医学成像中带来了显著挑战。磁共振成像（MRI）在诊断和管理缺血性中风中发挥着关键作用，但现有的分割技术往往无法准确勾勒出病灶。本研究提出了一种基于深度学习的新方法，用于利用多通道MRI模态（包括扩散加权成像(DWI)、表观扩散系数(ADC)和增强扩散加权成像(eDWI)）分割缺血性中风病灶。所提出的架构结合了DenseNet121作为编码器，并在解码器中使用自组织操作神经网络（SelfONN），并通过通道和空间复合注意（CSCA）以及双挤压和激励（DSE）块进行了增强。此外，还引入了一种结合Dice损失和交集并集比损失（Jaccard Loss）并进行加权平均的自定义损失函数，以提高模型性能。该模型在ISLES 2022数据集上进行训练和评估，使用单独的DWI时获得了83.88%的Dice相似系数（DSC），使用DWI和ADC时获得了85.86%的DSC，而在整合了DWI、ADC和eDWI后获得了87.49%的DSC。这种方法不仅超过了现有的方法，还解决了当前分割实践中的关键局限性。这些进步显著提升了缺血性中风的诊断精度和治疗计划，为临床决策提供了宝贵的支持。
2501.02200	 | Learning Evolution via Optimization Knowledge Adaptation	 | Chao Wang,Licheng Jiao,Jiaxuan Zhao,Lingling Li,Fang Liu,Shuyuan Yang	 | 本文提出了一种优化知识适应性进化模型（OKAEM），通过利用积累的知识动态调整进化算子的参数，从而增强进化算法的优化能力。实验结果表明，OKAEM在利用先验知识和自我调整方面表现出色，并且随着知识的增长优化能力不断提高。	 | Evolutionary algorithms (EAs) maintain populations through evolutionary operators to discover diverse solutions for complex tasks while gathering valuable knowledge, such as historical population data and fitness evaluations. However, traditional EAs face challenges in dynamically adapting to expanding knowledge bases, hindering the efficient exploitation of accumulated information and limiting adaptability to new situations. To address these issues, we introduce an Optimization Knowledge Adaptation Evolutionary Model (OKAEM), which features dynamic parameter adjustment using accumulated knowledge to enhance its optimization capabilities. OKAEM employs attention mechanisms to model the interactions among individuals, fitness landscapes, and genetic components separately, thereby parameterizing the evolutionary operators of selection, crossover, and mutation. These powerful learnable operators enable OKAEM to benefit from pre-learned extensive prior knowledge and self-tune with real-time evolutionary insights. Experimental results demonstrate that OKAEM: 1) exploits prior knowledge for significant performance gains across various knowledge transfer settings; 2) achieves competitive performance through self-tuning alone, even without prior knowledge; 3) outperforms state-of-the-art black-box baselines in a vision-language model tuning case; 4) can improve its optimization capabilities with growing knowledge; 5) is capable of emulating principles of natural selection and genetic recombination.	 | 进化算法（EAs）通过进化算子维护种群，以发现复杂任务的多样化解决方案，同时收集有价值的知识，例如历史种群数据和适应度评估。然而，传统的进化算法在动态适应不断扩大的知识库时面临挑战，这妨碍了对累积信息的有效利用，并限制了在新情况下的适应性。为了解决这些问题，我们提出了一种优化知识适应性进化模型（OKAEM），该模型通过利用积累的知识动态调整参数来增强其优化能力。OKAEM 使用注意力机制分别建模个体之间的交互、适应度景观和遗传组件，从而参数化选择、交叉和变异等进化算子。这些强大的可学习算子使OKAEM 能够受益于预先学习的大量先验知识，并根据实时的进化洞察进行自我调整。实验结果表明：1) OKAEM 在各种知识转移设置中利用先验知识显著提升了性能；2) 即使没有先验知识，仅通过自我调整即可实现竞争力的性能；3) 在视觉-语言模型调优案例中优于最先进的黑盒基线；4) 随着知识的增长，优化能力可以不断提升；5) 能够模仿自然选择和遗传重组的基本原理。
2501.02198	 | Fresh-CL: Feature Realignment through Experts on Hypersphere in Continual Learning	 | Zhongyi Zhou,Yaxin Peng,Pin Yi,Minjie Zhu,Chaomin Shen	 | Fresh-CL是一种持续学习方法，通过利用固定且简单的超球体等角紧框架分类器来提高特征分离能力，同时提出了一种动态扩展方法以适应新任务，从而增强特征表示。实验结果显示，Fresh-CL在多个数据集上显著提高了准确率，特别是在细粒度数据集上。	 | Continual Learning enables models to learn and adapt to new tasks while retaining prior knowledge.Introducing new tasks, however, can naturally lead to feature entanglement across tasks, limiting the model's capability to distinguish between new domain data.In this work, we propose a method called Feature Realignment through Experts on hyperSpHere in Continual Learning (Fresh-CL). By leveraging predefined and fixed simplex equiangular tight frame (ETF) classifiers on a hypersphere, our model improves feature separation both intra and inter tasks.However, the projection to a simplex ETF shifts with new tasks, disrupting structured feature representation of previous tasks and degrading performance. Therefore, we propose a dynamic extension of ETF through mixture of experts, enabling adaptive projections onto diverse subspaces to enhance feature representation.Experiments on 11 datasets demonstrate a 2\% improvement in accuracy compared to the strongest baseline, particularly in fine-grained datasets, confirming the efficacy of combining ETF and MoE to improve feature distinction in continual learning scenarios.	 | 持续学习使模型能够在学习新任务的同时保留先前的知识。然而，引入新任务时，自然会导致任务之间特征纠缠，限制了模型区分新领域数据的能力。在这项工作中，我们提出了一种名为基于超球体的专家特征重新对齐持续学习（Fresh-CL）的方法。通过利用预定义且固定的简单超球体等角紧框架（ETF）分类器，我们的模型在任务内外均提高了特征分离能力。然而，向简单ETF投影会随着新任务的引入而变化，破坏了之前任务的结构化特征表示并降低了性能。因此，我们提出了通过混合专家的ETF动态扩展方法，使投影能够适应不同的子空间，从而增强特征表示。在11个数据集上的实验表明，与最强基线相比，Fresh-CL在准确率上提高了2%，特别是在细粒度数据集上，这证实了结合ETF和MoE以提高持续学习场景中特征区分效果的有效性。
2501.02166	 | ROLO-SLAM: Rotation-Optimized LiDAR-Only SLAM in Uneven Terrain with Ground Vehicle	 | Yinchuan Wang,Bin Ren,Xiang Zhang,Pengyu Wang,Chaoqun Wang,Rui Song,Yibin Li,Max Q.-H. Meng	 | 本文提出了一种名为旋转优化的LiDAR-only（ROLO）SLAM的方法，旨在提高地面车辆在恶劣地形中的姿态估计准确性，通过前向位置预测和球形对齐引导的旋转注册等技术，显著减少了姿态估计的漂移问题；实验结果表明，ROLO-SLAM在多种场景下均优于现有的LiDAR SLAM框架。	 | LiDAR-based SLAM is recognized as one effective method to offer localization guidance in rough environments. However, off-the-shelf LiDAR-based SLAM methods suffer from significant pose estimation drifts, particularly components relevant to the vertical direction, when passing to uneven terrains. This deficiency typically leads to a conspicuously distorted global map. In this article, a LiDAR-based SLAM method is presented to improve the accuracy of pose estimations for ground vehicles in rough terrains, which is termed Rotation-Optimized LiDAR-Only (ROLO) SLAM. The method exploits a forward location prediction to coarsely eliminate the location difference of consecutive scans, thereby enabling separate and accurate determination of the location and orientation at the front-end. Furthermore, we adopt a parallel-capable spatial voxelization for correspondence-matching. We develop a spherical alignment-guided rotation registration within each voxel to estimate the rotation of vehicle. By incorporating geometric alignment, we introduce the motion constraint into the optimization formulation to enhance the rapid and effective estimation of LiDAR's translation. Subsequently, we extract several keyframes to construct the submap and exploit an alignment from the current scan to the submap for precise pose estimation. Meanwhile, a global-scale factor graph is established to aid in the reduction of cumulative errors. In various scenes, diverse experiments have been conducted to evaluate our method. The results demonstrate that ROLO-SLAM excels in pose estimation of ground vehicles and outperforms existing state-of-the-art LiDAR SLAM frameworks.	 | 基于LiDAR的SLAM被认为是一种在恶劣环境中提供定位指导的有效方法。然而，商用的基于LiDAR的SLAM方法在通过不平地形时，会遭受显著的姿态估计漂移，特别是垂直方向的相关部分。这种缺陷通常会导致全局地图明显失真。本文提出了一种基于LiDAR的SLAM方法，旨在提高地面车辆在恶劣地形中的姿态估计准确性，这种方法称为旋转优化的LiDAR-only（ROLO）SLAM。该方法利用前向位置预测粗略消除连续扫描之间的位置差异，从而在前端分别准确确定位置和姿态。此外，我们采用并行计算的时空体素化进行对应匹配。我们开发了一种球形对齐引导的旋转注册方法，在每个体素中进行，以估计车辆的旋转。通过结合几何对齐，我们引入了运动约束到优化公式中，以增强LiDAR平移的快速且有效的估计。随后，我们提取几个关键帧构建子地图，并利用当前扫描对子地图的对齐进行精确的姿态估计。同时，我们构建了一个全局尺度因子图以帮助减少累积误差。在各种场景下，进行了多种实验来评估我们的方法。结果表明，ROLO-SLAM在地面车辆的姿态估计方面表现出色，并优于现有的最先进的LiDAR SLAM框架。
2501.02029	 | Spot Risks Before Speaking! Unraveling Safety Attention Heads in Large Vision-Language Models	 | Ziwei Zheng,Junyao Zhao,Le Yang,Lijun He,Fan Li	 | 大型视觉-语言模型（LVLMs）在集成额外模态后比仅语言模型更易受到安全攻击，本文发现LVLMs在生成第一个词令牌期间的内部激活可以识别恶意提示，并提出了“安全头”机制来抵御这些攻击，通过这些机制构建的恶意提示检测器表现出强大的零样本泛化能力。	 | With the integration of an additional modality, large vision-language models (LVLMs) exhibit greater vulnerability to safety risks (e.g., jailbreaking) compared to their language-only predecessors. Although recent studies have devoted considerable effort to the post-hoc alignment of LVLMs, the inner safety mechanisms remain largely unexplored. In this paper, we discover that internal activations of LVLMs during the first token generation can effectively identify malicious prompts across different attacks. This inherent safety perception is governed by sparse attention heads, which we term ``safety heads." Further analysis reveals that these heads act as specialized shields against malicious prompts; ablating them leads to higher attack success rates, while the model's utility remains unaffected. By locating these safety heads and concatenating their activations, we construct a straightforward but powerful malicious prompt detector that integrates seamlessly into the generation process with minimal extra inference overhead. Despite its simple structure of a logistic regression model, the detector surprisingly exhibits strong zero-shot generalization capabilities. Experiments across various prompt-based attacks confirm the effectiveness of leveraging safety heads to protect LVLMs. Code is available at \url{https://github.com/Ziwei-Zheng/SAHs}.	 | 随着额外模态的集成，大型视觉-语言模型（LVLMs）在安全风险（例如，被破解）方面的脆弱性比其仅语言的前辈更为明显。尽管最近的研究已经投入了相当多的努力来对LVLMs进行事后对齐，但其内部的安全机制仍然很大程度上未被探索。在本文中，我们发现LVLMs在生成第一个词令牌期间的内部激活可以有效地识别来自不同攻击的恶意提示。这种固有的安全感知是由稀疏的注意力头控制的，我们称这些头为“安全头”（Safety Heads）。进一步的分析表明，这些头充当了专门的保护盾，用于抵御恶意提示；删除它们会导致更高的攻击成功率，而模型的实用性不会受到影响。通过定位这些安全头并将它们的激活值连接起来，我们构建了一个简单但强大的恶意提示检测器，该检测器无缝集成到生成过程中，并且额外的推理开销很小。尽管其结构是一个简单的逻辑回归模型，但检测器却表现出强大的零样本泛化能力。在各种基于提示的攻击实验中，利用安全头保护LVLMs的有效性得到了证实。代码可在 \url{https://github.com/Ziwei-Zheng/SAHs} 获取。
2501.02025	 | RealDiffFusionNet: Neural Controlled Differential Equation Informed Multi-Head Attention Fusion Networks for Disease Progression Modeling Using Real-World Data	 | Aashish Cheruvu,Nathaniel Rigoni	 | 本文提出了一种名为RealDiffFusionNet的新颖深度学习方法，该方法结合了神经控制微分方程和多头注意力机制，用于处理多模态时间序列数据，并在OSIC和ADNI数据集上的实验中显示出优于LSTM和其他基线模型的性能。	 | This paper presents a novel deep learning-based approach named RealDiffFusionNet incorporating Neural Controlled Differential Equations (Neural CDE) - time series models that are robust in handling irregularly sampled data - and multi-head attention to align relevant multimodal context (image data, time invariant data, etc.) at each time point. Long short-term memory (LSTM) models were also used as a baseline. Two different datasets were used: a data from the Open-Source Imaging Consortium (OSIC) containing structured time series data of demographics and lung function with a baseline CT scan of the lungs and the second from the Alzheimer's Disease Neuroimaging Initiative (ADNI) containing a series of MRI scans along with demographics, physical examinations, and cognitive assessment data. An ablation study was performed to understand the role of CDEs, multimodal data, attention fusion, and interpolation strategies on model performance. When the baseline models were evaluated, the use of multimodal data resulted in an improvement in Neural CDE performance, with a lower test RMSE. Additionally, the performance of multimodal Neural CDE was also superior to multimodal LSTM. In the attention-based architectures, fusion through concatenation and rectilinear interpolation were found to improve model performance. The performance of the proposed RealDiffFusionNet was found to be superior (0.2570) to all models. For the ADNI dataset, between the Neural-CDE and LSTM models trained only on the structured data, the test RMSE were comparable (0.471 for LSTM vs. 0.4581 Neural-CDE). Furthermore, the addition of image features from patients' MRI series resulted in an improvement in performance, with a lower test RMSE (0.4372 with multimodal vs 0.4581 with structured data). RealDiffFusionNet has shown promise in utilizing CDEs and multimodal data to accurately predict disease progression.	 | 本文提出了一种结合神经控制微分方程（Neural CDE）和多头注意力机制的新型深度学习方法，名为RealDiffFusionNet。Neural CDE是一种稳健处理不规则采样时间序列数据的时间序列模型，而多头注意力机制则用于在每个时间点对相关多模态上下文（图像数据、时间不变数据等）进行对齐。还使用了长短期记忆（LSTM）模型作为基线。使用了两个不同的数据集：一个是来自Open-Source Imaging Consortium (OSIC) 的数据，包含有关人口统计和肺功能的结构化时间序列数据，以及肺部的基线CT扫描；另一个是来自Alzheimer's Disease Neuroimaging Initiative (ADNI) 的数据，包含一系列MRI扫描以及人口统计、体格检查和认知评估数据。进行了一项消融研究，以了解控制微分方程（CDEs）、多模态数据、注意力融合和插值策略在模型性能中的作用。当基线模型被评估时，使用多模态数据提高了Neural CDE的性能，测试RMSE较低。此外，多模态Neural CDE的性能也优于多模态LSTM。在基于注意力的架构中，通过拼接和线性插值进行的融合被发现可以提高模型的性能。所提出的RealDiffFusionNet的性能优于所有模型（0.2570）。对于ADNI数据集，仅在结构化数据上训练的Neural-CDE模型和LSTM模型的测试RMSE相当（LSTM为0.471，Neural-CDE为0.4581）。此外，通过添加来自患者MRI系列的图像特征，性能得到了提高，测试RMSE较低（使用多模态数据为0.4372，使用结构化数据为0.4581）。实验证明，RealDiffFusionNet在利用CDEs和多模态数据方面具有准确预测疾病进展的潜力。
2501.02003	 | SurfPatch: Enabling Patch Matching for Exploratory Stream Surface Visualization	 | Delin An,Chaoli Wang	 | SurfPatch 是一种新的框架，用于探索性流表面可视化，通过将表面位置问题转化为表面选择问题并采用自底向上的三阶段过程（顶点级分类、补丁级匹配和表面级聚类），实现了精细、多尺度的流表面匹配和灵活的查询。该框架不仅适用于稳定流数据集，还能处理不稳定流和从标量场等值面生成的流表面，提供了一种有效的可视化和分析方法。	 | Unlike their line-based counterparts, surface-based techniques have yet to be thoroughly investigated in flow visualization due to their significant placement, speed, perception, and evaluation challenges. This paper presents SurfPatch, a novel framework supporting exploratory stream surface visualization. To begin with, we translate the issue of surface placement to surface selection and trace a large number of stream surfaces from a given flow field dataset. Then, we introduce a three-stage process: vertex-level classification, patch-level matching, and surface-level clustering that hierarchically builds the connection between vertices and patches and between patches and surfaces. This bottom-up approach enables fine-grained, multiscale patch-level matching, sharply contrasts surface-level matching offered by existing works, and provides previously unavailable flexibility during querying. We design an intuitive visual interface for users to conveniently visualize and analyze the underlying collection of stream surfaces in an exploratory manner. SurfPatch is not limited to stream surfaces traced from steady flow datasets. We demonstrate its effectiveness through experiments on stream surfaces produced from steady and unsteady flows as well as isosurfaces extracted from scalar fields. The code is available at https://github.com/adlsn/SurfPatch.	 | 与基于线条的方法相比，基于表面的技术在流场可视化中尚未得到充分研究，原因是它们在位置、速度、感知和评估方面存在重大挑战。本文介绍了SurfPatch，这是一种新的框架，支持探索性流表面可视化。首先，我们将表面位置问题转换为表面选择问题，并从给定的流场数据集中跟踪大量流表面。然后，我们引入了一个三阶段过程：顶点级分类、补丁级匹配和表面级聚类，通过分级构建顶点与补丁之间以及补丁与表面之间的连接。自底向上的方法使得可以进行精细、多尺度的补丁级匹配，与现有工作提供的表面级匹配形成鲜明对比，并在查询过程中提供了前所未有的灵活性。我们设计了一个直观的可视化界面，使用户能够方便地以探索性方式可视化和分析底层的流表面集合。SurfPatch不仅限于从稳定流数据集中跟踪的流表面。通过在稳定流和不稳定流以及从标量场提取的等值面产生的流表面进行的实验，表明了其有效性。代码可在https://github.com/adlsn/SurfPatch获取。
2501.02000	 | Multi-Center Study on Deep Learning-Assisted Detection and Classification of Fetal Central Nervous System Anomalies Using Ultrasound Imaging	 | Yang Qi,Jiaxin Cai,Jing Lu,Runqing Xiong,Rongshang Chen,Liping Zheng,Duo Ma	 | 研究人员构建了一个深度学习模型，用于辅助产前诊断胎儿中枢神经系统异常，该模型在涵盖四种典型胎儿中枢神经系统异常的数据集中达到了94.5%的患者级别预测准确率和99.3%的AUROC值，显著提高了诊断准确性，并结合了可视化热图以增强医生的诊断效率。	 | Prenatal ultrasound evaluates fetal growth and detects congenital abnormalities during pregnancy, but the examination of ultrasound images by radiologists requires expertise and sophisticated equipment, which would otherwise fail to improve the rate of identifying specific types of fetal central nervous system (CNS) abnormalities and result in unnecessary patient examinations. We construct a deep learning model to improve the overall accuracy of the diagnosis of fetal cranial anomalies to aid prenatal diagnosis. In our collected multi-center dataset of fetal craniocerebral anomalies covering four typical anomalies of the fetal central nervous system (CNS): anencephaly, encephalocele (including meningocele), holoprosencephaly, and rachischisis, patient-level prediction accuracy reaches 94.5%, with an AUROC value of 99.3%. In the subgroup analyzes, our model is applicable to the entire gestational period, with good identification of fetal anomaly types for any gestational period. Heatmaps superimposed on the ultrasound images not only provide a visual interpretation for the algorithm but also provide an intuitive visual aid to the physician by highlighting key areas that need to be reviewed, helping the physician to quickly identify and validate key areas. Finally, the retrospective reader study demonstrates that by combining the automatic prediction of the DL system with the professional judgment of the radiologist, the diagnostic accuracy and efficiency can be effectively improved and the misdiagnosis rate can be reduced, which has an important clinical application prospect.	 | 超声波检查在孕期评估胎儿生长和检测先天性异常方面具有重要作用，但放射科医生检查超声图像需要专业知识和高级设备，否则将无法提高识别特定类型的胎儿中枢神经系统（CNS）异常的准确性，导致不必要的患者检查。我们构建了一个深度学习模型，以提高诊断胎儿颅面异常的整体准确性，从而辅助产前诊断。在涵盖四种典型胎儿中枢神经系统（CNS）异常（无脑畸形、脑膨出（包括脊膜膨出）、半球面畸形和脊柱裂）的多中心数据集中，患者级别的预测准确率达到了94.5%，且AUROC值为99.3%。在子组分析中，我们的模型在整个妊娠期间都适用，能够有效地识别任何妊娠时期的胎儿异常类型。将热图叠加在超声图像上不仅为算法提供了视觉解释，还为医生提供了直观的视觉辅助，突出显示需要审查的关键区域，帮助医生迅速识别和验证关键区域。最后，回顾性读者研究显示，通过结合深度学习系统的自动预测与放射科医生的专业判断，可以有效提高诊断准确性和效率，减少误诊率，具有重要的临床应用前景。
2501.01984	 | Leveraging AI for Automatic Classification of PCOS Using Ultrasound Imaging	 | Atharva Divekar,Atharva Sonawane	 | 该研究利用InceptionV3架构进行迁移学习，构建了一个用于自动分类健康和不健康超声图像的AI系统，实现了90.52%的验证准确率，并通过解释性方法提供了对模型决策过程的洞察，展示了AI在诊断多囊卵巢综合征方面的潜力。	 | The AUTO-PCOS Classification Challenge seeks to advance the diagnostic capabilities of artificial intelligence (AI) in identifying Polycystic Ovary Syndrome (PCOS) through automated classification of healthy and unhealthy ultrasound frames. This report outlines our methodology for building a robust AI pipeline utilizing transfer learning with the InceptionV3 architecture to achieve high accuracy in binary classification. Preprocessing steps ensured the dataset was optimized for training, validation, and testing, while interpretability methods like LIME and saliency maps provided valuable insights into the model's decision-making. Our approach achieved an accuracy of 90.52%, with precision, recall, and F1-score metrics exceeding 90% on validation data, demonstrating its efficacy. The project underscores the transformative potential of AI in healthcare, particularly in addressing diagnostic challenges like PCOS. Key findings, challenges, and recommendations for future enhancements are discussed, highlighting the pathway for creating reliable, interpretable, and scalable AI-driven medical diagnostic tools.	 | 《自动PCOS分类挑战》旨在通过自动化分类健康和不健康的超声图像，推动人工智能（AI）在诊断多囊卵巢综合征（PCOS）方面的诊断能力。本报告概述了我们利用InceptionV3架构进行迁移学习，构建稳健的AI流水线的方法，以实现二分类中的高精度。预处理步骤确保了数据集在训练、验证和测试中的优化，而解释性方法如LIME和梯度加权类激活映射（Saliency Maps）提供了对模型决策过程的宝贵见解。我们的方法在验证数据上实现了90.52%的准确率，精确度、召回率和F1分数均超过90%，展示了其有效性。该项目强调了AI在医疗健康领域的变革潜力，特别是在解决如PCOS等诊断挑战方面。报告讨论了关键发现、面临的挑战和对未来改进的建议，突显了创建可靠、可解释和可扩展的AI驱动医疗诊断工具的道路。
