| ID | Title | Authors | Summary (zh) | Abstract (en) | Abstract (zh) | 
| --- | --- | --- | --- | --- | --- |
2501.03226	 | BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning	 | Beichen Zhang,Yuhong Liu,Xiaoyi Dong,Yuhang Zang,Pan Zhang,Haodong Duan,Yuhang Cao,Dahua Lin,Jiaqi Wang	 | 最新的大型语言模型在解决复杂数学问题时表现出色，但受限于ICL示例中的粒度不匹配和负面噪声问题。为解决这些问题，提出了一种名为BoostStep的方法，通过在步骤粒度上对ICL示例进行对齐并采用“初次尝试”策略，逐步提高每一步的推理质量，从而显著提升了模型的性能。	 | Cutting-edge large language models (LLMs) demonstrate promising performance in solving complex math problems with a divide-and-conquer pipeline and the assistance of in-context learning (ICL) examples. However, their potential for improvement is limited by two critical problems within their ICL examples: granularity-mismatch and the ensuing negative-effect noise problem. Specifically, the LLMs are capable of the dividing process yet mostly failed by inaccurate reasoning within a few conquer steps, while the ICL examples retrieved in question-grained sometimes lack relevant steps for a specific challenging reasoning step. Further, this disconnect may hinder the correct reasoning due to its irrelevance. To this end, we focus on improving the reasoning quality within each step and present BoostStep. BoostStep aligns the granularity between the retrieving and reasoning on step grained, and provides highly related ICL examples for each reasoning step with a novel `first-try' strategy. BoostStep provides more relevant examples than the coarse question-grained strategy, enhancing the model reasoning quality within each step steadily. BoostStep is a general and robust reasoning-enhancing method that not only improves standalone reasoning performance but also integrates seamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate generation and decision-making. Quantitatively, it improves GPT-4o and Qwen2.5-Math-72B by 3.6\% and 2.0\% respectively on various mathematical benchmarks, and 7.5\% gain combined with MCTS.	 | 最新的大型语言模型（LLMs）在利用分而治之的管道和上下文相关学习（ICL）示例解决复杂数学问题方面表现出令人鼓舞的性能。然而，它们的改进潜力受到两种关键问题的限制：ICL示例中的粒度不匹配和由此产生的负面噪声问题。具体来说，LLMs能够进行分而治之的过程，但在几个征服步骤中的推理不准确而导致失败。此外，在问题粒度下检索到的ICL示例有时缺乏针对特定挑战性推理步骤的相关步骤，这种不相关性可能会阻碍正确的推理。为了解决这些问题，我们重点关注提高每一步的推理质量，并提出了BoostStep。BoostStep在步骤粒度上对检索和推理进行对齐，并为每一步提供高度相关的ICL示例，采用一种新颖的“初次尝试”策略。BoostStep提供了比粗略问题粒度策略更多的相关示例，逐步提高模型在每一步的推理质量。BoostStep是一种通用且稳健的推理增强方法，不仅可以提高独立推理性能，还可以无缝集成蒙特卡洛树搜索方法（MCTS），以改进候选生成和决策过程。定量而言，BoostStep分别将GPT-4o和Qwen2.5-Math-72B在各种数学基准上的性能提高了3.6%和2.0%，结合MCTS时性能提升7.5%。
2501.03212	 | Leveraging Explainable AI for LLM Text Attribution: Differentiating Human-Written and Multiple LLMs-Generated Text	 | Ayat Najjar,Huthaifa I. Ashqar,Omar Darwish,Eman Hammad	 | 该研究利用机器学习和深度学习算法识别由生成型AI工具生成的文本，结果显示其准确率高达98.5%，显著优于现有工具，同时采用可解释的人工智能技术帮助理解文本的特征，以支持内容的准确归因和剽窃检测。	 | The development of Generative AI Large Language Models (LLMs) raised the alarm regarding identifying content produced through generative AI or humans. In one case, issues arise when students heavily rely on such tools in a manner that can affect the development of their writing or coding skills. Other issues of plagiarism also apply. This study aims to support efforts to detect and identify textual content generated using LLM tools. We hypothesize that LLMs-generated text is detectable by machine learning (ML), and investigate ML models that can recognize and differentiate texts generated by multiple LLMs tools. We leverage several ML and Deep Learning (DL) algorithms such as Random Forest (RF), and Recurrent Neural Networks (RNN), and utilized Explainable Artificial Intelligence (XAI) to understand the important features in attribution. Our method is divided into 1) binary classification to differentiate between human-written and AI-text, and 2) multi classification, to differentiate between human-written text and the text generated by the five different LLM tools (ChatGPT, LLaMA, Google Bard, Claude, and Perplexity). Results show high accuracy in the multi and binary classification. Our model outperformed GPTZero with 98.5\% accuracy to 78.3\%. Notably, GPTZero was unable to recognize about 4.2\% of the observations, but our model was able to recognize the complete test dataset. XAI results showed that understanding feature importance across different classes enables detailed author/source profiles. Further, aiding in attribution and supporting plagiarism detection by highlighting unique stylistic and structural elements ensuring robust content originality verification.	 | 生成型人工智能大型语言模型（LLMs）的发展引发了对通过生成型AI或人类所生成内容的识别问题的关注。在一种情况下，学生过度依赖这些工具可能会影响他们的写作或编程技能的发展，还会引发剽窃问题。本研究旨在支持检测和识别使用LLM工具生成的文本的努力。我们假设通过机器学习（ML）可以识别出LLM生成的文本，并调查可用于识别和区分不同LLM工具生成文本的ML模型。我们利用了多种ML和深度学习（DL）算法，如随机森林（RF）和递归神经网络（RNN），并使用可解释的人工智能（XAI）来理解归因的重要特征。我们的方法分为两部分：1) 二元分类，以区分人类撰写的文本和AI生成的文本；2) 多分类，以区分人类撰写的文本和由五种不同LLM工具（ChatGPT、LLaMA、Google Bard、Claude和Perplexity）生成的文本。结果显示，在二元分类和多分类中的准确性都很高。我们的模型在准确率方面超过了GPTZero，达到了98.5%，而GPTZero的准确率为78.3%。值得注意的是，GPTZero无法识别大约4.2%的观测值，但我们的模型能够识别全部测试数据集。XAI结果表明，理解不同类别中的特征重要性可以创建详细的作者/来源档案。此外，通过突出显示独特的风格和结构元素，该方法有助于归因和剽窃检测，确保内容原始性的坚实验证。
2501.03203	 | Detecting AI-Generated Text in Educational Content: Leveraging Machine Learning and Explainable AI for Academic Integrity	 | Ayat A. Najjar,Huthaifa I. Ashqar,Omar A. Darwish,Eman Hammad	 | 本研究利用CyberHumanAI数据集评估了多种机器学习和深度学习算法，以检测学生作品中的AI生成内容，并发现特定的传统机器学习算法在分类准确性上优于深度学习方法，同时提出的新模型在分类任务中表现优于通用系统GPTZero。	 | This study seeks to enhance academic integrity by providing tools to detect AI-generated content in student work using advanced technologies. The findings promote transparency and accountability, helping educators maintain ethical standards and supporting the responsible integration of AI in education. A key contribution of this work is the generation of the CyberHumanAI dataset, which has 1000 observations, 500 of which are written by humans and the other 500 produced by ChatGPT. We evaluate various machine learning (ML) and deep learning (DL) algorithms on the CyberHumanAI dataset comparing human-written and AI-generated content from Large Language Models (LLMs) (i.e., ChatGPT). Results demonstrate that traditional ML algorithms, specifically XGBoost and Random Forest, achieve high performance (83% and 81% accuracies respectively). Results also show that classifying shorter content seems to be more challenging than classifying longer content. Further, using Explainable Artificial Intelligence (XAI) we identify discriminative features influencing the ML model's predictions, where human-written content tends to use a practical language (e.g., use and allow). Meanwhile AI-generated text is characterized by more abstract and formal terms (e.g., realm and employ). Finally, a comparative analysis with GPTZero show that our narrowly focused, simple, and fine-tuned model can outperform generalized systems like GPTZero. The proposed model achieved approximately 77.5% accuracy compared to GPTZero's 48.5% accuracy when tasked to classify Pure AI, Pure Human, and mixed class. GPTZero showed a tendency to classify challenging and small-content cases as either mixed or unrecognized while our proposed model showed a more balanced performance across the three classes.	 | 本研究旨在通过使用先进科技提供工具，来增强学术诚信，检测学生作品中的AI生成内容。研究发现促进透明度和责任感，帮助教育者维护道德标准，并支持AI在教育中的负责任整合。本研究的一个关键贡献是生成了CyberHumanAI数据集，包含1000个观测值，其中500个由人类撰写，另外500个由ChatGPT生成。我们对CyberHumanAI数据集中的各类机器学习（ML）和深度学习（DL）算法进行了评估，对比了人类撰写的内容和大型语言模型（LLMs，如ChatGPT）生成的内容。结果表明，传统ML算法，特别是XGBoost和随机森林，表现出高精度（分别达到83%和81%的准确率）。此外，结果还显示，分类较短的内容似乎比分类较长的内容更具挑战性。进一步使用可解释的人工智能（XAI）技术，我们识别出影响ML模型预测的关键特征，其中人类撰写的内容倾向于使用实际语言（例如，使用和允许），而AI生成的文本则以更加抽象和正式的术语（例如，领域和使用）为特征。最后，与GPTZero的比较分析显示，我们的专注于特定任务、简洁且微调的模型在分类纯AI、纯人类和混合类内容时的表现优于通用系统GPTZero。我们提出的模型达到了约77.5%的准确率，而GPTZero在同样任务下的准确率为48.5%。GPTZero在处理具有挑战性和小型内容的情况时更倾向于将其归类为混合或未识别，而我们提出的模型则在三种类别中表现出更均衡的表现。
2501.03200	 | The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input	 | Alon Jacovi,Andrew Wang,Chris Alberti,Connie Tao,Jon Lipovetz,Kate Olszewska,Lukas Haas,Michelle Liu,Nate Keating,Adam Bloniarz,Carl Saroufim,Corey Fry,Dror Marcus,Doron Kukliansky,Gaurav Singh Tomar,James Swirhun,Jinwei Xing,Lily Wang,Madhu Gurumurthy,Michael Aaron,Moran Ambar,Rachana Fellinger,Rui Wang,Zizhao Zhang,Sasha Goldshtein,Dipanjan Das	 | FACTS Grounding 是一个实时排行榜及其基准测试，用于评估语言模型生成与用户提示背景信息相符的准确文本的能力，模型通过两个阶段的自动化裁判模型进行评估，排行榜将随着时间维护并包含公共和私有分割以保持完整性。	 | We introduce FACTS Grounding, an online leaderboard and associated benchmark that evaluates language models' ability to generate text that is factually accurate with respect to given context in the user prompt. In our benchmark, each prompt includes a user request and a full document, with a maximum length of 32k tokens, requiring long-form responses. The long-form responses are required to be fully grounded in the provided context document while fulfilling the user request. Models are evaluated using automated judge models in two phases: (1) responses are disqualified if they do not fulfill the user request; (2) they are judged as accurate if the response is fully grounded in the provided document. The automated judge models were comprehensively evaluated against a held-out test-set to pick the best prompt template, and the final factuality score is an aggregate of multiple judge models to mitigate evaluation bias. The FACTS Grounding leaderboard will be actively maintained over time, and contains both public and private splits to allow for external participation while guarding the integrity of the leaderboard. It can be found at https://www.kaggle.com/facts-leaderboard.	 | 我们介绍了 FACTS Grounding，这是一个实时排行榜及其相关基准，用于评估语言模型生成与给定用户提示背景信息相符的事实准确文本的能力。在我们的基准测试中，每个提示包含用户请求和一个完整的文档，最大长度为32k个标记，要求生成长篇幅的响应。这些长篇幅的响应需要完全基于提供的上下文文档，同时满足用户请求。模型通过两个阶段的自动化裁判模型进行评估：（1）如果响应未能满足用户请求，则予以取消资格；（2）如果响应完全基于提供的文档，则被判定为准确。自动化裁判模型经过全面评估，以选择最佳提示模板，最终的事实评分是多个裁判模型的综合结果，以减轻评估偏见。FACTS Grounding 的排行榜将随着时间的推移持续维护，并包含公共和私有分割，以允许外部参与同时保持排行榜的完整性。更多详细信息可访问 https://www.kaggle.com/facts-leaderboard。
2501.03191	 | CLIX: Cross-Lingual Explanations of Idiomatic Expressions	 | Aaron Gluck,Katharina von der Wense,Maria Pacheco	 | CLIX是一个跨语言形象表达解释的任务，旨在帮助语言学习者扩展词汇量，尽管大型语言模型在该任务上仍具有挑战性，但显示出潜力，未来需要进一步解决关键挑战以实现可靠的应用。	 | Automated definition generation systems have been proposed to support vocabulary expansion for language learners. The main barrier to the success of these systems is that learners often struggle to understand definitions due to the presence of potentially unfamiliar words and grammar, particularly when non-standard language is involved. To address these challenges, we propose CLIX, the task of Cross-Lingual explanations of Idiomatic eXpressions. We explore the capabilities of current NLP models for this task, and observe that while it remains challenging, large language models show promise. Finally, we perform a detailed error analysis to highlight the key challenges that need to be addressed before we can reliably incorporate these systems into educational tools.	 | 自动定义生成系统已被提出，以支持语言学习者的词汇扩展。这些系统成功的关键障碍在于，学习者往往难以理解定义，特别是由于其中可能包含不熟悉的词汇和语法，尤其是在涉及非标准语言时。为了应对这些挑战，我们提出了CLIX，即跨语言形象表达解释的任务。我们探讨了当前NLP模型在这种任务上的能力，并观察到虽然仍具有挑战性，但大型语言模型显示出潜力。最后，我们进行了详细的错误分析，以突出在我们能够可靠地将这些系统纳入教育工具之前需要解决的关键挑战。
2501.03183	 | Classifier-Guided Captioning Across Modalities	 | Ariel Shaulov,Tal Shaharabany,Eitan Shaar,Gal Chechik,Lior Wolf	 | 本文提出了一种方法，通过冻结语言模型和训练文本分类器来适应不同场景的语义，从而改善字幕生成的质量，特别是在音频字幕领域表现出色，并在零样本音频字幕中达到了最先进的性能。	 | Most current captioning systems use language models trained on data from specific settings, such as image-based captioning via Amazon Mechanical Turk, limiting their ability to generalize to other modality distributions and contexts. This limitation hinders performance in tasks like audio or video captioning, where different semantic cues are needed. Addressing this challenge is crucial for creating more adaptable and versatile captioning frameworks applicable across diverse real-world contexts. In this work, we introduce a method to adapt captioning networks to the semantics of alternative settings, such as capturing audibility in audio captioning, where it is crucial to describe sounds and their sources. Our framework consists of two main components: (i) a frozen captioning system incorporating a language model (LM), and (ii) a text classifier that guides the captioning system. The classifier is trained on a dataset automatically generated by GPT-4, using tailored prompts specifically designed to enhance key aspects of the generated captions. Importantly, the framework operates solely during inference, eliminating the need for further training of the underlying captioning model. We evaluate the framework on various models and modalities, with a focus on audio captioning, and report promising results. Notably, when combined with an existing zero-shot audio captioning system, our framework improves its quality and sets state-of-the-art performance in zero-shot audio captioning.	 | 当前大多数字幕系统都是基于特定场景的数据训练语言模型，如通过亚马逊 Mechanical Turk 进行的基于图像的字幕生成，这限制了它们在其他模态分布和上下文中的泛化能力。这种局限性阻碍了在音频或视频字幕等任务中的表现，因为这些任务需要不同的语义提示。解决这一挑战对于创建更适应性强且多功能的字幕框架至关重要，这些框架可以应用于各种现实世界的情境中。在这项工作中，我们提出了一种方法，将字幕网络适应不同场景的语义，例如在音频字幕中捕捉清晰度，需要描述声音及其来源。我们的框架由两个主要组件组成：(i) 一个冻结的字幕系统，包含语言模型 (LM)，以及 (ii) 一个文本分类器，该分类器指导字幕系统。分类器通过 GPT-4 自动生成的数据集训练，使用特定设计的提示来增强生成字幕的关键方面。重要的是，该框架仅在推断过程中运行，无需进一步训练底层字幕模型。我们在各种模型和模态下评估了该框架，重点是音频字幕领域，并报告了令人鼓舞的结果。值得注意的是，当与现有的零样本音频字幕系统结合使用时，我们的框架改善了其质量，并在零样本音频字幕中设置了最先进的性能。
2501.03182	 | Boosting Explainability through Selective Rationalization in Pre-trained Language Models	 | Libing Yuan,Shuaibo Hu,Kui Yu,Le Wu	 | 本文提出了PLMR方法，通过将预训练语言模型（PLMs）拆分为生成器和预测器来解决选择性理性化在PLMs中的退化问题，并通过修剪无关令牌缓解同质性问题，从而提供有效的可解释理由。实验结果表明，PLMR在多个PLMs上是有效的。	 | The widespread application of pre-trained language models (PLMs) in natural language processing (NLP) has led to increasing concerns about their explainability. Selective rationalization is a self-explanatory framework that selects human-intelligible input subsets as rationales for predictions. Recent studies have shown that applying existing rationalization frameworks to PLMs will result in severe degeneration and failure problems, producing sub-optimal or meaningless rationales. Such failures severely damage trust in rationalization methods and constrain the application of rationalization techniques on PLMs. In this paper, we find that the homogeneity of tokens in the sentences produced by PLMs is the primary contributor to these problems. To address these challenges, we propose a method named Pre-trained Language Model's Rationalization (PLMR), which splits PLMs into a generator and a predictor to deal with NLP tasks while providing interpretable rationales. The generator in PLMR also alleviates homogeneity by pruning irrelevant tokens, while the predictor uses full-text information to standardize predictions. Experiments conducted on two widely used datasets across multiple PLMs demonstrate the effectiveness of the proposed method PLMR in addressing the challenge of applying selective rationalization to PLMs. Codes: https://github.com/ylb777/PLMR.	 | 预训练语言模型（PLMs）在自然语言处理（NLP）中的广泛应用引发了对其解释性的关注。选择性理性化是一种自解释框架，它选择人类可理解的输入子集作为预测的理由。最近的研究表明，将现有理性化框架应用于PLMs会导致严重退化和失败问题，产生次优或无意义的理由。这些失败严重损害了理性化方法的信任，并限制了理性化技术在PLMs中的应用。在本文中，我们发现PLMs生成句子中的同质性是这些问题的主要原因。为了解决这些挑战，我们提出了一种名为Pre-trained Language Model's Rationalization（PLMR）的方法，该方法将PLMs拆分为生成器和预测器，以处理NLP任务并提供可解释的理由。PLMR中的生成器还通过修剪无关的令牌来缓解同质性问题，而预测器则使用全文信息来标准化预测。在多个PLMs上广泛使用的两个数据集上的实验结果表明，PLMR在将选择性理性化应用于PLMs方面是有效的。代码：https://github.com/ylb777/PLMR。
2501.03172	 | GLiREL -- Generalist Model for Zero-Shot Relation Extraction	 | Jack Boylan,Chris Hokamp,Demian Gholipour Ghalandari	 | 我们提出了GLiREL，一种用于零样本关系分类的高效模型，能够在单次前向传递中准确预测多个实体间的零样本关系标签，并在FewRel和WikiZSL基准测试中达到最先进的性能。此外，我们还提供了一个合成生成多样化关系标签数据集的方法。	 | We introduce GLiREL (Generalist Lightweight model for zero-shot Relation Extraction), an efficient architecture and training paradigm for zero-shot relation classification. Inspired by recent advancements in zero-shot named entity recognition, this work presents an approach to efficiently and accurately predict zero-shot relationship labels between multiple entities in a single forward pass. Experiments using the FewRel and WikiZSL benchmarks demonstrate that our approach achieves state-of-the-art results on the zero-shot relation classification task. In addition, we contribute a protocol for synthetically-generating datasets with diverse relation labels.	 | 我们引入了GLiREL（通用轻量级零样本关系抽取模型），这是一种用于零样本关系分类的高效架构和训练范式。受最近零样本命名实体识别进展的启发，这项工作提出了一种方法，在单次前向传递中高效且准确地预测多个实体之间的零样本关系标签。使用FewRel和WikiZSL基准的实验表明，我们的方法在零样本关系分类任务上达到了最先进的效果。此外，我们还贡献了一个合成生成具有多样化关系标签的数据集的协议。
2501.03166	 | Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot In-Context Learning for SQL2Text	 | Ali Al-Lawati,Jason Lucas,Prasenjit Mitra	 | 本文专注于SQL查询到自然语言的转换任务（SQL2Text），以增强大型语言模型（LLMs）在代码生成和安全分析中的安全性，并通过引入迭代性实例级检索（ICL）提示，利用Text2SQL数据集提高LLM在SQL2Text任务上的表现；研究发现，基于SQL固有的图性质选择ICL样本能显著提高模型性能，BLEU分数相较于随机选择提高了39%。	 | Large Language Models (LLMs) have demonstrated remarkable performance in various NLP tasks, including semantic parsing, which trans lates natural language into formal code representations. However, the reverse process, translating code into natural language, termed semantic captioning, has received less attention. This task is becoming increasingly important as LLMs are integrated into platforms for code generation, security analysis, and educational purposes. In this paper, we focus on the captioning of SQL query (SQL2Text) to address the critical need for understanding and explaining SQL queries in an era where LLM-generated code poses potential security risks. We repurpose Text2SQL datasets for SQL2Text by introducing an iterative ICL prompt using GPT-4o to generate multiple additional utterances, which enhances the robustness of the datasets for the reverse task. We conduct our experiments using in-context learning (ICL) based on different sample selection methods, emphasizing smaller, more computationally efficient LLMs. Our findings demonstrate that leveraging the inherent graph properties of SQL for ICL sample selection significantly outperforms random selection by up to 39% on BLEU score and provides better results than alternative methods. Dataset and codes are published: \url{https://github.com/aliwister/ast-icl}.	 | 大型语言模型（LLMs）在各种自然语言处理（NLP）任务中表现出色，包括语义解析，即将自然语言转换为正式的代码表示。然而，在代码转换为自然语言的反向过程中，被称为语义描述的任务则受到较少的关注。随着LLMs被集成到代码生成、安全分析和教育平台中，这一任务变得越来越重要。在本文中，我们重点关注SQL查询的描述（SQL2Text），以应对LLM生成代码可能带来的潜在安全风险时对SQL查询的理解和解释的需求。我们通过使用GPT-4o引入迭代性实例级检索（Instance-Level Caching, ICL）提示，将Text2SQL数据集重新用于SQL2Text任务，从而增强了反向任务的数据集稳健性。我们的实验基于不同的样本选择方法采用上下文学习（ICL），强调使用更小、计算效率更高的LLMs。我们的研究发现表明，利用SQL固有的图性质进行ICL样本选择，在BLEU分数上比随机选择高39%，并且比其他方法提供了更好的结果。我们已将数据集和代码发布在：\url{https://github.com/aliwister/ast-icl}。
2501.03139	 | VicSim: Enhancing Victim Simulation with Emotional and Linguistic Fidelity	 | Yerong Li,Yiren Liu,Yun Huang	 | 本文介绍了VictSim（受害者模拟器），这是一种新型语言模型，用于基于场景的培训，特别关注信息忠实性、情感动态和语言风格的真实再现。通过结合GAN训练和基于关键信息的提示，VictSim在人类评估中优于GPT-4，提高了模拟受害者的真实感。	 | Scenario-based training has been widely adopted in many public service sectors. Recent advancements in Large Language Models (LLMs) have shown promise in simulating diverse personas to create these training scenarios. However, little is known about how LLMs can be developed to simulate victims for scenario-based training purposes. In this paper, we introduce VicSim (victim simulator), a novel model that addresses three key dimensions of user simulation: informational faithfulness, emotional dynamics, and language style (e.g., grammar usage). We pioneer the integration of scenario-based victim modeling with GAN-based training workflow and key-information-based prompting, aiming to enhance the realism of simulated victims. Our adversarial training approach teaches the discriminator to recognize grammar and emotional cues as reliable indicators of synthetic content. According to evaluations by human raters, the VicSim model outperforms GPT-4 in terms of human-likeness.	 | 基于场景的培训已在许多公共服务领域得到广泛应用。近年来，大型语言模型（LLMs）的进步显示出模拟各种角色以创建这些培训场景的潜力。然而，关于如何开发LLMs来模拟受害者以用于基于场景的培训目的，人们了解甚少。在本文中，我们介绍了VictSim（受害者模拟器），这是一种新型模型，它解决了用户模拟的三个关键维度：信息忠实性、情感动态和语言风格（例如语法使用）。我们首次将基于场景的受害者建模与GAN为基础的培训工作流和基于关键信息的提示结合起来，旨在提高模拟受害者的真实感。我们的对抗训练方法教会判别器将语法和情感线索识别为合成内容的可靠指标。根据人类评估员的评价，VictSim模型在人类相似度方面优于GPT-4。
2501.03124	 | PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models	 | Mingyang Song,Zhaochen Su,Xiaoye Qu,Jiawei Zhou,Yu Cheng	 | PRMBench 是一个专门用于评估过程级奖励模型细粒度错误检测能力的新基准测试，包含6,216个精心设计的问题和83,456个步骤级别标签，发现当前模型存在显著弱点，突显了进一步研究的关键方向。	 | Process-level Reward Models (PRMs) are crucial for complex reasoning and decision-making tasks, where each intermediate step plays an important role in the reasoning process. Since language models are prone to various types of errors during the reasoning process, PRMs are required to possess nuanced capabilities for detecting various implicit error types in real-world scenarios. However, current benchmarks primarily focus on step correctness, failing to evaluate PRMs' performance systematically. To address this gap, we introduce PRMBench, a process-level benchmark specifically designed to assess the fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216 carefully designed problems and 83,456 step-level labels, evaluating models across multiple dimensions, including simplicity, soundness, and sensitivity. In our experiments on 15 models, spanning both open-source PRMs and closed-source large language models prompted as critic models, we uncover significant weaknesses in current PRMs. These findings underscore the challenges inherent in process-level evaluation and highlight key directions for future research. We hope PRMBench can be a robust bench for advancing research on PRM evaluation and development.	 | 过程级奖励模型（PRMs）对于复杂的推理和决策任务至关重要，因为在这些任务中，每个中间步骤都在推理过程中起着重要作用。由于语言模型在推理过程中容易出现各种类型的错误，因此PRMs需要具备精细的能力，以便在实际场景中检测各种隐式的错误类型。然而，当前的基准测试主要集中在步骤的正确性上，未能系统地评估PRMs的表现。为了解决这一差距，我们引入了PRMBench，这是一个专门用于评估PRMs细粒度错误检测能力的过程级基准测试。PRMBench包含6,216个精心设计的问题和83,456个步骤级别标签，从多个维度评估模型，包括简单性、正确性和敏感性。在对15个模型的实验中，包括开源的PRMs和作为批评模型的闭源大型语言模型，我们发现当前PRMs存在显著的弱点。这些发现突显了过程级评估固有的挑战，并指出了未来研究的关键方向。我们希望PRMBench能够成为推动PRM评估和开发研究的一个可靠基准。
2501.03112	 | LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases	 | Dylan Bouchard,Mohit Singh Chauhan,David Skarbrevik,Viren Bajaj,Zeya Ahmad	 | LangFair是一个开源的Python包，旨在帮助大型语言模型的使用者评估和管理他们特定应用场景中的偏见和公平性风险，通过提供生成评估数据集和选择相关指标的工具。	 | Large Language Models (LLMs) have been observed to exhibit bias in numerous ways, potentially creating or worsening outcomes for specific groups identified by protected attributes such as sex, race, sexual orientation, or age. To help address this gap, we introduce LangFair, an open-source Python package that aims to equip LLM practitioners with the tools to evaluate bias and fairness risks relevant to their specific use cases. The package offers functionality to easily generate evaluation datasets, comprised of LLM responses to use-case-specific prompts, and subsequently calculate applicable metrics for the practitioner's use case. To guide in metric selection, LangFair offers an actionable decision framework.	 | 大型语言模型（LLMs）在多种方式上表现出偏见，这可能会加剧被保护属性（如性别、种族、性取向或年龄）所标识的特定群体的不利结果。为了解决这一问题，我们引入了LangFair，这是一个开源的Python包，旨在为LLM实践者提供评估与他们具体应用场景相关的偏见和公平性风险的工具。该包提供了生成评估数据集的功能，这些数据集由LLM对特定应用场景的提示生成的响应组成，然后可以计算适用于实践者应用场景的相关指标。为了指导指标选择，LangFair提供了一个可操作的决策框架。
2501.03088	 | Sentiment-guided Commonsense-aware Response Generation for Mental Health Counseling	 | Aseem Srivastava,Gauri Naik,Alison Cerezo,Tanmoy Chakraborty,Md. Shad Akhtar	 | 研究提出了一种名为EmpRes的情感导向机制，结合常识知识来生成能够有效引导客户情感的回复，显著提升了虚拟心理健康助手的效果，并通过用户研究验证了其实际应用价值和用户满意度。	 | The crisis of mental health issues is escalating. Effective counseling serves as a critical lifeline for individuals suffering from conditions like PTSD, stress, etc. Therapists forge a crucial therapeutic bond with clients, steering them towards positivity. Unfortunately, the massive shortage of professionals, high costs, and mental health stigma pose significant barriers to consulting therapists. As a substitute, Virtual Mental Health Assistants (VMHAs) have emerged in the digital healthcare space. However, most existing VMHAs lack the commonsense to understand the nuanced sentiments of clients to generate effective responses. To this end, we propose EmpRes, a novel sentiment-guided mechanism incorporating commonsense awareness for generating responses. By leveraging foundation models and harnessing commonsense knowledge, EmpRes aims to generate responses that effectively shape the client's sentiment towards positivity. We evaluate the performance of EmpRes on HOPE, a benchmark counseling dataset, and observe a remarkable performance improvement compared to the existing baselines across a suite of qualitative and quantitative metrics. Moreover, our extensive empirical analysis and human evaluation show that the generation ability of EmpRes is well-suited and, in some cases, surpasses the gold standard. Further, we deploy EmpRes as a chat interface for users seeking mental health support. We address the deployed system's effectiveness through an exhaustive user study with a significant positive response. Our findings show that 91% of users find the system effective, 80% express satisfaction, and over 85.45% convey a willingness to continue using the interface and recommend it to others, demonstrating the practical applicability of EmpRes in addressing the pressing challenges of mental health support, emphasizing user feedback, and ethical considerations in a real-world context.	 | 心理健康危机正在加剧。有效的咨询为患有PTSD、压力等状况的个体提供了至关重要的生命线。治疗师与客户建立重要的治疗关系，引导他们走向积极的一面。然而，专业人员严重短缺、高昂的成本以及心理健康污名化等问题构成了咨询治疗的主要障碍。为此，虚拟心理健康助手（VMHA）已在数字化医疗领域出现。然而，大多数现有的VMHA缺乏足够的常识来理解客户的微妙情感，从而生成有效的回复。为解决这一问题，我们提出了一种名为EmpRes的新型情感导向机制，该机制结合了常识意识来生成回复。通过利用基础模型并利用常识知识，EmpRes旨在生成能够有效引导客户情感向积极方向转变的回复。我们利用HOPE基准咨询数据集评估了EmpRes的表现，并在一系列定性和定量指标上观察到比现有基线显著的性能提升。此外，我们广泛的实证分析和用户评估表明，EmpRes的生成能力非常适用于某些情况甚至超过了黄金标准。进一步地，我们将EmpRes部署为一个聊天界面，供寻求心理健康支持的用户使用。我们通过一项详尽的用户研究来验证部署系统的有效性，研究结果表明，91%的用户认为系统有效，80%的用户表示满意，并且超过85.45%的用户表示愿意继续使用该界面并推荐给他人。这些发现表明，EmpRes在解决心理健康支持的紧迫挑战方面具有实际应用价值，并强调了在实际环境中提供用户反馈和伦理考量的重要性。
2501.03064	 | Trust Modeling in Counseling Conversations: A Benchmark Study	 | Aseem Srivastava,Zuhair Hasan Shaik,Tanmoy Chakraborty,Md Shad Akhtar	 | 该研究旨在通过分析咨询过程中患者的文本交互来评估治疗关系中的信任，提出了一个包含212次咨询会话的新数据集MENTAL-TRUST，并开发了一个基准TrustBench来量化和分类信任水平。	 | In mental health counseling, a variety of earlier studies have focused on dialogue modeling. However, most of these studies give limited to no emphasis on the quality of interaction between a patient and a therapist. The therapeutic bond between a patient and a therapist directly correlates with effective mental health counseling. It involves developing the patient's trust on the therapist over the course of counseling. To assess the therapeutic bond in counseling, we introduce trust as a therapist-assistive metric. Our definition of trust involves patients' willingness and openness to express themselves and, consequently, receive better care. We conceptualize it as a dynamic trajectory observable through textual interactions during the counseling. To facilitate trust modeling, we present MENTAL-TRUST, a novel counseling dataset comprising manual annotation of 212 counseling sessions with first-of-its-kind seven expert-verified ordinal trust levels. We project our problem statement as an ordinal classification task for trust quantification and propose a new benchmark, TrustBench, comprising a suite of classical and state-of-the-art language models on MENTAL-TRUST. We evaluate the performance across a suite of metrics and lay out an exhaustive set of findings. Our study aims to unfold how trust evolves in therapeutic interactions.	 | 在心理卫生咨询中，许多早期研究都集中在对话建模上。然而，大多数这些研究对患者和治疗师之间的互动质量关注较少。患者和治疗师之间的治疗关系直接与有效的心理卫生咨询相关。它涉及到在咨询过程中建立患者对治疗师的信任。为了评估咨询中的治疗关系，我们引入了信任作为一种治疗师辅助指标。我们对信任的定义包括患者愿意并开放地表达自己，从而获得更好的护理。我们将之视为通过咨询期间的文本交互可观察到的动态轨迹。为了促进信任建模，我们提出了一种新型咨询数据集MENTAL-TRUST，该数据集包括212次咨询会话的手动标注，并且首次采用七种专家验证的顺序信任等级。我们将问题陈述作为顺序分类任务，用于信任量化，并提出了一种新的基准——TrustBench，其中包括在MENTAL-TRUST数据集上使用经典和最先进的语言模型的多种模型。我们通过一系列指标评估了性能，并列出了详尽的研究发现。我们的研究旨在揭示信任在治疗互动中的演变过程。
2501.03035	 | Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization Degradation for Mathematical Reasoning	 | Zhen Li,Yupeng Su,Runming Yang,Zhongwei Xie,Ngai Wong,Hongxia Yang	 | 该研究评估了量化技术对大规模语言模型在数学推理任务中的影响，发现量化虽然能减少计算成本，但会以不同方式影响数值计算和推理规划能力，特别是在某些关键领域表现下降。	 | Large language models have achieved significant advancements in complex mathematical reasoning benchmarks, such as MATH. However, their substantial computational requirements present challenges for practical deployment. Model quantization has emerged as an effective strategy to reduce memory usage and computational costs by employing lower precision and bit-width representations. In this study, we systematically evaluate the impact of quantization on mathematical reasoning tasks. We introduce a multidimensional evaluation framework that qualitatively assesses specific capability dimensions and conduct quantitative analyses on the step-by-step outputs of various quantization methods. Our results demonstrate that quantization differentially affects numerical computation and reasoning planning abilities, identifying key areas where quantized models experience performance degradation.	 | 大规模语言模型在数学推理基准测试，如MATH，方面取得了显著进展。然而，它们巨大的计算需求为实际部署带来了挑战。模型量化作为一种有效策略，通过采用低精度和位宽表示，减少了内存使用和计算成本。在本研究中，我们系统地评估了量化对数学推理任务的影响。我们引入了一个多维度的评估框架，从定性的角度评估特定能力维度，并对各种量化方法的逐步输出进行了定量分析。研究结果表明，量化以不同的方式影响数值计算和推理规划能力，识别出量化模型性能下降的关键领域。
2501.03008	 | Quality Estimation based Feedback Training for Improving Pronoun Translation	 | Harshit Dhankhar,Baban Gain,Asif Ekbal,Yogesh Mani Tripathi	 | ProNMT 是一种新型框架，通过结合质量评估模型和代词生成反馈机制，逐步微调预训练的神经机器翻译模型，有效提升了代词翻译的准确性和整体翻译质量。	 | Pronoun translation is a longstanding challenge in neural machine translation (NMT), often requiring inter-sentential context to ensure linguistic accuracy. To address this, we introduce ProNMT, a novel framework designed to enhance pronoun and overall translation quality in context-aware machine translation systems. ProNMT leverages Quality Estimation (QE) models and a unique Pronoun Generation Likelihood-Based Feedback mechanism to iteratively fine-tune pre-trained NMT models without relying on extensive human annotations. The framework combines QE scores with pronoun-specific rewards to guide training, ensuring improved handling of linguistic nuances. Extensive experiments demonstrate significant gains in pronoun translation accuracy and general translation quality across multiple metrics. ProNMT offers an efficient, scalable, and context-aware approach to improving NMT systems, particularly in translating context-dependent elements like pronouns.	 | 以下是该摘要的中文翻译：  代词翻译是神经机器翻译（NMT）中的一个长期挑战，通常需要句子间的上下文来确保语言的准确性。为了解决这一问题，我们提出了ProNMT，这是一种新型框架，旨在增强上下文意识机器翻译系统中的代词和整体翻译质量。ProNMT 利用质量评估（QE）模型和一种独特的代词生成可能性反馈机制，无需依赖大量的人工标注，逐步微调预训练的NMT模型。该框架将QE分数与代词特定的奖励相结合，以引导训练，确保更好地处理语言细微差别。广泛的实验结果显示，ProNMT 在多个指标上显著提高了代词翻译准确度和整体翻译质量。ProNMT 提供了一种高效、可扩展且上下文感知的方法，以改进NMT系统，特别是在翻译依赖上下文的元素（如代词）方面。
2501.02979	 | Registering Source Tokens to Target Language Spaces in Multilingual Neural Machine Translation	 | Zhi Qu,Yiran Wang,Jiannan Mao,Chenchen Ding,Hideki Tanaka,Masao Utiyama,Taro Watanabe	 | 本文提出了一种新的解码器Only多语言神经机器翻译（MNMT）模型，通过引入注册标记改进了目标语言标记的生成，使得模型在多种语言之间的翻译性能显著提升，甚至超越了商用大规模语言模型，并且模型已开源以促进进一步研究。	 | The multilingual neural machine translation (MNMT) enables arbitrary translations across multiple languages by training a model with limited parameters using parallel data only. However, the performance of such MNMT models still lags behind that of large language models (LLMs), limiting their practicality. In this work, we address this limitation by introducing registering to achieve the new state-of-the-art of decoder-only MNMT models. Specifically, we insert a set of artificial tokens specifying the target language, called registers, into the input sequence between the source and target tokens. By modifying the attention mask, the target token generation only pays attention to the activation of registers, representing the source tokens in the target language space. Experiments on EC-40, a large-scale benchmark, show that our method outperforms related methods driven by optimizing multilingual representations. We further scale up and collect 9.3 billion sentence pairs across 24 languages from public datasets to pre-train two models, namely MITRE (multilingual translation with registers). One of them, MITRE-913M, outperforms NLLB-3.3B, achieves comparable performance with commercial LLMs, and shows strong adaptability in fine-tuning. Finally, we open-source our models to facilitate further research and development in MNMT: https://github.com/zhiqu22/mitre.	 | 多语言神经机器翻译（MNMT）通过使用平行数据训练具有有限参数的模型，能够在多种语言之间实现任意翻译。然而，这类MNMT模型的性能仍然落后于大规模语言模型（LLM），限制了其实用性。本文通过引入注册机制解决了这一限制，实现了解码器Only MNMT模型的新前沿。具体而言，我们在源语言和目标语言标记之间插入一组指明目标语言的人工标记，称为注册标记。通过修改注意力掩码，目标语言标记生成仅关注注册标记的激活，这些激活代表了源语言在目标语言空间中的表示。在EC-40大规模基准上的实验表明，我们的方法优于通过优化多语言表示驱动的相关方法。我们进一步扩大了规模，并从公共数据集中收集了跨越24种语言的93亿个句子对进行预训练，构建了两个模型，分别是MITRE（多语言翻译带有注册）。其中，MITRE-913M超越了NLLB-3.3B，在性能上与商用LLM相当，并展示了强大的微调适应性。最后，我们开源了我们的模型，以促进MNMT领域的进一步研究和开发：https://github.com/zhiqu22/mitre。
2501.02891	 | Explaining Humour Style Classifications: An XAI Approach to Understanding Computational Humour Analysis	 | Mary Ogbuka Kenneth,Foaad Khosmood,Abbas Edalat	 | 本文提出了一种可解释的人工智能（XAI）框架，用于理解幽默风格分类，并通过分析关键因素如情感模糊性、上下文误解和目标识别，揭示了不同幽默风格在特征刻画和分类错误中的不同模式，从而为计算幽默分析提供了可解释的洞察，并对心理健康和数字人文研究有所贡献。	 | Humour styles can have either a negative or a positive impact on well-being. Given the importance of these styles to mental health, significant research has been conducted on their automatic identification. However, the automated machine learning models used for this purpose are black boxes, making their prediction decisions opaque. Clarity and transparency are vital in the field of mental health. This paper presents an explainable AI (XAI) framework for understanding humour style classification, building upon previous work in computational humour analysis. Using the best-performing single model (ALI+XGBoost) from prior research, we apply comprehensive XAI techniques to analyse how linguistic, emotional, and semantic features contribute to humour style classification decisions. Our analysis reveals distinct patterns in how different humour styles are characterised and misclassified, with particular emphasis on the challenges in distinguishing affiliative humour from other styles. Through detailed examination of feature importance, error patterns, and misclassification cases, we identify key factors influencing model decisions, including emotional ambiguity, context misinterpretation, and target identification. The framework demonstrates significant utility in understanding model behaviour, achieving interpretable insights into the complex interplay of features that define different humour styles. Our findings contribute to both the theoretical understanding of computational humour analysis and practical applications in mental health, content moderation, and digital humanities research.	 | 幽默风格对幸福感可能产生负面影响，也可能产生正面影响。由于这些风格对心理健康的重要性，相关研究对它们的自动识别进行了大量探索。然而，用于这一目的的自动化机器学习模型如同“黑箱”，其预测决策过程不透明。在心理健康领域，透明性和清晰性是非常重要的。本文提出了一种可解释的人工智能（XAI）框架，用于理解幽默风格分类，并在此基础上建立在计算幽默分析领域的前期工作之上。我们使用之前研究中表现最佳的单个模型（ALI+XGBoost），并应用全面的XAI技术来分析语言、情感和语义特征如何影响幽默风格分类决策。我们的分析揭示了不同幽默风格在特征刻画和分类错误中的不同模式，特别是区分关联性幽默与其他风格的挑战尤为突出。通过对特征重要性、错误模式和分类错误案例的详细分析，我们确定了影响模型决策的关键因素，包括情感模糊性、上下文误解和目标识别。该框架在理解模型行为方面展示了显著的实用性，提供了对不同幽默风格所定义的复杂特征间相互作用的可解释洞察。我们的研究结果不仅有助于计算幽默分析的理论理解，还为心理健康、内容审核和数字人文研究的实际应用提供了贡献。
2501.02844	 | Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text Classification	 | Yubo Wang,Haoyang Li,Fei Teng,Lei Chen	 | GORAG是一种基于图的在线检索增强生成框架，用于动态少量标注的文本分类，通过构建自适应信息图和使用最小成本生成树来优先检索相关上下文，从而提供更为全面和准确的分类结果。	 | Text classification is a fundamental task in natural language processing, pivotal to various applications such as query optimization, data integration, and schema matching. While neural network-based models, such as CNN and BERT, have demonstrated remarkable performance in text classification, their effectiveness heavily relies on abundant labeled training data. This dependency makes these models less effective in dynamic few-shot text classification, where labeled data is scarce, and target labels frequently evolve based on application needs. Recently, large language models (LLMs) have shown promise due to their extensive pretraining and contextual understanding. Current approaches provide LLMs with text inputs, candidate labels, and additional side information (e.g., descriptions) to predict text labels. However, their effectiveness is hindered by the increased input size and the noise introduced through side information processing. To address these limitations, we propose a graph-based online retrieval-augmented generation framework, namely GORAG, for dynamic few-shot text classification. GORAG constructs and maintains an adaptive information graph by extracting side information across all target texts, rather than treating each input independently. It employs a weighted edge mechanism to prioritize the importance and reliability of extracted information and dynamically retrieves relevant context using a minimum-cost spanning tree tailored for each text input. Empirical evaluations demonstrate that GORAG outperforms existing approaches by providing more comprehensive and accurate contextual information.	 | 文本分类是自然语言处理中的一个基本任务，对于诸如查询优化、数据集成和模式匹配等各种应用至关重要。虽然基于神经网络的模型，如CNN和BERT，在文本分类中表现出色，但它们的有效性很大程度上依赖于充足的标注训练数据。这种依赖性使得这些模型在动态少量标注的文本分类中不太有效，因为在这种情况下，标注数据稀缺，目标标签会频繁根据应用需求发生变化。最近，大型语言模型（LLMs）因其广泛的预训练和上下文理解能力显示出潜力。当前的方法通过向LLMs提供文本输入、候选标签以及额外的辅助信息（例如描述）来预测文本标签。然而，这些方法的有效性受到输入大小增加以及通过处理辅助信息引入的噪声的限制。为了解决这些限制，我们提出了一种基于图的在线检索增强生成框架，称为GORAG，用于动态少量标注的文本分类。GORAG通过从所有目标文本中提取辅助信息来构建和维护一个自适应信息图，而不是独立处理每个输入。它采用加权边机制来优先考虑提取信息的重要性和可靠性，并使用为每个文本输入量身定制的最小成本生成树动态检索相关上下文。实证评估表明，GORAG通过提供更为全面和准确的上下文信息优于现有方法。
2501.02832	 | Samba-asr state-of-the-art speech recognition leveraging structured state-space models	 | Syed Abdul Gaffar Shakhadri,Kruthika KR,Kartik Basavaraj Angadi	 | 本文提出了一种名为Samba ASR的新颖自动语音识别模型，采用Mamba架构并基于状态空间模型，通过有效建模局部和全局时间依赖关系，显著提升了识别准确性和效率，超越了现有开源的基于Transformer的ASR模型。	 | We propose Samba ASR, the first state-of-the-art Automatic Speech Recognition (ASR) model leveraging the novel Mamba architecture as both encoder and decoder, built on the foundation of state-space models (SSMs). Unlike transformer-based ASR models, which rely on self-attention mechanisms to capture dependencies, Samba ASR effectively models both local and global temporal dependencies using efficient state-space dynamics, achieving remarkable performance gains. By addressing the limitations of transformers, such as quadratic scaling with input length and difficulty in handling long-range dependencies, Samba ASR achieves superior accuracy and efficiency.   Experimental results demonstrate that Samba ASR surpasses existing open-source transformer-based ASR models across various standard benchmarks, establishing it as the new state of the art in ASR. Extensive evaluations on benchmark datasets show significant improvements in Word Error Rate (WER), with competitive performance even in low-resource scenarios. Furthermore, the computational efficiency and parameter optimization of the Mamba architecture make Samba ASR a scalable and robust solution for diverse ASR tasks.   Our contributions include:   A new Samba ASR architecture demonstrating the superiority of SSMs over transformer-based models for speech sequence processing. A comprehensive evaluation on public benchmarks showcasing state-of-the-art performance. An analysis of computational efficiency, robustness to noise, and sequence generalization. This work highlights the viability of Mamba SSMs as a transformer-free alternative for efficient and accurate ASR. By leveraging state-space modeling advancements, Samba ASR sets a new benchmark for ASR performance and future research.	 | 我们提出了一种名为Samba ASR的新颖自动语音识别（ASR）模型，该模型采用了全新的Mamba架构作为编码器和解码器，并基于状态空间模型（SSMs）构建。与依赖于自我注意机制来捕捉依赖关系的基于Transformer的ASR模型不同，Samba ASR 利用高效的状态空间动力学来有效建模局部和全局的时间依赖关系，从而实现了显著的性能提升。通过克服Transformer的局限性，如输入长度的二次扩展和处理长距离依赖困难的问题，Samba ASR 达到了更高的准确性和效率。  实验证明，Samba ASR 在各种标准基准测试中超越了现有的开源基于Transformer的ASR模型，确立了其在ASR领域的最新前沿地位。在基准数据集上的广泛测试显示，在Word Error Rate (WER) 方面取得了显著改进，即使在资源有限的情况下也能保持竞争力。此外，Mamba架构的计算效率和参数优化使Samba ASR 成为了一个可扩展且稳健的ASR解决方案，适用于各种任务。  我们的贡献包括：  1. 一种新的Samba ASR架构，展示了状态空间模型（SSMs）在语音序列处理方面优于基于Transformer模型的优势。 2. 在开源基准测试上的全面评估，展示了最先进的性能。 3. 关于计算效率、对噪声的鲁棒性和序列泛化能力的分析。这项工作突显了Mamba状态空间模型作为无Transformer替代方案的可行性，能够实现高效和准确的ASR。通过利用状态空间建模的进步，Samba ASR 为ASR性能和未来研究设定了新的基准。
2501.02795	 | InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via LLM Fusion	 | Zhaoyi Yan,Zhijie Sang,Yiming Zhang,Yuhao Fu,Baoyi He,Qi Zhou,Yining Di,Chunlin Ji,Shengyu Zhang,Fei Wu,Hongxia Yang	 | 本文提出了一种将多个领域专业化的大模型整合到一个高效枢纽模型中的策略，通过两两多步融合方法和统一融合方法结合多个大型语言模型的优势，并引入了速率-偏斜自适应融合（RSAF）技术及基于不确定性加权的方法以改进融合过程，最终在多项推理任务中显著提升了模型的性能。	 | Large Language Models (LLMs) have demonstrated strong performance across various reasoning tasks, yet building a single model that consistently excels across all domains remains challenging. This paper addresses this problem by exploring strategies to integrate multiple domain-specialized models into an efficient pivot model.We propose two fusion strategies to combine the strengths of multiple LLMs: (1) a pairwise, multi-step fusion approach that sequentially distills each source model into the pivot model, followed by a weight merging step to integrate the distilled models into the final model. This method achieves strong performance but requires substantial training effort; and (2) a unified fusion approach that aggregates all source models' outputs simultaneously.To improve the fusion process, we introduce a novel Rate-Skewness Adaptive Fusion (RSAF) technique, which dynamically adjusts top-K ratios during parameter merging for enhanced flexibility and stability.Furthermore, we propose an uncertainty-based weighting method for the unified approach, which dynamically balances the contributions of source models and outperforms other logits/distribution ensemble methods.We achieved accuracy improvements of 9.27%, 8.80%, and 8.89% on the GSM8K, MATH, and HumanEval tasks, respectively.	 | 大型语言模型（LLMs）已经在各种推理任务中展示了出色的性能，但构建一个能够在所有领域都持续表现出色的单一模型仍然具有挑战性。本文通过探索将多个领域专业化模型整合到一个高效枢纽模型中的策略来解决这一问题。我们提出了两种融合策略，以结合多个LLM的优势：（1）一种两两多步融合方法，该方法首先逐步将每个源模型提炼到枢纽模型中，然后通过权重合并步骤将提炼后的模型集成到最终模型中。这种方法取得了很好的性能，但需要大量的训练努力；（2）一种统一的融合方法，该方法同时聚合所有源模型的输出。  为了改进融合过程，我们引入了一种新的速率-偏斜自适应融合（RSAF）技术，该技术在参数合并过程中动态调整Top-K比率，以增强灵活性和稳定性。此外，我们还为统一方法提出了基于不确定性加权的方法，该方法动态平衡了源模型的贡献，并在其他概率/分布集成方法中表现出色。在GSM8K、MATH和HumanEval任务中，我们分别实现了9.27%、8.80%和8.89%的准确率提升。
2501.02790	 | Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model	 | Yueqin Yin,Shentao Yang,Yujia Xie,Ziyi Yang,Yuting Sun,Hany Awadalla,Weizhu Chen,Mingyuan Zhou	 | 本文提出了一种结合片段级别的奖励模型和位置感知的归一化方法的强化学习结合人类反馈（RLHF）技术，以改进语言模型与人类偏好的一致性，该方法在多个基准测试中取得了与AlpacaEval 2.0、Arena-Hard和MT-Bench相当的性能。	 | Reinforcement learning from human feedback (RLHF) has been widely adopted to align language models (LMs) with human preference. Prior RLHF works typically take a bandit formulation, which, though intuitive, ignores the sequential nature of LM generation and can suffer from the sparse reward issue. While recent works propose dense token-level RLHF, treating each token as an action may be oversubtle to proper reward assignment. In this paper, we seek to get the best of both by training and utilizing a segment-level reward model, which assigns a reward to each semantically complete text segment that spans over a short sequence of tokens. For reward learning, our method allows dynamic text segmentation and compatibility with standard sequence-preference datasets. For effective RL-based LM training against segment reward, we generalize the classical scalar bandit reward normalizers into location-aware normalizer functions and interpolate the segment reward for further densification. With these designs, our method performs competitively on three popular RLHF benchmarks for LM policy: AlpacaEval 2.0, Arena-Hard, and MT-Bench. Ablation studies are conducted to further demonstrate our method.	 | 强化学习结合人类反馈（RLHF）被广泛用于使语言模型（LMs）与人类偏好相一致。之前的RLHF工作通常采用多臂老虎机的形式，尽管直观，但忽略了LM生成的序列性特征，并且可能会受到稀疏奖励问题的影响。近期的工作提出了基于密集标记级别的RLHF，但这可能过于精细地将每个标记视为动作，从而影响奖励分配。在本文中，我们试图兼顾这两个方面，通过训练和利用基于片段的奖励模型来获取最佳效果，该模型为跨越短序列标记的每个语义完整文本片段分配一个奖励。在奖励学习方面，我们的方法允许动态文本分割，并兼容标准序列偏好数据集。为了有效地针对片段奖励进行基于强化学习的LM训练，我们将经典的标量多臂老虎机奖励归一化方法推广为位置感知的归一化函数，并对片段奖励进行插值以进一步密集化。凭借这些设计，我们的方法在三个流行的RLHF基准测试中表现得与AlpacaEval 2.0、Arena-Hard和MT-Bench相当。我们还进行了消融研究以进一步证明我们的方法。
2501.02739	 | TARDiS : Text Augmentation for Refining Diversity and Separability	 | Kyungmin Kim,SangHun Im,GiBaeg Kim,Heung-Seon Oh	 | 本文提出了一种基于大型语言模型（LLM）的新型文本增强方法TARDiS，通过两种生成过程和类别自适应对齐方法解决文本分类中少量样本的挑战，并在多个任务中表现出色。	 | Text augmentation (TA) is a critical technique for text classification, especially in few-shot settings. This paper introduces a novel LLM-based TA method, TARDiS, to address challenges inherent in the generation and alignment stages of two-stage TA methods. For the generation stage, we propose two generation processes, SEG and CEG, incorporating multiple class-specific prompts to enhance diversity and separability. For the alignment stage, we introduce a class adaptation (CA) method to ensure that generated examples align with their target classes through verification and modification. Experimental results demonstrate TARDiS's effectiveness, outperforming state-of-the-art LLM-based TA methods in various few-shot text classification tasks. An in-depth analysis confirms the detailed behaviors at each stage.	 | 文本增强（TA）是文本分类中的关键技术，特别是在少量样本设置中尤为重要。本文介绍了一种基于LLM的新颖TA方法TARDiS，以解决两类TA方法在生成和对齐阶段固有挑战。对于生成阶段，我们提出两种生成过程，即SEG和CEG，并结合多个类特定的提示，以增强多样性和可区分性。对于对齐阶段，我们引入了一种类别自适应（CA）方法，通过验证和修改确保生成的示例与目标类对齐。实验结果表明，TARDiS在各种少量样本文本分类任务中优于最新的基于LLM的TA方法。深入分析进一步确认了每个阶段的具体行为。
2501.02702	 | QuIM-RAG: Advancing Retrieval-Augmented Generation with Inverted Question Matching for Enhanced QA Performance	 | Binita Saha,Utsha Saha,Muhammad Zubair Malik	 | 本文提出了一种新的检索增强生成（RAG）系统架构，通过将语料库转换为特定领域的数据集，并引入QuIM-RAG机制，从文档片段中生成潜在问题并与用户查询匹配，以提高目标语料库中复杂问题的回答准确性。实验结果表明，该方法在BERT-Score和RAGAS指标上优于传统的RAG架构。	 | This work presents a novel architecture for building Retrieval-Augmented Generation (RAG) systems to improve Question Answering (QA) tasks from a target corpus. Large Language Models (LLMs) have revolutionized the analyzing and generation of human-like text. These models rely on pre-trained data and lack real-time updates unless integrated with live data tools. RAG enhances LLMs by integrating online resources and databases to generate contextually appropriate responses. However, traditional RAG still encounters challenges like information dilution and hallucinations when handling vast amounts of data. Our approach addresses these challenges by converting corpora into a domain-specific dataset and RAG architecture is constructed to generate responses from the target document. We introduce QuIM-RAG (Question-to-question Inverted Index Matching), a novel approach for the retrieval mechanism in our system. This strategy generates potential questions from document chunks and matches these with user queries to identify the most relevant text chunks for generating accurate answers. We have implemented our RAG system on top of the open-source Meta-LLaMA3-8B-instruct model by Meta Inc. that is available on Hugging Face. We constructed a custom corpus of 500+ pages from a high-traffic website accessed thousands of times daily for answering complex questions, along with manually prepared ground truth QA for evaluation. We compared our approach with traditional RAG models using BERT-Score and RAGAS, state-of-the-art metrics for evaluating LLM applications. Our evaluation demonstrates that our approach outperforms traditional RAG architectures on both metrics.	 | 本文提出了一种新的架构，用于构建检索增强生成（RAG）系统，以提高来自目标语料库的问题回答（QA）任务。大规模语言模型（LLMs）已经彻底改变了人类样式的文本分析和生成。这些模型依赖于预训练数据，并且除非与实时数据工具集成外，否则缺乏实时更新。RAG通过集成在线资源和数据库来增强LLMs，生成上下文相关性回答。然而，传统的RAG在处理大量数据时仍然面临信息稀释和幻觉等挑战。我们的方法通过将语料库转换为特定领域的数据集来解决这些问题，并构建了一个RAG架构，可以从目标文档中生成回答。我们引入了QuIM-RAG（Question-to-question Inverted Index Matching），这是一种在系统中检索机制的新颖方法。该策略从文档片段中生成潜在问题，并将这些问题与用户查询匹配，以识别生成准确答案的最相关文本片段。我们基于Meta Inc.开源的Meta-LLaMA3-8B-instruct模型构建了我们的RAG系统，该模型可在Hugging Face获得。我们从一个每日被访问数千次的高流量网站中构建了一个包含500多页的自定义语料库，用于回答复杂问题，并准备了手动准备的地面真实QA进行评估。我们使用BERT-Score和RAGAS等最先进的指标与传统的RAG模型进行了比较，RAGAS是评估LLMs应用的指标。我们的评估表明，我们的方法在两个指标上都优于传统的RAG架构。
2501.02688	 | Decoding specialised feature neurons in LLMs with the final projection layer	 | Harry J Davies	 | 该研究通过TARS知识移除方法揭示了大型语言模型（LLMs）中残差流线性层权重与特定概念的关联，并进一步通过模型的最终投影层直接解码神经元权重为标记概率，展示了对模型内部机制的可解释性改进，从而能够控制模型输出。	 | Large Language Models (LLMs) typically have billions of parameters and are thus often difficult to interpret in their operation. Such black-box models can pose a significant risk to safety when trusted to make important decisions. The lack of interpretability of LLMs is more related to their sheer size, rather than the complexity of their individual components. The TARS method for knowledge removal (Davies et al 2024) provides strong evidence for the hypothesis that that linear layer weights which act directly on the residual stream may have high correlation with different concepts encoded in the residual stream. Building upon this, we attempt to decode neuron weights directly into token probabilities through the final projection layer of the model (the LM-head). Firstly, we show that with Llama 3.1 8B we can utilise the LM-head to decode specialised feature neurons that respond strongly to certain concepts, with examples such as "dog" and "California". This is then confirmed by demonstrating that these neurons can be clamped to affect the probability of the concept in the output. This extends to the fine-tuned assistant Llama 3.1 8B instruct model, where we find that over 75% of neurons in the up-projection layers have the same top associated token compared to the pretrained model. Finally, we demonstrate that clamping the "dog" neuron leads the instruct model to always discuss dogs when asked about its favourite animal. Through our method, it is possible to map the entirety of Llama 3.1 8B's up-projection neurons in less than 15 minutes with no parallelization.	 | 大型语言模型（LLMs）通常包含数十亿个参数，因此在解释其运行机制时往往非常困难。这类“黑箱”模型在承担重要决策任务时可能会带来重大的安全风险。LLMs 的不可解释性更多与其庞大的规模有关，而非其组成部分的复杂性。Davies 等人（2024）提出的 TARS 知识移除方法提供了强有力的证据，证明直接作用于残差流的线性层权重可能与残差流中编码的不同概念高度相关。基于这一发现，我们尝试通过模型的最终投影层（即语言模型头）将神经元权重直接解码为标记概率。首先，我们展示，使用 Llama 3.1 8B，可以利用语言模型头解码对某些概念有强烈响应的专门特征神经元，例如“狗”和“加利福尼亚”。随后，我们通过证明这些神经元可以通过钳制来影响输出中概念的概率，进一步验证了这一点。这一方法扩展到微调后的助手 Llama 3.1 8B 指令模型，我们发现超过 75% 的上投影层神经元与预训练模型具有相同的最相关标记。最后，我们展示了钳制“狗”神经元会导致指令模型在被问及最喜爱的动物时总是谈论狗。通过我们的方法，可以不到 15 分钟内不使用并行化的方法映射 Llama 3.1 8B 的所有上投影神经元。
2501.02683	 | From Superficial Patterns to Semantic Understanding: Fine-Tuning Language Models on Contrast Sets	 | Daniel Petrov	 | 大规模预训练语言模型在标准自然语言推理数据集上表现出色，但在分布外测试集上表现不佳，尤其是在包含经过微小改动的数据实例的对比集上。通过在训练中引入少量更复杂的对比集数据，模型的鲁棒性得到提升，在对比集上的准确率恢复到接近90%。	 | Large scale pretrained language models have demonstrated high performance on standard datasets for natural language inference (NLI) tasks. Unfortunately, these evaluations can be misleading, as although the models can perform well on in-distribution data, they perform poorly on out-of-distribution test sets, such as contrast sets. Contrast sets consist of perturbed instances of data that have very minor, but meaningful, changes to the input that alter the gold label, revealing how models can learn superficial patterns in the training data rather than learning more sophisticated language nuances. As an example, the ELECTRA-small language model achieves nearly 90% accuracy on an SNLI dataset but drops to 75% when tested on an out-of-distribution contrast set. The research performed in this study explores how a language models' robustness can be improved by exposing it to small amounts of more complex contrast sets during training to help it better learn language patterns. With this approach, the model regains performance and achieves nearly 90% accuracy on contrast sets, highlighting the importance of diverse and challenging training data.	 | 大规模预训练语言模型在自然语言推理（NLI）任务的标准数据集上表现出色。不幸的是，这些评估可能具有误导性，因为尽管模型在分布内数据上表现良好，但在分布外测试集（如对比集）上表现不佳。对比集包含经过微小但有意义的改动的数据实例，这些改动改变了正确答案，揭示了模型如何学习训练数据中的表面模式而非更复杂的语言细微差别。例如，ELECTRA-small 模型在 SNLI 数据集上的准确率接近 90%，但在分布外对比集上的准确率下降到 75%。本研究中进行的研究探讨了通过在训练过程中让模型暴露于少量更复杂的对比集，如何提高模型的鲁棒性。通过这种方法，模型恢复了性能，并在对比集上达到了近 90% 的准确率，强调了多样性和挑战性训练数据的重要性。
2501.02654	 | Tougher Text, Smarter Models: Raising the Bar for Adversarial Defence Benchmarks	 | Yang Wang,Chenghua Lin	 | 本文提出了一个广泛的文本对抗防御基准，涵盖了多种数据集和关键自然语言处理任务，旨在全面评估现有防御机制，并为未来研究提供资源。	 | vulnerability of deep learning models to adversarial attacks. While various defence mechanisms have been proposed, there is a lack of comprehensive benchmarks that evaluate these defences across diverse datasets, models, and tasks. In this work, we address this gap by presenting an extensive benchmark for textual adversarial defence that significantly expands upon previous work. Our benchmark incorporates a wide range of datasets, evaluates state-of-the-art defence mechanisms, and extends the assessment to include critical tasks such as single-sentence classification, similarity and paraphrase identification, natural language inference, and commonsense reasoning. This work not only serves as a valuable resource for researchers and practitioners in the field of adversarial robustness but also identifies key areas for future research in textual adversarial defence. By establishing a new standard for benchmarking in this domain, we aim to accelerate progress towards more robust and reliable natural language processing systems.	 | 深度学习模型对对抗攻击的脆弱性。虽然已经提出了各种防御机制，但在不同数据集、模型和任务上的全面基准评估仍然不足。在本文中，我们通过提出一个广泛的文本对抗防御基准来填补这一空白，显著扩展了先前的工作。我们的基准涵盖了广泛的數據集，评估了最先进的防御机制，并将评估扩展到包括单句分类、相似性和同义句识别、自然语言推理和常识推理等关键任务。本研究不仅为对抗鲁棒性领域的研究人员和实践者提供了宝贵的资源，还指出了文本对抗防御中未来研究的关键领域。通过在该领域建立新的基准标准，我们旨在加速朝着更稳健和可靠的自然语言处理系统的发展。
2501.02631	 | Prune or Retrain: Optimizing the Vocabulary of Multilingual Models for Estonian	 | Aleksei Dorkin,Taido Purason,Kairit Sirts	 | 这项研究探讨了通过改进多语言编码器模型的词汇表来更好地适应爱沙尼亚语，从而提升命名实体识别任务的性能和效率，并评估了重新训练分词器和去除未使用标记两种方法的影响。研究表明，尽管重新训练分词器降低了NER任务的性能，但剪枝未对模型性能产生负面影响。	 | Adapting multilingual language models to specific languages can enhance both their efficiency and performance. In this study, we explore how modifying the vocabulary of a multilingual encoder model to better suit the Estonian language affects its downstream performance on the Named Entity Recognition (NER) task. The motivations for adjusting the vocabulary are twofold: practical benefits affecting the computational cost, such as reducing the input sequence length and the model size, and performance enhancements by tailoring the vocabulary to the particular language. We evaluate the effectiveness of two vocabulary adaptation approaches -- retraining the tokenizer and pruning unused tokens -- and assess their impact on the model's performance, particularly after continual training. While retraining the tokenizer degraded the performance of the NER task, suggesting that longer embedding tuning might be needed, we observed no negative effects on pruning.	 | 适应多语言语言模型特定语言的能力可以同时提高它们的效率和性能。在这项研究中，我们探讨了如何通过改进多语言编码器模型的词汇表以更好地适应爱沙尼亚语，从而影响其在命名实体识别（NER）任务上的下游性能。调整词汇表的动机有两个方面：一是实践上的好处，如减少输入序列长度和模型大小，从而降低计算成本；二是通过针对特定语言进行词汇表调整来提高性能。我们评估了两种词汇表适应方法——重新训练分词器和去除未使用的标记，并评估这些方法对模型性能的影响，尤其是在持续训练之后。尽管重新训练分词器降低了NER任务的性能，暗示可能需要更长时间的嵌入调整，但我们并未观察到剪枝有任何负面影响。
2501.02599	 | Empowering Bengali Education with AI: Solving Bengali Math Word Problems through Transformer Models	 | Jalisha Jashim Era,Bidyarthi Paul,Tahmid Sattar Aothoi,Mirazur Rahman Zim,Faisal Muhammad Shah	 | 本文通过开发基于变换器的模型，特别是mT5，解决了孟加拉语数学文字问题的挑战，并通过“PatiGonit”数据集的构建与模型微调，取得了97.30%的高准确性，显著推动了孟加拉语自然语言处理领域的发展。	 | Mathematical word problems (MWPs) involve the task of converting textual descriptions into mathematical equations. This poses a significant challenge in natural language processing, particularly for low-resource languages such as Bengali. This paper addresses this challenge by developing an innovative approach to solving Bengali MWPs using transformer-based models, including Basic Transformer, mT5, BanglaT5, and mBART50. To support this effort, the "PatiGonit" dataset was introduced, containing 10,000 Bengali math problems, and these models were fine-tuned to translate the word problems into equations accurately. The evaluation revealed that the mT5 model achieved the highest accuracy of 97.30%, demonstrating the effectiveness of transformer models in this domain. This research marks a significant step forward in Bengali natural language processing, offering valuable methodologies and resources for educational AI tools. By improving math education, it also supports the development of advanced problem-solving skills for Bengali-speaking students.	 | 数学文字问题（MWPs）涉及将文本描述转换为数学方程的任务。这在自然语言处理中提出了重大挑战，特别是在如孟加拉语这类低资源语言中。本文通过开发一种创新方法来解决孟加拉语MWPs问题，采用了基于变换器的模型，包括基本变换器、mT5、BanglaT5和mBART50。为了支持这一努力，引入了“PatiGonit”数据集，包含10,000个孟加拉数学问题，并对这些模型进行了微调，使其能够准确地将文字问题翻译成方程。评估结果显示，mT5模型的准确性最高，达到97.30%，这表明变换器模型在这一领域非常有效。这项研究标志着孟加拉语自然语言处理领域的重要进展，提供了有价值的方法和资源，以支持教育AI工具的发展。通过提高数学教育，它还支持孟加拉语 Speaking 学生的高级问题解决技能的发展。
2501.02598	 | GIT-CXR: End-to-End Transformer for Chest X-Ray Report Generation	 | Iustin Sîrbu,Iulia-Renata Sîrbu,Jasmina Bogojeska,Traian Rebedea	 | 该研究设计并评估了一种基于变压器的端到端方法，用于自动化生成准确的放射学报告，并在MIMIC-CXR-JPG数据库上展示了其在多个评估指标上的优越性能，包括临床准确性和自然语言生成质量。	 | Medical imaging is crucial for diagnosing, monitoring, and treating medical conditions. The medical reports of radiology images are the primary medium through which medical professionals attest their findings, but their writing is time consuming and requires specialized clinical expertise. The automated generation of radiography reports has thus the potential to improve and standardize patient care and significantly reduce clinicians workload. Through our work, we have designed and evaluated an end-to-end transformer-based method to generate accurate and factually complete radiology reports for X-ray images. Additionally, we are the first to introduce curriculum learning for end-to-end transformers in medical imaging and demonstrate its impact in obtaining improved performance. The experiments have been conducted using the MIMIC-CXR-JPG database, the largest available chest X-ray dataset. The results obtained are comparable with the current state-of-the-art on the natural language generation (NLG) metrics BLEU and ROUGE-L, while setting new state-of-the-art results on F1 examples-averaged, F1-macro and F1-micro metrics for clinical accuracy and on the METEOR metric widely used for NLG.	 | 医学成像是诊断、监测和治疗医学状况的关键。放射影像报告是医疗专业人员确认其发现的主要媒介，但撰写这些报告耗时且需要特定的临床专业知识。因此，自动化生成放射学报告有可能提高和标准化患者护理标准，并显著减轻临床工作者的工作负担。通过我们的工作，我们设计并评估了一种端到端的变压器基方法，用于生成准确且事实完整的X射线影像报告。此外，我们首次在医学影像中引入了课程学习方法，并展示了其在提高性能方面的影响。这些实验使用了MIMIC-CXR-JPG数据库进行，这是目前可用的最大规模胸部X射线数据集。所获得的成果在自然语言生成（NLG）指标BLEU和ROUGE-L上与当前最先进的技术相当，同时在临床准确性指标F1例子平均值、F1宏平均值和F1微平均值上建立了新的最先进的结果，并且在广泛用于NLG的METEOR指标上也有所提高。
2501.02552	 | Multi-LLM Collaborative Caption Generation in Scientific Documents	 | Jaeyoung Kim,Jongho Lee,Hong-Jun Choi,Ting-Yao Hsu,Chieh-Yang Huang,Sungchul Kim,Ryan Rossi,Tong Yu,Clyde Lee Giles,Ting-Hao 'Kenneth' Huang,Sungchul Choi	 | 本文提出了一种名为MLBCAP的多大型语言模型协作科学图表标注框架，该框架通过多模态LLM质量评估、多样化的描述生成和最终判断模块，生成高质量的图表描述，有效解决了现有方法的不足，并通过人类评估证明了其优越性。	 | Scientific figure captioning is a complex task that requires generating contextually appropriate descriptions of visual content. However, existing methods often fall short by utilizing incomplete information, treating the task solely as either an image-to-text or text summarization problem. This limitation hinders the generation of high-quality captions that fully capture the necessary details. Moreover, existing data sourced from arXiv papers contain low-quality captions, posing significant challenges for training large language models (LLMs). In this paper, we introduce a framework called Multi-LLM Collaborative Figure Caption Generation (MLBCAP) to address these challenges by leveraging specialized LLMs for distinct sub-tasks. Our approach unfolds in three key modules: (Quality Assessment) We utilize multimodal LLMs to assess the quality of training data, enabling the filtration of low-quality captions. (Diverse Caption Generation) We then employ a strategy of fine-tuning/prompting multiple LLMs on the captioning task to generate candidate captions. (Judgment) Lastly, we prompt a prominent LLM to select the highest quality caption from the candidates, followed by refining any remaining inaccuracies. Human evaluations demonstrate that informative captions produced by our approach rank better than human-written captions, highlighting its effectiveness. Our code is available at https://github.com/teamreboott/MLBCAP	 | 科学图表标注是一项复杂的任务，需要生成与上下文相适应的视觉内容描述。然而，现有的方法往往陷入困境，仅利用不完整的信息，或单纯地将其视为图像到文本或文本总结问题。这种限制妨碍了生成能够充分捕捉必要细节的高质量描述。此外，现有数据来源自arXiv论文，其中包含低质量的描述，这对训练大型语言模型（LLMs）构成了重大挑战。在本文中，我们提出了一种名为Multi-LLM Collaborative Figure Caption Generation (MLBCAP) 的框架，通过利用专门针对不同子任务的LLM来应对这些挑战。我们的方法分为三个关键模块：（质量评估）我们使用多模态LLM来评估训练数据的质量，从而筛选出低质量的描述。（多样化的描述生成）然后，我们采用对多个LLM进行微调/提示的方法来完成描述任务，生成候选描述。（判断）最后，我们提示一个 prominant LLM 从候选描述中选择最高质量的描述，并进一步修正任何剩余的不准确性。人类评估表明，我们方法生成的信息性描述比人工撰写的描述排名更高，突显了其有效性。我们的代码可在 https://github.com/teamreboott/MLBCAP 获取。
2501.02549	 | From Language To Vision: A Case Study of Text Animation	 | Ping Chen,Richard Alo,Justin Rundell	 | 本文提出了一种文本可视化系统，利用动画将自由文本转换为可视化形式，并通过示例句子说明了如何基于基础物理定律进行可视化。	 | Information can be expressed in multiple formats including natural language, images, and motions. Human intelligence usually faces little difficulty to convert from one format to another format, which often shows a true understanding of encoded information. Moreover, such conversions have broad application in many real-world applications. In this paper, we present a text visualization system that can visualize free text with animations. Our system is illustrated by visualizing example sentences of elementary Physics laws.	 | 信息可以以多种格式表达，包括自然语言、图像和动作。人类智能通常在从一种格式转换为另一种格式时几乎没有困难，这通常表明对编码信息的真正理解。此外，这种转换在许多实际应用中都有广泛的应用。在本文中，我们提出了一种文本可视化系统，该系统可以使用动画来可视化自由文本。我们的系统通过可视化基础物理定律的示例句子进行了说明。
2501.02546	 | TreeMatch: A Fully Unsupervised WSD System Using Dependency Knowledge on a Specific Domain	 | Andrew Tran,Chris Bowes,David Brown,Ping Chen,Max Choly,Wei Ding	 | TreeMatch是一个无监督的词义消歧系统，最初用于SemEval 2007任务7，后改编用于SemEval 2010任务17，该系统在特定领域全词消歧任务中表现出了较高的准确性，超过了最常见基线。	 | Word sense disambiguation (WSD) is one of the main challenges in Computational Linguistics. TreeMatch is a WSD system originally developed using data from SemEval 2007 Task 7 (Coarse-grained English All-words Task) that has been adapted for use in SemEval 2010 Task 17 (All-words Word Sense Disambiguation on a Specific Domain). The system is based on a fully unsupervised method using dependency knowledge drawn from a domain specific knowledge base that was built for this task. When evaluated on the task, the system precision performs above the Most Frequent Selection baseline.	 | 词义消歧（WSD）是计算语言学中的主要挑战之一。TreeMatch 是一个最初使用 SemEval 2007 任务 7（粗粒度英语全词任务）数据开发的 WSD 系统，后来被改编用于 SemEval 2010 任务 17（特定领域全词词义消歧）。该系统基于一种完全无监督的方法，利用从特定领域知识库中获取的依存知识。在该任务中评估时，该系统的精准度超过了最频繁选择基线。
2501.02532	 | Evaluating Large Language Models Against Human Annotators in Latent Content Analysis: Sentiment, Political Leaning, Emotional Intensity, and Sarcasm	 | Ljubisa Bojic,Olga Zagovora,Asta Zelenkauskaite,Vuk Vukovic,Milan Cabarkapa,Selma Veseljević Jerkovic,Ana Jovančevic	 | 本研究评估了七种最先进的语言模型在情感分析、政治倾向、情绪强度和讽刺检测方面的表现，结果显示这些模型在情感分析和政治倾向评估方面与人类高度一致，且在时间上表现出稳定性和优于人类的内部一致性，尽管在情绪强度和讽刺检测方面仍存在一些差异。	 | In the era of rapid digital communication, vast amounts of textual data are generated daily, demanding efficient methods for latent content analysis to extract meaningful insights. Large Language Models (LLMs) offer potential for automating this process, yet comprehensive assessments comparing their performance to human annotators across multiple dimensions are lacking. This study evaluates the reliability, consistency, and quality of seven state-of-the-art LLMs, including variants of OpenAI's GPT-4, Gemini, Llama, and Mixtral, relative to human annotators in analyzing sentiment, political leaning, emotional intensity, and sarcasm detection. A total of 33 human annotators and eight LLM variants assessed 100 curated textual items, generating 3,300 human and 19,200 LLM annotations, with LLMs evaluated across three time points to examine temporal consistency. Inter-rater reliability was measured using Krippendorff's alpha, and intra-class correlation coefficients assessed consistency over time. The results reveal that both humans and LLMs exhibit high reliability in sentiment analysis and political leaning assessments, with LLMs demonstrating higher internal consistency than humans. In emotional intensity, LLMs displayed higher agreement compared to humans, though humans rated emotional intensity significantly higher. Both groups struggled with sarcasm detection, evidenced by low agreement. LLMs showed excellent temporal consistency across all dimensions, indicating stable performance over time. This research concludes that LLMs, especially GPT-4, can effectively replicate human analysis in sentiment and political leaning, although human expertise remains essential for emotional intensity interpretation. The findings demonstrate the potential of LLMs for consistent and high-quality performance in certain areas of latent content analysis.	 | 在快速发展的数字通信时代，每天生成了大量文本数据，需要高效的方法来进行潜在内容分析以提取有意义的见解。大型语言模型（LLMs）提供了自动化的可能性，但缺乏全面评估，这些评估将它们在多个维度上的表现与人类注释者的表现进行比较。本研究评估了七种最先进的LLMs，包括OpenAI的GPT-4变体、Gemini、Llama和Mixtral，在分析情感、政治倾向、情绪强度和讽刺检测方面的可靠性和一致性。共有33名人类注释者和八种LLM变体评估了100个精心挑选的文本项，产生了3,300个人类和19,200个LLM注释。LLMs在三个时间点进行了评估，以检查时间的一致性。使用克里普特多夫的阿尔法（Krippendorff's alpha）衡量注释者之间的互评可靠性和时间一致性指标。结果表明，人类和LLMs在情感分析和政治倾向评估方面表现出高度的一致性，而LLMs在内部一致性方面优于人类。在情绪强度方面，LLMs表现出更高的共识，尽管人类对情绪强度的评价显著更高。两个群体在讽刺检测方面都遇到了困难，这一点体现在较低的共识度中。LLMs在所有维度上都显示了优异的时间一致性，表明其性能在时间上是稳定的。这项研究得出结论，LLMs，特别是GPT-4，可以有效地复制人类在情感和政治倾向分析方面的表现，尽管人类的专业知识对于情绪强度的解释仍然至关重要。研究结果展示了LLMs在某些潜在内容分析领域的持续且高质量表现的潜力。
2501.02518	 | CHAIR-Classifier of Hallucination as Improver	 | Ao Sun	 | 该研究提出了一种监督方法，通过分析LLaMA模型各层的标记得分特征（如最大值、最小值、平均值、标准差和斜率），并使用逻辑回归进行分类，以检测大型语言模型中的幻觉，在零样本场景中显著提升了性能。	 | This paper presents a supervised method for detecting hallucinations in large language models. By analyzing token scores (logitis) across layers of the LLaMA model, we derive a small set, aiming to reduce overfitting, of features-including maximum, minimum, mean, standard deviation, and slope. We use logistic regression for classification and validate the model on the TruthfulQA and MMLU datasets. The results demonstrate significant performance gains, especially in zero-shot scenarios, highlighting the effectiveness and potential for generalization.	 | 本文介绍了一种监督方法，用于检测大型语言模型中的幻觉。通过分析LLaMA模型各层的标记得分（logitis），我们提取了一组小特征，旨在减少过拟合，包括最大值、最小值、平均值、标准差和斜率。我们使用逻辑回归进行分类，并在TruthfulQA和MMLU数据集上验证了模型。结果表明，在零样本场景中尤其显著提升了性能，突显了该方法的有效性和泛化潜力。
2501.02511	 | Can Impressions of Music be Extracted from Thumbnail Images?	 | Takashi Harada,Takehiro Motomitsu,Katsuhiko Hayashi,Yusuke Sakai,Hidetaka Kamigaito	 | 近年来，音乐检索和生成系统的研究显著增加，但由于缺乏包含音乐数据及其自然语言描述的大规模数据集，特别是包含非音乐信息的数据集，限制了模型的发展。为此，研究提出了一种利用从音乐缩略图图像推断非音乐信息生成音乐标题的方法，并通过一个包含约36万个数据集训练了一个音乐检索模型，展示了其在音乐检索任务中的有效性。	 | In recent years, there has been a notable increase in research on machine learning models for music retrieval and generation systems that are capable of taking natural language sentences as inputs. However, there is a scarcity of large-scale publicly available datasets, consisting of music data and their corresponding natural language descriptions known as music captions. In particular, non-musical information such as suitable situations for listening to a track and the emotions elicited upon listening is crucial for describing music. This type of information is underrepresented in existing music caption datasets due to the challenges associated with extracting it directly from music data. To address this issue, we propose a method for generating music caption data that incorporates non-musical aspects inferred from music thumbnail images, and validated the effectiveness of our approach through human evaluations. Additionally, we created a dataset with approximately 360,000 captions containing non-musical aspects. Leveraging this dataset, we trained a music retrieval model and demonstrated its effectiveness in music retrieval tasks through evaluation.	 | 近年来，对于能够接受自然语言句子作为输入的音乐检索和生成系统的机器学习模型的研究显著增加。然而，大型的公开可用数据集仍然稀缺，这些数据集包含音乐数据及其相应的自然语言描述，即音乐标题。特别地，适合聆听歌曲的情况以及听歌曲时引起的情感等非音乐信息对于描述音乐至关重要。由于从音乐数据直接提取这些信息的挑战，这些信息在现有的音乐标题数据集中代表性不足。为了解决这一问题，我们提出了一种方法，该方法利用从音乐缩略图图像推断出的非音乐方面生成音乐标题数据，并通过人类评估验证了该方法的有效性。此外，我们创建了一个包含约36万个具有非音乐方面数据集。利用这个数据集，我们训练了一个音乐检索模型，并通过评估在音乐检索任务中展示了其有效性。
2501.02506	 | ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use	 | Junjie Ye,Zhengyin Du,Xuesong Yao,Weijian Lin,Yufei Xu,Zehui Chen,Zaiyuan Wang,Sining Zhu,Zhiheng Xi,Siyu Yuan,Tao Gui,Qi Zhang,Xuanjing Huang,Jiechao Chen	 | ToolHop是一个包含995个用户查询和3,912个相关工具的数据集，旨在严格评估大型语言模型在多跳工具使用场景下的理解、推理和调用能力；评估结果显示，尽管领先模型GPT-4o的准确率为49.04%，但仍然存在显著改进空间。	 | Effective evaluation of multi-hop tool use is critical for analyzing the understanding, reasoning, and function-calling capabilities of large language models (LLMs). However, progress has been hindered by a lack of reliable evaluation datasets. To address this, we present ToolHop, a dataset comprising 995 user queries and 3,912 associated tools, specifically designed for rigorous evaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach that includes tool creation, document refinement, and code generation. We evaluate 14 LLMs across five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and GPT), uncovering significant challenges in handling multi-hop tool-use scenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%, underscoring substantial room for improvement. Further analysis reveals variations in tool-use strategies for various families, offering actionable insights to guide the development of more effective approaches. Code and data can be found in https://huggingface.co/bytedance-research/ToolHop.	 | 多跳工具使用评估对于分析大型语言模型（LLMs）的理解、推理和功能调用能力至关重要。然而，缺乏可靠的评估数据集阻碍了这一进展。为了解决这一问题，我们提出了ToolHop，这是一个包含995个用户查询和3,912个相关工具的数据集，专门用于严格评估多跳工具使用。ToolHop确保了查询的多样性、有意义的相互依赖性、可本地执行的工具、详细的反馈以及通过一种新颖的查询驱动的数据构建方法（包括工具创建、文档精炼和代码生成）可验证的答案。我们对14个LLM（包括LLaMA3.1、Qwen2.5、Gemini1.5、Claude3.5和GPT）进行了评估，揭示了在处理多跳工具使用场景时存在的显著挑战。领先模型GPT-4o的准确率为49.04%，显示出巨大的改进空间。进一步分析揭示了不同模型家庭在工具使用策略上的差异，提供了可操作的见解，以指导更有效方法的发展。代码和数据可以在https://huggingface.co/bytedance-research/ToolHop找到。
2501.02482	 | Decoding News Bias: Multi Bias Detection in News Articles	 | Bhushan Santosh Shah,Deven Santosh Shah,Vahida Attar	 | 该研究探讨了新闻文章中各种类型的偏见，利用大型语言模型构建数据集，并采用多种检测技术展示了偏见的存在，强调了广泛偏见检测的重要性以提高新闻文章的完整性。	 | News Articles provides crucial information about various events happening in the society but they unfortunately come with different kind of biases. These biases can significantly distort public opinion and trust in the media, making it essential to develop techniques to detect and address them. Previous works have majorly worked towards identifying biases in particular domains e.g., Political, gender biases. However, more comprehensive studies are needed to detect biases across diverse domains. Large language models (LLMs) offer a powerful way to analyze and understand natural language, making them ideal for constructing datasets and detecting these biases. In this work, we have explored various biases present in the news articles, built a dataset using LLMs and present results obtained using multiple detection techniques. Our approach highlights the importance of broad-spectrum bias detection and offers new insights for improving the integrity of news articles.	 | 新闻文章提供了关于社会上各种事件的关键信息，但它们不可避免地伴随着不同类型的偏见。这些偏见可能会显著扭曲公众对媒体的看法和信任，因此开发检测和解决这些偏见的技术变得至关重要。以往的研究主要集中在识别特定领域的偏见，例如政治偏见和性别偏见。然而，还需要进行更加全面的研究来检测跨多种领域的偏见。大型语言模型（LLMs）提供了一种强大的分析和理解自然语言的方式，使它们成为构建数据集和检测这些偏见的理想工具。在这项工作中，我们探索了新闻文章中存在的各种偏见，使用LLMs构建了一个数据集，并使用多种检测技术展示了所获得的结果。我们的方法强调了广泛偏见检测的重要性，并为提高新闻文章的完整性提供了新的见解。
2501.02471	 | Hengqin-RA-v1: Advanced Large Language Model for Diagnosis and Treatment of Rheumatoid Arthritis with Dataset based Traditional Chinese Medicine	 | Yishen Liu,Shengda Luo,Zishao Zhong,Tongtong Wu,Jianguo Zhang,Peiyao Ou,Yong Liang,Liang Liu,Hudan Pan	 | Hengqin-RA-v1 是首个专门针对中医领域，专注于诊断和治疗类风湿性关节炎的大型语言模型，结合了HQ-GCM-RA-C1数据集，该数据集包含古代中医文献、古典文献和现代临床研究，显著提高了模型在提供准确且文化敏感的回应方面的性能，超越了现有最先进的模型。	 | Large language models (LLMs) primarily trained on English texts, often face biases and inaccuracies in Chinese contexts. Their limitations are pronounced in fields like Traditional Chinese Medicine (TCM), where cultural and clinical subtleties are vital, further hindered by a lack of domain-specific data, such as rheumatoid arthritis (RA). To address these issues, this paper introduces Hengqin-RA-v1, the first large language model specifically tailored for TCM with a focus on diagnosing and treating RA. We also present HQ-GCM-RA-C1, a comprehensive RA-specific dataset curated from ancient Chinese medical literature, classical texts, and modern clinical studies. This dataset empowers Hengqin-RA-v1 to deliver accurate and culturally informed responses, effectively bridging the gaps left by general-purpose models. Extensive experiments demonstrate that Hengqin-RA-v1 outperforms state-of-the-art models, even surpassing the diagnostic accuracy of TCM practitioners in certain cases.	 | 大型语言模型（LLMs）主要基于英文文本训练，往往在中文环境中表现出偏见和不准确的问题。在如中医（TCM）这样的领域，文化与临床的细微差别至关重要，而缺乏特定领域的数据，如类风湿性关节炎（RA），进一步加剧了这些问题。为了解决这些问题，本文介绍了Hengqin-RA-v1，这是首个专门针对中医领域，专注于诊断和治疗RA的大型语言模型。我们还介绍了HQ-GCM-RA-C1，这是一个综合性的RA特定数据集，从古代中医文献、古典文献和现代临床研究中精心整理而来。这一数据集赋予Hengqin-RA-v1提供准确且文化敏感的回应的能力，有效地填补了通用模型留下的空白。广泛的实验表明，Hengqin-RA-v1在某些情况下甚至超过了中医从业者在诊断准确性上的表现，超越了最先进的模型。
2501.02448	 | Understand, Solve and Translate: Bridging the Multilingual Mathematical Reasoning Gap	 | Hyunwoo Ko,Guijin Son,Dasol Choi	 | 该研究通过引入包含8,011个英韩双语数学问题的HRM8K基准，揭示了LLMs在非英语语言上的推理能力差距主要源于对非英语输入的理解困难，并提出了一种UST方法来提高多语言性能，最终将性能差距从11.6%降低到0.7%。	 | Large language models (LLMs) demonstrate exceptional performance on complex reasoning tasks. However, despite their strong reasoning capabilities in high-resource languages (e.g., English and Chinese), a significant performance gap persists in other languages. To investigate this gap in Korean, we introduce HRM8K, a benchmark comprising 8,011 English-Korean parallel bilingual math problems. Through systematic analysis of model behaviors, we identify a key finding: these performance disparities stem primarily from difficulties in comprehending non-English inputs, rather than limitations in reasoning capabilities. Based on these findings, we propose UST (Understand, Solve, and Translate), a method that strategically uses English as an anchor for reasoning and solution generation. By fine-tuning the model on 130k synthetically generated data points, UST achieves a 10.91% improvement on the HRM8K benchmark and reduces the multilingual performance gap from 11.6% to 0.7%. Additionally, we show that improvements from UST generalize effectively to different Korean domains, demonstrating that capabilities acquired from machine-verifiable content can be generalized to other areas. We publicly release the benchmark, training dataset, and models.	 | 大型语言模型（LLMs）在复杂的推理任务中表现出色。然而，尽管它们在高资源语言（如英语和中文）上具有强大的推理能力，但在其他语言上仍然存在显著的能力差距。为了研究这种差距，我们引入了HRM8K基准，该基准包含8,011个英韩双语数学问题。通过系统分析模型的行为，我们发现一个关键发现：这些性能差异主要源于理解非英语输入的困难，而不是推理能力的不足。基于这些发现，我们提出了UST（Understand, Solve, and Translate，理解、解决和翻译）方法，该方法战略性地使用英语作为推理和解决方案生成的锚点。通过在130,000个合成生成的数据点上微调模型，UST在HRM8K基准上实现了10.91%的改进，并将多语言性能差距从11.6%降低到0.7%。此外，我们展示了UST在不同韩语领域的改进具有良好的泛化能力，证明了从机器可验证内容中获得的能力可以迁移到其他领域。我们公开发布了基准、训练数据集和模型。
2501.02434	 | Towards Multimodal Metaphor Understanding: A Chinese Dataset and Model for Metaphor Mapping Identification	 | Dongyu Zhang,Shengcheng Yin,Jingwei Yu,Zhiyao Wu,Zhen Li,Chengpei Xu,Xiaoxia Wang,Feng Xia	 | 该研究开发了一个中文多模态比喻广告数据集（CM3D），并提出了一种基于链式思维提示的比喻映射识别模型（CPMMIM），旨在促进比喻理解的研究，尤其是非英语语言中的研究，并证明了该模型在NLP中的有效性。	 | Metaphors play a crucial role in human communication, yet their comprehension remains a significant challenge for natural language processing (NLP) due to the cognitive complexity involved. According to Conceptual Metaphor Theory (CMT), metaphors map a target domain onto a source domain, and understanding this mapping is essential for grasping the nature of metaphors. While existing NLP research has focused on tasks like metaphor detection and sentiment analysis of metaphorical expressions, there has been limited attention to the intricate process of identifying the mappings between source and target domains. Moreover, non-English multimodal metaphor resources remain largely neglected in the literature, hindering a deeper understanding of the key elements involved in metaphor interpretation. To address this gap, we developed a Chinese multimodal metaphor advertisement dataset (namely CM3D) that includes annotations of specific target and source domains. This dataset aims to foster further research into metaphor comprehension, particularly in non-English languages. Furthermore, we propose a Chain-of-Thought (CoT) Prompting-based Metaphor Mapping Identification Model (CPMMIM), which simulates the human cognitive process for identifying these mappings. Drawing inspiration from CoT reasoning and Bi-Level Optimization (BLO), we treat the task as a hierarchical identification problem, enabling more accurate and interpretable metaphor mapping. Our experimental results demonstrate the effectiveness of CPMMIM, highlighting its potential for advancing metaphor comprehension in NLP. Our dataset and code are both publicly available to encourage further advancements in this field.	 | 比喻在人类沟通中扮演着至关重要的角色，但由于涉及的认知复杂性，其理解仍然是自然语言处理（NLP）的一个重大挑战。根据概念隐喻理论（CMT），比喻将目标领域映射到源领域，理解这种映射对于把握比喻的本质至关重要。尽管现有的NLP研究主要集中在如比喻检测和比喻性表达的情感分析等任务上，但对源领域和目标领域之间映射的复杂识别过程的关注却相对有限。此外，非英语多模态比喻资源在文献中仍然被忽视，阻碍了对比喻解读中关键要素的深入理解。为填补这一空白，我们开发了一个包含特定目标和源领域注释的中文多模态比喻广告数据集（即CM3D），旨在促进对比喻理解的研究，尤其是非英语语言中的研究。此外，我们提出了基于链式思维（Chain-of-Thought，CoT）提示的比喻映射识别模型（CPMMIM），该模型模拟了人类认知过程中的比喻映射识别。该模型借鉴了链式思维推理和双层优化（BLO）的思想，将任务视为层次识别问题，从而实现更准确和更具解释性的比喻映射。我们的实验结果表明了CPMMIM的有效性，突显了其在NLP中推进比喻理解的潜力。我们的数据集和代码均已公开，以鼓励该领域的进一步发展。
2501.02432	 | Swift Cross-Dataset Pruning: Enhancing Fine-Tuning Efficiency in Natural Language Understanding	 | Binh-Nguyen Nguyen,Yang He	 | Swift跨数据集剪枝（SCDP）通过使用TF-IDF嵌入和几何中位数快速评估样本的重要性，并结合自适应剪枝策略，有效减少了数据集大小以提高模型训练效率，同时保持模型性能。该方法在多个不同数据集上表现出色，减少了计算资源的使用。	 | Dataset pruning aims to select a subset of a dataset for efficient model training. While data efficiency in natural language processing has primarily focused on within-corpus scenarios during model pre-training, efficient dataset pruning for task-specific fine-tuning across diverse datasets remains challenging due to variability in dataset sizes, data distributions, class imbalance and label spaces. Current cross-dataset pruning techniques for fine-tuning often rely on computationally expensive sample ranking processes, typically requiring full dataset training or reference models. We address this gap by proposing Swift Cross-Dataset Pruning (SCDP). Specifically, our approach uses TF-IDF embeddings with geometric median to rapidly evaluate sample importance. We then apply dataset size-adaptive pruning to ensure diversity: for smaller datasets, we retain samples far from the geometric median, while for larger ones, we employ distance-based stratified pruning. Experimental results on six diverse datasets demonstrate the effectiveness of our method, spanning various tasks and scales while significantly reducing computational resources. Source code is available at: https://github.com/he-y/NLP-Dataset-Pruning	 | 数据集剪枝旨在选择数据集的一个子集，以实现高效的模型训练。虽然自然语言处理中的数据效率主要集中在模型预训练过程中的单个语料库内部场景，但在跨多种数据集的任务特定微调中，由于数据集大小、数据分布、类别不平衡和标签空间的差异性，高效的数据集剪枝仍然具有挑战性。目前，用于微调的跨数据集剪枝技术往往依赖于计算成本高昂的样本排名过程，通常需要完整的数据集训练或参考模型。我们通过提出Swift跨数据集剪枝（SCDP）来解决这一问题。具体而言，我们的方法使用TF-IDF嵌入和几何中位数快速评估样本的重要性。然后，我们应用根据数据集大小自适应的剪枝策略以确保多样性：对于小数据集，我们保留远离几何中位数的样本；而对于大数据集，则采用基于距离的分层剪枝。实验结果表明，我们的方法在六个不同数据集上表现出色，涵盖了各种任务和规模，同时显著减少了计算资源的使用。源代码可在以下链接获取：https://github.com/he-y/NLP-Dataset-Pruning
2501.02407	 | Anonymization by Design of Language Modeling	 | Antoine Boutet,Zakaria El Kazdam,Lucas Magnana,Helain Zimmermann	 | 本文提出了一种通过掩码语言模型（MLM）和因果语言模型（CLM）方法解决自然语言模型隐私问题的新方案，以避免模型记住敏感标识信息，从而在保持模型性能的同时提高医疗数据的隐私保护水平。	 | Rapid advances in Natural Language Processing (NLP) have revolutionized many fields, including healthcare. However, these advances raise significant privacy concerns, especially when models specialized on sensitive data can memorize and then expose and regurgitate confidential information. This paper presents a privacy-by-design language modeling approach to address the problem of language models anonymization, and thus promote their sharing. Specifically, we propose both a Masking Language Modeling (MLM) methodology to specialize a BERT-like language model, and a Causal Language Modeling (CLM) methodology to specialize a GPT-like model that avoids the model from memorizing direct and indirect identifying information present in the training data. We have comprehensively evaluated our approaches using medical datasets and compared them against different baselines. Our results indicate that by avoiding memorizing both direct and indirect identifiers during model specialization, our masking and causal language modeling schemes offer the best tradeoff for maintaining high privacy while retaining high utility.	 | 自然语言处理（NLP）的迅速发展已经革新了许多领域，包括医疗保健。然而，这些进步引发了重大的隐私问题，特别是在处理敏感数据的模型能够记住并暴露和重复机密信息时尤为突出。本文提出了一种隐私设计语言模型的方法，以解决语言模型匿名化的问题，从而促进其共享。具体地，我们提出了一种掩码语言模型（MLM）方法来专业化一种类似于BERT的模型，并提出了一种因果语言模型（CLM）方法来专业化一种类似于GPT的模型，从而避免模型记住训练数据中直接和间接的标识信息。我们使用医疗数据集对我们的方法进行了全面评估，并与不同的基线进行了比较。我们的结果显示，通过在模型专业化过程中避免记住直接和间接标识信息，我们的掩码和因果语言模型方案提供了在保持高水平隐私的同时保留高实用性的最佳权衡。
2501.02392	 | Syntactic Evolution in Language Usage	 | Surbhit Kumar	 | 本研究通过分析2004年来自blogger.com的博客数据集，探讨了从青少年到老年这一生命周期中语言风格的动态变化，旨在揭示年龄与语言使用之间的复杂关系，对于语言学、心理学和传播学等领域具有重要意义。	 | This research aims to investigate the dynamic nature of linguistic style throughout various stages of life, from post teenage to old age. By employing linguistic analysis tools and methodologies, the study will delve into the intricacies of how individuals adapt and modify their language use over time. The research uses a data set of blogs from blogger.com from 2004 and focuses on English for syntactic analysis. The findings of this research can have implications for linguistics, psychology, and communication studies, shedding light on the intricate relationship between age and language.	 | 本研究旨在探讨语言风格在生命不同阶段的动态特性，从青少年之后到老年。通过运用语言分析工具和方法，研究将深入探讨个体如何随时间适应和修改其语言使用方式。该研究利用来自blogger.com的2004年的博客数据集，并专注于英语的句法分析。本研究的发现对于语言学、心理学和传播学等领域具有重要意义，有助于揭示年龄与语言之间错综复杂的关系。
2501.02370	 | Prepending or Cross-Attention for Speech-to-Text? An Empirical Comparison	 | Tsz Kin Lam,Marco Gaido,Sara Papi,Luisa Bentivogli,Barry Haddow	 | 研究比较了密集特征预接（DFP）和交叉注意力两种架构在语音到文本识别和翻译任务中的表现，尽管DFP在普遍性上表现出色，但实验结果并未显示其有明显优势。	 | Following the remarkable success of Large Language Models (LLMs) in NLP tasks, there is increasing interest in extending their capabilities to speech -- the most common form in communication. To integrate speech into LLMs, one promising approach is dense feature prepending (DFP) which prepends the projected speech representations to the textual representations, allowing end-to-end training with the speech encoder. However, DFP typically requires connecting a text decoder to a speech encoder. This raises questions about the importance of having a sophisticated speech encoder for DFP, and how its performance compares with a standard encoder-decoder (i.e. cross-attention) architecture. In order to perform a controlled architectural comparison, we train all models from scratch, rather than using large pretrained models, and use comparable data and parameter settings, testing speech-to-text recognition (ASR) and translation (ST) on MuST-C v1.0 and CoVoST2 datasets. We study the influence of a speech encoder in DFP. More importantly, we compare DFP and cross-attention under a variety of configurations, such as CTC compression, sequence-level knowledge distillation, generation speed and GPU memory footprint on monolingual, bilingual and multilingual models. Despite the prevalence of DFP over cross-attention, our overall results do not indicate a clear advantage of DFP.	 | 在大型语言模型（LLMs）在自然语言处理（NLP）任务上取得显著成功之后，人们越来越关注将它们的能力扩展到语音——这是交流中最常见的形式。为了将语音整合到LLMs中，一种有前途的方法是密集特征预接（DFP），这种方法是在文本表示之前插入投影的语音表示，从而允许端到端训练，其中包括语音编码器。然而，DFP通常需要将文本解码器连接到语音编码器。这引发了关于DFP是否需要一个复杂的语音编码器，以及它与标准编码器-解码器（即交叉注意力）架构相比的表现如何的问题。为了进行受控的架构比较，我们从头开始训练所有模型，而不是使用大型预训练模型，并使用可比的数据和参数设置，在MuST-C v1.0和CoVoST2数据集上测试语音到文本识别（ASR）和翻译（ST）。我们研究了DFP中语音编码器的影响。更重要的是，我们在单语、双语和多语模型的各种配置下比较了DFP和交叉注意力，例如CTC压缩、序列级知识蒸馏、生成速度和GPU内存占用。尽管DFP在普遍性上超过交叉注意力，但我们的总体结果显示DFP并没有明显的优势。
2501.02361	 | Context Aware Lemmatization and Morphological Tagging Method in Turkish	 | Cagri Sayallar	 | 这项研究介绍了专门为土耳其语开发的词干提取模型和形态标注模型，这些模型在预测单词意义时考虑了单词的意义，并且表现优于2019年SIGMORPHON竞赛的现有结果。	 | The smallest part of a word that defines the word is called a word root. Word roots are used to increase success in many applications since they simplify the word. In this study, the lemmatization model, which is a word root finding method, and the morphological tagging model, which predicts the grammatical knowledge of the word, are presented. The presented model was developed for Turkish, and both models make predictions by taking the meaning of the word into account. In the literature, there is no lemmatization study that is sensitive to word meaning in Turkish. For this reason, the present study shares the model and the results obtained from the model on Turkish lemmatization for the first time in the literature. In the present study, in the lemmatization and morphological tagging models, bidirectional LSTM is used for the spelling of words, and the Turkish BERT model is used for the meaning of words. The models are trained using the IMST and PUD datasets from Universal Dependencies. The results from the training of the models were compared with the results from the SIGMORPHON 2019 competition. The results of the comparisons revealed that our models were superior.	 | 构成单词意义最小部分被称为词干。词干在许多应用中用于提高成功率，因为它们简化了单词。在这项研究中，介绍了词干提取模型和形态标注模型。词干提取模型用于查找词干，而形态标注模型则用于预测单词的语法知识。这些模型专门为土耳其语开发，两者在进行预测时都会考虑单词的意义。文献中没有针对土耳其语的敏感于单词意义的词干提取研究。因此，本研究首次在文献中分享了土耳其语词干提取模型及其从该模型获得的结果。在词干提取和形态标注模型中，本研究使用双向LSTM进行单词拼写，并使用土耳其语BERT模型来确定单词的意义。模型使用Universal Dependencies中的IMST和PUD数据集进行训练。训练模型的结果与2019年SIGMORPHON竞赛的结果进行了比较。比较结果显示，我们的模型表现更佳。
2501.02348	 | Thinking with Many Minds: Using Large Language Models for Multi-Perspective Problem-Solving	 | Sanghyun Park,Boris Maciejovsky,Phanish Puranam	 | 合成讨论利用大型语言模型增强了个体的认知灵活性，使其能够同时处理和探索多种观点，超越了传统心理模拟的局限性，适用于战略规划、政策制定和冲突解决等领域。	 | Complex problem-solving requires cognitive flexibility--the capacity to entertain multiple perspectives while preserving their distinctiveness. This flexibility replicates the "wisdom of crowds" within a single individual, allowing them to "think with many minds." While mental simulation enables imagined deliberation, cognitive constraints limit its effectiveness. We propose synthetic deliberation, a Large Language Model (LLM)-based method that simulates discourse between agents embodying diverse perspectives, as a solution. Using a custom GPT-based model, we showcase its benefits: concurrent processing of multiple viewpoints without cognitive degradation, parallel exploration of perspectives, and precise control over viewpoint synthesis. By externalizing the deliberative process and distributing cognitive labor between parallel search and integration, synthetic deliberation transcends mental simulation's limitations. This approach shows promise for strategic planning, policymaking, and conflict resolution.	 | 复杂的解决问题能力要求认知灵活性——即在保持各类观点独特性的同时能够同时容纳多种视角的能力。这种灵活性可以在单一个体内部复制“群众的智慧”，使他们能够“用众智思考”。虽然心理模拟能够使个体进行想象性讨论，但认知限制限制了其效果。我们提出了一种基于大型语言模型（LLM）的方法——合成讨论，作为一种解决方案。通过使用自定义的GPT模型，我们展示了该方法的优势：能够在不造成认知下降的情况下同时处理多种观点，平行探索不同视角，并且能够精确控制视角的合成。通过将推理过程外化，并在平行搜索和整合之间分配认知劳动，合成讨论超越了心理模拟的局限性。这种方法在战略规划、政策制定和冲突解决方面具有潜在应用前景。
2501.02336	 | AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference	 | Zhuomin He,Yizhen Yao,Pengfei Zuo,Bin Gao,Qinya Li,Zhenzhe Zheng,Fan Wu	 | 本文提出了一个名为 \sysname 的自适应子层跳过方法，该方法专为长上下文推理设计，能够在保持推理性能的同时加速预填充和解码阶段；\sysname 能够通过利用实时相似性信息自适应地识别较不重要的层，并支持子层级别的跳过。	 | Long-context large language models (LLMs) inference is increasingly critical, motivating a number of studies devoted to alleviating the substantial storage and computational costs in such scenarios. Layer-wise skipping methods are promising optimizations but rarely explored in long-context inference. We observe that existing layer-wise skipping strategies have several limitations when applied in long-context inference, including the inability to adapt to model and context variability, disregard for sublayer significance, and inapplicability for the prefilling phase. This paper proposes \sysname, an adaptive sublayer skipping method specifically designed for long-context inference. \sysname adaptively identifies less important layers by leveraging on-the-fly similarity information, enables sublayer-wise skipping, and accelerates both the prefilling and decoding phases. The effectiveness of \sysname is demonstrated through extensive experiments on various long-context benchmarks and models, showcasing its superior inference performance over existing baselines.	 | 长上下文大规模语言模型（LLMs）推理越来越关键，这促使了大量研究致力于减轻此类场景中的巨大存储和计算成本。逐层跳过方法是很有前景的优化手段，但在长上下文推理中鲜有探索。我们发现，现有的逐层跳过策略在应用于长上下文推理时存在几个局限性，包括无法适应模型和上下文的变异性、忽略子层的重要性以及在预填充阶段不适用。本文提出了一种名为 \sysname 的自适应子层跳过方法，该方法专为长上下文推理设计。\sysname 能够通过利用实时相似性信息自适应地识别较不重要的层，支持子层级别的跳过，并加速预填充和解码阶段。通过在多种长上下文基准和模型上的广泛实验，展示了 \sysname 在推理性能方面的优越性，优于现有基线。
2501.02334	 | Validity Arguments For Constructed Response Scoring Using Generative Artificial Intelligence Applications	 | Jodi M. Casabianca,Daniel F. McCaffrey,Matthew S. Johnson,Naim Alper,Vladimir Zubenko	 | 本文探讨了生成型人工智能在构造性应答评分中的应用，强调了与特征导向方法相比，生成型AI需要更广泛的有效性证据，并提出了一套最佳实践来支持其使用和解释。研究还讨论了如何结合多种AI评分来源来覆盖更多构念，特别是在缺乏人工评分的情况下。	 | The rapid advancements in large language models and generative artificial intelligence (AI) capabilities are making their broad application in the high-stakes testing context more likely. Use of generative AI in the scoring of constructed responses is particularly appealing because it reduces the effort required for handcrafting features in traditional AI scoring and might even outperform those methods. The purpose of this paper is to highlight the differences in the feature-based and generative AI applications in constructed response scoring systems and propose a set of best practices for the collection of validity evidence to support the use and interpretation of constructed response scores from scoring systems using generative AI. We compare the validity evidence needed in scoring systems using human ratings, feature-based natural language processing AI scoring engines, and generative AI. The evidence needed in the generative AI context is more extensive than in the feature-based NLP scoring context because of the lack of transparency and other concerns unique to generative AI such as consistency. Constructed response score data from standardized tests demonstrate the collection of validity evidence for different types of scoring systems and highlights the numerous complexities and considerations when making a validity argument for these scores. In addition, we discuss how the evaluation of AI scores might include a consideration of how a contributory scoring approach combining multiple AI scores (from different sources) will cover more of the construct in the absence of human ratings.	 | 大型语言模型和生成型人工智能（AI）能力的快速发展使其在高风险测试环境中得到广泛应用的可能性大大增加。生成型AI在评分构造性应答中的应用尤其具有吸引力，因为它减少了传统AI评分中手工特征构建所需的努力，并且甚至可能超越这些方法。本文旨在突出特征导向和生成型AI在构造性应答评分系统中的差异，并提出一套最佳实践，以收集有效证据支持使用和解释基于生成型AI的构造性应答评分。我们比较了使用人工评分、基于特征的自然语言处理（NLP）评分引擎评分系统以及生成型AI评分系统所需的有效性证据。生成型AI环境下所需的有效性证据比基于特征的NLP评分环境更广泛，这是因为生成型AI缺乏透明度及其他特有的问题，例如一致性问题。标准化测试中的构造性应答评分数据展示了不同评分系统收集有效性证据的过程，并突显了为这些评分做出有效论证时的诸多复杂性和考虑因素。此外，我们讨论了在评分过程中考虑结合多个来源的AI评分进行贡献评分的方法，以在没有人工评分的情况下覆盖更多构念。
2501.02295	 | Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection	 | Yachao Zhao,Bo Wang,Yan Wang	 | 本文提出了一种新的系统框架，结合社会心理学理论来研究和比较大型语言模型（LLMs）中的显式和隐式偏见，并通过实验发现，尽管随着训练数据和模型规模的增加，LLMs的显式偏见有所减轻，但隐式偏见却有所增加，当前的对齐方法对隐式偏见的抑制效果有限。	 | Large Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content. While extensive research has investigated bias in LLMs, prior work has predominantly focused on explicit bias, leaving the more nuanced implicit biases largely unexplored. This paper presents a systematic framework grounded in social psychology theories to investigate and compare explicit and implicit biases in LLMs. We propose a novel "self-reflection" based evaluation framework that operates in two phases: first measuring implicit bias through simulated psychological assessment methods, then evaluating explicit bias by prompting LLMs to analyze their own generated content. Through extensive experiments on state-of-the-art LLMs across multiple social dimensions, we demonstrate that LLMs exhibit a substantial inconsistency between explicit and implicit biases, where explicit biases manifest as mild stereotypes while implicit biases show strong stereotypes. Furthermore, we investigate the underlying factors contributing to this explicit-implicit bias inconsistency. Our experiments examine the effects of training data scale, model parameters, and alignment techniques. Results indicate that while explicit bias diminishes with increased training data and model size, implicit bias exhibits a contrasting upward trend. Notably, contemporary alignment methods (e.g., RLHF, DPO) effectively suppress explicit bias but show limited efficacy in mitigating implicit bias. These findings suggest that while scaling up models and alignment training can address explicit bias, the challenge of implicit bias requires novel approaches beyond current methodologies.	 | 大型语言模型（LLMs）在其生成的内容中表现出各种偏见和刻板印象。尽管有大量的研究探讨了LLMs中的偏见，但先前的工作主要集中在显式偏见上，而较为复杂的隐式偏见则没有被充分探索。本文提出了一种基于社会心理学理论的系统框架，用于研究和比较LLMs中的显式和隐式偏见。我们提出了一种新颖的“自我反思”为基础的评估框架，该框架分为两个阶段：首先通过模拟的心理评估方法测量隐式偏见，然后通过促使LLMs分析其生成的内容来评估显式偏见。  通过在多个社会维度上对最先进的LLMs进行广泛的实验，我们表明，LLMs在显式偏见和隐式偏见之间存在显著的不一致性，其中显式偏见表现为轻度的刻板印象，而隐式偏见则表现出强烈的刻板印象。此外，我们还研究了导致这种显性-隐性偏见不一致的根本因素。实验考察了训练数据规模、模型参数以及对齐技术的影响。结果显示，虽然随着训练数据量和模型规模的增大，显式偏见有所减少，但隐式偏见却表现出相反的上升趋势。值得注意的是，当前的对齐方法（如RLHF、DPO）有效地抑制了显式偏见，但在减轻隐式偏见方面效果有限。这些发现表明，虽然通过扩大模型规模和对齐训练可以解决显式偏见问题，但对于隐式偏见的挑战则需要超出当前方法的新颖方案。
2501.02266	 | LLMzSz{\L}: a comprehensive LLM benchmark for Polish	 | Krzysztof Jassem,Michał Ciesiółka,Filip Graliński,Piotr Jabłoński,Jakub Pokrywka,Marek Kubis,Monika Jabłońska,Ryszard Staruch	 | 本文介绍了LLMzSzŁ基准，这是迄今为止规模最大的波兰语综合基准，包含近1.9万道封闭式问题，涵盖了4种类型和154个领域，用于评估开源多语言和单语言大型语言模型在波兰语考试中的表现。研究表明，多语言LLM在某些情况下表现更好，但在模型大小成为关键因素时，单语言模型可能更有优势。	 | This article introduces the first comprehensive benchmark for the Polish language at this scale: LLMzSzŁ (LLMs Behind the School Desk). It is based on a coherent collection of Polish national exams, including both academic and professional tests extracted from the archives of the Polish Central Examination Board. It covers 4 types of exams, coming from 154 domains. Altogether, it consists of almost 19k closed-ended questions. We investigate the performance of open-source multilingual, English, and Polish LLMs to verify LLMs' abilities to transfer knowledge between languages. Also, the correlation between LLMs and humans at model accuracy and exam pass rate levels is examined. We show that multilingual LLMs can obtain superior results over monolingual ones; however, monolingual models may be beneficial when model size matters. Our analysis highlights the potential of LLMs in assisting with exam validation, particularly in identifying anomalies or errors in examination tasks.	 | 本文介绍了迄今为止规模最大的波兰语综合基准：LLMzSzŁ（LLMs Behind the School Desk）。该基准基于一套连贯的波兰全国考试数据集合，包括从波兰中央考试委员会档案中提取的学术和职业测试。该基准涵盖了4种类型、来自154个领域的考试。总计包含近1.9万道封闭式问题。我们研究了开源多语言、英语和波兰语大型语言模型（LLM）的表现，以验证LLM在语言间转移知识的能力。同时，还分析了模型准确性及考试通过率方面LLM与人类之间的相关性。结果显示，多语言LLM在某些情况下可以获得优于单语言LLM的结果；然而，当模型大小成为关键因素时，单语言模型可能更有优势。我们的分析突显了LLM在考试验证方面的潜力，特别是在识别考试任务中的异常或错误方面。
2501.02237	 | Financial Named Entity Recognition: How Far Can LLM Go?	 | Yi-Te Lu,Yintong Huo	 | 该研究系统评估了先进语言模型在金融命名实体识别任务中的表现和局限性，并识别了五种主要的失败类型，为深化理解通用语言模型在特定领域应用中的潜力与挑战提供了见解。	 | The surge of large language models (LLMs) has revolutionized the extraction and analysis of crucial information from a growing volume of financial statements, announcements, and business news. Recognition for named entities to construct structured data poses a significant challenge in analyzing financial documents and is a foundational task for intelligent financial analytics. However, how effective are these generic LLMs and their performance under various prompts are yet need a better understanding. To fill in the blank, we present a systematic evaluation of state-of-the-art LLMs and prompting methods in the financial Named Entity Recognition (NER) problem. Specifically, our experimental results highlight their strengths and limitations, identify five representative failure types, and provide insights into their potential and challenges for domain-specific tasks.	 | 大规模语言模型（LLMs）的激增已经彻底改变了从不断增加的财务报表、公告和商业新闻中提取和分析关键信息的方式。命名实体识别（NER）以构建结构化数据来分析财务文件，是智能财务分析的基础任务。然而，通用LLMs在各种提示下的有效性和性能尚需进一步了解。为填补这一空白，我们系统评估了最先进的LLMs及其在金融命名实体识别问题中的提示方法。具体而言，我们的实验结果突显了它们的优势和局限性，识别了五种代表性的失败类型，并提供了其在特定领域任务中的潜力和挑战的洞见。
2501.02235	 | Survey on Question Answering over Visually Rich Documents: Methods, Challenges, and Trends	 | Camille Barboule,Benjamin Piwowarski,Yoan Chabot	 | 本文概述了利用大型语言模型（LLMs）增强富视觉文档理解（VrDU）的方法，并讨论了将VrD特征集成到LLMs中的策略及其面临的挑战。	 | Using Large Language Models (LLMs) for Visually-rich Document Understanding (VrDU) has significantly improved performance on tasks requiring both comprehension and generation, such as question answering, albeit introducing new challenges. This survey explains how VrDU models enhanced by LLMs function, covering methods for integrating VrD features into LLMs and highlighting key challenges.	 | 使用大型语言模型（LLMs）进行富视觉文档理解（VrDU）在提高诸如问答等既需要理解又需要生成的任务性能方面取得了显著进步，尽管引入了新的挑战。本文综述了LLMs增强的VrDU模型的工作原理，涵盖了将VrD特征集成到LLMs中的方法，并强调了关键挑战。
2501.02196	 | CPTuning: Contrastive Prompt Tuning for Generative Relation Extraction	 | Jiaxin Duan,Fengyu Lu,Junfei Liu	 | CPTuning 是一种新的对比提示调优方法，它通过学习实体对之间候选关系的概率质量并使用 Trie 约束解码来实现多关系提取，从而在多个数据集上显著优于现有方法。	 | Generative relation extraction (RE) commonly involves first reformulating RE as a linguistic modeling problem easily tackled with pre-trained language models (PLM) and then fine-tuning a PLM with supervised cross-entropy loss. Although having achieved promising performance, existing approaches assume only one deterministic relation between each pair of entities without considering real scenarios where multiple relations may be valid, i.e., entity pair overlap, causing their limited applications. To address this problem, we introduce a novel contrastive prompt tuning method for RE, CPTuning, which learns to associate a candidate relation between two in-context entities with a probability mass above or below a threshold, corresponding to whether the relation exists. Beyond learning schema, CPTuning also organizes RE as a verbalized relation generation task and uses Trie-constrained decoding to ensure a model generates valid relations. It adaptively picks out the generated candidate relations with a high estimated likelihood in inference, thereby achieving multi-relation extraction. We conduct extensive experiments on four widely used datasets to validate our method. Results show that T5-large fine-tuned with CPTuning significantly outperforms previous methods, regardless of single or multiple relations extraction.	 | 生成式关系提取（RE）通常首先将关系提取重新表述为一个语言建模问题，这种问题可以通过预训练语言模型（PLM）轻松解决，然后使用监督交叉熵损失微调PLM。尽管现有方法取得了令人鼓舞的性能，但它们假设每个实体对之间只有一个确定的关系，而没有考虑到实际场景中可能存在多个有效关系的情况，即实体对之间的重叠，导致其应用受到限制。为了解决这一问题，我们提出了一种新的对比提示调优方法，CPTuning，该方法学习将两个上下文实体之间的候选关系与高于或低于阈值的概率质量关联起来，对应于关系是否存在。除了学习模式外，CPTuning 还将关系提取组织为一个口头化的关系生成任务，并使用Trie约束解码以确保模型生成有效的关系。在推理时，它会自适应地选择具有高估计概率的生成候选关系，从而实现多关系提取。我们在四个广泛使用的数据集上进行了大量实验以验证该方法。结果显示，使用CPTuning微调的T5-large在单关系和多关系提取方面均显著优于之前的方法。
2501.02157	 | Personalized Graph-Based Retrieval for Large Language Models	 | Steven Au,Cameron J. Dimacali,Ojasmitha Pedirappagari,Namyong Park,Franck Dernoncourt,Yu Wang,Nikos Kanakaris,Hanieh Deilamsalehy,Ryan A. Rossi,Nesreen K. Ahmed	 | 本文提出了一种基于个性化图检索增强生成（PGraphRAG）的框架，通过利用用户中心的知识图谱来提高上下文理解和输出质量，特别是在数据稀疏的冷启动场景中。实验结果表明，PGraphRAG 在多种任务中显著优于现有方法，展示了图检索在个性化文本生成中的独特优势。	 | As large language models (LLMs) evolve, their ability to deliver personalized and context-aware responses offers transformative potential for improving user experiences. Existing personalization approaches, however, often rely solely on user history to augment the prompt, limiting their effectiveness in generating tailored outputs, especially in cold-start scenarios with sparse data. To address these limitations, we propose Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG), a framework that leverages user-centric knowledge graphs to enrich personalization. By directly integrating structured user knowledge into the retrieval process and augmenting prompts with user-relevant context, PGraphRAG enhances contextual understanding and output quality. We also introduce the Personalized Graph-based Benchmark for Text Generation, designed to evaluate personalized text generation tasks in real-world settings where user history is sparse or unavailable. Experimental results show that PGraphRAG significantly outperforms state-of-the-art personalization methods across diverse tasks, demonstrating the unique advantages of graph-based retrieval for personalization.	 | 随着大型语言模型（LLMs）的发展，它们提供个性化和上下文感知响应的能力具有改变用户体验的潜力。然而，现有的个性化方法通常仅依赖于用户历史来增强提示，这在数据稀疏的冷启动场景中限制了其生成定制输出的有效性。为了解决这些问题，我们提出了一种基于个性化图检索增强生成（PGraphRAG）的框架，该框架利用用户中心的知识图谱来丰富个性化。通过直接将结构化用户知识集成到检索过程中，并通过用户相关背景增强提示，PGraphRAG 提高了上下文理解能力和输出质量。我们还引入了基于个性化图的文本生成基准，旨在评估在用户历史数据稀疏或不可用的现实场景中的个性化文本生成任务。实验结果表明，PGraphRAG 在多种任务中显著优于最先进的个性化方法，展示了基于图的检索在个性化方面的独特优势。
2501.02090	 | Applying Text Mining to Analyze Human Question Asking in Creativity Research	 | Anna Wróblewska,Marceli Korbin,Yoed N. Kenett,Daniel Dan,Maria Ganzha,Marcin Paprzycki	 | 这项研究探讨了通过文本挖掘方法衡量问题的认知潜力，以理解问题在创造力中的作用，并提出了一种新方法应用于五个数据集，结果显示自然语言处理在创造性研究中有一定作用。	 | Creativity relates to the ability to generate novel and effective ideas in the areas of interest. How are such creative ideas generated? One possible mechanism that supports creative ideation and is gaining increased empirical attention is by asking questions. Question asking is a likely cognitive mechanism that allows defining problems, facilitating creative problem solving. However, much is unknown about the exact role of questions in creativity. This work presents an attempt to apply text mining methods to measure the cognitive potential of questions, taking into account, among others, (a) question type, (b) question complexity, and (c) the content of the answer. This contribution summarizes the history of question mining as a part of creativity research, along with the natural language processing methods deemed useful or helpful in the study. In addition, a novel approach is proposed, implemented, and applied to five datasets. The experimental results obtained are comprehensively analyzed, suggesting that natural language processing has a role to play in creative research.	 | 创造力涉及到在感兴趣的领域生成新颖且有效的想法的能力。这些创造性想法是如何生成的？支持创造性构思的一种可能机制，并且越来越多地受到实证研究的关注，就是通过提问。提问可能是允许界定问题、促进创造性问题解决的认知机制之一。然而，关于问题在创造力中的确切作用还知之甚少。这项工作试图应用文本挖掘方法来衡量问题的认知潜力，考虑了包括但不限于（a）问题类型，（b）问题复杂性，以及（c）答案内容等因素。本文总结了作为创造力研究一部分的提问挖掘的历史，并介绍了在研究中认为有用或有帮助的自然语言处理方法。此外，还提出了一种新的方法，并应用于五个数据集。实验结果的全面分析表明，自然语言处理在创造性研究中发挥着一定作用。
2501.02086	 | Instruction-Following Pruning for Large Language Models	 | Bairu Hou,Qibin Chen,Jianyu Wang,Guoli Yin,Chong Wang,Nan Du,Ruoming Pang,Shiyu Chang,Tao Lei	 | 本文提出了一种“遵循指令的剪枝”方法，通过动态调整剪枝掩码来优化大型语言模型，使得模型在保持性能的同时大幅减小规模。实验结果显示，该方法在多个评估基准上显著提升了模型性能，特别是在数学和编程领域，性能与更大规模的模型相当。	 | With the rapid scaling of large language models (LLMs), structured pruning has become a widely used technique to learn efficient, smaller models from larger ones, delivering superior performance compared to training similarly sized models from scratch. In this paper, we move beyond the traditional static pruning approach of determining a fixed pruning mask for a model, and propose a dynamic approach to structured pruning. In our method, the pruning mask is input-dependent and adapts dynamically based on the information described in a user instruction. Our approach, termed "instruction-following pruning", introduces a sparse mask predictor that takes the user instruction as input and dynamically selects the most relevant model parameters for the given task. To identify and activate effective parameters, we jointly optimize the sparse mask predictor and the LLM, leveraging both instruction-following data and the pre-training corpus. Experimental results demonstrate the effectiveness of our approach on a wide range of evaluation benchmarks. For example, our 3B activated model improves over the 3B dense model by 5-8 points of absolute margin on domains such as math and coding, and rivals the performance of a 9B model.	 | 随着大型语言模型（LLMs）的快速扩展，结构化剪枝已成为一种广泛使用的技术，可以从较大的模型中学习更高效、更小的模型，其性能优于从零开始训练同样大小的模型。在本文中，我们超越了传统的静态剪枝方法，即确定一个固定的剪枝掩码，而是提出了一种动态的结构化剪枝方法。在我们的方法中，剪枝掩码依赖于输入，并基于用户指令中的信息进行动态调整。我们提出的这种方法称为“遵循指令的剪枝”，它引入了一个稀疏掩码预测器，该预测器将用户指令作为输入，并根据给定任务动态选择最相关的模型参数。为了识别并激活有效的参数，我们同时优化了稀疏掩码预测器和LLM，利用了遵循指令的数据和预训练语料库。实验结果表明，我们的方法在广泛的评估基准上非常有效。例如，我们的激活模型在数学和编程等领域的绝对差距提高了3B稠密模型5-8分，其性能与9B模型相当。
2501.02068	 | The interplay between domain specialization and model size: a case study in the legal domain	 | Roseval Malaquias Junior,Ramon Pires,Thales Sales Almeida,Kenzo Sakiyama,Roseli Romero,Rodrigo Nogueira	 | 该研究探讨了在计算资源受限情况下，持续预训练期间领域专业化与模型规模之间的关系，通过在通用和法律专用数据集上分别对不同参数量的模型进行预训练和评估，发现随着模型规模的增加，专用模型和通用模型之间的计算效率差距逐渐扩大。	 | Scaling laws for language models so far focused on finding the compute-optimal model size and token count for training from scratch. However, achieving this optimal balance requires significant compute resources due to the extensive data demands when training models from randomly-initialized weights. Continual pre-training offers a cost-effective alternative, leveraging the compute investment from pre-trained models to incorporate new knowledge without requiring extensive new data. Recent findings suggest that data quality influences constants in scaling laws, thereby altering the optimal parameter-token allocation ratio. Building on this insight, we investigate the interplay between domain specialization and model size during continual pre-training under compute-constrained scenarios. Our goal is to identify a compute-efficient training regime for this scenario and, potentially, detect patterns in this interplay that can be generalized across different model sizes and domains. To compare general and specialized training, we filtered a web-based dataset to extract legal domain data. We pre-trained models with 1.5B, 3B, 7B and 14B parameters on both the unfiltered and filtered datasets, then evaluated their performance on legal exams. Results show that as model size increases, the compute-effectiveness gap between specialized and general models widens.	 | 到目前为止，语言模型的标度规律主要集中在找到从头训练时计算最优的模型大小和 token 数量。然而，实现这一最优平衡需要大量的计算资源，因为在使用随机初始化权重训练模型时，需要大量的数据。持续预训练提供了一种成本效益更高的替代方案，通过利用预训练模型的计算投资来纳入新知识，而不需要大量的新数据。最近的研究表明，数据质量影响标度法则中的常数，从而改变最优参数-token 分配比率。基于这一洞见，我们研究了在计算受限场景下持续预训练期间领域专业化与模型规模之间的相互作用。我们的目标是确定适合这一场景的计算高效训练制度，并且可能发现可以跨不同模型规模和领域泛化的这种相互作用的模式。为了比较通用和专用训练，我们从一个基于网络的数据集中筛选出法律领域的数据。我们分别在未筛选和筛选后的数据集上对具有 1.5B、3B、7B 和 14B 参数的模型进行预训练，然后评估它们在法律考试中的表现。结果显示，随着模型规模的增加，专用模型和通用模型之间的计算效率差距逐渐扩大。
2501.02063	 | AGGA: A Dataset of Academic Guidelines for Generative AI and Large Language Models	 | Junfeng Jiao,Saleh Afroogh,Kevin Chen,David Atkinson,Amit Dhurandhar	 | 本研究介绍了AGGA数据集，包含80份来自全球大学的学术指南，旨在指导在学术环境中使用生成型人工智能和大型语言模型，该数据集可用于自然语言处理任务，并涵盖多个学术领域和教育机构类型。	 | This study introduces AGGA, a dataset comprising 80 academic guidelines for the use of Generative AIs (GAIs) and Large Language Models (LLMs) in academic settings, meticulously collected from official university websites. The dataset contains 188,674 words and serves as a valuable resource for natural language processing tasks commonly applied in requirements engineering, such as model synthesis, abstraction identification, and document structure assessment. Additionally, AGGA can be further annotated to function as a benchmark for various tasks, including ambiguity detection, requirements categorization, and the identification of equivalent requirements. Our methodologically rigorous approach ensured a thorough examination, with a selection of universities that represent a diverse range of global institutions, including top-ranked universities across six continents. The dataset captures perspectives from a variety of academic fields, including humanities, technology, and both public and private institutions, offering a broad spectrum of insights into the integration of GAIs and LLMs in academia.	 | 本研究介绍了AGGA数据集，该数据集包含80份学术指南，旨在指导在学术环境中使用生成型人工智能（GAIs）和大型语言模型（LLMs）。这些指南详细收集自各大正规大学的官方网站。数据集包含188,674个单词，可作为自然语言处理任务的重要资源，这些任务在需求工程中广泛应用于模型综合、抽象识别和文档结构评估。此外，AGGA还可以进一步注释，用作各种任务的基准，包括模糊性检测、需求分类以及等效需求识别。我们严谨的方法确保了全面的审查，选取的大学涵盖了六大洲的多种类型的教育机构，包括全球顶尖大学。数据集涵盖了人文学科、技术、公立和私立机构等多个学术领域的视角，为生成型人工智能和大型语言模型在学术界的应用提供了广泛的研究见解。
2501.02044	 | Advancing Pancreatic Cancer Prediction with a Next Visit Token Prediction Head on top of Med-BERT	 | Jianping He,Laila Rasmy,Degui Zhi,Cui Tao	 | 该研究利用Med-BERT基础模型，通过重新定义疾病预测任务为令牌预测和后续访问掩码令牌预测，显著提高了胰腺癌预测的准确性，特别是在少量标注数据的情况下，从而改善了早期检测和干预效果。	 | Background: Recently, numerous foundation models pretrained on extensive data have demonstrated efficacy in disease prediction using Electronic Health Records (EHRs). However, there remains some unanswered questions on how to best utilize such models especially with very small fine-tuning cohorts. Methods: We utilized Med-BERT, an EHR-specific foundation model, and reformulated the disease binary prediction task into a token prediction task and a next visit mask token prediction task to align with Med-BERT's pretraining task format in order to improve the accuracy of pancreatic cancer (PaCa) prediction in both few-shot and fully supervised settings. Results: The reformulation of the task into a token prediction task, referred to as Med-BERT-Sum, demonstrates slightly superior performance in both few-shot scenarios and larger data samples. Furthermore, reformulating the prediction task as a Next Visit Mask Token Prediction task (Med-BERT-Mask) significantly outperforms the conventional Binary Classification (BC) prediction task (Med-BERT-BC) by 3% to 7% in few-shot scenarios with data sizes ranging from 10 to 500 samples. These findings highlight that aligning the downstream task with Med-BERT's pretraining objectives substantially enhances the model's predictive capabilities, thereby improving its effectiveness in predicting both rare and common diseases. Conclusion: Reformatting disease prediction tasks to align with the pretraining of foundation models enhances prediction accuracy, leading to earlier detection and timely intervention. This approach improves treatment effectiveness, survival rates, and overall patient outcomes for PaCa and potentially other cancers.	 | 背景：近年来，基于大量数据预训练的多种基础模型在电子健康记录（EHRs）疾病预测方面显示出了显著的效果。然而，如何最好地利用这些模型，特别是在非常小的微调数据集的情况下，仍然存在一些未解之谜。方法：我们利用了Med-BERT，一个针对EHR特定的基础模型，并将疾病二分类预测任务重新定义为一个令牌预测任务和一个后续访问掩码令牌预测任务，以与Med-BERT的预训练任务格式对齐，从而在少量样本和完全监督设置下提高胰腺癌（PaCa）预测的准确性。结果：将任务重新定义为一个令牌预测任务，称为Med-BERT-Sum，在少量样本场景和大量数据样本中都显示出了略微优越的表现。此外，将预测任务重新定义为一个后续访问掩码令牌预测任务（Med-BERT-Mask），在少量样本数据量从10到500不等的情况下，与传统的二分类（BC）预测任务（Med-BERT-BC）相比，提高了3%到7%的准确性。这些发现表明，将下游任务与Med-BERT的预训练目标对齐显著增强了模型的预测能力，从而提高了其在预测罕见和常见疾病方面的有效性。结论：重新格式化疾病预测任务以适应基础模型的预训练，可以提高预测准确性，有助于早期检测和及时干预。这种方法可以提高胰腺癌（PaCa）和其他癌症的治疗效果、生存率和整体患者预后。
2501.02039	 | An Investigation into Value Misalignment in LLM-Generated Texts for Cultural Heritage	 | Fan Bu,Zheng Wang,Siyi Wang,Ziyao Liu	 | 该研究系统地评估了大型语言模型在生成与文化遗产相关的文化上一致文本时的可靠性，发现超过65%的生成文本存在显著的文化不一致；研究还提供了基准数据集和评估工作流，旨在为未来的研究提供资源。	 | As Large Language Models (LLMs) become increasingly prevalent in tasks related to cultural heritage, such as generating descriptions of historical monuments, translating ancient texts, preserving oral traditions, and creating educational content, their ability to produce accurate and culturally aligned texts is being increasingly relied upon by users and researchers. However, cultural value misalignments may exist in generated texts, such as the misrepresentation of historical facts, the erosion of cultural identity, and the oversimplification of complex cultural narratives, which may lead to severe consequences. Therefore, investigating value misalignment in the context of LLM for cultural heritage is crucial for mitigating these risks, yet there has been a significant lack of systematic and comprehensive study and investigation in this area. To fill this gap, we systematically assess the reliability of LLMs in generating culturally aligned texts for cultural heritage-related tasks. We conduct a comprehensive evaluation by compiling an extensive set of 1066 query tasks covering 5 widely recognized categories with 17 aspects within the knowledge framework of cultural heritage across 5 open-source LLMs, and examine both the type and rate of cultural value misalignments in the generated texts. Using both automated and manual approaches, we effectively detect and analyze the cultural value misalignments in LLM-generated texts. Our findings are concerning: over 65% of the generated texts exhibit notable cultural misalignments, with certain tasks demonstrating almost complete misalignment with key cultural values. Beyond these findings, this paper introduces a benchmark dataset and a comprehensive evaluation workflow that can serve as a valuable resource for future research aimed at enhancing the cultural sensitivity and reliability of LLMs.	 | 作为大型语言模型（LLMs）在文化遗产相关任务中的应用越来越广泛，例如生成历史建筑的描述、翻译古代文本、保护口头传统以及创造教育内容，它们生成准确且文化上一致的文本的能力越来越受到用户和研究人员的依赖。然而，在生成的文本中可能存在文化价值不一致的情况，例如对历史事实的误代表、文化身份的侵蚀以及对复杂文化叙事的过度简化，这些都可能导致严重后果。因此，在文化遗产领域调查LLM中的文化价值不一致对于减轻这些风险至关重要。然而，在这一领域，系统且全面的研究和调查仍然严重缺乏。为了填补这一空白，我们系统地评估了LLM在生成与文化遗产相关的文化上一致的文本时的可靠性。我们通过收集覆盖5个广泛认可类别和17个方面的一共1066个查询任务，对5个开源LLM进行了全面评估，考察了生成文本中的文化价值不一致的类型和频率。我们使用自动化和手动方法有效检测和分析了LLM生成文本中的文化价值不一致。我们的研究结果令人担忧：超过65%的生成文本存在显著的文化不一致，某些任务甚至几乎完全不与关键文化价值一致。除了这些发现，本文还介绍了基准数据集和全面的评估工作流，这些可以为未来旨在增强LLM的文化敏感性和可靠性的研究提供宝贵的资源。
2501.02031	 | CarbonChat: Large Language Model-Based Corporate Carbon Emission Analysis and Climate Knowledge Q&A System	 | Zhixuan Cao,Ming Han,Jingtao Wang,Meng Jia	 | 本文提出了一种名为CarbonChat的系统，旨在通过改进的大型语言模型处理企业碳排放分析和气候变化知识问答，该系统通过多元化指标模块和增强的自我提示检索架构提高了精确度和效率，同时通过多层切块机制和幻觉检测确保了分析结果的准确性和可验证性。	 | As the impact of global climate change intensifies, corporate carbon emissions have become a focal point of global attention. In response to issues such as the lag in climate change knowledge updates within large language models, the lack of specialization and accuracy in traditional augmented generation architectures for complex problems, and the high cost and time consumption of sustainability report analysis, this paper proposes CarbonChat: Large Language Model-based corporate carbon emission analysis and climate knowledge Q&A system, aimed at achieving precise carbon emission analysis and policy understanding.First, a diversified index module construction method is proposed to handle the segmentation of rule-based and long-text documents, as well as the extraction of structured data, thereby optimizing the parsing of key information.Second, an enhanced self-prompt retrieval-augmented generation architecture is designed, integrating intent recognition, structured reasoning chains, hybrid retrieval, and Text2SQL, improving the efficiency of semantic understanding and query conversion.Next, based on the greenhouse gas accounting framework, 14 dimensions are established for carbon emission analysis, enabling report summarization, relevance evaluation, and customized responses.Finally, through a multi-layer chunking mechanism, timestamps, and hallucination detection features, the accuracy and verifiability of the analysis results are ensured, reducing hallucination rates and enhancing the precision of the responses.	 | 随着全球气候变化的影响加剧，企业的碳排放已成为全球关注的焦点。针对大型语言模型中气候变化知识更新滞后、传统增强生成架构在处理复杂问题时缺乏专门性和准确性，以及可持续性报告分析成本高、耗时长等问题，本文提出了一种名为CarbonChat的基于大型语言模型的企业碳排放分析和气候变化知识问答系统，旨在实现精确的碳排放分析和政策理解。首先，提出了一种多元化的指标模块构建方法，处理基于规则和长文本文档的分割，以及结构化数据的提取，从而优化关键信息的解析。其次，设计了一种增强的自我提示检索增强生成架构，整合了意图识别、结构化推理链、混合检索和Text2SQL，提高了语义理解和查询转换的效率。基于温室气体核算框架，建立了14个维度进行碳排放分析，实现报告总结、相关性评估和个性化响应。最后，通过多层切块机制、时间戳检测和幻觉检测功能，保证了分析结果的准确性和可验证性，降低了幻觉率并提高了响应的精度。
2501.02026	 | Recursive Decomposition of Logical Thoughts: Framework for Superior Reasoning and Knowledge Propagation in Large Language Models	 | Kaleem Ullah Qasim,Jiashu Zhang,Tariq Alsahfi,Ateeq Ur Rehman Butt	 | RDoLT（递归分解逻辑思考提示）是一种创新框架，通过递归分解任务、选择评分机制和知识传播模块显著提升了大型语言模型的推理性能，在多个基准测试中均优于现有技术，特别是在GSM8K基准测试中将准确率提升了6.28%至90.98%。	 | Enhancing the reasoning capabilities of Large Language Models remains a critical challenge in artificial intelligence. We introduce RDoLT, Recursive Decomposition of Logical Thought prompting, a novel framework that significantly boosts LLM reasoning performance. RDoLT is built on three key innovations: (1) recursively breaking down complex reasoning tasks into sub-tasks of progressive complexity; (2) employing an advanced selection and scoring mechanism to identify the most promising reasoning thoughts; and (3) integrating a knowledge propagation module that mimics human learning by keeping track of strong and weak thoughts for information propagation. Our approach was evaluated across multiple benchmarks, including GSM8K, SVAMP, MultiArith, LastLetterConcatenation, and Gaokao2023 Math. The results demonstrate that RDoLT consistently outperforms existing state-of-the-art techniques, achieving a 90.98 percent accuracy on GSM8K with ChatGPT-4, surpassing state-of-the-art techniques by 6.28 percent. Similar improvements were observed on other benchmarks, with accuracy gains ranging from 5.5 percent to 6.75 percent. These findings highlight RDoLT's potential to advance prompt engineering, offering a more effective and generalizable approach to complex reasoning tasks.	 | 增强大型语言模型的推理能力仍然是人工智能领域的一项关键挑战。我们提出了RDoLT（递归分解逻辑思维提示），这是一种创新框架，显著提升了LLM的推理性能。RDoLT基于三个关键创新：（1）递归地将复杂的推理任务分解为逐级复杂性的子任务；（2）采用先进的选择和评分机制，识别出最有前途的推理思考；（3）整合了一个知识传播模块，该模块模仿人类学习的过程，跟踪强弱思考以促进信息传播。我们的方法在多个基准测试中进行了评估，包括GSM8K、SVAMP、MultiArith、LastLetterConcatenation和Gaokao2023 数学。结果显示，RDoLT在所有基准测试中都优于现有最先进的技术。在使用ChatGPT-4的GSM8K基准测试中，RDoLT达到了90.98%的准确率，比最先进的技术高出了6.28%。在其他基准测试中也观察到了类似的改进，准确率提升范围从5.5%到6.75%不等。这些发现突显了RDoLT在推进提示工程方面的潜力，提供了一种更有效且更具通用性的复杂推理任务解决方案。
2501.02020	 | Enhancing Uncertainty Modeling with Semantic Graph for Hallucination Detection	 | Kedi Chen,Qin Chen,Jie Zhou,Xinqi Tao,Bowen Ding,Jingwen Xie,Mingchen Xie,Peilong Li,Feng Zheng,Liang He	 | 本文提出了一种通过语义图增强不确定性建模的方法来检测大型语言模型的幻觉，该方法不仅提高了句子级幻觉检测的性能，还在段落级检测上取得了显著改进，提升了19.78%。	 | Large Language Models (LLMs) are prone to hallucination with non-factual or unfaithful statements, which undermines the applications in real-world scenarios. Recent researches focus on uncertainty-based hallucination detection, which utilizes the output probability of LLMs for uncertainty calculation and does not rely on external knowledge or frequent sampling from LLMs. Whereas, most approaches merely consider the uncertainty of each independent token, while the intricate semantic relations among tokens and sentences are not well studied, which limits the detection of hallucination that spans over multiple tokens and sentences in the passage. In this paper, we propose a method to enhance uncertainty modeling with semantic graph for hallucination detection. Specifically, we first construct a semantic graph that well captures the relations among entity tokens and sentences. Then, we incorporate the relations between two entities for uncertainty propagation to enhance sentence-level hallucination detection. Given that hallucination occurs due to the conflict between sentences, we further present a graph-based uncertainty calibration method that integrates the contradiction probability of the sentence with its neighbors in the semantic graph for uncertainty calculation. Extensive experiments on two datasets show the great advantages of our proposed approach. In particular, we obtain substantial improvements with 19.78% in passage-level hallucination detection.	 | 大型语言模型（LLMs）容易产生与事实不符或不忠实的陈述，这在实际应用场景中削弱了其应用价值。最近的研究重点在于基于不确定性进行幻觉检测，这种方法利用LLMs的输出概率进行不确定性计算，无需依赖外部知识或频繁从LLMs中进行抽样。然而，大多数方法仅仅考虑每个独立词的不确定性，而忽略了词与句子之间复杂的语义关系，这限制了检测跨越多个词和句子的幻觉。在本文中，我们提出了一种通过语义图增强不确定性建模来进行幻觉检测的方法。具体而言，我们首先构建了一个能够很好地捕捉实体词和句子之间关系的语义图。然后，我们结合两个实体之间的关系来进行不确定性传递，以增强句子级幻觉检测的性能。由于幻觉通常是由于句子之间的冲突造成的，我们进一步提出了一个基于图的不确定性校准方法，该方法将句子与其语义图中邻居之间的矛盾概率整合到不确定性计算中。在两个数据集上进行的广泛实验表明，我们提出的方法具有显著的优势，特别是在段落级幻觉检测方面取得了19.78%的显著改进。
2501.02018	 | Safeguarding Large Language Models in Real-time with Tunable Safety-Performance Trade-offs	 | Joao Fonseca,Andrew Bell,Julia Stoyanovich	 | SafeNudge 是一种结合了受控文本生成与“引导”技术的新颖保护方法，可以在不显著增加推理时间和影响语义流畅性的前提下，降低大型语言模型受到越狱攻击的成功率约30%。	 | Large Language Models (LLMs) have been shown to be susceptible to jailbreak attacks, or adversarial attacks used to illicit high risk behavior from a model. Jailbreaks have been exploited by cybercriminals and blackhat actors to cause significant harm, highlighting the critical need to safeguard widely-deployed models. Safeguarding approaches, which include fine-tuning models or having LLMs "self-reflect", may lengthen the inference time of a model, incur a computational penalty, reduce the semantic fluency of an output, and restrict ``normal'' model behavior. Importantly, these Safety-Performance Trade-offs (SPTs) remain an understudied area. In this work, we introduce a novel safeguard, called SafeNudge, that combines Controlled Text Generation with "nudging", or using text interventions to change the behavior of a model. SafeNudge triggers during text-generation while a jailbreak attack is being executed, and can reduce successful jailbreak attempts by 30% by guiding the LLM towards a safe responses. It adds minimal latency to inference and has a negligible impact on the semantic fluency of outputs. Further, we allow for tunable SPTs. SafeNudge is open-source and available through https://pypi.org/, and is compatible with models loaded with the Hugging Face "transformers" library.	 | 大型语言模型（LLMs）已被证明容易受到“越狱”攻击，即用于诱使模型执行高风险行为的对抗性攻击。这些越狱攻击已被网络犯罪分子和黑客利用，造成了重大危害，凸显了保护广泛部署的模型的重要性。保护措施，包括微调模型或让LLMs“自我反思”，可能会延长模型的推理时间，增加计算负担，降低输出的语义流畅性，限制“正常”模型行为。重要的是，这些安全性-性能权衡（SPTs）仍然是一个研究不足的领域。在这项工作中，我们介绍了一种新颖的保护方法，称为SafeNudge，它结合了受控文本生成与“引导”，即使用文本干预来改变模型的行为。SafeNudge 在执行越狱攻击时触发，在文本生成过程中引导LLM产生安全响应，可降低成功越狱尝试30%。它对推理时间几乎没有延迟增加，并且对输出的语义流畅性影响微乎其微。此外，我们允许调节SPT。SafeNudge 是开源的，并可通过 https://pypi.org/ 获得，兼容使用 Hugging Face “transformers”库加载的模型。
2501.02009	 | Cross-model Transferability among Large Language Models on the Platonic Representations of Concepts	 | Youcheng Huang,Chen Huang,Duanyu Feng,Wenqiang Lei,Jiancheng Lv	 | 研究发现，通过引入线性变换方法可以有效对齐不同大型语言模型（LLMs）之间的概念表示，类似于柏拉图的洞穴寓言，从而实现跨模型的高效转移和行为控制，并且较小的LLM概念表示对较大的LLM具有从弱到强的可转移性。	 | Understanding the inner workings of Large Language Models (LLMs) is a critical research frontier. Prior research has shown that a single LLM's concept representations can be captured as steering vectors (SVs), enabling the control of LLM behavior (e.g., towards generating harmful content). Our work takes a novel approach by exploring the intricate relationships between concept representations across different LLMs, drawing an intriguing parallel to Plato's Allegory of the Cave. In particular, we introduce a linear transformation method to bridge these representations and present three key findings: 1) Concept representations across different LLMs can be effectively aligned using simple linear transformations, enabling efficient cross-model transfer and behavioral control via SVs. 2) This linear transformation generalizes across concepts, facilitating alignment and control of SVs representing different concepts across LLMs. 3) A weak-to-strong transferability exists between LLM concept representations, whereby SVs extracted from smaller LLMs can effectively control the behavior of larger LLMs.	 | 了解大型语言模型（LLMs）的内部工作机制是研究的一个关键前沿领域。此前的研究表明，单一LLM的概念表示可以被捕捉为引导向量（SVs），这使得能够控制LLM的行为（例如，使其生成有害内容）。我们的研究采取了一种新颖的方法，通过探索不同LLM之间概念表示的复杂关系，得出了一个引人入胜的类比，类似于柏拉图的洞穴寓言。特别地，我们引入了一种线性变换方法来连接这些表示，并提出了三个关键发现：1）使用简单的线性变换可以有效对齐不同LLM之间的概念表示，从而通过SVs实现跨模型的高效转移和行为控制。2）这种线性变换在不同概念之间具有泛化能力，使不同概念的SVs在不同LLM之间的对齐和控制成为可能。3）较小的LLM概念表示对较大的LLM具有从弱到强的可转移性，即从中型LLM提取的SVs能够有效地控制大型LLM的行为。
2501.03225	 | Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation	 | Yuhui Zhang,Yuchang Su,Yiming Liu,Xiaohan Wang,James Burgess,Elaine Sui,Chenyu Wang,Josiah Aklilu,Alejandro Lozano,Anjiang Wei,Ludwig Schmidt,Serena Yeung-Levy	 | 为了进行更严格可靠的评估，该研究引入了AutoConverter框架，能够自动将开放性问题转换为多项选择题，从而减少了主观性并构建了VMCBench基准。VMCBench不仅统一了20个现有VQA数据集，还全面评估了33个最先进的视觉语言模型，确立了新的评估标准。	 | The rapid development of vision language models (VLMs) demands rigorous and reliable evaluation. However, current visual question answering (VQA) benchmarks often depend on open-ended questions, making accurate evaluation difficult due to the variability in natural language responses. To address this, we introduce AutoConverter, an agentic framework that automatically converts these open-ended questions into multiple-choice format, enabling objective evaluation while reducing the costly question creation process. Our experiments demonstrate that AutoConverter can generate correct and challenging multiple-choice questions, with VLMs demonstrating consistently similar or lower accuracy on these questions compared to human-created ones. Using AutoConverter, we construct VMCBench, a benchmark created by transforming 20 existing VQA datasets into a unified multiple-choice format, totaling 9,018 questions. We comprehensively evaluate 33 state-of-the-art VLMs on VMCBench, setting a new standard for scalable, consistent, and reproducible VLM evaluation.	 | 视觉语言模型（VLMs）的快速发展迫切需要严格的可靠评估。然而，当前的视觉问答（VQA）基准通常依赖开放性问题，这使得准确评估变得困难，因为自然语言响应的变异性较大。为了解决这个问题，我们引入了AutoConverter，这是一种自主框架，能够自动将开放性问题转换为多项选择题格式，从而实现客观评估，并减少昂贵的问题创建过程。我们的实验表明，AutoConverter能够生成正确且具有挑战性的多项选择题，VLMs在这些问题上的准确率与人类创建的问题相比一致或更低。利用AutoConverter，我们构建了VMCBench，这是一个将20个现有VQA数据集统一转换为多项选择格式的基准，总共包含9,018个问题。我们全面评估了33个最先进的VLMs在VMCBench上的表现，为可扩展、一致和可重复的VLM评估设定了新的标准。
2501.03040	 | ChronoSense: Exploring Temporal Understanding in Large Language Models with Time Intervals of Events	 | Duygu Sezen Islakoglu,Jan-Christoph Kalo	 | ChronoSense 是一个新的基准测试，旨在评估大型语言模型在时间理解方面的能力，特别是识别Allen区间关系和进行时间算术运算，结果显示不同模型在处理这些任务时表现差异显著。	 | Large Language Models (LLMs) have achieved remarkable success in various NLP tasks, yet they still face significant challenges in reasoning and arithmetic. Temporal reasoning, a critical component of natural language understanding, has raised increasing research attention. However, comprehensive testing of Allen's interval relations (e.g., before, after, during) -- a fundamental framework for temporal relationships -- remains underexplored. To fill this gap, we present ChronoSense, a new benchmark for evaluating LLMs' temporal understanding. It includes 16 tasks, focusing on identifying the Allen relation between two temporal events and temporal arithmetic, using both abstract events and real-world data from Wikidata. We assess the performance of seven recent LLMs using this benchmark and the results indicate that models handle Allen relations, even symmetrical ones, quite differently. Moreover, the findings suggest that the models may rely on memorization to answer time-related questions. Overall, the models' low performance highlights the need for improved temporal understanding in LLMs and ChronoSense offers a robust framework for future research in this area. Our dataset and the source code are available at https://github.com/duyguislakoglu/chronosense.	 | 大型语言模型（LLMs）在各种自然语言处理（NLP）任务中取得了显著成功，但在推理和数学运算方面仍然面临重大挑战。时间推理是自然语言理解的关键组成部分，近年来引起了越来越多的研究关注。然而，全面测试Allen区间关系（例如，之前、之后、期间）——时间关系的基本框架——仍然不足。为了弥补这一空白，我们提出ChronoSense，一个新的基准来评估LLMs的时间理解能力。它包含16项任务，重点关注识别两个时间事件之间的Allen关系以及时间算术运算，使用抽象事件和来自Wikidata的现实生活数据。我们使用此基准测试了七个最新的LLM模型，并且结果表明，模型在处理Allen关系（即使是对称的关系）方面表现得相当不同。此外，研究发现表明，模型可能依赖于记忆来回答与时间相关的问题。总体来说，模型的低性能突显了LLMs需要改进的时间理解需求，而ChronoSense为未来在这个领域进行研究提供了一个稳健的框架。我们的数据集和源代码可在https://github.com/duyguislakoglu/chronosense找到。
2501.03012	 | Analyzing Fine-tuning Representation Shift for Multimodal LLMs Steering alignment	 | Pegah Khayatan,Mustafa Shukor,Jayneel Parekh,Matthieu Cord	 | 该研究系统地分析了多模态大语言模型在微调过程中的隐藏状态表示演变，揭示了模型如何适应新的多模态任务，并通过概念驱动的方法追踪各模态编码概念的变化，最终展示了如何在无需额外训练的情况下调整模型的行为。	 | Multimodal LLMs have reached remarkable levels of proficiency in understanding multimodal inputs, driving extensive research to develop increasingly powerful models. However, much less attention has been paid to understanding and explaining the underlying mechanisms of these models. Most existing explainability research examines these models only in their final states, overlooking the dynamic representational shifts that occur during training. In this work, we systematically analyze the evolution of hidden state representations to reveal how fine-tuning alters the internal structure of a model to specialize in new multimodal tasks. Using a concept-based approach, we map hidden states to interpretable visual and textual concepts, enabling us to trace changes in encoded concepts across modalities as training progresses. We also demonstrate the use of shift vectors to capture these concepts changes. These shift vectors allow us to recover fine-tuned concepts by shifting those in the original model. Finally, we explore the practical impact of our findings on model steering, showing that we can adjust multimodal LLMs behaviors without any training, such as modifying answer types, captions style, or biasing the model toward specific responses. Our work sheds light on how multimodal representations evolve through fine-tuning and offers a new perspective for interpreting model adaptation in multimodal tasks. The code for this project is publicly available at https://github.com/mshukor/xl-vlms.	 | 多模态大语言模型在理解和处理多模态输入方面已经达到了令人瞩目的水平，这推动了大量研究致力于开发越来越强大的模型。然而，对这些模型的内部运作机制的理解和解释却相对较少。目前大多数现有的解释性研究仅关注这些模型的最终状态，而忽视了训练过程中发生的动态表示变化。在这项工作中，我们系统地分析了隐藏状态表示的演变，揭示了微调如何改变模型的内部结构以适应新的多模态任务。通过概念驱动的方法，我们将隐藏状态映射到可解释的视觉和文本概念，从而使我们能够追踪训练过程中各模态编码概念的变化。我们还展示了如何使用移位向量来捕捉这些概念的变化。这些移位向量允许我们通过将原始模型中的概念移位来恢复微调后的概念。最后，我们探讨了我们发现的实际应用，展示了如何在无需任何训练的情况下调整多模态大语言模型的行为，例如修改答案类型、调整图描述风格或使模型偏向特定响应。我们的工作揭示了多模态表示在微调过程中如何演变，并为多模态任务中的模型适应提供了新的解释视角。该项目的代码已公开，可从 https://github.com/mshukor/xl-vlms 获取。
2501.02997	 | CALM: Curiosity-Driven Auditing for Large Language Models	 | Xiang Zheng,Longxiang Wang,Yi Liu,Xingjun Ma,Chao Shen,Cong Wang	 | 本研究提出了一种基于好奇心驱动的审计方法（CALM），使用内驱动的强化学习来发现目标大型语言模型中潜在有害和偏见的输入-输出对，从而在无法访问模型参数的情况下审计黑盒LLM。	 | Auditing Large Language Models (LLMs) is a crucial and challenging task. In this study, we focus on auditing black-box LLMs without access to their parameters, only to the provided service. We treat this type of auditing as a black-box optimization problem where the goal is to automatically uncover input-output pairs of the target LLMs that exhibit illegal, immoral, or unsafe behaviors. For instance, we may seek a non-toxic input that the target LLM responds to with a toxic output or an input that induces the hallucinative response from the target LLM containing politically sensitive individuals. This black-box optimization is challenging due to the scarcity of feasible points, the discrete nature of the prompt space, and the large search space. To address these challenges, we propose Curiosity-Driven Auditing for Large Language Models (CALM), which uses intrinsically motivated reinforcement learning to finetune an LLM as the auditor agent to uncover potential harmful and biased input-output pairs of the target LLM. CALM successfully identifies derogatory completions involving celebrities and uncovers inputs that elicit specific names under the black-box setting. This work offers a promising direction for auditing black-box LLMs. Our code is available at https://github.com/x-zheng16/CALM.git.	 | 大型语言模型（LLMs）的审计是一项至关重要的挑战性任务。在本研究中，我们集中于在无法访问模型参数的情况下审计黑盒LLMs，仅可访问其提供的服务。我们将这种类型的审计视为一种黑盒优化问题，其目标是自动发现目标LLM中表现出非法、不道德或不安全行为的输入-输出对。例如，我们可能寻找一种无毒的输入，而目标LLM对其的响应却具有毒性；或者寻找一种能够引发目标LLM产生包含政治敏感人物的幻觉响应的输入。由于可行点的稀缺性、提示空间的离散性质以及庞大的搜索空间，这种黑盒优化问题极具挑战性。为了应对这些挑战，我们提出了一种针对大型语言模型的基于好奇心驱动的审计方法（CALM），该方法使用内驱动的强化学习来微调一个LLM作为审计代理，以发现目标LLM中潜在有害和偏见的输入-输出对。CALM成功地识别出涉及名人的一些贬损完成，并在黑盒设置中发现了能够引发特定名称的输入。本研究为审计黑盒LLM提供了有前景的方向。我们的代码可在 https://github.com/x-zheng16/CALM.git 获取。
2501.02772	 | GeAR: Generation Augmented Retrieval	 | Haoyu Liu,Shaohan Huang,Jianfeng Liu,Yuefeng Zhan,Hao Sun,Weiwei Deng,Feng Sun,Furu Wei,Qi Zhang	 | 本文提出了一种名为GeAR的新方法，通过结合融合和解码模块，能够在查询和文档融合表示的基础上生成相关文本，从而提高检索结果的准确性和理解性。GeAR在多种场景和数据集上展示了竞争力的检索和定位性能，并提供了对检索结果的新颖见解。	 | Document retrieval techniques form the foundation for the development of large-scale information systems. The prevailing methodology is to construct a bi-encoder and compute the semantic similarity. However, such scalar similarity is difficult to reflect enough information and impedes our comprehension of the retrieval results. In addition, this computational process mainly emphasizes the global semantics and ignores the fine-grained semantic relationship between the query and the complex text in the document. In this paper, we propose a new method called $\textbf{Ge}$neration $\textbf{A}$ugmented $\textbf{R}$etrieval ($\textbf{GeAR}$) that incorporates well-designed fusion and decoding modules. This enables GeAR to generate the relevant text from documents based on the fused representation of the query and the document, thus learning to "focus on" the fine-grained information. Also when used as a retriever, GeAR does not add any computational burden over bi-encoders. To support the training of the new framework, we have introduced a pipeline to efficiently synthesize high-quality data by utilizing large language models. GeAR exhibits competitive retrieval and localization performance across diverse scenarios and datasets. Moreover, the qualitative analysis and the results generated by GeAR provide novel insights into the interpretation of retrieval results. The code, data, and models will be released after completing technical review to facilitate future research.	 | 文档检索技术构成了大规模信息系统开发的基础。目前的主要方法是构建双编码器并计算语义相似度。然而，这种标量相似度很难反映出足够的信息，妨碍了我们对检索结果的理解。此外，在计算过程中主要强调全局语义，而忽视了查询与文档中复杂文本之间的精细语义关系。在本文中，我们提出了一种名为**Ge**neration **A**ugmented **R**etrieval (**GeAR**)**的新方法，该方法结合了精心设计的融合和解码模块。这使GeAR能够在查询和文档融合表示的基础上生成相关文本，从而学会“聚焦”于精细信息。此外，在用作检索器时，GeAR并不增加额外的计算负担。为了支持新框架的训练，我们引入了一种利用大型语言模型高效合成高质量数据的流水线。GeAR在多种场景和数据集上展示了竞争力的检索和定位性能。此外，GeAR的定性分析及其生成的结果为理解检索结果提供了新的见解。在完成技术审核后，我们将发布代码、数据和模型，以促进未来的研究。
2501.02754	 | MBTSAD: Mitigating Backdoors in Language Models Based on Token Splitting and Attention Distillation	 | Yidong Ding,Jiafei Niu,Ping Yi	 | 本研究提出了MBTSAD方法，通过利用少量干净数据重新训练受污染的语言模型，并在不依赖预训练权重的情况下有效缓解后门攻击，同时保持对干净数据的良好性能。MBTSAD通过注意力蒸馏和标记分割生成离群数据，促使模型学习更一般的特征以消除后门模式。	 | In recent years, attention-based models have excelled across various domains but remain vulnerable to backdoor attacks, often from downloading or fine-tuning on poisoned datasets. Many current methods to mitigate backdoors in NLP models rely on the pre-trained (unfine-tuned) weights, but these methods fail in scenarios where the pre-trained weights are not available. In this work, we propose MBTSAD, which can mitigate backdoors in the language model by utilizing only a small subset of clean data and does not require pre-trained weights. Specifically, MBTSAD retrains the backdoored model on a dataset generated by token splitting. Then MBTSAD leverages attention distillation, the retrained model is the teacher model, and the original backdoored model is the student model. Experimental results demonstrate that MBTSAD achieves comparable backdoor mitigation performance as the methods based on pre-trained weights while maintaining the performance on clean data. MBTSAD does not rely on pre-trained weights, enhancing its utility in scenarios where pre-trained weights are inaccessible. In addition, we simplify the min-max problem of adversarial training and visualize text representations to discover that the token splitting method in MBTSAD's first step generates Out-of-Distribution (OOD) data, leading the model to learn more generalized features and eliminate backdoor patterns.	 | 近年来，注意力模型在各个领域表现出色，但仍然容易受到后门攻击的影响，这些攻击通常来自受污染数据集的下载或微调。目前用于缓解NLP模型后门攻击的方法大多依赖于预训练（未微调）权重，但在这些权重不可用的情况下，这些方法会失效。在本研究中，我们提出了MBTSAD，通过利用一小部分干净数据来缓解语言模型中的后门攻击，而不需要预训练权重。具体而言，MBTSAD 在通过标记分割生成的数据集上重新训练受污染的模型，然后利用注意力蒸馏方法，重新训练的模型作为教师模型，原始受污染模型作为学生模型。实验结果显示，MBTSAD 在后门攻击缓解性能上与依赖预训练权重的方法具有可比性，同时保持对干净数据的性能。MBTSAD 不依赖于预训练权重，这增强了其在预训练权重不可用场景中的实用性。此外，我们简化了对抗训练中的最小最大问题，并可视化文本表示，发现MBTSAD 第一步中的标记分割方法生成了离群数据（OOD数据），促使模型学习更一般的特征，从而消除后门模式。
2501.02711	 | KG-CF: Knowledge Graph Completion with Context Filtering under the Guidance of Large Language Models	 | Zaiyi Zheng,Yushun Dong,Song Wang,Haochen Liu,Qi Wang,Jundong Li	 | KG-CF框架利用大型语言模型的推理能力专门针对基于排名的知识图谱补全任务，通过过滤无关背景在实际数据集上取得了优异结果。	 | Large Language Models (LLMs) have shown impressive performance in various tasks, including knowledge graph completion (KGC). However, current studies mostly apply LLMs to classification tasks, like identifying missing triplets, rather than ranking-based tasks, where the model ranks candidate entities based on plausibility. This focus limits the practical use of LLMs in KGC, as real-world applications prioritize highly plausible triplets. Additionally, while graph paths can help infer the existence of missing triplets and improve completion accuracy, they often contain redundant information. To address these issues, we propose KG-CF, a framework tailored for ranking-based KGC tasks. KG-CF leverages LLMs' reasoning abilities to filter out irrelevant contexts, achieving superior results on real-world datasets. The code and datasets are available at \url{https://anonymous.4open.science/r/KG-CF}.	 | 大型语言模型（LLMs）在各种任务上表现出色，包括知识图谱补全（KGC）。然而，当前的研究主要将LLMs应用于分类任务，如识别缺失的三元组，而不是基于排名的任务，在排名任务中，模型会根据可验证性对候选实体进行排序。这种关注限制了LLMs在KGC中的实际应用，因为实际应用更倾向于选择高度可信的三元组。此外，尽管图路径可以帮助推断缺失三元组的存在并提高补全准确性，但它们通常包含冗余信息。为了解决这些问题，我们提出了KG-CF框架，该框架专门针对基于排名的知识图谱补全任务。KG-CF利用LLMs的推理能力过滤掉无关背景，从而在实际数据集上取得了优异的结果。相关代码和数据集可在 \url{https://anonymous.4open.science/r/KG-CF} 获取。
2501.02669	 | Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?	 | Simon Park,Abhishek Panigrahi,Yun Cheng,Dingli Yu,Anirudh Goyal,Sanjeev Arora	 | 该研究引入了一个合成框架来评估视觉语言模型在算法视觉推理任务上的多步推理能力，并通过表格读取、网格导航和视觉类比三个任务及其不同难度级别，揭示了模型在模态不平衡和S2H泛化上的不足，强调了明确的图像到文本转换在自回归训练中的关键作用。	 | While Vision Language Models (VLMs) are impressive in tasks such as visual question answering (VQA) and image captioning, their ability to apply multi-step reasoning to images has lagged, giving rise to perceptions of modality imbalance or brittleness. Towards systematic study of such issues, we introduce a synthetic framework for assessing the ability of VLMs to perform algorithmic visual reasoning (AVR), comprising three tasks: Table Readout, Grid Navigation, and Visual Analogy. Each has two levels of difficulty, SIMPLE and HARD, and even the SIMPLE versions are difficult for frontier VLMs. We seek strategies for training on the SIMPLE version of the tasks that improve performance on the corresponding HARD task, i.e., S2H generalization. This synthetic framework, where each task also has a text-only version, allows a quantification of the modality imbalance, and how it is impacted by training strategy. Ablations highlight the importance of explicit image-to-text conversion in promoting S2H generalization when using auto-regressive training. We also report results of mechanistic study of this phenomenon, including a measure of gradient alignment that seems to identify training strategies that promote better S2H generalization.	 | 尽管视觉语言模型（VLMs）在视觉问答（VQA）和图像描述等任务上表现出色，但在对图像进行多步推理的能力上仍然存在不足，这导致了对模态不平衡或脆弱性的感知。为了系统地研究这些问题，我们引入了一个合成框架来评估VLMs执行算法视觉推理（AVR）的能力，该框架包括三个任务：表格读取、网格导航和视觉类比。每个任务都有简单的（SIMPLE）和困难的（HARD）两个难度级别，即使是简单的版本对最先进的VLMs来说也很具有挑战性。我们寻求在简单的任务版本上进行训练以提高对应困难任务性能的策略，即S2H泛化。这个合成框架中每个任务还都有仅文本版本，允许我们量化模态不平衡及其受训练策略影响的程度。消融实验强调了明确的图像到文本转换在使用自回归训练促进S2H泛化方面的关键作用。我们还报告了对这一现象的机制性研究结果，包括一种梯度对齐度量，这似乎能够识别出促进更好S2H泛化的训练策略。
2501.02629	 | Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for Jailbreak Attack Defense	 | Yang Ouyang,Hengrui Gu,Shuhang Lin,Wenyue Hua,Jie Peng,Bhavya Kailkhura,Tianlong Chen,Kaixiong Zhou	 | 本文介绍了一种名为Layer-AdvPatcher的新方法，通过修补大型语言模型（LLMs）内部特定层来防御“越狱”攻击，从而减少有害输出并提高模型安全性，同时保持对良性查询的响应不变。实验结果表明，该方法在多个模型和数据集上显著降低了“越狱”攻击的有害性和成功率。	 | As large language models (LLMs) are increasingly deployed in diverse applications, including chatbot assistants and code generation, aligning their behavior with safety and ethical standards has become paramount. However, jailbreak attacks, which exploit vulnerabilities to elicit unintended or harmful outputs, threaten LLMs' safety significantly. In this paper, we introduce Layer-AdvPatcher, a novel methodology designed to defend against jailbreak attacks by utilizing an unlearning strategy to patch specific layers within LLMs through self-augmented datasets. Our insight is that certain layer(s), tend to produce affirmative tokens when faced with harmful prompts. By identifying these layers and adversarially exposing them to generate more harmful data, one can understand their inherent and diverse vulnerabilities to attacks. With these exposures, we then "unlearn" these issues, reducing the impact of affirmative tokens and hence minimizing jailbreak risks while keeping the model's responses to safe queries intact. We conduct extensive experiments on two models, four benchmark datasets, and multiple state-of-the-art jailbreak benchmarks to demonstrate the efficacy of our approach. Results indicate that our framework reduces the harmfulness and attack success rate of jailbreak attacks without compromising utility for benign queries compared to recent defense methods.	 | 随着大型语言模型（LLMs）在各种应用中的日益部署，包括聊天机器人助手和代码生成，使其行为符合安全和伦理标准变得尤为重要。然而，利用漏洞促使LLMs产生意外或有害输出的“越狱”攻击，严重威胁着LLMs的安全性。在本文中，我们介绍了Layer-AdvPatcher这一新型方法，通过利用未学习策略对LLMs内部特定层进行修补，从而防御“越狱”攻击。我们的洞察是，某些层在面对有害提示时倾向于生成肯定性标记。通过识别这些层并对其进行对抗性暴露以生成更多有害数据，可以理解它们对攻击的内在和多样化的脆弱性。通过这些暴露，我们随后“遗忘”这些问题，减少了肯定性标记的影响，从而降低了“越狱”风险，同时保持模型对安全查询的响应不变。我们在两个模型、四个基准数据集和多个最先进的“越狱”基准上进行了广泛的实验，以证明我们方法的有效性。结果表明，与最近的防御方法相比，我们的框架减少了“越狱”攻击的有害性和攻击成功率，而不会牺牲对良性查询的实用性。
2501.02584	 | Efficient Architectures for High Resolution Vision-Language Models	 | Miguel Carvalho,Bruno Martins	 | Pheye是一种新颖的架构，能够高效处理高分辨率图像，相较于同等大小的视觉语言模型训练参数更少，尤其在需要精细图像理解和场景文本处理的任务中表现出色。	 | Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This work introduces Pheye, a novel architecture that efficiently processes high-resolution images while training fewer parameters than similarly sized VLMs. Notably, Pheye achieves a high efficiency while maintaining strong performance, particularly in tasks that demand fine-grained image understanding and/or the handling of scene-text.	 | 视觉语言模型（VLMs）近年来取得了显著的进步。然而，在高分辨率图像中准确识别细微细节仍然存在挑战，这限制了其在多个任务中的性能。本研究介绍了一种新颖的架构Pheye，它能够高效处理高分辨率图像，并且相较于同等大小的VLMs训练参数更少。值得注意的是，Pheye在保持较强性能的同时实现了高效率，特别是在需要精细图像理解和/或处理场景文本的任务中表现尤为突出。
2501.02573	 | LeetDecoding: A PyTorch Library for Exponentially Decaying Causal Linear Attention with CUDA Implementations	 | Jiaping Wang,Simiao Zhang,Qiao-Chu He,Yifan Chen	 | LeetDecoding是首个提供指数衰减因果线性注意力基本操作计算例行程序的Python包，旨在帮助研究者更容易地集成和测试这一新型注意力机制，加速大型语言模型的研究进展。	 | The machine learning and data science community has made significant while dispersive progress in accelerating transformer-based large language models (LLMs), and one promising approach is to replace the original causal attention in a generative pre-trained transformer (GPT) with \emph{exponentially decaying causal linear attention}. In this paper, we present LeetDecoding, which is the first Python package that provides a large set of computation routines for this fundamental operator. The launch of LeetDecoding was motivated by the current lack of (1) clear understanding of the complexity regarding this operator, (2) a comprehensive collection of existing computation methods (usually spread in seemingly unrelated fields), and (3) CUDA implementations for fast inference on GPU. LeetDecoding's design is easy to integrate with existing linear-attention LLMs, and allows for researchers to benchmark and evaluate new computation methods for exponentially decaying causal linear attention. The usage of LeetDecoding does not require any knowledge of GPU programming and the underlying complexity analysis, intentionally making LeetDecoding accessible to LLM practitioners. The source code of LeetDecoding is provided at \href{https://github.com/Computational-Machine-Intelligence/LeetDecoding}{this GitHub repository}, and users can simply install LeetDecoding by the command \texttt{pip install leet-decoding}.	 | 机器学习和数据科学社区在加速基于变压器的大型语言模型（LLMs）方面取得了显著但分散的进步，一种有前途的方法是用具有指数衰减因果线性注意力的替换生成预训练变压器（GPT）中的原始因果注意力。在本文中，我们介绍了LeetDecoding，这是第一个提供大量此类基本操作计算例行程序的Python包。LeetDecoding的推出主要是为了应对目前存在的以下问题：（1）缺乏对这一操作复杂性的清晰理解，（2）没有全面的现有计算方法集（这些方法通常分散在看似不相关的领域），以及（3）缺乏CUDA实现以加快GPU上的推理速度。LeetDecoding的设计易于与现有的线性注意力LLMs集成，并允许研究人员对指数衰减因果线性注意力的新计算方法进行基准测试和评估。使用LeetDecoding无需任何关于GPU编程和潜在的复杂性分析的知识，这使得LeetDecoding对LLMs从业者更加友好。LeetDecoding的源代码可以在\href{https://github.com/Computational-Machine-Intelligence/LeetDecoding}{这个GitHub仓库}找到，用户可以通过命令\texttt{pip install leet-decoding}简单地安装LeetDecoding。
2501.02570	 | Decoding fMRI Data into Captions using Prefix Language Modeling	 | Vyacheslav Shen,Kassymzhomart Kunanbayev,Dae-Shik Kim	 | 本文提出了一种新的脑解码方法，通过从fMRI信号预测DINOv2模型的图像嵌入，并使用GPT-2语言模型生成图像描述，从而减少计算需求并更好地利用体素的空间位置信息。	 | With the advancements in Large Language and Latent Diffusion models, brain decoding has achieved remarkable results in recent years. The works on the NSD dataset, with stimuli images from the COCO dataset, leverage the embeddings from the CLIP model for image reconstruction and GIT for captioning. However, the current captioning approach introduces the challenge of potential data contamination given that the GIT model was trained on the COCO dataset. In this work, we present an alternative method for decoding brain signals into image captions by predicting a DINOv2 model's embedding of an image from the corresponding fMRI signal and then providing its [CLS] token as the prefix to the GPT-2 language model which decreases computational requirements considerably. Additionally, instead of commonly used Linear Regression, we explore 3D Convolutional Neural Network mapping of fMRI signals to image embedding space for better accounting positional information of voxels.	 | 随着大型语言模型和隐变量扩散模型的进展，近年来脑解码取得了显著成果。基于NSD数据集和来自COCO数据集的刺激图像，这些研究利用CLIP模型的嵌入进行图像重建，并使用GIT进行图像描述。然而，当前的图像描述方法存在潜在的数据污染挑战，因为GIT模型是基于COCO数据集进行训练的。本文中，我们提出了一种替代方法，通过从对应的fMRI信号预测DINOv2模型的图像嵌入，然后将其[CLS]标记作为前缀提供给GPT-2语言模型，从而大大降低了计算需求。此外，我们探索了从fMRI信号到图像嵌入空间的3D卷积神经网络映射，而不是常用的线性回归，以更好地考虑体素的空间位置信息。
2501.02531	 | Towards New Benchmark for AI Alignment & Sentiment Analysis in Socially Important Issues: A Comparative Study of Human and LLMs in the Context of AGI	 | Ljubisa Bojic,Dylan Seychell,Milan Cabarkapa	 | 该研究通过李克特量表调查，评估了包括GPT-4和Bard在内的七种大型语言模型的情感倾向，并发现这些模型间存在多样性的情感评分，而人类的情感评分则相对较低，这表明大型语言模型有可能发展出独特的感情并影响社会观点。	 | With the expansion of neural networks, such as large language models, humanity is exponentially heading towards superintelligence. As various AI systems are increasingly integrated into the fabric of societies-through recommending values, devising creative solutions, and making decisions-it becomes critical to assess how these AI systems impact humans in the long run. This research aims to contribute towards establishing a benchmark for evaluating the sentiment of various Large Language Models in socially importan issues. The methodology adopted was a Likert scale survey. Seven LLMs, including GPT-4 and Bard, were analyzed and compared against sentiment data from three independent human sample populations. Temporal variations in sentiment were also evaluated over three consecutive days. The results highlighted a diversity in sentiment scores among LLMs, ranging from 3.32 to 4.12 out of 5. GPT-4 recorded the most positive sentiment score towards AGI, whereas Bard was leaning towards the neutral sentiment. The human samples, contrastingly, showed a lower average sentiment of 2.97. The temporal comparison revealed differences in sentiment evolution between LLMs in three days, ranging from 1.03% to 8.21%. The study's analysis outlines the prospect of potential conflicts of interest and bias possibilities in LLMs' sentiment formation. Results indicate that LLMs, akin to human cognitive processes, could potentially develop unique sentiments and subtly influence societies' perceptions towards various opinions formed within the LLMs.	 | 随着神经网络的扩展，例如大型语言模型的增长，人类正以指数级的速度朝着超智能的方向前进。随着各种人工智能系统越来越多地被整合到社会的各个方面——通过推荐价值观、提供创意解决方案和做出决策——评估这些AI系统对人类的长期影响变得至关重要。本研究旨在为此类问题的社会重要性建立评估大型语言模型情感的基准。采用的方法是李克特量表调查。分析了包括GPT-4和 Bard在内的七种大型语言模型，并将其与三个独立的人类样本群体的情感数据进行了比较。还对三天内的情感变化进行了评估。结果显示，在大型语言模型之间，情感评分存在多样性，范围从3.32到4.12分（满分5分）。GPT-4在AGI方面获得了最积极的情感评分，而Bard则倾向于中性情感。相比之下，人类样本群体的平均情感评分较低，仅为2.97。时间比较显示，在三天内，大型语言模型之间的情感演变存在差异，范围从1.03%到8.21%。研究分析指出了大型语言模型情感形成中潜在的利益冲突和偏见的可能性。结果表明，大型语言模型可能像人类的认知过程一样，有可能发展出独特的感情，并微妙地影响社会对各种观点的看法。
2501.02497	 | Test-time Computing: from System-1 Thinking to System-2 Thinking	 | Yixin Ji,Juntao Li,Hai Ye,Kaixin Wu,Jia Xu,Linjian Mo,Min Zhang	 | 该研究展示了测试时计算缩放能增强模型的System-2思考能力，并指出当前关于这一技术的全面调查仍显不足，未来应进一步探索其在不同模型类型中的作用和潜力。	 | The remarkable performance of the o1 model in complex reasoning demonstrates that test-time computing scaling can further unlock the model's potential, enabling powerful System-2 thinking. However, there is still a lack of comprehensive surveys for test-time computing scaling. We trace the concept of test-time computing back to System-1 models. In System-1 models, test-time computing addresses distribution shifts and improves robustness and generalization through parameter updating, input modification, representation editing, and output calibration. In System-2 models, it enhances the model's reasoning ability to solve complex problems through repeated sampling, self-correction, and tree search. We organize this survey according to the trend of System-1 to System-2 thinking, highlighting the key role of test-time computing in the transition from System-1 models to weak System-2 models, and then to strong System-2 models. We also point out a few possible future directions.	 | o1模型在复杂推理中的出色表现表明，测试时计算缩放可以进一步发掘模型的潜力，使其具备强大的System-2思考能力。然而，关于测试时计算缩放的全面调查仍然不足。我们追溯测试时计算的概念至System-1模型。在System-1模型中，测试时计算通过参数更新、输入修改、表征编辑和输出校准来应对分布偏移并提高鲁棒性和泛化能力。在System-2模型中，测试时计算通过重复抽样、自我校正和树搜索增强模型的推理能力以解决复杂问题。我们按照从System-1到System-2思维的趋势组织了这一调查，强调测试时计算在从System-1模型过渡到弱System-2模型，再到强System-2模型中关键的作用。同时，我们也指出了几个可能的未来发展方向。
2501.02486	 | LLMPC: Large Language Model Predictive Control	 | Gabriel Maher	 | 本文从模型预测控制（MPC）的角度研究了大规模语言模型（LLMs）的提示技术，发现LLMs在使用规划提示时相当于隐式地最小化规划成本函数，并提出通过引入实际的规划成本函数和评估器来进一步提升其规划性能。	 | Recent advancements in prompting techniques for Large Language Models (LLMs) have improved their reasoning, planning, and action abilities. This paper examines these prompting techniques through the lens of model predictive control (MPC). We show that LLMs act as implicit planning cost function minimizers when planning prompts are used. Under our framework we demonstrate that LLM planning performance can be improved further by incorporating real planning cost functions and evaluators.	 | 近年来，对于大规模语言模型（LLMs）的提示技术取得了显著进步，这提升了它们的推理、规划和执行能力。本文通过模型预测控制（MPC）的视角研究这些提示技术。我们展示，在使用规划提示时，LLMs 实际上充当了隐式的规划成本函数最小化器。在我们的框架下，我们证明通过引入实际的规划成本函数和评估器，可以进一步提高LLMs的规划性能。
2501.02441	 | A Statistical Hypothesis Testing Framework for Data Misappropriation Detection in Large Language Models	 | Yinpeng Cai,Lexin Li,Linjun Zhang	 | 本文针对大规模语言模型（LLMs）训练中数据滥用问题，特别是未经授权使用其他LLM生成内容的情况，提出了一种新颖的检测方法，通过在版权训练数据中嵌入水印并将其形式化为假设检验问题来识别数据来源。该方法不仅构建了统计检验框架，还通过理论分析和实验验证了其有效性。	 | Large Language Models (LLMs) are rapidly gaining enormous popularity in recent years. However, the training of LLMs has raised significant privacy and legal concerns, particularly regarding the inclusion of copyrighted materials in their training data without proper attribution or licensing, which falls under the broader issue of data misappropriation. In this article, we focus on a specific problem of data misappropriation detection, namely, to determine whether a given LLM has incorporated data generated by another LLM. To address this issue, we propose embedding watermarks into the copyrighted training data and formulating the detection of data misappropriation as a hypothesis testing problem. We develop a general statistical testing framework, construct a pivotal statistic, determine the optimal rejection threshold, and explicitly control the type I and type II errors. Furthermore, we establish the asymptotic optimality properties of the proposed tests, and demonstrate its empirical effectiveness through intensive numerical experiments.	 | 近年来，大规模语言模型（LLMs）迅速获得了巨大的 popularity。然而，LLMs 的训练引发了显著的隐私和法律问题，尤其是关于在训练数据中未适当标注或许可地包含版权材料的问题，这属于更广泛的数据滥用问题。本文我们专注于一个具体的数据滥用检测问题，即确定给定的 LLM 是否包含了由另一个 LLM 生成的数据。为解决这一问题，我们提出在版权训练数据中嵌入水印，并将数据滥用检测形式化为假设检验问题。我们发展了一种通用的统计检验框架，构建了一个关键统计量，确定了最优拒绝阈值，并明确控制了 I 类和 II 类错误。此外，我们建立了所提检验的渐近最优性质，并通过密集的数值实验验证了其实际有效性。
2501.02438	 | Efficient Deployment of Large Language Models on Resource-constrained Devices	 | Zhiwei Yao,Yang Xu,Hongli Xu,Yunming Liao,Zuan Xie	 | FedSpine是一种结合参数高效微调和结构化剪枝的联邦学习框架，旨在提高大型语言模型在资源受限设备上的部署效率，通过迭代剪枝和参数调整，结合在线多臂老虎机算法自适应确定剪枝比例和LoRA秩，从而提升细调效率和准确性。实验结果显示，与基线方法相比，FedSpine在保持较高推断准确性的基础上，显著提升了细调效率。	 | Deploying Large Language Models (LLMs) on resource-constrained (or weak) devices presents significant challenges due to limited resources and heterogeneous data distribution. To address the data concern, it is necessary to fine-tune LLMs using on-device private data for various downstream tasks. While Federated Learning (FL) offers a promising privacy-preserving solution, existing fine-tuning methods retain the original LLM size, leaving issues of high inference latency and excessive memory demands unresolved. Hence, we design FedSpine, an FL framework that combines Parameter- Efficient Fine-Tuning (PEFT) with structured pruning for efficient deployment of LLMs on resource-constrained devices. Specifically, FedSpine introduces an iterative process to prune and tune the parameters of LLMs. To mitigate the impact of device heterogeneity, an online Multi-Armed Bandit (MAB) algorithm is employed to adaptively determine different pruning ratios and LoRA ranks for heterogeneous devices without any prior knowledge of their computing and communication capabilities. As a result, FedSpine maintains higher inference accuracy while improving fine-tuning efficiency. Experimental results conducted on a physical platform with 80 devices demonstrate that FedSpine can speed up fine-tuning by 1.4$\times$-6.9$\times$ and improve final accuracy by 0.4%-4.5% under the same sparsity level compared to other baselines.	 | 将大型语言模型（LLMs）部署在资源受限（或较弱）的设备上，由于资源有限和数据分布异质性，面临显著挑战。为了应对数据方面的顾虑，有必要使用设备上的私有数据对LLMs进行细调，以适应各种下游任务。虽然联邦学习（Federated Learning，FL）提供了一种有前景的隐私保护解决方案，但现有的细调方法保留了原来的LLM大小，未能解决推断延迟高和内存需求过大的问题。因此，我们设计了FedSpine，这是一种结合参数高效微调（PEFT）和结构化剪枝的FED框架，旨在提高LLMs在资源受限设备上的高效部署。具体来说，FedSpine引入了一个迭代过程来剪枝和调整LLMs的参数。为减轻设备异质性的影响，我们采用了在线多臂老虎机（MAB）算法，可以在没有任何关于设备计算和通信能力先验知识的情况下，自适应地确定不同设备的剪枝比例和LoRA秩。结果，FedSpine在保持较高推断准确性的基础上，提高了细调效率。在具有80个物理设备的平台上进行的实验表明，相比于其他基线，当稀疏度水平相同时，FedSpine可以将细调速度提升1.4$\times$至6.9$\times$，并在最终准确性上提高0.4%至4.5%。
2501.02423	 | Scaling Laws for Floating Point Quantization Training	 | Xingwu Sun,Shuaipeng Li,Ruobing Xie,Weidong Han,Kan Wu,Zhen Yang,Yixing Li,An Wang,Shuai Li,Jinbao Xue,Yu Cheng,Yangyu Tao,Zhanhui Kang,Chengzhong Xu,Di Wang,Jie Jiang	 | 本文探讨了浮点量化目标、指数位、尾数位以及缩放因子计算粒度对大规模语言模型（LLM）浮点量化训练性能的影响，并提出了一套精确的浮点量化扩展定律，指出在低精度训练中最佳指数-尾数位比例、关键数据量及最优成本-性能精度范围等关键因素。	 | Low-precision training is considered an effective strategy for reducing both training and downstream inference costs. Previous scaling laws for precision mainly focus on integer quantization, which pay less attention to the constituents in floating-point quantization and thus cannot well fit the LLM losses in this scenario. In contrast, while floating-point quantization training is more commonly implemented in production, the research on it has been relatively superficial. In this paper, we thoroughly explore the effects of floating-point quantization targets, exponent bits, mantissa bits, and the calculation granularity of the scaling factor in floating-point quantization training performance of LLM models. While presenting an accurate floating-point quantization unified scaling law, we also provide valuable suggestions for the community: (1) Exponent bits contribute slightly more to the model performance than mantissa bits. We provide the optimal exponent-mantissa bit ratio for different bit numbers, which is available for future reference by hardware manufacturers; (2) We discover the formation of the critical data size in low-precision LLM training. Too much training data exceeding the critical data size will inversely bring in degradation of LLM performance; (3) The optimal floating-point quantization precision is directly proportional to the computational power, but within a wide computational power range, we estimate that the best cost-performance precision lies between 4-8 bits.	 | 低精度训练被认为是一种有效的方法，可以减少训练和下游推理的成本。之前的精度扩展定律主要集中在整数量化上，较少关注浮点量化中的构成部分，因此在低层次语言模型（LLM）训练场景中不能很好地拟合损失。相比之下，尽管在生产中浮点量化训练更为常见，但关于其的研究相对较少。本文深入探讨了浮点量化目标、指数位、尾数位以及缩放因子计算粒度对LLM模型浮点量化训练性能的影响。同时，我们提出了一个精确的浮点量化统一扩展定律，并为社区提供了宝贵的建议：(1) 指数位对模型性能的贡献略高于尾数位。我们提供了适用于不同位数的最优指数-尾数位比例，这可供未来硬件制造商参考；(2) 我们发现低精度LLM训练中的关键数据量。超过关键数据量的过多训练数据会反向导致LLM性能下降；(3) 最佳浮点量化精度与计算能力成正比，但在广泛的计算能力范围内，我们估计最佳成本-性能精度位于4-8位之间。
2501.02406	 | Who Wrote This? Zero-Shot Statistical Tests for LLM-Generated Text Detection using Finite Sample Concentration Inequalities	 | Tara Radvand,Mojtaba Abdolmaleki,Mohamed Mostagir,Ambuj Tewari	 | 本文提出了一种零样本统计测试方法，通过分析由不同语言模型生成的文本在对数困惑度和平均熵方面的差异，以确定文本是由内部受控语言模型还是外部非许可语言模型生成的，从而帮助识别并打击虚假信息。	 | Verifying the provenance of content is crucial to the function of many organizations, e.g., educational institutions, social media platforms, firms, etc. This problem is becoming increasingly difficult as text generated by Large Language Models (LLMs) becomes almost indistinguishable from human-generated content. In addition, many institutions utilize in-house LLMs and want to ensure that external, non-sanctioned LLMs do not produce content within the institution. In this paper, we answer the following question: Given a piece of text, can we identify whether it was produced by LLM $A$ or $B$ (where $B$ can be a human)? We model LLM-generated text as a sequential stochastic process with complete dependence on history and design zero-shot statistical tests to distinguish between (i) the text generated by two different sets of LLMs $A$ (in-house) and $B$ (non-sanctioned) and also (ii) LLM-generated and human-generated texts. We prove that the type I and type II errors for our tests decrease exponentially in the text length. In designing our tests, we derive concentration inequalities on the difference between log-perplexity and the average entropy of the string under $A$. Specifically, for a given string, we demonstrate that if the string is generated by $A$, the log-perplexity of the string under $A$ converges to the average entropy of the string under $A$, except with an exponentially small probability in string length. We also show that if $B$ generates the text, except with an exponentially small probability in string length, the log-perplexity of the string under $A$ converges to the average cross-entropy of $B$ and $A$. Lastly, we present preliminary experimental results to support our theoretical results. By enabling guaranteed (with high probability) finding of the origin of harmful LLM-generated text with arbitrary size, we can help fight misinformation.	 | 验证内容的来源对于许多组织的功能至关重要，例如教育机构、社交媒体平台、企业等。随着大型语言模型（LLMs）生成的文本变得几乎难以与人类生成的内容区分开来，这个问题变得越来越难以解决。此外，许多机构使用内部LLMs，并希望确保外部未经许可的LLMs不生成机构内的内容。在本文中，我们回答了以下问题：给定一段文本，我们能否确定它是由LLM \(A\) 还是 \(B\)（其中 \(B\) 可以是人类）生成的？我们将LLM生成的文本视为一个完全依赖于历史的序列随机过程，并设计零样本统计测试来区分（i）由两个不同集的LLM \(A\)（内部）和 \(B\)（未经许可）生成的文本，以及（ii）LLM生成的文本和人类生成的文本。我们证明，对于我们的测试，I型错误和II型错误随着文本长度的增加呈指数级降低。在设计我们的测试时，我们推导了关于字符串在LLM \(A\) 下的对数困惑度与平均熵之间的差异的集中不等式。具体来说，对于给定的字符串，我们证明了如果字符串由 \(A\) 生成，那么字符串在 \(A\) 下的对数困惑度将收敛到字符串在 \(A\) 下的平均熵，除非字符串长度中以指数级概率不发生。我们还证明，如果 \(B\) 生成了文本，除非字符串长度中以指数级概率不发生，那么字符串在 \(A\) 下的对数困惑度将收敛到 \(B\) 和 \(A\) 的平均交叉熵。最后，我们给出了初步的实验结果以支持我们的理论结果。通过提供高概率下识别任意大小的有害LLM生成文本的来源的能力，我们可以帮助打击虚假信息。
2501.02393	 | Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers	 | Markus J. Buehler	 | 本文提出了一种将图神经网络与注意力机制结合的新方法——图感知同构注意力（Graph-Aware Isomorphic Attention），并通过稀疏GIN-注意力进一步增强了其适应性，提高了模型在关系推理和序列数据建模任务中的性能和泛化能力。	 | We present an approach to modifying Transformer architectures by integrating graph-aware relational reasoning into the attention mechanism, merging concepts from graph neural networks and language modeling. Building on the inherent connection between attention and graph theory, we reformulate the Transformer's attention mechanism as a graph operation and propose Graph-Aware Isomorphic Attention. This method leverages advanced graph modeling strategies, including Graph Isomorphism Networks (GIN) and Principal Neighborhood Aggregation (PNA), to enrich the representation of relational structures. Our approach captures complex dependencies and generalizes across tasks, as evidenced by a reduced generalization gap and improved learning performance. Additionally, we expand the concept of graph-aware attention to introduce Sparse GIN-Attention, a fine-tuning approach that employs sparse GINs. By interpreting attention matrices as sparse adjacency graphs, this technique enhances the adaptability of pre-trained foundational models with minimal computational overhead, endowing them with graph-aware capabilities. Sparse GIN-Attention fine-tuning achieves improved training dynamics and better generalization compared to alternative methods like low-rank adaption (LoRA). We discuss latent graph-like structures within traditional attention mechanisms, offering a new lens through which Transformers can be understood. By evolving Transformers as hierarchical GIN models for relational reasoning. This perspective suggests profound implications for foundational model development, enabling the design of architectures that dynamically adapt to both local and global dependencies. Applications in bioinformatics, materials science, language modeling, and beyond could benefit from this synthesis of relational and sequential data modeling, setting the stage for interpretable and generalizable modeling strategies.	 | 我们提出了一种通过将图感知关系推理整合到注意力机制中来修改Transformer架构的方法，融合了图神经网络和语言模型的概念。基于注意力机制与图论之间的固有联系，我们将Transformer的注意力机制重新表述为图操作，并提出了图感知同构注意力（Graph-Aware Isomorphic Attention）。该方法利用了先进的图建模策略，包括图同构网络（GIN）和主小区聚合（PNA），以丰富关系结构的表示。我们的方法捕获了复杂的依赖关系，并在任务迁移中表现出较好的泛化能力。此外，我们将图感知注意力的概念扩展到了稀疏GIN-注意力，这是一种采用稀疏GIN的微调方法。通过将注意力矩阵解释为稀疏邻接图，该技术通过最小的计算开销增强了预训练基础模型的适应性，赋予了它们图感知的能力。稀疏GIN-注意力微调比低秩适应（LoRA）等替代方法在训练动态和泛化性能方面表现更优。我们讨论了传统注意力机制中的潜在图状结构，为理解Transformer提供了新的视角。通过将Transformer演化为用于关系推理的分层GIN模型，这种观点为基础模型的发展提供了深远的影响，使得能够设计出能够动态适应局部和全局依赖的架构。这种关系和序列数据建模的综合应用于生物信息学、材料科学、语言建模等领域，为可解释和泛化建模策略奠定了基础。
2501.02342	 | Optimizing Small Language Models for In-Vehicle Function-Calling	 | Yahya Sowti Khiabani,Farris Atif,Chieh Hsu,Sven Stahlmann,Tobias Michels,Sebastian Kramer,Benedikt Heidrich,M. Saquib Sarfraz,Julian Merten,Faezeh Tafazzoli	 | 本文提出了一种利用小型语言模型作为车辆边缘设备功能调用代理的方法，通过模型压缩和优化技术，使SLMs能够在资源受限的车载硬件上实现高效、实时的本地推理，提升驾驶体验。研究表明，即使大幅减少模型规模，该方法仍能保持处理复杂车载任务的能力，并实现每秒11个词元的生成速度。	 | We propose a holistic approach for deploying Small Language Models (SLMs) as function-calling agents within vehicles as edge devices, offering a more flexible and robust alternative to traditional rule-based systems. By leveraging SLMs, we simplify vehicle control mechanisms and enhance the user experience. Given the in-vehicle hardware constraints, we apply state-of-the-art model compression techniques, including structured pruning, healing, and quantization, ensuring that the model fits within the resource limitations while maintaining acceptable performance. Our work focuses on optimizing a representative SLM, Microsoft's Phi-3 mini, and outlines best practices for enabling embedded models, including compression, task-specific fine-tuning, and vehicle integration. We demonstrate that, despite significant reduction in model size which removes up to 2 billion parameters from the original model, our approach preserves the model's ability to handle complex in-vehicle tasks accurately and efficiently. Furthermore, by executing the model in a lightweight runtime environment, we achieve a generation speed of 11 tokens per second, making real-time, on-device inference feasible without hardware acceleration. Our results demonstrate the potential of SLMs to transform vehicle control systems, enabling more intuitive interactions between users and their vehicles for an enhanced driving experience.	 | 我们提出了一种全面的方法，将小型语言模型（SLMs）部署为车辆中的边缘设备功能调用代理，作为一种比传统基于规则的系统更灵活和稳健的替代方案。通过利用SLMs，我们可以简化车辆控制机制并提升用户体验。考虑到车载硬件的限制，我们应用了最先进的模型压缩技术，包括结构化剪枝、修复和量化，确保模型能在资源限制内运行同时保持可接受的性能。我们的工作集中在优化一个代表性的SLM——微软的Phi-3 mini，并概述了使嵌入式模型可工作的最佳实践，包括压缩、任务特定微调以及车辆集成。我们证明，尽管模型规模有显著减少，移除了原始模型中的多达2亿个参数，我们的方法仍能保持模型处理复杂车载任务的能力。此外，通过在轻量级运行时环境中执行模型，我们实现了每秒11个词元的生成速度，使得在不使用硬件加速的情况下进行实时、本地推理成为可能。我们的结果展示了SLMs有潜力彻底改变车辆控制系统，使用户与车辆之间的互动更加直观，从而提升驾驶体验。
2501.02211	 | Examining the Robustness of Homogeneity Bias to Hyperparameter Adjustments in GPT-4	 | Messi H.J. Lee	 | 该研究通过探索GPT-4的超参数调整，发现虽然可以一定程度上缓解种族同质性偏见，但不能有效解决性别同质性偏见，且不同超参数对偏见的影响表现出非线性关系。研究揭示了视觉-语言模型中同质性偏见的普遍性和复杂性。	 | Vision-Language Models trained on massive collections of human-generated data often reproduce and amplify societal stereotypes. One critical form of stereotyping reproduced by these models is homogeneity bias-the tendency to represent certain groups as more homogeneous than others. We investigate how this bias responds to hyperparameter adjustments in GPT-4, specifically examining sampling temperature and top p which control the randomness of model outputs. By generating stories about individuals from different racial and gender groups and comparing their similarities using vector representations, we assess both bias robustness and its relationship with hyperparameter values. We find that (1) homogeneity bias persists across most hyperparameter configurations, with Black Americans and women being represented more homogeneously than White Americans and men, (2) the relationship between hyperparameters and group representations shows unexpected non-linear patterns, particularly at extreme values, and (3) hyperparameter adjustments affect racial and gender homogeneity bias differently-while increasing temperature or decreasing top p can reduce racial homogeneity bias, these changes show different effects on gender homogeneity bias. Our findings suggest that while hyperparameter tuning may mitigate certain biases to some extent, it cannot serve as a universal solution for addressing homogeneity bias across different social group dimensions.	 | 基于大量人类生成数据训练的视觉-语言模型往往会复制和放大社会刻板印象。这些模型复制和放大的一种关键形式是同质性偏见——趋势是将某些群体描绘得比其他群体更同质。我们研究了这种偏见如何响应GPT-4的超参数调整，特别是探索控制模型输出随机性的采样温度和top p。通过生成来自不同种族和性别群体的个人故事，并使用向量表示比较它们的相似性，我们评估了偏见的稳健性和其与超参数值的关系。我们发现：（1）同质性偏见在大多数超参数配置中持续存在，黑人美国人和女性比白人美国人和男性更被描绘成更同质；（2）超参数与群体表示之间的关系显示出不寻常的非线性模式，特别是在极端值时；（3）超参数调整对种族和性别同质性偏见的影响不同——增加温度或减少top p可以减少种族同质性偏见，但这些变化对性别同质性偏见的影响不同。我们的研究结果表明，虽然超参数调整可能在一定程度上缓解某些偏见，但它不能作为解决不同社会群体维度同质性偏见的通用解决方案。
2501.02189	 | Benchmark Evaluations, Applications, and Challenges of Large Vision Language Models: A Survey	 | Zongxia Li,Xiyang Wu,Hongyang Du,Huy Nghiem,Guangyao Shi	 | 多模态视觉语言模型（VLMs）已成为连接计算机视觉和自然语言处理的关键技术，显著提升了机器在视觉和文本数据上的推理和理解能力，但仍缺乏对此领域的系统性综述；本文对此进行了全面回顾，涵盖了过去五年主要的VLM模型、架构、训练方法、评估指标、应用场景以及面临的挑战。	 | Multimodal Vision Language Models (VLMs) have emerged as a transformative technology at the intersection of computer vision and natural language processing, enabling machines to perceive and reason about the world through both visual and textual modalities. For example, models such as CLIP, Claude, and GPT-4V demonstrate strong reasoning and understanding abilities on visual and textual data and beat classical single modality vision models on zero-shot classification. Despite their rapid advancements in research and growing popularity in applications, a comprehensive survey of existing studies on VLMs is notably lacking, particularly for researchers aiming to leverage VLMs in their specific domains. To this end, we provide a systematic overview of VLMs in the following aspects: model information of the major VLMs developed over the past five years (2019-2024); the main architectures and training methods of these VLMs; summary and categorization of the popular benchmarks and evaluation metrics of VLMs; the applications of VLMs including embodied agents, robotics, and video generation; the challenges and issues faced by current VLMs such as hallucination, fairness, and safety. Detailed collections including papers and model repository links are listed in https://github.com/zli12321/Awesome-VLM-Papers-And-Models.git.	 | 多模态视觉语言模型（Multimodal Vision Language Models, VLMs）已成为计算机视觉和自然语言处理交汇领域的变革性技术，使机器能够通过视觉和文本两种模态感知和推理世界。例如，CLIP、Claude和GPT-4V等模型在视觉和文本数据上的推理和理解能力表现出色，并在零样本分类中击败了经典的单模态视觉模型。尽管这些模型在研究和应用中的发展非常迅速，但关于VLMs的研究综述仍然非常缺乏，尤其是对于希望在特定领域利用VLMs的研究人员而言。为了解决这一问题，我们从以下几个方面提供了系统性的综述：过去五年（2019-2024）主要开发的VLMs的模型信息；这些VLMs的主要架构和训练方法；VLMs的流行基准和评估指标的总结和分类；VLMs的应用领域，包括具身智能体、机器人技术和视频生成；当前VLMs面临的一些挑战和问题，如幻觉、公平性和安全性。详细的文献和模型存储库链接收藏在https://github.com/zli12321/Awesome-VLM-Papers-And-Models.git。
2501.02152	 | Table as Thought: Exploring Structured Thoughts in LLM Reasoning	 | Zhenjie Sun,Naihao Deng,Haofei Yu,Jiaxuan You	 | 提出了一种名为“Table as Thought”的框架，该框架通过在表格中组织思维步骤和上下文信息来改进大型语言模型的推理能力，并在规划任务和数学推理中表现出色，显著提升了模型性能。	 | Large language models' reasoning abilities benefit from methods that organize their thought processes, such as chain-of-thought prompting, which employs a sequential structure to guide the reasoning process step-by-step. However, existing approaches focus primarily on organizing the sequence of thoughts, leaving structure in individual thought steps underexplored. To address this gap, we propose Table as Thought, a framework inspired by cognitive neuroscience theories on human thought. Table as Thought organizes reasoning within a tabular schema, where rows represent sequential thought steps and columns capture critical constraints and contextual information to enhance reasoning. The reasoning process iteratively populates the table until self-verification ensures completeness and correctness. Our experiments show that Table as Thought excels in planning tasks and demonstrates a strong potential for enhancing LLM performance in mathematical reasoning compared to unstructured thought baselines. This work provides a novel exploration of refining thought representation within LLMs, paving the way for advancements in reasoning and AI cognition.	 | 大型语言模型的推理能力可以从组织其思维过程的方法中受益，例如链式思考提示，这种方法通过逐步的顺序结构来引导推理过程。然而，现有的方法主要集中在组织思维的顺序上，而忽略了单个思维步骤中的结构挖掘不足。为了解决这一问题，我们提出了一种名为“Table as Thought”的框架，该框架受到认知神经科学中关于人类思维理论的启发。Table as Thought以表格模式组织推理，其中行代表顺序的思维步骤，列则捕捉关键的约束条件和上下文信息，以增强推理能力。推理过程会逐步填充表格，直到自我验证确保推理的完整性和准确性。我们的实验表明，Table as Thought在规划任务中表现出色，并且在数学推理方面相较于无结构思维基线，具有显著提升大型语言模型性能的潜力。这项工作为改进大型语言模型中的思维表示提供了新的探索，为推理和人工智能认知的进步开辟了道路。
2501.02045	 | METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring	 | Ollie Liu,Sami Jaghouar,Johannes Hagemann,Shangshang Wang,Jason Wiemels,Jeff Kaufman,Willie Neiswanger	 | 研究人员开发了一个名为METAGENE-1的70亿参数自回归变压器模型，该模型基于多样宏基因组DNA和RNA序列训练，旨在捕捉下水道中基因组信息的完整分布，用于流行病监测和病原体检测；在预训练和评估中，METAGENE-1显示出优越的性能，特别是在基因组基准测试和人类病原体检测方面。	 | We pretrain METAGENE-1, a 7-billion-parameter autoregressive transformer model, which we refer to as a metagenomic foundation model, on a novel corpus of diverse metagenomic DNA and RNA sequences comprising over 1.5 trillion base pairs. This dataset is sourced from a large collection of human wastewater samples, processed and sequenced using deep metagenomic (next-generation) sequencing methods. Unlike genomic models that focus on individual genomes or curated sets of specific species, the aim of METAGENE-1 is to capture the full distribution of genomic information present within this wastewater, to aid in tasks relevant to pandemic monitoring and pathogen detection. We carry out byte-pair encoding (BPE) tokenization on our dataset, tailored for metagenomic sequences, and then pretrain our model. In this paper, we first detail the pretraining dataset, tokenization strategy, and model architecture, highlighting the considerations and design choices that enable the effective modeling of metagenomic data. We then show results of pretraining this model on our metagenomic dataset, providing details about our losses, system metrics, and training stability over the course of pretraining. Finally, we demonstrate the performance of METAGENE-1, which achieves state-of-the-art results on a set of genomic benchmarks and new evaluations focused on human-pathogen detection and genomic sequence embedding, showcasing its potential for public health applications in pandemic monitoring, biosurveillance, and early detection of emerging health threats.	 | 我们预训练了一个名为METAGENE-1的70亿参数自回归变压器模型，我们将其称为宏基因组基础模型，该模型基于一个包含超过1.5万亿碱基对的多样宏基因组DNA和RNA序列的新型语料库进行训练。这些数据来源于人类下水道样本的大量集合，并通过深度宏基因组（下一代）测序方法进行处理和测序。与专注于个体基因组或特定物种集合的基因组模型不同，METAGENE-1的目标是捕捉下水道中基因组信息的完整分布，以辅助与流行病监测和病原体检测相关的任务。我们在宏基因组序列上进行了字节对编码（BPE）分词，并预训练了我们的模型。在这篇论文中，我们首先详细介绍了预训练数据集、分词策略和模型架构，突出了使宏基因组数据有效建模的考虑和设计选择。然后，我们展示了在宏基因组数据集上预训练该模型的结果，提供了关于我们的损失、系统指标和预训练期间训练稳定性的详细信息。最后，我们展示了METAGENE-1的性能，该模型在一组基因组基准测试和新的侧重于人类病原体检测和基因组序列嵌入的评估中取得了最先进的结果，展示了其在流行病监测、生物监控和早期检测新兴健康威胁方面的潜在应用前景。
2501.01982	 | Is Your Image a Good Storyteller?	 | Xiujie Song,Xiaoyi Pang,Haifeng Tang,Mengyue Wu,Kenny Q. Zhu	 | 该研究强调了评估图像语义复杂性的必要性，指出尽管量化图像的视觉复杂性相对容易，但语义复杂性却被广泛忽视，而富含语义的图像对视觉模型的发展和人类认知评估具有重要意义。为此，研究提出了图像语义评估（ISA）任务，并构建了首个ISA数据集，利用语言解决视觉问题的新方法，实验表明该方法有效。	 | Quantifying image complexity at the entity level is straightforward, but the assessment of semantic complexity has been largely overlooked. In fact, there are differences in semantic complexity across images. Images with richer semantics can tell vivid and engaging stories and offer a wide range of application scenarios. For example, the Cookie Theft picture is such a kind of image and is widely used to assess human language and cognitive abilities due to its higher semantic complexity. Additionally, semantically rich images can benefit the development of vision models, as images with limited semantics are becoming less challenging for them. However, such images are scarce, highlighting the need for a greater number of them. For instance, there is a need for more images like Cookie Theft to cater to people from different cultural backgrounds and eras. Assessing semantic complexity requires human experts and empirical evidence. Automatic evaluation of how semantically rich an image will be the first step of mining or generating more images with rich semantics, and benefit human cognitive assessment, Artificial Intelligence, and various other applications. In response, we propose the Image Semantic Assessment (ISA) task to address this problem. We introduce the first ISA dataset and a novel method that leverages language to solve this vision problem. Experiments on our dataset demonstrate the effectiveness of our approach. Our data and code are available at: https://github.com/xiujiesong/ISA.	 | 量化图像在实体层面上的复杂性是很简单的，但对语义复杂性的评估却 largely 被忽略了。实际上，不同图像之间的语义复杂性是有差异的。富含语义的图像可以讲述生动且引人入胜的故事，并提供广泛的应用场景。例如，The Cookie Theft 图像就是这样一种图像，并且由于其较高的语义复杂性，常被用于评估人类语言和认知能力。此外，富含语义的图像对视觉模型的发展有益，因为具有有限语义的图像对它们来说越来越不具有挑战性。然而，这样的图像稀缺，突显了需要更多这类图像的需求。例如，为了适应不同的文化背景和时代，需要更多类似 The Cookie Theft 的图像。评估语义复杂性需要人类专家和实证证据。自动评估图像的语义丰富程度是挖掘或生成更多富含语义图像的第一步，并有助于人类的认知评估、人工智能以及各种其他应用。为此，我们提出了图像语义评估（ISA）任务来解决这个问题。我们介绍了首个 ISA 数据集以及一种利用语言来解决这一视觉问题的新方法。在我们的数据集上进行的实验展示了我们方法的有效性。我们的数据和代码可在 https://github.com/xiujiesong/ISA 获取。
