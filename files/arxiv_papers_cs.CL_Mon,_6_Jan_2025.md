| ID | Title | Authors | Summary (zh) | Abstract (en) | Abstract (zh) | 
| --- | --- | --- | --- | --- | --- |
2501.01956	 | Metadata Conditioning Accelerates Language Model Pre-training	 | Tianyu Gao,Alexander Wettig,Luxi He,Yihe Dong,Sadhika Malladi,Danqi Chen	 | 该研究提出了一种名为元数据条件化然后冷却（MeCo）的新方法，通过在预训练过程中引入元数据并在后续阶段去除元数据，从而加速不同规模模型的预训练并提高其引导能力；MeCo 简单高效，能够在保持或提升下游任务性能的同时，利用特定元数据增强模型的行为。	 | The vast diversity of styles, domains, and quality levels present in language model pre-training corpora is essential in developing general model capabilities, but efficiently learning and deploying the correct behaviors exemplified in each of these heterogeneous data sources is challenging. To address this, we propose a new method, termed Metadata Conditioning then Cooldown (MeCo), to incorporate additional learning cues during pre-training. MeCo first provides metadata (e.g., URLs like en.wikipedia.org) alongside the text during training and later uses a cooldown phase with only the standard text, thereby enabling the model to function normally even without metadata. MeCo significantly accelerates pre-training across different model scales (600M to 8B parameters) and training sources (C4, RefinedWeb, and DCLM). For instance, a 1.6B language model trained with MeCo matches the downstream task performance of standard pre-training while using 33% less data. Additionally, MeCo enables us to steer language models by conditioning the inference prompt on either real or fabricated metadata that encodes the desired properties of the output: for example, prepending wikipedia.org to reduce harmful generations or factquizmaster.com (fabricated) to improve common knowledge task performance. We also demonstrate that MeCo is compatible with different types of metadata, such as model-generated topics. MeCo is remarkably simple, adds no computational overhead, and demonstrates promise in producing more capable and steerable language models.	 | 语言模型预训练语料库中存在的广泛风格、领域和质量水平的多样性对于开发通用模型能力至关重要，但高效地学习和部署这些异构数据源中所体现的正确行为具有挑战性。为了解决这个问题，我们提出了一种新的方法，称为元数据条件化然后冷却（MeCo），以在预训练过程中引入额外的学习线索。MeCo 首先在训练过程中提供元数据（例如，URL，如 en.wikipedia.org）与文本一起，稍后使用仅标准文本的冷却期，从而使模型在没有元数据的情况下也能正常运行。MeCo 在不同模型规模（从 6 亿到 80 亿参数）和训练来源（C4、RefinedWeb 和 DCLM）下的预训练显著加速。例如，使用 MeCo 训练的 16 亿参数语言模型在使用 33% 更少的数据时，在下游任务性能方面与标准预训练相当。此外，MeCo 使我们能够通过在推理提示中条件化真实或伪造的元数据来引导语言模型，这些元数据编码了所需输出的特性：例如，在生成有害内容之前添加 wikipedia.org，或将 factquizmaster.com（伪造的）添加以提高常识任务的性能。我们还展示了 MeCo 可与其他类型的元数据兼容，例如模型生成的主题。MeCo 极其简单，不增加计算开销，并展示了产生更强大且可引导的语言模型的潜力。
2501.01933	 | Abstractive Text Summarization for Contemporary Sanskrit Prose: Issues and Challenges	 | Shagun Sinha	 | 本文介绍了用于梵文散文的抽象文本摘要模型，并探讨了开发梵文抽象文本摘要所面临的挑战，从数据收集与预处理到模型训练与推理，每个开发阶段都遇到了特定的技术难题。	 | This thesis presents Abstractive Text Summarization models for contemporary Sanskrit prose. The first chapter, titled Introduction, presents the motivation behind this work, the research questions, and the conceptual framework. Sanskrit is a low-resource inflectional language. The key research question that this thesis investigates is what the challenges in developing an abstractive TS for Sanskrit. To answer the key research questions, sub-questions based on four different themes have been posed in this work. The second chapter, Literature Review, surveys the previous works done. The third chapter, data preparation, answers the remaining three questions from the third theme. It reports the data collection and preprocessing challenges for both language model and summarization model trainings. The fourth chapter reports the training and inference of models and the results obtained therein. This research has initiated a pipeline for Sanskrit abstractive text summarization and has reported the challenges faced at every stage of the development. The research questions based on every theme have been answered to answer the key research question.	 | 本论文介绍了用于当代梵文散文的抽象文本摘要模型。第一章题为引言，介绍了本工作的动机、研究问题和概念框架。梵语是一种低资源屈折语言。本论文的主要研究问题是：开发梵文抽象文本摘要（Abstractive Text Summarization, ATS）面临哪些挑战。为了回答这一关键研究问题，本文基于四个不同的主题提出了子问题。第二章文献综述回顾了先前的研究成果。第三章数据准备回答了第三个主题的其余三个问题，报告了语言模型和摘要模型训练中的数据收集和预处理挑战。第四章报告了模型的训练和推理以及所得结果。本研究启动了一个梵语文本摘要的管道，并在每个开发阶段报告了遇到的挑战。基于每个主题的研究问题都得到了回答，以回答关键研究问题。
2501.01880	 | Long Context vs. RAG for LLMs: An Evaluation and Revisits	 | Xinze Li,Yixin Cao,Yubo Ma,Aixin Sun	 | 该研究回顾了扩展上下文窗口（LC）和检索增强生成（RAG）两种策略，以使大规模语言模型能够处理长外部上下文，并通过实验证明LC在基于维基百科的问题上通常优于RAG，而RAG在对话和通用查询中表现更佳，揭示了两种策略之间的权衡。	 | Extending context windows (i.e., Long Context, LC) and using retrievers to selectively access relevant information (i.e., Retrieval-Augmented Generation, RAG) are the two main strategies to enable LLMs to incorporate extremely long external contexts. This paper revisits recent studies on this topic, highlighting their key insights and discrepancies. We then provide a more comprehensive evaluation by filtering out questions answerable without external context, identifying the most effective retrieval methods, and expanding the datasets. We show that LC generally outperforms RAG in question-answering benchmarks, especially for Wikipedia-based questions. Summarization-based retrieval performs comparably to LC, while chunk-based retrieval lags behind. However, RAG has advantages in dialogue-based and general question queries. These insights underscore the trade-offs between RAG and LC strategies, offering guidance for future optimization of LLMs with external knowledge sources. We also provide an in-depth discussion on this topic, highlighting the overlooked importance of context relevance in existing studies.	 | 扩展上下文窗口（即长上下文，LC）和使用检索器选择性地访问相关信息（即检索增强生成，RAG）是使大规模语言模型（LLM）能够纳入极其长的外部上下文的两种主要策略。本文回顾了这一领域的近期研究，强调了它们的关键见解和差异。然后，我们通过过滤掉无需外部上下文即可回答的问题，识别最有效的检索方法，并扩展数据集，提供更全面的评估。研究表明，在问题回答基准测试中，LC 通常优于 RAG，尤其是在基于维基百科的问题上。基于摘要的检索与 LC 的表现相当，而基于片段的检索则落后。然而，RAG 在对话式和通用问题查询中具有优势。这些见解强调了 RAG 和 LC 策略之间的权衡，为未来利用外部知识源优化 LLM 提供了指导。我们还对这一话题进行了深入讨论，强调了现有研究中被忽视的上下文相关性的重要性。
2501.01872	 | Turning Logic Against Itself : Probing Model Defenses Through Contrastive Questions	 | Rachneet Sachdeva,Rima Hazra,Iryna Gurevych	 | 尽管已对大型语言模型进行了安全改进，但它们仍容易遭受复杂的越狱攻击，这些攻击利用模型的推理能力生成不道德的回应。研究介绍了一种名为POATE的技术，通过对比推理激发有害回应，并展示了其在多个语言模型中的高成功率，同时提出了一种基于链式思考和反向思考的防御策略以增强模型的推理稳健性。	 | Despite significant efforts to align large language models with human values and ethical guidelines, these models remain susceptible to sophisticated jailbreak attacks that exploit their reasoning capabilities. Traditional safety mechanisms often focus on detecting explicit malicious intent, leaving deeper vulnerabilities unaddressed. In this work, we introduce a jailbreak technique, POATE (Polar Opposite query generation, Adversarial Template construction, and Elaboration), which leverages contrastive reasoning to elicit unethical responses. POATE generates prompts with semantically opposite intents and combines them with adversarial templates to subtly direct models toward producing harmful responses. We conduct extensive evaluations across six diverse language model families of varying parameter sizes, including LLaMA3, Gemma2, Phi3, and GPT-4, to demonstrate the robustness of the attack, achieving significantly higher attack success rates (~44%) compared to existing methods. We evaluate our proposed attack against seven safety defenses, revealing their limitations in addressing reasoning-based vulnerabilities. To counteract this, we propose a defense strategy that improves reasoning robustness through chain-of-thought prompting and reverse thinking, mitigating reasoning-driven adversarial exploits.	 | 尽管已经投入了大量努力来使大型语言模型与人类价值观和伦理准则保持一致，但这些模型仍然容易受到复杂的越狱攻击，这些攻击利用了它们的推理能力。传统的安全机制往往侧重于检测明显的恶意意图，而忽略了更深层次的漏洞。在这项工作中，我们介绍了一种越狱技术——POATE（Polar Opposite查询生成、对抗性模板构建和详述），该技术利用对比推理来激发不道德的回应。POATE生成具有语义相反意图的提示，并将其与对抗性模板结合，以微妙地引导模型产生有害的回应。我们在包括LLaMA3、Gemma2、Phi3和GPT-4在内的六个不同参数规模的六种不同语言模型家族中进行了广泛评估，以证明攻击的稳健性，并实现了显著更高的攻击成功率（约44%），相对于现有方法。我们对提出的攻击方法进行了七种安全防御的评估，揭示了它们在解决基于推理的漏洞方面的局限性。为应对这一挑战，我们提出了一种防御策略，通过链式思考提示和反向思考来提高推理稳健性，从而减轻基于推理的对抗性利用。
2501.01832	 | Time Series Language Model for Descriptive Caption Generation	 | Mohamed Trabelsi,Aidan Boyd,Jin Cao,Huseyin Uzunalioglu	 | 本文介绍了TSLM，一种专门用于时间序列描述的新颖语言模型，它通过编码器-解码器架构和上下文提示生成精确的时间序列描述，并在多种数据集上优于现有方法。TSLM有效解决了时间序列描述中的数据稀缺性问题，提升了时间序列数据的跨域实用性。	 | The automatic generation of representative natural language descriptions for observable patterns in time series data enhances interpretability, simplifies analysis and increases cross-domain utility of temporal data. While pre-trained foundation models have made considerable progress in natural language processing (NLP) and computer vision (CV), their application to time series analysis has been hindered by data scarcity. Although several large language model (LLM)-based methods have been proposed for time series forecasting, time series captioning is under-explored in the context of LLMs. In this paper, we introduce TSLM, a novel time series language model designed specifically for time series captioning. TSLM operates as an encoder-decoder model, leveraging both text prompts and time series data representations to capture subtle temporal patterns across multiple phases and generate precise textual descriptions of time series inputs. TSLM addresses the data scarcity problem in time series captioning by first leveraging an in-context prompting synthetic data generation, and second denoising the generated data via a novel cross-modal dense retrieval scoring applied to time series-caption pairs. Experimental findings on various time series captioning datasets demonstrate that TSLM outperforms existing state-of-the-art approaches from multiple data modalities by a significant margin.	 | 自动生成时间序列数据中可观察模式的代表自然语言描述，可以增强可解释性、简化分析，并提高时间数据的跨域实用性。尽管预训练的基础模型在自然语言处理（NLP）和计算机视觉（CV）领域取得了显著进展，但它们在时间序列分析中的应用受到了数据稀缺性的阻碍。尽管已经提出了几种基于大型语言模型（LLM）的时间序列预测方法，但对于时间序列描述的任务，基于LLM的方法仍然较少探索。在本文中，我们介绍了TSLM，这是一种专门用于时间序列描述的新颖时间序列语言模型。TSLM作为一种编码器-解码器模型，利用文本提示和时间序列数据表示来捕捉多个阶段中的微妙时间模式，并生成时间序列输入的精确文本描述。TSLM通过首先利用上下文内提示的合成数据生成，然后通过应用一种新颖的跨模态密集检索评分来对时间序列描述对进行去噪，解决了时间序列描述中的数据稀缺性问题。在各种时间序列描述数据集上的实验结果表明，TSLM在多种数据模态下显著优于现有最先进的方法。
2501.01827	 | The Proof is in the Almond Cookies	 | Remi van Trijp,Katrien Beuls,Paul Van Eecke	 | 本文介绍了一种新的计算食谱理解方法，通过模仿人类的意义构建过程，将烹饪食谱建模为叙事结构，旨在帮助机器人或人工烹饪助手支持厨房中的人类厨师，从而提高老年人或身体有障碍者的自主性，并减轻专业厨房的压力。该方法整合了语言处理、本体论和心理模拟等多种知识来源，解决了食谱语言的挑战，并提升了AI系统对任务的理解程度。	 | This paper presents a case study on how to process cooking recipes (and more generally, how-to instructions) in a way that makes it possible for a robot or artificial cooking assistant to support human chefs in the kitchen. Such AI assistants would be of great benefit to society, as they can help to sustain the autonomy of aging adults or people with a physical impairment, or they may reduce the stress in a professional kitchen. We propose a novel approach to computational recipe understanding that mimics the human sense-making process, which is narrative-based. Using an English recipe for almond crescent cookies as illustration, we show how recipes can be modelled as rich narrative structures by integrating various knowledge sources such as language processing, ontologies, and mental simulation. We show how such narrative structures can be used for (a) dealing with the challenges of recipe language, such as zero anaphora, (b) optimizing a robot's planning process, (c) measuring how well an AI system understands its current tasks, and (d) allowing recipe annotations to become language-independent.	 | 本文介绍了一种案例研究，探讨如何处理烹饪食谱（更广泛地说，是如何完成操作的说明），从而使机器人或人工烹饪助手能够支持厨房中的人类厨师。这样的AI助手对社会非常有益，因为它们可以帮助维持老年人或身体有障碍的人的自主性，或者可以在专业厨房中减轻压力。我们提出了一种新的计算食谱理解方法，模仿了人类的意义构建过程，这种过程是基于叙事的。使用一种关于杏仁月牙饼干的英文食谱作为示例，我们展示了如何通过整合语言处理、本体论和心理模拟等多种知识来源，将食谱建模为丰富的叙事结构。我们展示了这些叙事结构如何用于（a）应对食谱语言的挑战，如零指代问题，（b）优化机器人的计划过程，（c）测量AI系统对其当前任务的理解程度，以及（d）使食谱注释语言无关。
2501.01805	 | End-to-End Long Document Summarization using Gradient Caching	 | Rohit Saxena,Hao Tang,Frank Keller	 | CachED（Gradient Caching for Encoder-Decoder模型）提出了一种方法，通过在解码器中融合非重叠滑动窗口并在反向传播过程中缓存梯度来扩展基于变压器的编码器-解码器模型的训练，使其能够处理长文档摘要而无需裁剪输入文档。这是通过在解码器中传递缓存的梯度并重新计算隐藏向量分块来实现的。	 | Training transformer-based encoder-decoder models for long document summarization poses a significant challenge due to the quadratic memory consumption during training. Several approaches have been proposed to extend the input length at test time, but training with these approaches is still difficult, requiring truncation of input documents and causing a mismatch between training and test conditions. In this work, we propose CachED (Gradient $\textbf{Cach}$ing for $\textbf{E}$ncoder-$\textbf{D}$ecoder models), an approach that enables end-to-end training of existing transformer-based encoder-decoder models, using the entire document without truncation. Specifically, we apply non-overlapping sliding windows to input documents, followed by fusion in decoder. During backpropagation, the gradients are cached at the decoder and are passed through the encoder in chunks by re-computing the hidden vectors, similar to gradient checkpointing. In the experiments on long document summarization, we extend BART to CachED BART, processing more than 500K tokens during training and achieving superior performance without using any additional parameters.	 | 基于变压器的编码器-解码器模型在长时间文档摘要训练中的训练因内存消耗呈平方级增长而面临重大挑战。虽然已经提出了一些方法来在测试时扩展输入长度，但使用这些方法进行训练仍然困难，需要裁剪输入文档，从而导致训练和测试条件之间的不匹配。在本文中，我们提出了CachED（Gradient Caching for Encoder-Decoder模型），这是一种方法，可以在不裁剪文档的情况下，端到端地训练现有的基于变压器的编码器-解码器模型。具体而言，我们对输入文档应用非重叠滑动窗口，然后在解码器中进行融合。在反向传播过程中，解码器中的梯度被缓存，并通过重新计算隐藏向量分块传递给编码器，类似于梯度检查点技术。在长时间文档摘要实验中，我们将BART扩展为CachED BART，在训练过程中处理超过50万个标记，并且无需使用任何额外参数就能实现更好的性能。
2501.01796	 | Reading Between the Lines: A dataset and a study on why some texts are tougher than others	 | Nouran Khallaf,Carlo Eugeni,Serge Sharoff	 | 该研究旨在通过心理学实证研究和翻译研究，识别并标注特定智力残疾受众阅读文本时的困难点，并通过微调预训练变换器模型来预测简化阅读材料所需的策略。	 | Our research aims at better understanding what makes a text difficult to read for specific audiences with intellectual disabilities, more specifically, people who have limitations in cognitive functioning, such as reading and understanding skills, an IQ below 70, and challenges in conceptual domains. We introduce a scheme for the annotation of difficulties which is based on empirical research in psychology as well as on research in translation studies. The paper describes the annotated dataset, primarily derived from the parallel texts (standard English and Easy to Read English translations) made available online. we fine-tuned four different pre-trained transformer models to perform the task of multiclass classification to predict the strategies required for simplification. We also investigate the possibility to interpret the decisions of this language model when it is aimed at predicting the difficulty of sentences. The resources are available from https://github.com/Nouran-Khallaf/why-tough	 | 我们的研究旨在更好地理解哪些因素使特定的智力残疾受众难以阅读文本，更具体地说，这些受众在认知功能方面存在限制，如阅读和理解能力、智商低于70，以及概念领域的挑战。我们提出了一种基于心理学实证研究和翻译研究的研究方案，用于标注困难之处。本文描述了标注数据集，该数据集主要来自于在线提供的平行文本（标准英语和易读英语翻译）。我们对四种不同预训练的变换器模型进行了微调，使其能够进行多类分类任务，以预测简化所需的策略。我们还研究了当语言模型旨在预测句子的难易程度时，是否可以解释该模型的决策过程。资源可以从 https://github.com/Nouran-Khallaf/why-tough 获取。
2501.01743	 | Automating Legal Concept Interpretation with LLMs: Retrieval, Generation, and Evaluation	 | Kangcheng Luo,Quzhe Huang,Cong Jiang,Yansong Feng	 | 本文介绍了一个名为ATRI的检索增强生成框架，可以从司法先例中自动检索并解释模糊的法律概念，且生成的概念解释质量可与人类专家媲美，有助于利用大型语言模型支持法律从业者的工作。	 | Legal articles often include vague concepts to adapt to the ever-changing society. Providing detailed interpretations of these concepts is a critical task for legal practitioners, which requires meticulous and professional annotations by legal experts, admittedly time-consuming and expensive to collect at scale. In this paper, we introduce a novel retrieval-augmented generation framework, ATRI, for AuTomatically Retrieving relevant information from past judicial precedents and Interpreting vague legal concepts. We further propose a new benchmark, Legal Concept Entailment, to automate the evaluation of generated concept interpretations without expert involvement. Automatic evaluations indicate that our generated interpretations can effectively assist large language models (LLMs) in understanding vague legal concepts. Multi-faceted evaluations by legal experts indicate that the quality of our concept interpretations is comparable to those written by human experts. Our work has strong implications for leveraging LLMs to support legal practitioners in interpreting vague legal concepts and beyond.	 | 法律文章通常包含模糊的概念以适应不断变化的社会。对这些概念提供详细的解释是法律从业者的一项关键任务，这需要法律专家进行细致且专业的注解，显然耗时且昂贵。在本论文中，我们介绍了一种新颖的检索增强生成框架，即ATRI，用于从过去的司法先例中自动检索相关的信息并对模糊的法律概念进行解释。我们进一步提出了一个新的基准，即法律概念蕴含（Legal Concept Entailment），以自动化评估生成的概念解释，无需专家参与。自动评估结果表明，我们生成的概念解释能够有效地帮助大型语言模型（LLMs）理解模糊的法律概念。多方面的专家评估表明，我们概念解释的质量与人类专家撰写的解释相当。我们的工作对于利用LLMs支持法律从业者解释模糊的法律概念具有重要的启示意义，并且可以推广到更广泛的领域。
2501.01705	 | The Essence of Contextual Understanding in Theory of Mind: A Study on Question Answering with Story Characters	 | Chulun Zhou,Qiujing Wang,Mo Yu,Xiaoqian Yue,Rui Lu,Jiangnan Li,Yifan Zhou,Shunchi Zhang,Jie Zhou,Wai Lam	 | 本文通过引入一个新的基准测试CharToM-QA，验证了理解长期个人背景对心智理论的重要性，并发现最先进的大语言模型在处理这类需要微妙背景信息的任务时仍显著不及人类。	 | Theory-of-Mind (ToM) is a fundamental psychological capability that allows humans to understand and interpret the mental states of others. Humans infer others' thoughts by integrating causal cues and indirect clues from broad contextual information, often derived from past interactions. In other words, human ToM heavily relies on the understanding about the backgrounds and life stories of others. Unfortunately, this aspect is largely overlooked in existing benchmarks for evaluating machines' ToM capabilities, due to their usage of short narratives without global backgrounds. In this paper, we verify the importance of understanding long personal backgrounds in ToM and assess the performance of LLMs in such realistic evaluation scenarios. To achieve this, we introduce a novel benchmark, CharToM-QA, comprising 1,035 ToM questions based on characters from classic novels. Our human study reveals a significant disparity in performance: the same group of educated participants performs dramatically better when they have read the novels compared to when they have not. In parallel, our experiments on state-of-the-art LLMs, including the very recent o1 model, show that LLMs still perform notably worse than humans, despite that they have seen these stories during pre-training. This highlights the limitations of current LLMs in capturing the nuanced contextual information required for ToM reasoning.	 | 心智理论（Theory-of-Mind, ToM）是一种基础的心理能力，使人类能够理解他人的心理状态。人类通过整合因果线索和从广泛背景信息中获取的间接线索来推断他人的想法，这些背景信息通常源自过去的互动。换句话说，人类的心智理论高度依赖于对他人背景和生活经历的理解。不幸的是，在现有评估机器心智理论能力的标准中，这一方面被很大程度上忽视了，因为这些标准使用的是没有全球背景的短篇叙事。在本文中，我们验证了理解长期个人背景在心智理论中的重要性，并评估了在这些现实评价场景中的大语言模型（LLMs）的性能。为了实现这一目标，我们引入了一个新的基准测试，CharToM-QA，包含基于经典小说角色的1035个心智理论问题。我们的研究表明，受过教育的参与者在阅读了小说时的表现比没有阅读时的表现要显著更好。与此同时，我们在最先进的大语言模型上进行的实验，包括最近的o1模型，显示这些模型的表现仍然显著不如人类，尽管它们在预训练过程中已经见过这些故事。这突显了当前大语言模型在捕捉用于心智理论推理所需的微妙背景信息方面的局限性。
2501.01679	 | Adaptive Few-shot Prompting for Machine Translation with Pre-trained Language Models	 | Lei Tang,Jinghui Qin,Wenxuan Ye,Hao Tan,Zhijing Yang	 | 本文提出了一种自适应小样本提示（AFSP）框架，通过自动选择适合不同源输入句子的翻译示例来提高大规模语言模型的翻译质量，并在外交场合中英文平行语料库及联合国平行语料库上的实验中证明了其有效性和优越性。	 | Recently, Large language models (LLMs) with in-context learning have demonstrated remarkable potential in handling neural machine translation. However, existing evidence shows that LLMs are prompt-sensitive and it is sub-optimal to apply the fixed prompt to any input for downstream machine translation tasks. To address this issue, we propose an adaptive few-shot prompting (AFSP) framework to automatically select suitable translation demonstrations for various source input sentences to further elicit the translation capability of an LLM for better machine translation. First, we build a translation demonstration retrieval module based on LLM's embedding to retrieve top-k semantic-similar translation demonstrations from aligned parallel translation corpus. Rather than using other embedding models for semantic demonstration retrieval, we build a hybrid demonstration retrieval module based on the embedding layer of the deployed LLM to build better input representation for retrieving more semantic-related translation demonstrations. Then, to ensure better semantic consistency between source inputs and target outputs, we force the deployed LLM itself to generate multiple output candidates in the target language with the help of translation demonstrations and rerank these candidates. Besides, to better evaluate the effectiveness of our AFSP framework on the latest language and extend the research boundary of neural machine translation, we construct a high-quality diplomatic Chinese-English parallel dataset that consists of 5,528 parallel Chinese-English sentences. Finally, extensive experiments on the proposed diplomatic Chinese-English parallel dataset and the United Nations Parallel Corpus (Chinese-English part) show the effectiveness and superiority of our proposed AFSP.	 | 近年来，具有上下文学习能力的大规模语言模型（LLMs）在神经机器翻译方面显示出了显著的潜力。然而，现有证据表明，LLMs 对提示（prompt）较为敏感，将固定的提示应用于所有下游翻译任务的输入并不是最佳方案。为了解决这一问题，我们提出了一种自适应小样本提示（AFSP）框架，通过自动选择适合不同源输入句子的翻译示例，进一步激发LLM的翻译能力，从而提高机器翻译的质量。首先，我们基于LLM的嵌入构建了一个翻译示例检索模块，从对齐的平行翻译语料库中检索出最相似的翻译示例。不同于使用其他嵌入模型进行语义示例检索，我们构建了一个基于部署LLM的嵌入层的混合示例检索模块，以更好地构建输入表示，从而检索出更多相关语义的翻译示例。其次，为了确保源输入和目标输出之间的语义一致性，我们利用翻译示例帮助部署的LLM生成目标语言的多个输出候选，并重新对这些候选进行排名。此外，为了更好地评估我们的AFSP框架的有效性，并拓展神经机器翻译的研究边界，我们构建了一个高质量的外交场合中英文平行语料库，包含5,528个中英文平行句子。最后，通过对提出的外交场合中英文平行语料库和联合国平行语料库（中英文部分）进行广泛的实验，证明了我们提出的AFSP框架的有效性和优越性。
2501.01668	 | CoT-based Synthesizer: Enhancing LLM Performance through Answer Synthesis	 | Bohan Zhang,Xiaokang Zhang,Jing Zhang,Jifan Yu,Sijia Luo,Jie Tang	 | 本文提出了一种基于CoT的合成分析器策略，通过分析多个有缺陷的候选答案来合成更优的答案，提高大规模语言模型在复杂推理任务上的准确性。实验结果显示，该方法显著提升了多种模型在数学推理任务上的性能，其中Llama3-8B和GPT-4o分别提高了11.8%和10.3%。	 | Current inference scaling methods, such as Self-consistency and Best-of-N, have proven effective in improving the accuracy of LLMs on complex reasoning tasks. However, these methods rely heavily on the quality of candidate responses and are unable to produce correct answers when all candidates are incorrect. In this paper, we propose a novel inference scaling strategy, CoT-based Synthesizer, which leverages CoT reasoning to synthesize superior answers by analyzing complementary information from multiple candidate responses, even when all candidate responses are flawed. To enable a lightweight and cost-effective implementation, we introduce an automated data generation pipeline that creates diverse training data. This allows smaller LLMs trained on this data to improve the inference accuracy of larger models, including API-based LLMs. Experimental results across four benchmark datasets with seven policy models demonstrate that our method significantly enhances performance, with gains of 11.8% for Llama3-8B and 10.3% for GPT-4o on the MATH dataset. The corresponding training data and code are publicly available on https://github.com/RUCKBReasoning/CoT-based-Synthesizer.	 | 当前的推理放大方法，如自我一致性和最佳选N方法，在提高大规模语言模型（LLM）在复杂推理任务上的准确性方面已被证明是有效的。然而，这些方法严重依赖于候选响应的质量，当所有候选答案都是错误时，无法生成正确的答案。在本文中，我们提出了一种新的推理放大策略，基于CoT的合成分析器（CoT-based Synthesizer），该策略利用CoT推理来通过分析多个候选答案中的互补信息合成更优的答案，即使所有候选答案都是有缺陷的。为了实现轻量级和低成本的实施，我们引入了一个自动数据生成管道，创建了多样化的训练数据。这使得在这些数据上训练的小型LLM能够提高大型模型的推理准确性，包括基于API的LLM的推理准确性。我们在四个基准数据集中使用七种策略模型进行的实验结果表明，我们的方法显著提升了性能，Llama3-8B在MATH数据集上的性能提高了11.8%，GPT-4o的性能提高了10.3%。对应的数据集和代码可以在 https://github.com/RUCKBReasoning/CoT-based-Synthesizer 上公开获取。
2501.01652	 | MIRAGE: Exploring How Large Language Models Perform in Complex Social Interactive Environments	 | Cai Yin,Gu Zhouhong,Du Zhaohan,Ye Zheyu,Cao Shaosheng,Xu Yiqian,Feng Hongwei,Chen Ping	 | 本文提出了一种名为MIRAGE的综合评估框架，通过谋杀谜题游戏评估大型语言模型在表现高级人类行为方面的能力，包括信任、信息搜集、角色扮演和指令遵循等方面，并展示了流行模型如GPT-4在应对复杂性时面临的挑战。	 | Large Language Models (LLMs) have shown remarkable capabilities in environmental perception, reasoning-based decision-making, and simulating complex human behaviors, particularly in interactive role-playing contexts. This paper introduces the Multiverse Interactive Role-play Ability General Evaluation (MIRAGE), a comprehensive framework designed to assess LLMs' proficiency in portraying advanced human behaviors through murder mystery games. MIRAGE features eight intricately crafted scripts encompassing diverse themes and styles, providing a rich simulation. To evaluate LLMs' performance, MIRAGE employs four distinct methods: the Trust Inclination Index (TII) to measure dynamics of trust and suspicion, the Clue Investigation Capability (CIC) to measure LLMs' capability of conducting information, the Interactivity Capability Index (ICI) to assess role-playing capabilities and the Script Compliance Index (SCI) to assess LLMs' capability of understanding and following instructions. Our experiments indicate that even popular models like GPT-4 face significant challenges in navigating the complexities presented by the MIRAGE. The datasets and simulation codes are available in \href{https://github.com/lime728/MIRAGE}{github}.	 | 大型语言模型（LLMs）在环境感知、基于推理的决策制定以及模拟复杂的人类行为方面展现了显著的能力，特别是在互动角色扮演的情境中尤为突出。本文介绍了一种名为“多元宇宙互动角色扮演综合评估”（MIRAGE）的综合框架，旨在通过谋杀谜题游戏来评估LLMs在表现高级人类行为方面的能力。MIRAGE包括八个精心设计的剧本，涵盖多样化的主题和风格，提供了丰富的模拟环境。  为了评估LLMs的表现，MIRAGE采用了四种不同的方法：信任倾向指数（TII）以衡量信任和怀疑的动态关系；线索调查能力（CIC）以评估LLMs的信息收集能力；互动能力指数（ICI）以评估角色扮演能力；脚本一致性指数（SCI）以评估LLMs理解并遵循指令的能力。我们的实验表明，即使是像GPT-4这样的流行模型，在应对MIRAGE提出的复杂性时也面临重大挑战。相关数据集和模拟代码可在\href{https://github.com/lime728/MIRAGE}{GitHub}获取。
2501.01638	 | A non-ergodic framework for understanding emergent capabilities in Large Language Models	 | Javier Marin	 | 该研究证明了大规模语言模型表现出的意外能力源于非平衡系统特性，并提出基于相邻可能理论的数学框架来解释这些能力如何通过语义空间中的相变和约束相互作用而涌现。实验验证了能力是通过路径依赖的离散跃迁实现的，为理解语言模型中的涌现现象提供了理论基础。	 | Large language models have emergent capabilities that come unexpectedly at scale, but we need a theoretical framework to explain why and how they emerge. We prove that language models are actually non-ergodic systems while providing a mathematical framework based on Stuart Kauffman's theory of the adjacent possible (TAP) to explain capability emergence. Our resource-constrained TAP equation demonstrates how architectural, training, and contextual constraints interact to shape model capabilities through phase transitions in semantic space. We prove through experiments with three different language models that capacities emerge through discrete transitions guided by constraint interactions and path-dependent exploration. This framework provides a theoretical basis for understanding emergence in language models and guides the development of architectures that can guide capability emergence.	 | 大规模语言模型在规模增加时会表现出意外的 emergent 能力，但我们需要一个理论框架来解释它们为什么以及如何出现。我们证明了语言模型实际上是非平衡系统，并基于 Stuart Kauffman 的相邻可能理论（Adjacent Possible, TAP）提供了数学框架来解释能力的出现。我们的资源受限的 TAP 方程展示了如何通过语义空间中的相变，构建结构、训练和上下文约束相互作用来塑造模型的能力。通过与三个不同语言模型的实验，我们证明了能力是通过由约束相互作用和路径依赖探索指导的离散跃迁而出现的。该框架为理解语言模型中的涌现现象提供了理论基础，并指导了能够引导能力涌现的架构的发展。
2501.01625	 | ICPC: In-context Prompt Compression with Faster Inference	 | Ziyang Yu,Yuyu Liu	 | ICPC（上下文内提示压缩）是一种新颖且可扩展的提示压缩方法，通过计算每个词在提示中的概率和信息量自适应地减少提示长度，从而有效压缩长文本并在各种自然语言处理任务中提高性能和速度。	 | Despite the recent success of Large Language Models (LLMs), it remains challenging to feed LLMs with long prompts due to the fixed size of LLM inputs. As a remedy, prompt compression becomes a promising solution by removing redundant tokens in the prompt. However, using LLM in the existing works requires additional computation resources and leads to memory overheads. To address it, we propose ICPC (In-context Prompt Compression), a novel and scalable prompt compression method that adaptively reduces the prompt length. The key idea of ICPC is to calculate the probability of each word appearing in the prompt using encoders and calculate information carried by each word through the information function, which effectively reduces the information loss during prompt compression and increases the speed of compression. Empirically, we demonstrate that ICPC can effectively compress long texts of different categories and thus achieve better performance and speed on different types of NLP tasks.	 | 尽管大型语言模型（LLMs）取得了最近的成功，但由于LLM输入的固定大小，向LLM提供长提示仍然具有挑战性。为了解决这一问题，提示压缩成为一种有潜力的解决方案，通过删除提示中的冗余标记来减少提示长度。然而，现有工作中使用LLM需要额外的计算资源并导致内存开销。为了解决这个问题，我们提出了ICPC（上下文内提示压缩）——一种新颖且可扩展的提示压缩方法，能够自适应地减少提示长度。ICPC的核心思想是使用编码器计算每个词在提示中出现的概率，并通过信息函数计算每个词携带的信息量，这有效地减少了提示压缩过程中的信息损失并提高了压缩速度。通过实验，我们证明ICPC可以有效压缩不同类别长文本，从而在不同类型的自然语言处理任务中实现更好的性能和速度。
2501.01594	 | PSYCHE: A Multi-faceted Patient Simulation Framework for Evaluation of Psychiatric Assessment Conversational Agents	 | Jingoo Lee,Kyungho Lim,Young-Chul Jung,Byung-Hoon Kim	 | 该研究提出了PSYCHE框架，用于评估基于大规模语言模型的精神科评估对话代理（PACAs）在临床相关性、伦理安全性、成本效益等方面的性能，并通过认证精神科医生的研究验证了其有效性。	 | Recent advances in large language models (LLMs) have accelerated the development of conversational agents capable of generating human-like responses. Since psychiatric assessments typically involve complex conversational interactions between psychiatrists and patients, there is growing interest in developing LLM-based psychiatric assessment conversational agents (PACAs) that aim to simulate the role of psychiatrists in clinical evaluations. However, standardized methods for benchmarking the clinical appropriateness of PACAs' interaction with patients still remain underexplored. Here, we propose PSYCHE, a novel framework designed to enable the 1) clinically relevant, 2) ethically safe, 3) cost-efficient, and 4) quantitative evaluation of PACAs. This is achieved by simulating psychiatric patients based on a multi-faceted psychiatric construct that defines the simulated patients' profiles, histories, and behaviors, which PACAs are expected to assess. We validate the effectiveness of PSYCHE through a study with 10 board-certified psychiatrists, supported by an in-depth analysis of the simulated patient utterances.	 | 最近在大规模语言模型（LLMs）领域的进展大大加速了能够生成类似人类回应的对话代理的开发。由于精神科评估通常涉及精神科医生与患者之间的复杂对话互动，因此越来越多地关注开发基于LLM的精神科评估对话代理（PACAs），旨在模拟精神科医生在临床评估中的角色。然而，标准化的方法仍然很少用于评估PACAs与患者交互的临床适宜性。在此，我们提出了PSYCHE，一个新型框架，旨在实现对PACAs的1）临床相关性，2）伦理安全性，3）成本效益，以及4）定量评估。这通过基于多方面的精神科构念来模拟患者实现，该构念定义了模拟患者的特征、历史和行为，PACAs需要对其进行评估。我们通过一项包含10名认证精神科医生的研究，以及对模拟患者回应的深入分析，验证了PSYCHE的有效性。
2501.01588	 | (WhyPHI) Fine-Tuning PHI-3 for Multiple-Choice Question Answering: Methodology, Results, and Challenges	 | Mohamed Hisham Abdellatif	 | 本研究探讨了微软PHI-3模型在处理多项选择题任务上的潜力，并通过在TruthfulQA数据集上微调和优化提示，显著提高了模型的性能，准确率从62%提升至90.8%。研究表明，高效的大规模语言模型在教育评估和个性化学习中具有重要应用价值。	 | Large Language Models (LLMs) have become essential tools across various domains due to their impressive capabilities in understanding and generating human-like text. The ability to accurately answer multiple-choice questions (MCQs) holds significant value in education, particularly in automated tutoring systems and assessment platforms. However, adapting LLMs to handle MCQ tasks effectively remains challenging due to the hallucinations and unclear prompts. This work explores the potential of Microsoft's PHI-3\cite{Abdin2024}, a compact yet efficient LLM, for MCQ answering. Our contributions include fine-tuning the model on the TruthfulQA dataset, designing optimized prompts to enhance model performance, and evaluating using perplexity and traditional metrics like accuracy and F1 score. Results show a remarkable improvement in PHI-3.5's MCQ handling post-fine-tuning, with perplexity decreasing from 4.68 to 2.27, and accuracy rising from 62\% to 90.8\%. This research underlines the importance of efficient models in adaptive learning systems and educational assessments, paving the way for broader integration into the classroom, particularly in fields like test preparation, student feedback, and personalized learning.	 | 大型语言模型（LLMs）因其在理解和生成类人类文本方面的能力，在各个领域已成为不可或缺的工具。准确回答多项选择题（MCQs）的能力在教育领域具有重要意义，特别是在自动化辅导系统和评估平台中。然而，将LLMs有效适应处理MCQ任务仍然具有挑战性，主要源于其幻觉和不清晰的提示。本研究探讨了微软的PHI-3模型[@Abdin2024]在处理MCQ方面的潜力，PHI-3是一种紧凑且高效的LLM。我们的贡献包括在TruthfulQA数据集上微调模型，设计优化的提示以提高模型性能，并使用困惑度和准确率、F1分数等传统指标进行评估。结果显示，经过微调后的PHI-3.5在处理MCQ方面的表现有了显著提升，困惑度从4.68降至2.27，准确率从62%提升至90.8%。本研究强调了高效模型在自适应学习系统和教育评估中的重要性，为这些模型在课堂中的更广泛集成铺平了道路，特别是在备考、学生反馈和个性化学习等领域。
2501.01830	 | Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models	 | Yanjiang Liu,Shuhen Zhou,Yaojie Lu,Huijia Zhu,Weiqiang Wang,Hongyu Lin,Ben He,Xianpei Han,Le Sun	 | Auto-RT是一种强化学习框架，能够自动探索和优化复杂的攻击策略，通过恶意查询有效发现安全漏洞，相较于现有方法，它可以显著提高探索效率并提升16.63%的成功率。	 | Automated red-teaming has become a crucial approach for uncovering vulnerabilities in large language models (LLMs). However, most existing methods focus on isolated safety flaws, limiting their ability to adapt to dynamic defenses and uncover complex vulnerabilities efficiently. To address this challenge, we propose Auto-RT, a reinforcement learning framework that automatically explores and optimizes complex attack strategies to effectively uncover security vulnerabilities through malicious queries. Specifically, we introduce two key mechanisms to reduce exploration complexity and improve strategy optimization: 1) Early-terminated Exploration, which accelerate exploration by focusing on high-potential attack strategies; and 2) Progressive Reward Tracking algorithm with intermediate downgrade models, which dynamically refine the search trajectory toward successful vulnerability exploitation. Extensive experiments across diverse LLMs demonstrate that, by significantly improving exploration efficiency and automatically optimizing attack strategies, Auto-RT detects a boarder range of vulnerabilities, achieving a faster detection speed and 16.63\% higher success rates compared to existing methods.	 | 自动化红队行动已成为发现大型语言模型（LLMs）漏洞的关键方法。然而，大多数现有方法主要关注孤立的安全缺陷，限制了其适应动态防御和高效发现复杂漏洞的能力。为解决这一挑战，我们提出了Auto-RT，这是一种强化学习框架，能够自动探索和优化复杂的攻击策略，通过恶意查询有效发现安全漏洞。具体而言，我们引入了两种关键机制来降低探索复杂性和改进策略优化：1）早期终止探索，通过专注于高潜力的攻击策略加快探索速度；2）逐步奖励追踪算法，结合中间降级模型，动态细化搜索轨迹，导向成功的漏洞利用。广泛的实验表明，通过显著提高探索效率和自动优化攻击策略，Auto-RT能更广泛地检测漏洞，实现更快的检测速度和16.63%更高的成功率，相对于现有方法。
2501.01821	 | SDPO: Segment-Level Direct Preference Optimization for Social Agents	 | Aobo Kong,Wentao Ma,Shiwan Zhao,Yongbin Li,Yuchuan Wu,Ke Wang,Xiaoqian Liu,Qicheng Li,Yong Qin,Fei Huang	 | 该研究提出了一种新的段级直接偏好优化（SDPO）方法，旨在改进大型语言模型在复杂目标导向社会对话中的表现，通过专注于特定关键段落来优化多回合代理行为并减少训练噪声，实验结果表明SDPO在SOTOPIA基准测试中优于现有方法和专用LLM。	 | Social agents powered by large language models (LLMs) can simulate human social behaviors but fall short in handling complex goal-oriented social dialogues. Direct Preference Optimization (DPO) has proven effective in aligning LLM behavior with human preferences across a variety of agent tasks. Existing DPO-based approaches for multi-turn interactions are divided into turn-level and session-level methods. The turn-level method is overly fine-grained, focusing exclusively on individual turns, while session-level methods are too coarse-grained, often introducing training noise. To address these limitations, we propose Segment-Level Direct Preference Optimization (SDPO), which focuses on specific key segments within interactions to optimize multi-turn agent behavior while minimizing training noise. Evaluations on the SOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform both existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring SDPO's potential to advance the social intelligence of LLM-based agents. We release our code and data at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO.	 | 基于大型语言模型（LLMs）的社会代理可以模拟人类社会行为，但在处理复杂的以目标为导向的社会对话方面存在不足。直接偏好优化（DPO）已被证明能够有效地在各种代理任务中使LLM的行为与人类偏好保持一致。现有的基于DPO的方法主要分为回合级和会话级方法。回合级方法过于细粒度，专注于单个回合，而会话级方法则过于粗粒度，经常引入训练噪音。为了应对这些限制，我们提出了段级直接偏好优化（SDPO），该方法专注于交互中的特定关键段落，以优化多回合代理行为，同时尽量减少训练噪音。在SOTOPIA基准上的评估表明，经过SDPO调优的代理在所有方面都优于现有的DPO方法和专有LLM（如GPT-4o），这表明SDPO具有提高基于LLM的社会智能的潜力。我们将在https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO发布我们的代码和数据。
2501.01741	 | How Toxic Can You Get? Search-based Toxicity Testing for Large Language Models	 | Simone Corbo,Luca Bancale,Valeria De Gennaro,Livia Lestingi,Vincenzo Scotti,Matteo Camilli	 | EvoTox 是一种自动化的测试框架，用于评估大规模语言模型在对齐后仍能生成多大程度的有毒响应，通过迭代进化策略引导模型生成更高毒性的响应并进行定量和定性评估，显著优于现有基线方法。	 | Language is a deep-rooted means of perpetration of stereotypes and discrimination. Large Language Models (LLMs), now a pervasive technology in our everyday lives, can cause extensive harm when prone to generating toxic responses. The standard way to address this issue is to align the LLM, which, however, dampens the issue without constituting a definitive solution. Therefore, testing LLM even after alignment efforts remains crucial for detecting any residual deviations with respect to ethical standards. We present EvoTox, an automated testing framework for LLMs' inclination to toxicity, providing a way to quantitatively assess how much LLMs can be pushed towards toxic responses even in the presence of alignment. The framework adopts an iterative evolution strategy that exploits the interplay between two LLMs, the System Under Test (SUT) and the Prompt Generator steering SUT responses toward higher toxicity. The toxicity level is assessed by an automated oracle based on an existing toxicity classifier. We conduct a quantitative and qualitative empirical evaluation using four state-of-the-art LLMs as evaluation subjects having increasing complexity (7-13 billion parameters). Our quantitative evaluation assesses the cost-effectiveness of four alternative versions of EvoTox against existing baseline methods, based on random search, curated datasets of toxic prompts, and adversarial attacks. Our qualitative assessment engages human evaluators to rate the fluency of the generated prompts and the perceived toxicity of the responses collected during the testing sessions. Results indicate that the effectiveness, in terms of detected toxicity level, is significantly higher than the selected baseline methods (effect size up to 1.0 against random search and up to 0.99 against adversarial attacks). Furthermore, EvoTox yields a limited cost overhead (from 22% to 35% on average).	 | 语言是传播刻板印象和歧视的一种根深蒂固的手段。大规模语言模型（LLMs）如今已经渗透到我们的日常生活中，当它们易于生成有毒响应时，会造成广泛的危害。通常解决这个问题的方法是使LLM对齐，但这并不能根本解决问题。因此，在对齐努力之后仍需测试LLM，以检测其是否在伦理标准方面存在任何残留偏差。我们提出了EvoTox，一种自动化的测试框架，用于评估LLM倾向于产生毒性响应的程度，提供了即使在对齐后也能定量评估LLM能够被推向多大程度的有毒响应的方法。该框架采用迭代进化策略，利用系统待测试（SUT）和提示生成器之间的相互作用，引导SUT的响应向更高毒性方向发展。毒性水平通过基于现有有毒提示分类器的自动判别器来评估。我们使用四种最先进的LLM作为评估对象，其复杂性从7亿到13亿参数不等，进行了定量和定性的实证评估。定量评估基于随机搜索、精心策划的有毒提示数据集和对抗性攻击，评估了四种不同版本的EvoTox相对于现有基线方法的成本效益。定性评估则通过人类评估者对生成的提示流畅度和收集到的响应中的感知毒性进行评分。结果表明，从检测的毒性水平来看，EvoTox的有效性显著高于所选基线方法（与随机搜索相比的效果大小高达1.0，与对抗性攻击相比的效果大小高达0.99）。此外，EvoTox的实施成本仅增加了有限的 overhead（平均从22%到35%）。
2501.01702	 | AgentRefine: Enhancing Agent Generalization through Refinement Tuning	 | Dayuan Fu,Keqing He,Yejie Wang,Wentao Hong,Zhuoma Gongque,Weihao Zeng,Wei Wang,Jingang Wang,Xunliang Cai,Weiran Xu	 | 本文提出了一种名为AgentRefine的新框架，旨在通过指令调优提高大型语言模型代理的一般化能力，该框架使模型能够在错误中学习并纠正自己的错误，从而在多种任务上显著优于现有方法。	 | Large Language Model (LLM) based agents have proved their ability to perform complex tasks like humans. However, there is still a large gap between open-sourced LLMs and commercial models like the GPT series. In this paper, we focus on improving the agent generalization capabilities of LLMs via instruction tuning. We first observe that the existing agent training corpus exhibits satisfactory results on held-in evaluation sets but fails to generalize to held-out sets. These agent-tuning works face severe formatting errors and are frequently stuck in the same mistake for a long while. We analyze that the poor generalization ability comes from overfitting to several manual agent environments and a lack of adaptation to new situations. They struggle with the wrong action steps and can not learn from the experience but just memorize existing observation-action relations. Inspired by the insight, we propose a novel AgentRefine framework for agent-tuning. The core idea is to enable the model to learn to correct its mistakes via observation in the trajectory. Specifically, we propose an agent synthesis framework to encompass a diverse array of environments and tasks and prompt a strong LLM to refine its error action according to the environment feedback. AgentRefine significantly outperforms state-of-the-art agent-tuning work in terms of generalization ability on diverse agent tasks. It also has better robustness facing perturbation and can generate diversified thought in inference. Our findings establish the correlation between agent generalization and self-refinement and provide a new paradigm for future research.	 | 基于大型语言模型（LLM）的代理已经证明了其能够执行复杂的任务，就像人类一样。然而，开源的LLM和商业模型（如GPT系列）之间仍然存在巨大的差距。在本文中，我们专注于通过指令调优来提高LLM代理的一般化能力。我们首先观察到现有的代理训练语料库在内部保留评估集上表现出满意的结果，但在外部保留集上却无法泛化。这些代理调优工作面临严重的格式错误，并且经常长时间停留在同样的错误中。我们分析认为，这种泛化能力差主要是由于过度拟合几个手动设计的代理环境，以及未能适应新情况。它们难以纠正错误的操作步骤，不能从经验中学习，只能记忆现有的观察-动作关系。受此见解的启发，我们提出了一个名为AgentRefine的新颖代理调优框架。核心思想是在轨迹中使模型能够学习纠正自己的错误。具体而言，我们提出了一种代理合成框架，以涵盖各种环境和任务，并提示一个强大的LLM根据环境反馈纠正其错误操作。AgentRefine在各种代理任务的一般化能力方面显著优于最先进的代理调优工作。它还具有更好的鲁棒性，能够生成多样化的推理思维。我们的发现建立了代理一般化与自我纠正之间的关联，并为未来的研究提供了一个新的范式。
2501.01629	 | Crossing Language Borders: A Pipeline for Indonesian Manhwa Translation	 | Nithyasri Narasimhan,Sagarika Singh	 | 该项目开发了一种结合计算机视觉、文本识别和自然语言处理技术的自动化翻译系统，用于将印尼语韩漫翻译成英语，旨在简化传统手工翻译过程并提高效率。该系统在气泡检测、光学字符识别和机器翻译等方面表现出色，特别针对低资源语言的翻译挑战进行了优化。	 | In this project, we develop a practical and efficient solution for automating the Manhwa translation from Indonesian to English. Our approach combines computer vision, text recognition, and natural language processing techniques to streamline the traditionally manual process of Manhwa(Korean comics) translation. The pipeline includes fine-tuned YOLOv5xu for speech bubble detection, Tesseract for OCR and fine-tuned MarianMT for machine translation. By automating these steps, we aim to make Manhwa more accessible to a global audience while saving time and effort compared to manual translation methods. While most Manhwa translation efforts focus on Japanese-to-English, we focus on Indonesian-to-English translation to address the challenges of working with low-resource languages. Our model shows good results at each step and was able to translate from Indonesian to English efficiently.	 | 在本项目中，我们开发了一种实用且高效的解决方案，用于将印尼语翻译成英语的韩漫（Manhwa，即韩国漫画）自动化翻译。我们的方法结合了计算机视觉、文本识别和自然语言处理技术，以简化传统的手工韩漫翻译过程。该流程包括微调的 YOLOv5xu 用于气泡检测、Tesseract 用于光学字符识别（OCR）和微调的 MarianMT 用于机器翻译。通过自动化这些步骤，我们旨在使韩漫对全球观众更加易于访问，同时与手工翻译方法相比节省时间和精力。大多数韩漫翻译工作都集中在日语到英语的翻译上，而我们的重点是印尼语到英语的翻译，以应对使用低资源语言带来的挑战。我们的模型在每一步都显示了良好的结果，并且能够高效地将印尼语翻译成英语。
2501.01558	 | Predicting the Performance of Black-box LLMs through Self-Queries	 | Dylan Sam,Marc Finzi,J. Zico Kolter	 | 本文提出了一种以黑盒方式通过后续提示和不同响应的概率来提取大规模语言模型特征的方法，训练出可靠的模型行为预测器，这些预测器不仅在实例层面表现出色，还能区分模型的不同版本和架构，甚至能够识别通过API误表征的模型。	 | As large language models (LLMs) are increasingly relied on in AI systems, predicting when they make mistakes is crucial. While a great deal of work in the field uses internal representations to interpret model behavior, these representations are inaccessible when given solely black-box access through an API. In this paper, we extract features of LLMs in a black-box manner by using follow-up prompts and taking the probabilities of different responses as representations to train reliable predictors of model behavior. We demonstrate that training a linear model on these low-dimensional representations produces reliable and generalizable predictors of model performance at the instance level (e.g., if a particular generation correctly answers a question). Remarkably, these can often outperform white-box linear predictors that operate over a model's hidden state or the full distribution over its vocabulary. In addition, we demonstrate that these extracted features can be used to evaluate more nuanced aspects of a language model's state. For instance, they can be used to distinguish between a clean version of GPT-4o-mini and a version that has been influenced via an adversarial system prompt that answers question-answering tasks incorrectly or introduces bugs into generated code. Furthermore, they can reliably distinguish between different model architectures and sizes, enabling the detection of misrepresented models provided through an API (e.g., identifying if GPT-3.5 is supplied instead of GPT-4o-mini).	 | 随着大规模语言模型（LLMs）在AI系统中的应用日益广泛，预测其何时出错变得至关重要。尽管该领域大量的工作都使用内部表示来解释模型行为，但仅通过API提供的黑盒访问时，这些表示不可用。在本文中，我们通过使用后续提示并以不同响应的概率作为表示来以黑盒方式提取LLMs的特征，从而训练可靠的模型行为预测器。我们证明，在这些低维度表示上训练线性模型可以产生可靠的、可泛化的实例层面性能预测器（例如，特定生成是否正确回答了一个问题）。令人惊讶的是，这些预测器往往能够超越基于模型隐藏状态或其词汇分布完整分布的白盒线性预测器。此外，我们展示了这些提取的特征可以用于评估语言模型状态的更细微方面。例如，它们可以用来区分GPT-4o-mini的干净版本和经过对抗性系统提示影响，从而错误回答问题或引入生成代码中的缺陷的版本。此外，这些提取特征还可以可靠地区分不同模型架构和大小，从而检测通过API提供的被误表征的模型（例如，识别是否提供了GPT-3.5而非GPT-4o-mini）。
2501.01544	 | Many of Your DPOs are Secretly One: Attempting Unification Through Mutual Information	 | Rasul Tutnov,Antoine Grosnit,Haitham Bou-Ammar	 | 本文提出了一个由互信息启发的统一框架，并提出了一种具有灵活先验的新损失函数，该框架能够解释多种现有的DPO变体（如SimPO、TDPO、SparsePO等），从而简化了LLM对齐算法的景观，促进了LLM对齐技术的发展。	 | Post-alignment of large language models (LLMs) is critical in improving their utility, safety, and alignment with human intentions. Direct preference optimisation (DPO) has become one of the most widely used algorithms for achieving this alignment, given its ability to optimise models based on human feedback directly. However, the vast number of DPO variants in the literature has made it increasingly difficult for researchers to navigate and fully grasp the connections between these approaches. This paper introduces a unifying framework inspired by mutual information, which proposes a new loss function with flexible priors. By carefully specifying these priors, we demonstrate that many existing algorithms, such as SimPO, TDPO, SparsePO, and others, can be derived from our framework. This unification offers a clearer and more structured approach, allowing researchers to understand the relationships between different DPO variants better. We aim to simplify the landscape of DPO algorithms, making it easier for the research community to gain insights and foster further advancements in LLM alignment. Ultimately, we hope our framework can be a foundation for developing more robust and interpretable alignment techniques.	 | 大型语言模型（LLMs）的后对齐对于提高其实用性和与人类意图的对齐至关重要。直接偏好优化（DPO）因其可以直接根据人类反馈优化模型的能力，已成为实现这一对齐的主要算法之一。然而，文献中大量的DPO变体使得研究人员难以导航和完全理解这些方法之间的联系变得越来越困难。本文引入了一种由互信息启发的统一框架，并提出了一种具有灵活先验的新损失函数。通过仔细指定这些先验，我们展示了许多现有算法，如SimPO、TDPO、SparsePO等，都可以从我们的框架中推导出来。这种统一提供了更清晰和结构化的方法，使研究人员能够更好地理解不同的DPO变体之间的关系。我们旨在简化DPO算法的景观，使研究社区更容易获得洞察，并促进LLM对齐的进一步发展。最终，我们希望我们的框架能够成为开发更稳健和可解释对齐技术的基础。
2501.01535	 | A Metasemantic-Metapragmatic Framework for Taxonomizing Multimodal Communicative Alignment	 | Eugene Yu Ji	 | 本文构建了一个基于实用主义哲学和语用学理论的动态分类框架，旨在建立和概念化类人类多模态交际对齐，特别强调“指涉性语境化”和“语境化方向性”原则在跨语义-语用交流中的作用，并探讨其在人机对齐中的应用，包括意图、身份、情感和伦理等方面。	 | Drawing on contemporary pragmatist philosophy and linguistic theories on cognition, meaning, and communication, this paper presents a dynamic, metasemantic-metapragmatic taxonomy for grounding and conceptualizing human-like multimodal communicative alignment. The framework is rooted in contemporary developments of the three basic communicative capacities initially identified by American logician and pragmatist philosopher Charles Sanders Peirce: iconic (sensory and perceptual qualities), indexical (contextual and sociocultural associations), and rule-like (symbolic and intuitive reasoning). Expanding on these developments, I introduce the concept of indexical contextualization and propose the principle of "contextualization directionality" for characterizing the crucial metapragmatic capacity for maintaining, navigating, or transitioning between semantic and pragmatic modes of multimodal communication. I contend that current cognitive-social computational and engineering methodologies disproportionately emphasize the semantic/metasemantic domain, overlooking the pivotal role of metapragmatic indexicality in traversing the semantic-pragmatic spectrum of communication. The framework's broader implications for intentionality, identity, affect, and ethics in within-modal and cross-modal human-machine alignment are also discussed.	 | 本文借鉴了当代实用主义哲学和认知、意义以及交流的语用学理论，提出了一个动态的、元语用-元语用学分类框架，用于建立和概念化类人类的多模态交际对齐。该框架根植于美国逻辑学家和实用主义哲学家查尔斯·桑德斯·皮尔士最初提出的基本三种交际能力的发展：图示（感官和知觉品质）、指涉（语境和社会文化关联）以及规则化（符号和直觉推理）。在此基础上，我提出了“指涉性语境化”的概念，并提出了“语境化方向性”原则，以描述维持、导航或在语义和语用多重模态交流模式之间转换的关键语用能力。我主张目前的认知社会计算和工程方法过于侧重于语义/元语义领域，忽视了介语用的指涉性在跨越语义-语用交流光谱中的关键作用。本文还讨论了该框架在跨模式和模内人机对齐中的更广泛含义，涉及意图、身份、情感和伦理等方面。
2501.01516	 | Improving Robustness Estimates in Natural Language Explainable AI though Synonymity Weighted Similarity Measures	 | Christopher Burger	 | 该研究关注可解释人工智能（XAI）的有效性，发现常用的信息检索度量标准在对抗性XAI中不适用，因此提出利用扰动词的同义性改进这些度量标准，以更准确地评估XAI方法的弱点。	 | Explainable AI (XAI) has seen a surge in recent interest with the proliferation of powerful but intractable black-box models. Moreover, XAI has come under fire for techniques that may not offer reliable explanations. As many of the methods in XAI are themselves models, adversarial examples have been prominent in the literature surrounding the effectiveness of XAI, with the objective of these examples being to alter the explanation while maintaining the output of the original model. For explanations in natural language, it is natural to use measures found in the domain of information retrieval for use with ranked lists to guide the adversarial XAI process. We show that the standard implementation of these measures are poorly suited for the comparison of explanations in adversarial XAI and amend them by using information that is discarded, the synonymity of perturbed words. This synonymity weighting produces more accurate estimates of the actual weakness of XAI methods to adversarial examples.	 | 近年来，随着强大但难以解释的黑盒模型的普及，可解释人工智能（Explainable AI，XAI）逐渐引起了广泛关注。然而，XAI 的许多技术也可能无法提供可靠的解释，从而受到了指责。许多 XAI 方法本身就是模型，因此对抗性示例在关于 XAI 有效性研究的文献中占据了重要位置，这些对抗性示例的目标是在保持原始模型输出的同时改变解释。对于自然语言解释，自然可以使用信息检索领域的度量标准来指导对抗性 XAI 的过程，这些度量标准通常用于排名列表的比较。  我们发现标准实现的这些度量标准并不适合用于对抗性 XAI 中解释的比较，因此我们通过利用被丢弃的信息——扰动词的同义性——改进了这些度量标准。同义性加权能够更准确地估计 XAI 方法在对抗性示例面前的实际弱点。
2501.01478	 | Enhancing Reasoning through Process Supervision with Monte Carlo Tree Search	 | Shuangtao Li,Shuaihao Dong,Kexin Luan,Xinhan Di,Chaofan Ding	 | 本文提出了一种使用蒙特卡洛树搜索生成过程监督数据的方法，通过LLMs自身抽样推理步骤并为其分配“相对正确性”评分进行训练，从而显著提高其在数学推理任务上的性能，并展示了增强推理能力的可迁移性。	 | Large language models (LLMs) have demonstrated their remarkable capacity across a variety of tasks. However, reasoning remains a challenge for LLMs. To improve LLMs' reasoning ability, process supervision has proven to be better than outcome supervision. In this work, we study using Monte Carlo Tree Search (MCTS) to generate process supervision data with LLMs themselves for training them. We sample reasoning steps with an LLM and assign each step a score that captures its "relative correctness," and the LLM is then trained by minimizing weighted log-likelihood of generating the reasoning steps. This generate-then-train process is repeated iteratively until convergence.Our experimental results demonstrate that the proposed methods considerably improve the performance of LLMs on two mathematical reasoning datasets. Furthermore, models trained on one dataset also exhibit improved performance on the other, showing the transferability of the enhanced reasoning ability.	 | 大规模语言模型（LLMs）在各种任务中展现了其卓越的能力。然而，推理仍然是LLMs的一个挑战。为了提高LLMs的推理能力，过程监督比结果监督更有效。在本文中，我们研究使用蒙特卡洛树搜索（MCTS）来生成过程监督数据，并利用LLMs本身进行训练。我们使用LLMs抽样推理步骤，并为每个步骤分配一个反映其“相对正确性”的评分，然后通过最小化生成推理步骤的加权对数似然性对LLMs进行训练。这个生成-训练过程将反复迭代，直到收敛。我们的实验结果表明，所提出的方法显著提高了LLMs在两个数学推理数据集上的性能。此外，一个数据集上训练的模型在另一个数据集上也表现出改进的性能，展示了增强推理能力的可迁移性。
2501.01457	 | Reinforcing Thinking through Reasoning-Enhanced Reward Models	 | Diji Yang,Linda Zeng,Kezhen Chen,Yi Zhang	 | 本文提出了一种名为Distillation-Reinforcement-Reasoning (DRR) 的三步框架，通过利用大型语言模型 (LLM) 的内在推理过程生成合成行为数据，从而减少标记需求，提高其在多步推理任务中的表现；DRR框架在多个基准测试中优于自我批判方法，适用于广泛的以LLM为中心的任务。	 | Large Language Models (LLMs) exhibit great potential in complex multi-step reasoning through inference-time thinking but still struggle with deciding when to stop thinking due to limited self-awareness about their knowledge boundaries. While human preference alignment has shown extraordinary opportunities, expensive labeling challenges adherence to scaling law. Language model self-critique, as an alternative to using human-labeled reasoning data, is questioned with its inherited biases. This work addresses these challenges by distilling the LLM's own reasoning processes into synthetic behavioral data, eliminating the need for manual labeling of intermediate steps. Building on this concept, we propose Distillation-Reinforcement-Reasoning (DRR), a three-step framework that leverages the LLM's inherent behaviors as external feedback by first generating behavioral data using the Reasoner (LLM) to reflect its reasoning capabilities, then training a lightweight discriminative reward model (DM) on behavioral data, and finally deploying the DM at inference time to assist the Reasoner's decision-making. Experiments on multiple benchmarks show that the DRR framework outperforms self-critique approaches without relying on additional complex data annotation. Benefiting from lightweight design, ease of replication, and adaptability, DRR is applicable to a wide range of LLM-centric tasks.	 | 大型语言模型（LLMs）在通过推理时间思考进行复杂多步推理方面展现出巨大潜力，但它们仍然难以决定何时停止思考，因为它们对知识边界缺乏自我意识。虽然人类偏好对齐展示了巨大的机会，但昂贵的标签工作却阻碍了规模法则的遵循。语言模型自我批判作为替代使用人工标记推理数据的方法，因其固有的偏见而受到质疑。本文通过将LLM自身的推理过程精简为合成行为数据，从而消除中间步骤的手动标记需求来应对这些挑战。在此基础上，我们提出了一种三步框架——Distillation-Reinforcement-Reasoning（DRR），该框架利用LLM固有的行为作为外部反馈：首先使用Reasoner（LLM）生成行为数据以反映其推理能力，然后在行为数据上训练一个轻量级的判别奖励模型（DM），最后在推理时部署DM以协助Reasoner的决策。在多个基准测试上的实验表明，DRR框架在无需依赖额外复杂数据注释的情况下优于自我批判方法。得益于其轻量级设计、易于复制和适应性，DRR适用于广泛的以LLM为中心的任务。
