| ID | Title | Authors | Abstract (en) | Abstract (zh) | Summary (zh) | 
| --- | --- | --- | --- | --- | --- |
2501.01428	 | GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models	 | Zhangyang Qi,Zhixiong Zhang,Ye Fang,Jiaqi Wang,Hengshuang Zhao	 | In recent years, 2D Vision-Language Models (VLMs) have made significant strides in image-text understanding tasks. However, their performance in 3D spatial comprehension, which is critical for embodied intelligence, remains limited. Recent advances have leveraged 3D point clouds and multi-view images as inputs, yielding promising results. However, we propose exploring a purely vision-based solution inspired by human perception, which merely relies on visual cues for 3D spatial understanding. This paper empirically investigates the limitations of VLMs in 3D spatial knowledge, revealing that their primary shortcoming lies in the lack of global-local correspondence between the scene and individual frames. To address this, we introduce GPT4Scene, a novel visual prompting paradigm in VLM training and inference that helps build the global-local relationship, significantly improving the 3D spatial understanding of indoor scenes. Specifically, GPT4Scene constructs a 3D Bird's Eye View (BEV) image from the video and marks consistent object IDs across both frames and the BEV image. The model then inputs the concatenated BEV image and video frames with markers. In zero-shot evaluations, GPT4Scene improves performance over closed-source VLMs like GPT-4o. Additionally, we prepare a processed video dataset consisting of 165K text annotation to fine-tune open-source VLMs, achieving state-of-the-art performance on all 3D understanding tasks. Surprisingly, after training with the GPT4Scene paradigm, VLMs consistently improve during inference, even without visual prompting and BEV image as explicit correspondence. It demonstrates that the proposed paradigm helps VLMs develop an intrinsic ability to understand 3D scenes, which paves the way for a noninvasive approach to extending pre-trained VLMs for 3D scene understanding.	 | 近年来，2D 视觉-语言模型（VLMs）在图像-文本理解任务中取得了显著进展。然而，它们在三维空间理解方面的性能仍然有限，而三维空间理解对于实现体感智能至关重要。最近的研究利用了3D 点云和多视图图像作为输入，取得了令人鼓舞的结果。然而，我们提出了一种受人类感知启发的纯视觉解决方案，仅依赖于视觉线索进行三维空间理解。本文通过实证研究探讨了VLMs在三维空间知识方面的局限性，揭示其主要缺点在于场景与单帧之间的全局-局部对应关系缺失。为解决这一问题，我们引入了GPT4Scene，这是一种新颖的视觉提示范式，用于VLM训练和推理，有助于构建全局-局部关系，显著提高了对室内场景的三维空间理解能力。具体来说，GPT4Scene 从视频中构建一个三维鸟瞰图（BEV）图像，并在帧之间和BEV 图像中标记一致的对象ID。模型随后将连接的BEV图像和带有标记的视频帧作为输入。在零样本评估中，GPT4Scene 在性能上优于闭源VLMs如GPT-4o。此外，我们还准备了一个包含16.5万个文本注释的处理过的视频数据集，用于微调开源VLMs，使其在所有三维理解任务上取得了最先进的性能。令人惊讶的是，在使用GPT4Scene范式训练后，VLMs在推理过程中即使没有视觉提示和BEV图像作为显式对应关系，也表现出持续的性能提升。这表明所提出的方法帮助VLMs发展出了内在理解三维场景的能力，为非侵入性的方法实现预训练VLMs在三维场景理解上的扩展铺平了道路。	 | 本文探讨了2D视觉-语言模型在三维空间理解方面的局限性，并提出了一种名为GPT4Scene的新颖视觉提示范式，通过构建三维鸟瞰图和标记一致的对象ID，显著提高了模型对室内场景的三维理解能力。实验表明，在使用GPT4Scene训练后，模型即使在没有显式对应关系的情况下也能持续提升三维理解能力，为非侵入性地扩展VLMs的三维理解能力提供了新方法。
2501.01427	 | VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control	 | Yuanpeng Tu,Hao Luo,Xi Chen,Sihui Ji,Xiang Bai,Hengshuang Zhao	 | Despite significant advancements in video generation, inserting a given object into videos remains a challenging task. The difficulty lies in preserving the appearance details of the reference object and accurately modeling coherent motions at the same time. In this paper, we propose VideoAnydoor, a zero-shot video object insertion framework with high-fidelity detail preservation and precise motion control. Starting from a text-to-video model, we utilize an ID extractor to inject the global identity and leverage a box sequence to control the overall motion. To preserve the detailed appearance and meanwhile support fine-grained motion control, we design a pixel warper. It takes the reference image with arbitrary key-points and the corresponding key-point trajectories as inputs. It warps the pixel details according to the trajectories and fuses the warped features with the diffusion U-Net, thus improving detail preservation and supporting users in manipulating the motion trajectories. In addition, we propose a training strategy involving both videos and static images with a reweight reconstruction loss to enhance insertion quality. VideoAnydoor demonstrates significant superiority over existing methods and naturally supports various downstream applications (e.g., talking head generation, video virtual try-on, multi-region editing) without task-specific fine-tuning.	 | 尽管在视频生成方面取得了显著进展，但在视频中插入给定对象仍是一项具有挑战性的任务。难度在于同时保留参考对象的外观细节和准确建模连贯的运动。在本文中，我们提出了一种名为VideoAnydoor的零样本视频对象插入框架，该框架具有高保真细节保留和精确的运动控制。从一个文本到视频模型开始，我们利用ID提取器注入全局身份，并利用一个框序列来控制整体运动。为了同时保留详细的外观并支持精细的运动控制，我们设计了一个像素扭曲器。它以带有任意关键点的参考图像及其对应的关键点轨迹作为输入。它根据轨迹扭曲像素细节，并将扭曲特征与扩散U-Net融合，从而提高细节保留并支持用户操控运动轨迹。此外，我们提出了一种涉及视频和静态图像的训练策略，并采用重权重建损失以增强插入质量。VideoAnydoor在现有方法上表现出显著优越性，并且自然支持各种下游应用（例如，说话头生成、视频虚拟试穿、多区域编辑）而无需针对特定任务进行微调。	 | 本文提出了一种名为VideoAnydoor的零样本视频对象插入框架，该框架通过高保真细节保留和精确的运动控制实现了给定对象的高效插入，并且自然支持多种下游应用。该框架利用像素扭曲器根据参考图像和关键点轨迹生成精细的外观和运动，同时采用特殊的训练策略和损失函数来优化插入效果。
2501.01426	 | Unifying Specialized Visual Encoders for Video Language Models	 | Jihoon Chung,Tyler Zhu,Max Gonzalez Saez-Diez,Juan Carlos Niebles,Honglu Zhou,Olga Russakovsky	 | The recent advent of Large Language Models (LLMs) has ushered sophisticated reasoning capabilities into the realm of video through Video Large Language Models (VideoLLMs). However, VideoLLMs currently rely on a single vision encoder for all of their visual processing, which limits the amount and type of visual information that can be conveyed to the LLM. Our method, MERV, Multi-Encoder Representation of Videos, instead leverages multiple frozen visual encoders to create a unified representation of a video, providing the VideoLLM with a comprehensive set of specialized visual knowledge. Spatio-temporally aligning the features from each encoder allows us to tackle a wider range of open-ended and multiple-choice video understanding questions and outperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy than Video-LLaVA across the standard suite video understanding benchmarks, while also having a better Video-ChatGPT score. We also improve upon SeViLA, the previous best on zero-shot Perception Test accuracy, by 2.2%. MERV introduces minimal extra parameters and trains faster than equivalent single-encoder methods while parallelizing the visual processing. Finally, we provide qualitative evidence that MERV successfully captures domain knowledge from each of its encoders. Our results offer promising directions in utilizing multiple vision encoders for comprehensive video understanding.	 | 近期，大型语言模型（LLMs）的出现将复杂的推理能力引入了视频领域，通过视频大型语言模型（VideoLLMs）。然而，当前的VideoLLMs依赖单一的视觉编码器进行所有视觉处理，这限制了可以传达给LLM的视觉信息的类型和数量。我们提出的方法，MERV（多编码器视频表示），通过利用多个冻结的视觉编码器来创建视频的统一表示，为VideoLLM提供了一整套专门化的视觉知识。通过在时空上对每个编码器的特征进行对齐，我们可以解决更多开放性和多元选择的视频理解问题，并超越了先前的最佳成果。与Video-LLaVA在标准视频理解基准测试中的准确性相比，MERV 在准确性上提高了3.7%，同时也具有更高的Video-ChatGPT评分。我们还在零样本感知测试中将SeViLA的最佳表现提高了2.2%。相比等效的单编码器方法，MERV 引入了最少的额外参数，训练速度更快，并且可以并行化视觉处理。最后，我们提供了定性的证据，证明MERV 成功从每个编码器中捕捉到了其领域的知识。我们的结果为利用多个视觉编码器进行全面视频理解提供了令人振奋的方向。	 | MERV（多编码器视频表示）通过利用多个冻结的视觉编码器为VideoLLM提供专门化的视觉知识，提高了视频理解的准确性和多样性，在标准基准测试中超越了现有最佳成果，并在零样本感知测试中显著提升了性能。
2501.01425	 | Free-Form Motion Control: A Synthetic Video Generation Dataset with Controllable Camera and Object Motions	 | Xincheng Shuai,Henghui Ding,Zhenyuan Qin,Hao Luo,Xingjun Ma,Dacheng Tao	 | Controlling the movements of dynamic objects and the camera within generated videos is a meaningful yet challenging task. Due to the lack of datasets with comprehensive motion annotations, existing algorithms can not simultaneously control the motions of both camera and objects, resulting in limited controllability over generated contents. To address this issue and facilitate the research in this field, we introduce a Synthetic Dataset for Free-Form Motion Control (SynFMC). The proposed SynFMC dataset includes diverse objects and environments and covers various motion patterns according to specific rules, simulating common and complex real-world scenarios. The complete 6D pose information facilitates models learning to disentangle the motion effects from objects and the camera in a video. To validate the effectiveness and generalization of SynFMC, we further propose a method, Free-Form Motion Control (FMC). FMC enables independent or simultaneous control of object and camera movements, producing high-fidelity videos. Moreover, it is compatible with various personalized text-to-image (T2I) models for different content styles. Extensive experiments demonstrate that the proposed FMC outperforms previous methods across multiple scenarios.	 | 在生成视频中控制动态物体和摄像机的运动是一个有意义但极具挑战性的任务。由于缺乏包含全面运动标注的数据集，现有算法无法同时控制摄像机和物体的运动，导致生成内容的可控性有限。为解决这一问题并促进该领域的研究，我们引入了一个合成数据集——自由形态运动控制合成数据集（SynFMC）。该提出的SynFMC数据集包含了多样化的物体和环境，并根据特定规则涵盖了各种运动模式，模拟了常见和复杂的现实场景。完整的6D姿态信息有助于模型学习在视频中分离物体和摄像机的运动效应。为了验证SynFMC的有效性和泛化能力，我们进一步提出了一种方法——自由形态运动控制（FMC）。FMC能够独立或同时控制物体和摄像机的运动，生成高质量的视频。此外，它还兼容多种个性化的文本到图像（T2I）模型，以适应不同的内容风格。大量实验表明，提出的FMC在多种场景中均优于之前的算法。	 | 该研究提出了一种名为SynFMC的合成数据集，用于控制生成视频中动态物体和摄像机的运动，并开发了一种自由形态运动控制（FMC）方法，能够独立或同时控制物体和摄像机的运动，显著提高了生成视频的可控性和质量。
2501.01424	 | Object-level Visual Prompts for Compositional Image Generation	 | Gaurav Parmar,Or Patashnik,Kuan-Chieh Wang,Daniil Ostashev,Srinivasa Narasimhan,Jun-Yan Zhu,Daniel Cohen-Or,Kfir Aberman	 | We introduce a method for composing object-level visual prompts within a text-to-image diffusion model. Our approach addresses the task of generating semantically coherent compositions across diverse scenes and styles, similar to the versatility and expressiveness offered by text prompts. A key challenge in this task is to preserve the identity of the objects depicted in the input visual prompts, while also generating diverse compositions across different images. To address this challenge, we introduce a new KV-mixed cross-attention mechanism, in which keys and values are learned from distinct visual representations. The keys are derived from an encoder with a small bottleneck for layout control, whereas the values come from a larger bottleneck encoder that captures fine-grained appearance details. By mixing keys and values from these complementary sources, our model preserves the identity of the visual prompts while supporting flexible variations in object arrangement, pose, and composition. During inference, we further propose object-level compositional guidance to improve the method's identity preservation and layout correctness. Results show that our technique produces diverse scene compositions that preserve the unique characteristics of each visual prompt, expanding the creative potential of text-to-image generation.	 | 我们提出了一种在文本到图像扩散模型中组合物体级视觉提示的方法。我们的方法解决了在不同场景和风格中生成语义上连贯的组合的任务，类似于文本提示提供的灵活性和表达力。在这个任务中面临的一个关键挑战是在保留输入视觉提示中描绘的对象身份的同时，在不同图像中生成多样化的组合。为了应对这一挑战，我们引入了一种新的KV混合交叉注意力机制，其中键和值分别从不同的视觉表示中学习。键由一个具有小瓶颈以控制布局的编码器产生，而值则来自一个较大的瓶颈编码器，能够捕捉细粒度的外观细节。通过从这些互补来源混合键和值，我们的模型在保持视觉提示身份的同时支持对象排列、姿态和布局的灵活变化。在推理过程中，我们进一步提出了物体级组合指导，以提高方法的身份保持能力和布局准确性。实验结果表明，我们的技术能够生成多样化的场景组合，同时保留每个视觉提示的独特特征，从而扩展了文本到图像生成的创造性潜力。	 | 该研究提出了一种新的KV混合交叉注意力机制，用于在文本到图像扩散模型中生成具有多样化的组合和保留输入视觉提示身份的语义连贯场景，通过这种方法扩展了文本到图像生成的创造性潜力。
2501.01423	 | Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models	 | Jingfeng Yao,Xinggang Wang	 | Latent diffusion models with Transformer architectures excel at generating high-fidelity images. However, recent studies reveal an optimization dilemma in this two-stage design: while increasing the per-token feature dimension in visual tokenizers improves reconstruction quality, it requires substantially larger diffusion models and more training iterations to achieve comparable generation performance. Consequently, existing systems often settle for sub-optimal solutions, either producing visual artifacts due to information loss within tokenizers or failing to converge fully due to expensive computation costs. We argue that this dilemma stems from the inherent difficulty in learning unconstrained high-dimensional latent spaces. To address this, we propose aligning the latent space with pre-trained vision foundation models when training the visual tokenizers. Our proposed VA-VAE (Vision foundation model Aligned Variational AutoEncoder) significantly expands the reconstruction-generation frontier of latent diffusion models, enabling faster convergence of Diffusion Transformers (DiT) in high-dimensional latent spaces. To exploit the full potential of VA-VAE, we build an enhanced DiT baseline with improved training strategies and architecture designs, termed LightningDiT. The integrated system achieves state-of-the-art (SOTA) performance on ImageNet 256x256 generation with an FID score of 1.35 while demonstrating remarkable training efficiency by reaching an FID score of 2.11 in just 64 epochs--representing an over 21 times convergence speedup compared to the original DiT. Models and codes are available at: https://github.com/hustvl/LightningDiT.	 | 基于Transformer架构的隐式扩散模型在生成高保真图像方面表现出色。然而，最近的研究揭示了这种两阶段设计中的优化困境：虽然在视觉标记器中增加每令牌特征维度可以提高重建质量，但这也需要更大的扩散模型和更多的训练迭代次数才能达到可比的生成性能。因此，现有的系统往往选择次优解决方案，要么因标记器内的信息损失而产生视觉伪影，要么因高昂的计算成本而无法完全收敛。我们认为，这一困境源于学习不受约束的高维潜在空间固有的困难。为了解决这一问题，我们提出在训练视觉标记器时将潜在空间与预先训练的视觉基础模型对齐。我们提出的VA-VAE（视觉基础模型对齐的变分自动编码器）显著扩展了隐式扩散模型的重建-生成边界，使扩散转换器（DiT）在高维潜在空间中的收敛速度更快。为了充分发挥VA-VAE的潜力，我们构建了一个增强的DiT基线，采用改进的训练策略和架构设计，称为LightningDiT。综合系统在ImageNet 256x256生成任务中实现了最先进的性能（SOTA），FID得分为1.35，同时展示了显著的训练效率，在仅64个epoch中达到FID得分为2.11，相比原始DiT收敛速度快了21倍以上。相关模型和代码可在以下链接获取：https://github.com/hustvl/LightningDiT。	 | 研究发现，基于Transformer的隐式扩散模型在优化过程中存在重建质量和生成性能之间的trade-off问题，为解决这一问题，提出了一种将视觉标记器与预先训练的视觉基础模型对齐的VA-VAE方法，显著提升了扩散模型的性能和训练效率，使得LightningDiT在ImageNet 256x256生成任务中达到了最先进的FID分数1.35。
2501.01422	 | Multi-Modal Video Feature Extraction for Popularity Prediction	 | Haixu Liu,Wenning Wang,Haoxiang Zheng,Penghao Jiang,Qirui Wang,Ruiqing Yan,Qiuzhuang Sun	 | This work aims to predict the popularity of short videos using the videos themselves and their related features. Popularity is measured by four key engagement metrics: view count, like count, comment count, and share count. This study employs video classification models with different architectures and training methods as backbone networks to extract video modality features. Meanwhile, the cleaned video captions are incorporated into a carefully designed prompt framework, along with the video, as input for video-to-text generation models, which generate detailed text-based video content understanding. These texts are then encoded into vectors using a pre-trained BERT model. Based on the six sets of vectors mentioned above, a neural network is trained for each of the four prediction metrics. Moreover, the study conducts data mining and feature engineering based on the video and tabular data, constructing practical features such as the total frequency of hashtag appearances, the total frequency of mention appearances, video duration, frame count, frame rate, and total time online. Multiple machine learning models are trained, and the most stable model, XGBoost, is selected. Finally, the predictions from the neural network and XGBoost models are averaged to obtain the final result.	 | 本研究旨在通过自身特征及相关特征预测短视频的流行度。流行度通过四个关键的参与度指标进行衡量：观看次数、点赞次数、评论次数和分享次数。本研究采用具有不同架构和训练方法的视频分类模型作为基础网络，提取视频模态特征。同时，清理后的视频字幕被整合到一个精心设计的提示框架中，与视频一起作为输入，用于视频到文本生成模型，生成详细的视频内容理解文本。这些文本随后使用预训练的BERT模型编码成向量。基于上述六组向量，分别针对四种预测指标训练了一个神经网络。此外，本研究基于视频和表格数据进行数据挖掘和特征工程，构建了诸如标签出现总频率、提及出现总频率、视频时长、帧数、帧率和在线总时长等实用特征。多个机器学习模型被训练，最稳定的模型XGBoost被选中。最后，神经网络和XGBoost模型的预测结果被平均，以获得最终结果。	 | 该研究通过视频分类模型提取模态特征，并结合预处理的字幕，使用BERT编码生成详细文本，再通过神经网络和XGBoost模型预测短视频的观看次数、点赞次数、评论次数和分享次数等流行度指标，最终将两种模型的预测结果平均以提高准确性。
2501.01421	 | R-SCoRe: Revisiting Scene Coordinate Regression for Robust Large-Scale Visual Localization	 | Xudong Jiang,Fangjinhua Wang,Silvano Galliani,Christoph Vogel,Marc Pollefeys	 | Learning-based visual localization methods that use scene coordinate regression (SCR) offer the advantage of smaller map sizes. However, on datasets with complex illumination changes or image-level ambiguities, it remains a less robust alternative to feature matching methods. This work aims to close the gap. We introduce a covisibility graph-based global encoding learning and data augmentation strategy, along with a depth-adjusted reprojection loss to facilitate implicit triangulation. Additionally, we revisit the network architecture and local feature extraction module. Our method achieves state-of-the-art on challenging large-scale datasets without relying on network ensembles or 3D supervision. On Aachen Day-Night, we are 10$\times$ more accurate than previous SCR methods with similar map sizes and require at least 5$\times$ smaller map sizes than any other SCR method while still delivering superior accuracy. Code will be available at: https://github.com/cvg/scrstudio .	 | 基于学习的视觉定位方法，尤其是使用场景坐标回归（SCR）的方法，具有较小地图大小的优势。然而，在具有复杂光照变化或图像级歧义的数据集上，它仍然是特征匹配方法的一种不太稳健的替代方案。本工作旨在弥补这一差距。我们提出了一种基于共视图图的全局编码学习和数据增广策略，并引入了一种深度调整的重投影损失，以促进隐式三角测量。此外，我们重新审视了网络架构和局部特征提取模块。我们的方法在具有挑战性的大规模数据集上达到了最先进的性能，而无需依赖网络集成或3D监督。在Aachen Day-Night数据集上，我们的方法比类似地图大小的先前SCR方法准确性高10倍，并且所需的地图大小至少比其他任何SCR方法小5倍，同时仍能提供更高的准确性。代码将在此处获得：https://github.com/cvg/scrstudio 。	 | 本文提出了一种改进的视觉定位方法，通过引入全局编码学习、数据增强策略和深度调整的重投影损失，提高了复杂光照变化条件下的定位准确性，并且所需的地图大小远小于现有方法，同时保持了高精度。该方法在具有挑战性的大规模数据集上达到了最先进的性能，特别是在Aachen Day-Night数据集上，准确性比现有方法高10倍，而地图大小却小5倍。
2501.01420	 | A Multi-task Supervised Compression Model for Split Computing	 | Yoshitomo Matsubara,Matteo Mendula,Marco Levorato	 | Split computing ($\neq$ split learning) is a promising approach to deep learning models for resource-constrained edge computing systems, where weak sensor (mobile) devices are wirelessly connected to stronger edge servers through channels with limited communication capacity. State-of-theart work on split computing presents methods for single tasks such as image classification, object detection, or semantic segmentation. The application of existing methods to multitask problems degrades model accuracy and/or significantly increase runtime latency. In this study, we propose Ladon, the first multi-task-head supervised compression model for multi-task split computing. Experimental results show that the multi-task supervised compression model either outperformed or rivaled strong lightweight baseline models in terms of predictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets while learning compressed representations at its early layers. Furthermore, our models reduced end-to-end latency (by up to 95.4%) and energy consumption of mobile devices (by up to 88.2%) in multi-task split computing scenarios.	 | 分割计算（$\neq$ 分裂学习）是资源受限边缘计算系统中深度学习模型的一种有前景的方法，其中弱传感器（移动）设备通过有限通信能力的信道无线连接到更强的边缘服务器。关于分割计算的最新研究提出了针对单一任务（如图像分类、对象检测或语义分割）的方法。现有方法应用于多任务问题时会导致模型精度下降或显著增加运行时间延迟。在这项研究中，我们提出了Ladon，这是第一个用于多任务分割计算的监督压缩模型。实验结果表明，多任务监督压缩模型在ILSVRC 2012、COCO 2017 和 PASCAL VOC 2012 数据集上，在早期层学习压缩表示时，预测性能优于或可与强大的轻量级基线模型相媲美。此外，我们的模型在多任务分割计算场景中将端到端延迟（最高减少95.4%）和移动设备的能量消耗（最高减少88.2%）显著降低。	 | Ladon 是首个用于多任务分割计算的监督压缩模型，能够在保持预测性能的同时显著降低端到端延迟和移动设备的能量消耗。
2501.01416	 | Hierarchical Alignment-enhanced Adaptive Grounding Network for Generalized Referring Expression Comprehension	 | Yaxian Wang,Henghui Ding,Shuting He,Xudong Jiang,Bifan Wei,Jun Liu	 | In this work, we address the challenging task of Generalized Referring Expression Comprehension (GREC). Compared to the classic Referring Expression Comprehension (REC) that focuses on single-target expressions, GREC extends the scope to a more practical setting by further encompassing no-target and multi-target expressions. Existing REC methods face challenges in handling the complex cases encountered in GREC, primarily due to their fixed output and limitations in multi-modal representations. To address these issues, we propose a Hierarchical Alignment-enhanced Adaptive Grounding Network (HieA2G) for GREC, which can flexibly deal with various types of referring expressions. First, a Hierarchical Multi-modal Semantic Alignment (HMSA) module is proposed to incorporate three levels of alignments, including word-object, phrase-object, and text-image alignment. It enables hierarchical cross-modal interactions across multiple levels to achieve comprehensive and robust multi-modal understanding, greatly enhancing grounding ability for complex cases. Then, to address the varying number of target objects in GREC, we introduce an Adaptive Grounding Counter (AGC) to dynamically determine the number of output targets. Additionally, an auxiliary contrastive loss is employed in AGC to enhance object-counting ability by pulling in multi-modal features with the same counting and pushing away those with different counting. Extensive experimental results show that HieA2G achieves new state-of-the-art performance on the challenging GREC task and also the other 4 tasks, including REC, Phrase Grounding, Referring Expression Segmentation (RES), and Generalized Referring Expression Segmentation (GRES), demonstrating the remarkable superiority and generalizability of the proposed HieA2G.	 | 在本文中，我们探讨了通用参照表达理解（Generalized Referring Expression Comprehension，简称GREC）这一具有挑战性的任务。与经典参照表达理解（Classic Referring Expression Comprehension，简称REC）聚焦于单目标表达不同，GREC进一步扩展了范围，涵盖了无目标和多目标表达。现有的REC方法在处理GREC中遇到的复杂情况时面临挑战，主要原因是它们的固定输出和多模态表示的局限性。为了解决这些问题，我们提出了一种层次化对齐增强自适应语义定位网络（HieA2G），能够灵活应对各种类型的参照表达。首先，我们提出了层次化多模态语义对齐（Hierarchical Multi-modal Semantic Alignment，简称HMSA）模块，包括词对象、短语对象和文本图像对齐三个层次的对齐。该模块通过多层次的跨模态交互实现全面而稳健的多模态理解，极大地增强了复杂情况下的定位能力。其次，为了应对GREC中目标对象数量的变化，我们引入了自适应定位计数器（Adaptive Grounding Counter，简称AGC），以动态确定输出目标的数量。此外，我们还在AGC中使用了辅助对比损失，通过拉近具有相同计数的多模态特征并推开具有不同计数的特征，增强了对象计数能力。广泛实验结果表明，HieA2G 在具有挑战性的GREC任务以及包括REC、短语定位、参照表达分割（Referring Expression Segmentation，简称RES）和通用参照表达分割（Generalized Referring Expression Segmentation，简称GRES）在内的其他4个任务中取得了新的最佳性能，证明了所提出HieA2G 的显著优越性和泛化能力。	 | 本文提出了一种名为HieA2G的层次化对齐增强自适应语义定位网络，以解决通用参照表达理解中的挑战，该网络通过多层次的跨模态交互和自适应定位计数器有效应对多目标和无目标表达，并在多个任务中取得了最佳性能。
2501.01409	 | On Unifying Video Generation and Camera Pose Estimation	 | Chun-Hao Paul Huang,Jae Shin Yoon,Hyeonho Jeong,Niloy Mitra,Duygu Ceylan	 | Inspired by the emergent 3D capabilities in image generators, we explore whether video generators similarly exhibit 3D awareness. Using structure-from-motion (SfM) as a benchmark for 3D tasks, we investigate if intermediate features from OpenSora, a video generation model, can support camera pose estimation. We first examine native 3D awareness in video generation features by routing raw intermediate outputs to SfM-prediction modules like DUSt3R. Then, we explore the impact of fine-tuning on camera pose estimation to enhance 3D awareness. Results indicate that while video generator features have limited inherent 3D awareness, task-specific supervision significantly boosts their accuracy for camera pose estimation, resulting in competitive performance. The proposed unified model, named JOG3R, produces camera pose estimates with competitive quality without degrading video generation quality.	 | 受图像生成器中出现的3D能力启发，我们研究视频生成器是否也表现出3D意识。我们使用结构从运动（SfM）作为3D任务的基准标准，来探究OpenSora（一种视频生成模型）的中间特征是否能够支持相机姿态估计。我们首先通过将原始中间输出路由到类似于DUSt3R的SfM预测模块，来检查视频生成特征中固有的3D意识。然后，我们探索了微调对相机姿态估计的影响，以增强3D意识。结果表明，虽然视频生成器的特征本身具有有限的固有3D意识，但特定任务的监督显著提高了其在相机姿态估计中的准确性，从而实现了竞争力的表现。我们提出的统一大模型，名为JOG3R，在不降低视频生成质量的情况下，能够生成具有竞争力质量的相机姿态估计。	 | 研究发现，虽然视频生成器（如OpenSora）的中间特征本身具有有限的3D意识，但通过特定任务的监督微调，其在相机姿态估计上的表现显著提升，进而实现了与专门设计的3D模型竞争力相当的结果。此外，提出的统一大模型JOG3R能够在不降低视频生成质量的情况下，生成高质量的相机姿态估计。
2501.01407	 | Nested Attention: Semantic-aware Attention Values for Concept Personalization	 | Or Patashnik,Rinon Gal,Daniil Ostashev,Sergey Tulyakov,Kfir Aberman,Daniel Cohen-Or	 | Personalizing text-to-image models to generate images of specific subjects across diverse scenes and styles is a rapidly advancing field. Current approaches often face challenges in maintaining a balance between identity preservation and alignment with the input text prompt. Some methods rely on a single textual token to represent a subject, which limits expressiveness, while others employ richer representations but disrupt the model's prior, diminishing prompt alignment. In this work, we introduce Nested Attention, a novel mechanism that injects a rich and expressive image representation into the model's existing cross-attention layers. Our key idea is to generate query-dependent subject values, derived from nested attention layers that learn to select relevant subject features for each region in the generated image. We integrate these nested layers into an encoder-based personalization method, and show that they enable high identity preservation while adhering to input text prompts. Our approach is general and can be trained on various domains. Additionally, its prior preservation allows us to combine multiple personalized subjects from different domains in a single image.	 | 将特定文本转化为特定图像的模型进行个性化以生成不同场景和风格中的特定主题图像，这是一个迅速发展的领域。当前的方法在保持身份保留和与输入文本提示的对齐之间往往面临着平衡的挑战。一些方法依赖单一的文本令牌来表示一个主题，这限制了模型的表达能力，而另一些方法则使用更丰富的表示，但会导致模型先验的破坏，降低提示的对齐度。在本文中，我们引入了一种新颖的机制——嵌套注意机制，该机制将丰富的图像表示注入到模型现有的交叉注意层中。我们的核心思想是生成查询依赖的主题值，这些值由嵌套注意层学习生成，以选择生成图像中每个区域的相关主题特征。我们将这些嵌套层整合到基于编码器的个性化方法中，并证明它们能够实现高度的身份保留，同时遵循输入文本提示。我们的方法是通用的，可以在各种领域进行训练。此外，由于其先验保留能力，我们可以在单张图像中结合来自不同领域的多个个性化主题。	 | 本文提出了一种新颖的嵌套注意机制，将丰富的图像表示注入现有模型的交叉注意层中，以生成特定主题的图像，该方法能够在保持身份保留的同时与输入文本提示对齐，并且具备在不同领域训练的通用性，还能够在单张图像中结合多个个性化主题，保留模型的先验信息。
2501.01393	 | Learning 3D Garment Animation from Trajectories of A Piece of Cloth	 | Yidi Shao,Chen Change Loy,Bo Dai	 | Garment animation is ubiquitous in various applications, such as virtual reality, gaming, and film producing. Recently, learning-based approaches obtain compelling performance in animating diverse garments under versatile scenarios. Nevertheless, to mimic the deformations of the observed garments, data-driven methods require large scale of garment data, which are both resource-wise expensive and time-consuming. In addition, forcing models to match the dynamics of observed garment animation may hinder the potentials to generalize to unseen cases. In this paper, instead of using garment-wise supervised-learning we adopt a disentangled scheme to learn how to animate observed garments: 1). learning constitutive behaviors from the observed cloth; 2). dynamically animate various garments constrained by the learned constitutive laws. Specifically, we propose Energy Unit network (EUNet) to model the constitutive relations in the format of energy. Without the priors from analytical physics models and differentiable simulation engines, EUNet is able to directly capture the constitutive behaviors from the observed piece of cloth and uniformly describes the change of energy caused by deformations, such as stretching and bending. We further apply the pre-trained EUNet to animate various garments based on energy optimizations. The disentangled scheme alleviates the need of garment data and enables us to utilize the dynamics of a piece of cloth for animating garments. Experiments show that while EUNet effectively delivers the energy gradients due to the deformations, models constrained by EUNet achieve more stable and physically plausible performance comparing with those trained in garment-wise supervised manner. Code is available at https://github.com/ftbabi/EUNet_NeurIPS2024.git .	 | 服装动画在虚拟现实、游戏和影视制作等多种应用中普遍存在。最近，基于学习的方法在各种场景下动画化多种服装方面表现出色。然而，为了模仿观察到的服装的变形，数据驱动的方法需要大量的服装数据，这在资源和时间上都是昂贵且耗时的。此外，强制模型匹配观察到的服装动画的动力学可能会影响其在未见情况下的泛化能力。在本文中，我们不再使用基于服装的监督学习，而是采用解耦的方法来学习如何动画化观察到的服装：1) 从观察到的布料中学习根本行为；2) 根据学习到的根本法则动态动画化各种服装。具体而言，我们提出了Energy Unit网络（EUNet）来用能量的形式建模根本关系。在没有解析物理模型和可微模拟引擎先验知识的情况下，EUNet能够直接从观察到的布料中捕捉根本行为，并统一描述由变形（如拉伸和弯曲）引起的能量变化。我们进一步利用预训练的EUNet基于能量优化来动画化各种服装。解耦的方法减少了对服装数据的需求，并使我们能够利用布料的动力学来动画化服装。实验结果显示，尽管EUNet能够有效地传递由于变形引起的能量梯度，但受EUNet约束的模型在稳定性和物理合理性方面优于基于服装的监督学习训练的模型。代码可在https://github.com/ftbabi/EUNet_NeurIPS2024.git 获取。	 | 本文提出了一种解耦的方法来学习如何动画化观察到的服装，通过Energy Unit网络（EUNet）从布料中学习根本行为，并利用能量优化来动态动画化各种服装，从而减少了对大量服装数据的需求，提高了模型的稳定性和物理合理性。
2501.01375	 | Iris Recognition for Infants	 | Rasel Ahmed Bhuiyan,Mateusz Trokielewicz,Piotr Maciejewicz,Sherri Bucher,Adam Czajka	 | Non-invasive, efficient, physical token-less, accurate and stable identification methods for newborns may prevent baby swapping at birth, limit baby abductions and improve post-natal health monitoring across geographies, within the context of both the formal (i.e., hospitals) and informal (i.e., humanitarian and fragile settings) health sectors. This paper explores the feasibility of application iris recognition to build biometric identifiers for 4-6 week old infants. We (a) collected near infrared (NIR) iris images from 17 infants using a specially-designed NIR iris sensor; (b) evaluated six iris recognition methods to assess readiness of the state-of-the-art iris recognition to be applied to newborns and infants; (c) proposed a new segmentation model that correctly detects iris texture within infants iris images, and coupled it with several iris texture encoding approaches to offer, to the first of our knowledge, a fully-operational infant iris recognition system; and, (d) trained a StyleGAN-based model to synthesize iris images mimicking samples acquired from infants to deliver to the research community privacy-safe infant iris images. The proposed system, incorporating the specially-designed iris sensor and segmenter, and applied to the collected infant iris samples, achieved Equal Error Rate (EER) of 3\% and Area Under ROC Curve (AUC) of 99\%, compared to EER$\geq$20\% and AUC$\leq$88\% obtained for state of the art adult iris recognition systems. This suggests that it may be feasible to design methods that succesfully extract biometric features from infant irises.	 | 非侵入性、高效、无需物理令牌、准确且稳定的新生儿识别方法可能预防出生时婴儿调包、减少婴儿拐卖，并在不同地理区域和正式（如医院）及非正式（如人道主义和脆弱环境）卫生领域中改善产后健康监测。本文探讨了将虹膜识别应用于4-6周大婴儿的可行性，旨在构建生物特征标识符。我们（a）使用专门设计的近红外（NIR）虹膜传感器采集了17名婴儿的近红外虹膜图像；（b）评估了六种虹膜识别方法，以评估最先进的虹膜识别技术应用于新生儿和婴儿的可行性；（c）提出了一种新的分割模型，能够正确检测婴儿虹膜图像中的虹膜纹理，并与多种虹膜纹理编码方法结合，据我们所知，这是第一个完全可操作的婴儿虹膜识别系统；（d）训练了一个基于StyleGAN的模型，以合成模仿从婴儿处采集的样本的虹膜图像，为研究界提供隐私安全的婴儿虹膜图像。所提议的系统结合专门设计的虹膜传感器和分割器，并应用于采集的婴儿虹膜样本，其等错误率（EER）为3%，受试者操作特性曲线下面积（AUC）为99%，而最先进的成人虹膜识别系统的相关指标分别为EER≥20%和AUC≤88%。这表明，可能设计出成功从婴儿虹膜中提取生物特征的方法。	 | 本文提出了一种基于近红外虹膜识别的新生儿识别方法，通过专门设计的虹膜传感器和新的分割模型，实现了准确且稳定的婴儿虹膜识别，其性能显著优于现有的成人虹膜识别系统。
2501.01371	 | CLIP-UP: CLIP-Based Unanswerable Problem Detection for Visual Question Answering	 | Ben Vardi,Oron Nir,Ariel Shamir	 | Recent Vision-Language Models (VLMs) have demonstrated remarkable capabilities in visual understanding and reasoning, and in particular on multiple-choice Visual Question Answering (VQA). Still, these models can make distinctly unnatural errors, for example, providing (wrong) answers to unanswerable VQA questions, such as questions asking about objects that do not appear in the image. To address this issue, we propose CLIP-UP: CLIP-based Unanswerable Problem detection, a novel lightweight method for equipping VLMs with the ability to withhold answers to unanswerable questions. By leveraging CLIP to extract question-image alignment information, CLIP-UP requires only efficient training of a few additional layers, while keeping the original VLMs' weights unchanged. Tested across LLaVA models, CLIP-UP achieves state-of-the-art results on the MM-UPD benchmark for assessing unanswerability in multiple-choice VQA, while preserving the original performance on other tasks.	 | 近期的视觉-语言模型（VLMs）在视觉理解和推理方面展现出了显著的能力，尤其是在多项选择的视觉问答（VQA）方面。然而，这些模型仍然可能会犯出乎自然的错误，例如，对于那些无法回答的问题，例如询问图像中不存在的物体的问题，提供错误的答案。为了解决这一问题，我们提出了CLIP-UP：基于CLIP的无法回答问题检测，这是一种新颖的轻量级方法，用于使VLMs具备回答无法回答的问题的能力。通过利用CLIP提取问题-图像对齐信息，CLIP-UP只需要对少量额外的层进行高效的训练，而无需改变原始VLMs的权重。在LLaVA模型上测试时，CLIP-UP在MM-UPD基准测试中达到了评估多项选择VQA中的无法回答性的最新成果，同时保持了在其他任务上的原始性能。	 | CLIP-UP是一种轻量级方法，利用CLIP模型检测视觉问答中无法回答的问题，仅通过少量额外训练层即可增强VLMs的性能，在保持其他任务性能的同时，实现了无法回答性检测的最新成果。
2501.01368	 | Test-time Controllable Image Generation by Explicit Spatial Constraint Enforcement	 | Z. Zhang,B. Liu,J. Bao,L. Chen,S. Zhu,J. Yu	 | Recent text-to-image generation favors various forms of spatial conditions, e.g., masks, bounding boxes, and key points. However, the majority of the prior art requires form-specific annotations to fine-tune the original model, leading to poor test-time generalizability. Meanwhile, existing training-free methods work well only with simplified prompts and spatial conditions. In this work, we propose a novel yet generic test-time controllable generation method that aims at natural text prompts and complex conditions. Specifically, we decouple spatial conditions into semantic and geometric conditions and then enforce their consistency during the image-generation process individually. As for the former, we target bridging the gap between the semantic condition and text prompts, as well as the gap between such condition and the attention map from diffusion models. To achieve this, we propose to first complete the prompt w.r.t. semantic condition, and then remove the negative impact of distracting prompt words by measuring their statistics in attention maps as well as distances in word space w.r.t. this condition. To further cope with the complex geometric conditions, we introduce a geometric transform module, in which Region-of-Interests will be identified in attention maps and further used to translate category-wise latents w.r.t. geometric condition. More importantly, we propose a diffusion-based latents-refill method to explicitly remove the impact of latents at the RoI, reducing the artifacts on generated images. Experiments on Coco-stuff dataset showcase 30$\%$ relative boost compared to SOTA training-free methods on layout consistency evaluation metrics.	 | 近年来，文本到图像生成倾向于各种形式的空间条件，例如掩码、边界框和关键点。然而，大多数现有技术要求特定形式的注释才能微调原始模型，这导致了较差的测试时泛化能力。同时，现有的无需训练的方法仅在简化提示和空间条件方面表现良好。在本工作中，我们提出了一种新颖且通用的测试时可控生成方法，旨在处理自然文本提示和复杂条件。具体而言，我们将空间条件分解为语义条件和几何条件，并在图像生成过程中分别确保它们的一致性。对于前者，我们旨在弥合语义条件与文本提示之间的差距，以及这种条件与扩散模型的注意力图之间的差距。为此，我们提出首先根据语义条件补充提示，然后通过测量这些提示词在注意力图中的统计特征和词空间中的距离来消除它们带来的负面影响。为了进一步应对复杂的几何条件，我们引入了一个几何变换模块，在该模块中，将在注意力图中识别区域，并进一步用于根据几何条件翻译类别级别的潜在变量。更重要的是，我们提出了一个基于扩散的潜在变量填充方法，以明确地消除RoI处潜在变量的影响，从而减少生成图像中的伪影。在Coco-stuff数据集上的实验表明，与最佳训练前方法相比，该方法在布局一致性评估指标上相对提高了30%。	 | 本文提出了一种新的测试时可控生成方法，该方法能够处理自然文本提示和复杂空间条件，通过分解空间条件为语义和几何条件，并分别确保它们的一致性，特别是在处理类别级别的潜在变量和减少生成图像中的伪影方面。实验结果表明，该方法在Coco-stuff数据集上的布局一致性评估指标上相比最佳训练前方法提高了30%。
2501.01366	 | ViGiL3D: A Linguistically Diverse Dataset for 3D Visual Grounding	 | Austin T. Wang,ZeMing Gong,Angel X. Chang	 | 3D visual grounding (3DVG) involves localizing entities in a 3D scene referred to by natural language text. Such models are useful for embodied AI and scene retrieval applications, which involve searching for objects or patterns using natural language descriptions. While recent works have focused on LLM-based scaling of 3DVG datasets, these datasets do not capture the full range of potential prompts which could be specified in the English language. To ensure that we are scaling up and testing against a useful and representative set of prompts, we propose a framework for linguistically analyzing 3DVG prompts and introduce Visual Grounding with Diverse Language in 3D (ViGiL3D), a diagnostic dataset for evaluating visual grounding methods against a diverse set of language patterns. We evaluate existing open-vocabulary 3DVG methods to demonstrate that these methods are not yet proficient in understanding and identifying the targets of more challenging, out-of-distribution prompts, toward real-world applications.	 | 3D视觉定位（3DVG）涉及将自然语言文本所指代的实体在3D场景中进行定位。这种模型对于具有物理代理的AI和场景检索应用非常有用，这些应用需要使用自然语言描述来搜索对象或模式。虽然近期的研究主要集中在基于LLM的3DVG数据集的扩展上，但这些数据集未能捕捉到英语中可能出现的全部提示范围。为了确保我们扩展和测试的提示集是有用且具有代表性的，我们提出了一种语言分析3DVG提示的框架，并引入了Visual Grounding with Diverse Language in 3D（ViGiL3D），这是一个用于评估视觉定位方法的诊断数据集，该数据集涵盖了多种语言模式。我们评估现有的开放词汇3DVG方法，以证明这些方法尚未熟练理解并识别更具挑战性和分布外提示的目标，从而应用于现实世界的应用中。	 | 该研究提出了一个分析3D视觉定位提示的语言框架，并构建了ViGiL3D数据集，旨在评估现有方法在处理多样化和更具挑战性语言模式方面的有效性。通过该数据集，研究发现当前的开放词汇3D视觉定位方法在处理分布外和更具挑战性的提示时表现不足。
2501.01346	 | Large Vision-Language Model Alignment and Misalignment: A Survey Through the Lens of Explainability	 | Dong Shu,Haiyan Zhao,Jingyu Hu,Weiru Liu,Lu Cheng,Mengnan Du	 | Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in processing both visual and textual information. However, the critical challenge of alignment between visual and linguistic representations is not fully understood. This survey presents a comprehensive examination of alignment and misalignment in LVLMs through an explainability lens. We first examine the fundamentals of alignment, exploring its representational and behavioral aspects, training methodologies, and theoretical foundations. We then analyze misalignment phenomena across three semantic levels: object, attribute, and relational misalignment. Our investigation reveals that misalignment emerges from challenges at multiple levels: the data level, the model level, and the inference level. We provide a comprehensive review of existing mitigation strategies, categorizing them into parameter-frozen and parameter-tuning approaches. Finally, we outline promising future research directions, emphasizing the need for standardized evaluation protocols and in-depth explainability studies.	 | 大型视觉-语言模型（LVLMs）在处理视觉和文本信息方面展现了非凡的能力。然而，视觉表征与语言表征之间的对齐关键挑战尚未完全理解。本文综述通过解释性视角对LVLMs中的对齐与错配进行了全面的考察。我们首先探讨了对齐的基本原理，包括其表征和行为方面的内容、训练方法以及理论基础。然后，我们分析了在三个语义层面上的错配现象：对象错配、属性错配和关系错配。我们的研究揭示了错配源自多个层面的挑战：数据层面、模型层面和推理层面。我们全面回顾了现有的缓解策略，将其分类为参数冻结方法和参数调优方法。最后，我们概述了未来研究的有希望的方向，强调需要标准化的评估协议和深入的解释性研究的重要性。	 | 本文通过解释性视角全面考察了大型视觉-语言模型（LVLMs）中视觉表征与语言表征的对齐与错配问题，分析了错配现象在对象、属性和关系三个语义层面上的来源，并回顾了现有的缓解策略，最后指出了未来研究的方向。
2501.01320	 | SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration	 | Jianyi Wang,Zhijie Lin,Meng Wei,Yang Zhao,Ceyuan Yang,Chen Change Loy,Lu Jiang	 | Video restoration poses non-trivial challenges in maintaining fidelity while recovering temporally consistent details from unknown degradations in the wild. Despite recent advances in diffusion-based restoration, these methods often face limitations in generation capability and sampling efficiency. In this work, we present SeedVR, a diffusion transformer designed to handle real-world video restoration with arbitrary length and resolution. The core design of SeedVR lies in the shifted window attention that facilitates effective restoration on long video sequences. SeedVR further supports variable-sized windows near the boundary of both spatial and temporal dimensions, overcoming the resolution constraints of traditional window attention. Equipped with contemporary practices, including causal video autoencoder, mixed image and video training, and progressive training, SeedVR achieves highly-competitive performance on both synthetic and real-world benchmarks, as well as AI-generated videos. Extensive experiments demonstrate SeedVR's superiority over existing methods for generic video restoration.	 | 视频恢复在保持保真度的同时，从野生环境中未知的退化中恢复时间上一致的细节，面临着非平凡的挑战。尽管在基于扩散的方法方面取得了最近的进展，这些方法仍常常在生成能力和采样效率方面面临限制。在这项工作中，我们提出了SeedVR，一种专门设计用于处理任意长度和分辨率的真实世界视频恢复的扩散变换器。SeedVR的核心设计在于移位窗口注意力机制，这使得在长视频序列上实现有效的恢复成为可能。SeedVR还支持在空间和时间维度边界附近可变大小的窗口，克服了传统窗口注意力机制的分辨率限制。借助当代实践，包括因果视频自编码器、混合图像和视频训练以及渐进训练，SeedVR在合成和真实世界基准以及AI生成视频上都实现了高度竞争力的性能。广泛的实验表明，SeedVR在通用视频恢复方面优于现有方法。	 | SeedVR 是一种专门设计用于处理任意长度和分辨率的真实世界视频恢复的扩散变换器，通过移位窗口注意力机制和可变大小的窗口支持，在长视频序列上实现有效的恢复，并在合成和真实世界基准上表现出色，优于现有方法。
2501.01311	 | Multi-Head Explainer: A General Framework to Improve Explainability in CNNs and Transformers	 | Bohang Sun,Pietro Liò	 | In this study, we introduce the Multi-Head Explainer (MHEX), a versatile and modular framework that enhances both the explainability and accuracy of Convolutional Neural Networks (CNNs) and Transformer-based models. MHEX consists of three core components: an Attention Gate that dynamically highlights task-relevant features, Deep Supervision that guides early layers to capture fine-grained details pertinent to the target class, and an Equivalent Matrix that unifies refined local and global representations to generate comprehensive saliency maps. Our approach demonstrates superior compatibility, enabling effortless integration into existing residual networks like ResNet and Transformer architectures such as BERT with minimal modifications. Extensive experiments on benchmark datasets in medical imaging and text classification show that MHEX not only improves classification accuracy but also produces highly interpretable and detailed saliency scores.	 | 在本研究中，我们提出了一个多功能且模块化的框架——多头解释器（MHEX），该框架旨在提高卷积神经网络（CNNs）和基于变换器的模型的可解释性和准确性。MHEX 包含三个核心组件：注意力门控（Attention Gate），它能够动态地突出显示与任务相关的特征；深度监督（Deep Supervision），它引导早期层捕获与目标类相关的细微特征；以及等效矩阵（Equivalent Matrix），它统一了细化的局部和全局表示，以生成全面的显著性图。我们的方法展示了卓越的兼容性，能够轻松地无缝集成到现有的残差网络（如ResNet）和变换器架构（如BERT）中，只需进行少量修改。在医学影像和文本分类基准数据集上的广泛实验表明，MHEX 不仅提高了分类准确率，还产生了高度可解释和详细的显著性评分。	 | 该研究提出了一种名为多头解释器（MHEX）的模块化框架，旨在提升卷积神经网络和基于变换器模型的解释性和准确性，通过注意力门控、深度监督和等效矩阵三个核心组件实现；实验结果表明，MHEX 能显著提高分类准确率并生成高度可解释的显著性评分。
2501.01275	 | HybridTrack: A Hybrid Approach for Robust Multi-Object Tracking	 | Leandro Di Bella,Yangxintong Lyu,Bruno Cornelis,Adrian Munteanu	 | The evolution of Advanced Driver Assistance Systems (ADAS) has increased the need for robust and generalizable algorithms for multi-object tracking. Traditional statistical model-based tracking methods rely on predefined motion models and assumptions about system noise distributions. Although computationally efficient, they often lack adaptability to varying traffic scenarios and require extensive manual design and parameter tuning. To address these issues, we propose a novel 3D multi-object tracking approach for vehicles, HybridTrack, which integrates a data-driven Kalman Filter (KF) within a tracking-by-detection paradigm. In particular, it learns the transition residual and Kalman gain directly from data, which eliminates the need for manual motion and stochastic parameter modeling. Validated on the real-world KITTI dataset, HybridTrack achieves 82.08% HOTA accuracy, significantly outperforming state-of-the-art methods. We also evaluate our method under different configurations, achieving the fastest processing speed of 112 FPS. Consequently, HybridTrack eliminates the dependency on scene-specific designs while improving performance and maintaining real-time efficiency. The code will be publicly available at the time of publishing: https://github.com/leandro-svg/HybridTrack.git.	 | 高级驾驶辅助系统（ADAS）的演进增加了对多目标跟踪算法的稳健性和泛用性的需求。传统的基于统计模型的跟踪方法依赖于预定义的运动模型和关于系统噪声分布的假设。尽管计算效率高，但它们通常缺乏对不同交通场景的适应性，并且需要大量的手动设计和参数调整。为了解决这些问题，我们提出了一种新的车辆3D多目标跟踪方法，HybridTrack，它在跟踪-检测范式中结合了数据驱动的卡尔曼滤波器（KF）。具体而言，它直接从数据中学习转移残差和卡尔曼增益，从而消除了手动运动和统计参数建模的需要。在实际的KITTI数据集上验证，HybridTrack在HOTA准确性方面达到了82.08%，显著优于现有最先进的方法。我们还评估了该方法在不同配置下的表现，实现了最快的处理速度112 FPS。因此，HybridTrack消除了对特定场景设计的依赖，同时提高了性能并保持了实时效率。代码将在发布时公开：https://github.com/leandro-svg/HybridTrack.git。	 | HybridTrack 是一种新的车辆3D多目标跟踪方法，它结合了数据驱动的卡尔曼滤波器来直接从数据中学习转移残差和卡尔曼增益，从而在KITTI数据集上实现了82.08%的HOTA准确性，并保持了实时效率。
2501.01262	 | Detail Matters: Mamba-Inspired Joint Unfolding Network for Snapshot Spectral Compressive Imaging	 | Mengjie Qin,Yuchao Feng,Zongliang Wu,Yulun Zhang,Xin Yuan	 | In the coded aperture snapshot spectral imaging system, Deep Unfolding Networks (DUNs) have made impressive progress in recovering 3D hyperspectral images (HSIs) from a single 2D measurement. However, the inherent nonlinear and ill-posed characteristics of HSI reconstruction still pose challenges to existing methods in terms of accuracy and stability. To address this issue, we propose a Mamba-inspired Joint Unfolding Network (MiJUN), which integrates physics-embedded DUNs with learning-based HSI imaging. Firstly, leveraging the concept of trapezoid discretization to expand the representation space of unfolding networks, we introduce an accelerated unfolding network scheme. This approach can be interpreted as a generalized accelerated half-quadratic splitting with a second-order differential equation, which reduces the reliance on initial optimization stages and addresses challenges related to long-range interactions. Crucially, within the Mamba framework, we restructure the Mamba-inspired global-to-local attention mechanism by incorporating a selective state space model and an attention mechanism. This effectively reinterprets Mamba as a variant of the Transformer} architecture, improving its adaptability and efficiency. Furthermore, we refine the scanning strategy with Mamba by integrating the tensor mode-$k$ unfolding into the Mamba network. This approach emphasizes the low-rank properties of tensors along various modes, while conveniently facilitating 12 scanning directions. Numerical and visual comparisons on both simulation and real datasets demonstrate the superiority of our proposed MiJUN, and achieving overwhelming detail representation.	 | 在编码孔径快照光谱成像系统中，深度展开网络（DUNs）已经在从单个二维测量中恢复三维超光谱图像（HSIs）方面取得了显著进展。然而，超光谱图像（HSI）重建固有的非线性和病态特性仍然对现有方法在准确性和稳定性方面的表现构成了挑战。为了解决这一问题，我们提出了一种受Mamba启发的联合展开网络（MiJUN），将物理嵌入的DUNs与基于学习的HSI成像相结合。首先，利用梯形离散化概念扩展展开网络的表示空间，引入了加速展开网络方案。这种方法可以被解释为广义加速二次分裂式，结合了二阶微分方程，从而减少了对初始优化阶段的依赖，并解决了远程相互作用带来的挑战。关键的是，在Mamba框架中，我们通过引入选择性的状态空间模型和注意力机制来重新构架Mamba启发的全局到局部注意力机制。这有效地将Mamba重新解释为Transformer架构的一种变体，从而提高了其适应性和效率。此外，我们通过将张量模式-$k$ 展开整合到Mamba网络中，进一步改进了扫描策略。这种方法强调了在各种模式下张量的低秩性质，同时方便地提供了12个扫描方向。在仿真和真实数据集上的数值和视觉比较表明，我们提出的MiJUN在细节表示方面具有明显优势。	 | 本文提出了一种名为MiJUN的方法，结合了物理嵌入的深度展开网络和基于学习的HSI成像，通过引入加速展开网络方案、选择性的状态空间模型、注意力机制和张量模式-$k$展开，有效解决了超光谱图像重建中的非线性和病态特性问题，提高了重建质量和稳定性。
2501.01245	 | SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization	 | Yongle Huang,Haodong Chen,Zhenbang Xu,Zihan Jia,Haozhou Sun,Dian Shao	 | Human action understanding is crucial for the advancement of multimodal systems. While recent developments, driven by powerful large language models (LLMs), aim to be general enough to cover a wide range of categories, they often overlook the need for more specific capabilities. In this work, we address the more challenging task of Fine-grained Action Recognition (FAR), which focuses on detailed semantic labels within shorter temporal duration (e.g., "salto backward tucked with 1 turn"). Given the high costs of annotating fine-grained labels and the substantial data needed for fine-tuning LLMs, we propose to adopt semi-supervised learning (SSL). Our framework, SeFAR, incorporates several innovative designs to tackle these challenges. Specifically, to capture sufficient visual details, we construct Dual-level temporal elements as more effective representations, based on which we design a new strong augmentation strategy for the Teacher-Student learning paradigm through involving moderate temporal perturbation. Furthermore, to handle the high uncertainty within the teacher model's predictions for FAR, we propose the Adaptive Regulation to stabilize the learning process. Experiments show that SeFAR achieves state-of-the-art performance on two FAR datasets, FineGym and FineDiving, across various data scopes. It also outperforms other semi-supervised methods on two classical coarse-grained datasets, UCF101 and HMDB51. Further analysis and ablation studies validate the effectiveness of our designs. Additionally, we show that the features extracted by our SeFAR could largely promote the ability of multimodal foundation models to understand fine-grained and domain-specific semantics.	 | 人类行为理解对于多模态系统的进步至关重要。尽管最近的发展，受到强大大型语言模型（LLMs）的推动，旨在实现覆盖面广泛的能力，但它们往往忽略了更具体能力的必要性。在此项工作中，我们解决了一个更具挑战性的任务，即细粒度动作识别（FAR），该任务关注较短时间内的详细语义标签（例如，“向后翻腾一周转体”）。由于细粒度标签的注释成本高昂且需要大量的数据进行LLM的微调，我们提出采用半监督学习（SSL）方法。我们的框架SeFAR结合了多种创新设计以应对这些挑战。具体而言，为了捕捉足够的视觉细节，我们构建了双层次时间元素作为更有效的表示，并在此基础上设计了一种新的强增强策略，通过引入适度的时间扰动来实现教师-学生学习范式。此外，为了处理教师模型预测中的高不确定性，我们提出了自适应调节来稳定学习过程。实验结果显示，SeFAR在两个FAR数据集FineGym和FineDiving中取得了最先进的性能，在各种数据规模下均表现优异。同时，SeFAR在两个经典的粗粒度数据集UCF101和HMDB51上也超越了其他半监督方法。进一步的分析和消融研究验证了我们设计的有效性。此外，我们展示了由SeFAR提取的特征能够显著增强多模态基础模型理解和细粒度及领域特定语义的能力。	 | 该研究提出了SeFAR框架，通过半监督学习方法解决细粒度动作识别任务，结合双层次时间元素表示和自适应调节等创新设计，显著提高了在FineGym和FineDiving数据集上的性能，并在UCF101和HMDB51等经典粗粒度数据集上也超越了其他半监督方法。
2501.01243	 | Face-Human-Bench: A Comprehensive Benchmark of Face and Human Understanding for Multi-modal Assistants	 | Lixiong Qin,Shilong Ou,Miaoxuan Zhang,Jiangning Wei,Yuhang Zhang,Xiaoshuai Song,Yuchen Liu,Mei Wang,Weiran Xu	 | Faces and humans are crucial elements in social interaction and are widely included in everyday photos and videos. Therefore, a deep understanding of faces and humans will enable multi-modal assistants to achieve improved response quality and broadened application scope. Currently, the multi-modal assistant community lacks a comprehensive and scientific evaluation of face and human understanding abilities. In this paper, we first propose a hierarchical ability taxonomy that includes three levels of abilities. Then, based on this taxonomy, we collect images and annotations from publicly available datasets in the face and human community and build a semi-automatic data pipeline to produce problems for the new benchmark. Finally, the obtained Face-Human-Bench comprises a development set with 900 problems and a test set with 1800 problems, supporting both English and Chinese. We conduct evaluations over 25 mainstream multi-modal large language models (MLLMs) with our Face-Human-Bench, focusing on the correlation between abilities, the impact of the relative position of targets on performance, and the impact of Chain of Thought (CoT) prompting on performance. Moreover, inspired by multi-modal agents, we also explore which abilities of MLLMs need to be supplemented by specialist models.	 | 面部和人类是社交互动中的关键元素，广泛出现在日常生活中的照片和视频中。因此，对面部和人类的深入理解将使多模态助手能够提高响应质量并拓宽应用范围。目前，多模态助手社区缺乏对面部和人类理解能力的全面和科学评估。在这篇论文中，我们首先提出了一种层次能力分类体系，包括三个能力层次。然后，基于此分类体系，我们从面部和人类社区的公开数据集中收集图像和注释，并构建了一种半自动数据管道，以生成新基准的问题。最后，所获得的Face-Human-Bench包含一个由900个问题组成的发展集和一个由1800个问题组成的标准测试集，同时支持英文和中文。我们使用Face-Human-Bench对25种主流多模态大型语言模型（MLLMs）进行了评估，重点关注能力之间的相关性、目标相对位置对性能的影响以及Chain of Thought（CoT）提示对性能的影响。此外，受到多模态代理的启发，我们还探讨了哪些MLLM的能力需要通过专业模型进行补充。	 | 本文提出了一种新的评估框架Face-Human-Bench，包含900个英文和1800个中文问题，用于评估多模态大型语言模型在理解面部和人类方面的能力，并探讨了不同模型能力之间的相关性和Chain of Thought提示的影响。
2501.01240	 | Asymmetric Reinforcing against Multi-modal Representation Bias	 | Xiyuan Gao,Bing Cao,Pengfei Zhu,Nannan Wang,Qinghua Hu	 | The strength of multimodal learning lies in its ability to integrate information from various sources, providing rich and comprehensive insights. However, in real-world scenarios, multi-modal systems often face the challenge of dynamic modality contributions, the dominance of different modalities may change with the environments, leading to suboptimal performance in multimodal learning. Current methods mainly enhance weak modalities to balance multimodal representation bias, which inevitably optimizes from a partialmodality perspective, easily leading to performance descending for dominant modalities. To address this problem, we propose an Asymmetric Reinforcing method against Multimodal representation bias (ARM). Our ARM dynamically reinforces the weak modalities while maintaining the ability to represent dominant modalities through conditional mutual information. Moreover, we provide an in-depth analysis that optimizing certain modalities could cause information loss and prevent leveraging the full advantages of multimodal data. By exploring the dominance and narrowing the contribution gaps between modalities, we have significantly improved the performance of multimodal learning, making notable progress in mitigating imbalanced multimodal learning.	 | 多模态学习的优势在于其能够整合多种来源的信息，提供丰富且全面的见解。然而，在实际场景中，多模态系统常常面临动态模态贡献的挑战，不同模态的主导地位可能会随着环境的变化而改变，导致多模态学习性能不佳。当前的方法主要通过增强较弱模态来平衡多模态表示偏见，这不可避免地是从部分模态的角度进行优化，容易导致主导模态的性能下降。为此，我们提出了一种对抗多模态表示偏见的非对称强化方法（Asymmetric Reinforcing method against Multimodal representation bias，ARM）。ARM方法动态增强较弱模态，同时通过条件互信息保持对主导模态的表示能力。此外，我们深入分析了优化某些模态可能导致信息损失，以及妨碍充分利用多模态数据的全部优势。通过探索模态的主导地位并缩小模态贡献的差距，我们显著提升了多模态学习的性能，取得了在缓解不平衡多模态学习方面的重大进展。	 | 该研究提出了一种对抗多模态表示偏见的非对称强化方法（ARM），该方法动态增强较弱模态同时保持对主导模态的表示能力，从而显著提升了多模态学习的性能。
2501.01238	 | EHCTNet: Enhanced Hybrid of CNN and Transformer Network for Remote Sensing Image Change Detection	 | Junjie Yang,Haibo Wan,Zhihai Shang	 | Remote sensing (RS) change detection incurs a high cost because of false negatives, which are more costly than false positives. Existing frameworks, struggling to improve the Precision metric to reduce the cost of false positive, still have limitations in focusing on the change of interest, which leads to missed detections and discontinuity issues. This work tackles these issues by enhancing feature learning capabilities and integrating the frequency components of feature information, with a strategy to incrementally boost the Recall value. We propose an enhanced hybrid of CNN and Transformer network (EHCTNet) for effectively mining the change information of interest. Firstly, a dual branch feature extraction module is used to extract the multi scale features of RS images. Secondly, the frequency component of these features is exploited by a refined module I. Thirdly, an enhanced token mining module based on the Kolmogorov Arnold Network is utilized to derive semantic information. Finally, the semantic change information's frequency component, beneficial for final detection, is mined from the refined module II. Extensive experiments validate the effectiveness of EHCTNet in comprehending complex changes of interest. The visualization outcomes show that EHCTNet detects more intact and continuous changed areas and perceives more accurate neighboring distinction than state of the art models.	 | 遥感（RS）变化检测的成本高昂，主要是由于假阴性问题，假阴性比假阳性更为昂贵。现有框架尽管努力通过提高精确度指标来降低假阳性的成本，但在关注目标变化方面仍然存在局限性，导致检测遗漏和连续性问题。本研究通过增强特征学习能力和整合特征信息的频率成分，采用逐步提升召回率值的策略来解决这些问题。我们提出了一种增强的卷积神经网络和变换器网络混合模型（EHCTNet），以有效挖掘感兴趣的变化信息。首先，使用双分支特征提取模块提取RS图像的多尺度特征。其次，通过精细模块I利用这些特征的频率成分。第三，利用柯尔莫哥洛夫-阿诺德网络（KAN）增强的标记挖掘模块提取语义信息。最后，从精细模块II中挖掘语义变化信息的频率成分，这对最终检测有益。广泛实验验证了EHCTNet在理解和检测复杂变化方面的有效性。可视化结果表明，EHCTNet能够检测出更完整和连续的变化区域，并且能够更准确地区分邻近区域，优于现有模型。	 | 本研究提出了一种增强的卷积神经网络和变换器网络混合模型（EHCTNet），通过提升特征学习能力、整合频率成分信息并逐步提升召回率值，有效解决了遥感变化检测中的假阴性问题，特别是在复杂变化场景下的检测和连续性方面表现优异。
2501.01235	 | SVFR: A Unified Framework for Generalized Video Face Restoration	 | Zhiyao Wang,Xu Chen,Chengming Xu,Junwei Zhu,Xiaobin Hu,Jiangning Zhang,Chengjie Wang,Yuqi Liu,Yiyi Zhou,Rongrong Ji	 | Face Restoration (FR) is a crucial area within image and video processing, focusing on reconstructing high-quality portraits from degraded inputs. Despite advancements in image FR, video FR remains relatively under-explored, primarily due to challenges related to temporal consistency, motion artifacts, and the limited availability of high-quality video data. Moreover, traditional face restoration typically prioritizes enhancing resolution and may not give as much consideration to related tasks such as facial colorization and inpainting. In this paper, we propose a novel approach for the Generalized Video Face Restoration (GVFR) task, which integrates video BFR, inpainting, and colorization tasks that we empirically show to benefit each other. We present a unified framework, termed as stable video face restoration (SVFR), which leverages the generative and motion priors of Stable Video Diffusion (SVD) and incorporates task-specific information through a unified face restoration framework. A learnable task embedding is introduced to enhance task identification. Meanwhile, a novel Unified Latent Regularization (ULR) is employed to encourage the shared feature representation learning among different subtasks. To further enhance the restoration quality and temporal stability, we introduce the facial prior learning and the self-referred refinement as auxiliary strategies used for both training and inference. The proposed framework effectively combines the complementary strengths of these tasks, enhancing temporal coherence and achieving superior restoration quality. This work advances the state-of-the-art in video FR and establishes a new paradigm for generalized video face restoration.	 | 面部恢复（FR）是图像和视频处理中的一个重要领域，专注于从降级输入中重建高质量的人物肖像。尽管在图像FR方面取得了进展，但在视频FR方面仍然相对未被充分探索，这主要是由于时间一致性、运动伪影以及高质量视频数据的有限可用性等方面面临的挑战。此外，传统的面部恢复通常侧重于提升分辨率，可能不会太多考虑诸如面部着色和修复等关联任务。在本文中，我们提出了一种新颖的方法，用于通用视频面部恢复（GVFR）任务，该方法整合了视频BFR、修复和着色任务，并通过实验证明这些任务可以相互受益。我们提出了一种统一框架，称为稳定视频面部恢复（SVFR），该框架利用稳定视频扩散（SVD）的生成先验和运动先验，并通过统一的面部恢复框架整合任务特定的信息。引入了可学习的任务嵌入以增强任务识别。同时，采用了一种新颖的统一潜在正则化（ULR）方法，以促进不同子任务之间的共享特征表示学习。为进一步提高恢复质量和时间稳定性，我们引入了面部先验学习和自我参照精炼作为辅助策略，用于训练和推理。所提出的框架有效地结合了这些任务的互补优势，提高了时间连贯性，并实现了卓越的恢复质量。这项工作在视频FR方面推动了最先进的技术水平，并建立了通用视频面部恢复的新范式。	 | 本文提出了一种新颖的稳定视频面部恢复（SVFR）框架，该框架整合了视频超分辨率、修复和着色任务，并通过统一框架和先进策略提高了视频面部恢复的质量和时间连贯性，从而推动了视频面部恢复技术的发展。
2501.01231	 | Exploiting Latent Properties to Optimize Neural Codecs	 | Muhammet Balcilar,Bharath Bhushan Damodaran,Karam Naser,Franck Galpin,Pierre Hellier	 | End-to-end image and video codecs are becoming increasingly competitive, compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques, such as their straightforward adaptation to perceptual distortion metrics and high performance in specific fields thanks to their learning ability. However, current state-of-the-art neural codecs do not fully exploit the benefits of vector quantization and the existence of the entropy gradient in decoding devices. In this paper, we propose to leverage these two properties (vector quantization and entropy gradient) to improve the performance of off-the-shelf codecs. Firstly, we demonstrate that using non-uniform scalar quantization cannot improve performance over uniform quantization. We thus suggest using predefined optimal uniform vector quantization to improve performance. Secondly, we show that the entropy gradient, available at the decoder, is correlated with the reconstruction error gradient, which is not available at the decoder. We therefore use the former as a proxy to enhance compression performance. Our experimental results show that these approaches save between 1 to 3% of the rate for the same quality across various pretrained methods. In addition, the entropy gradient based solution improves traditional codec performance significantly as well.	 | 端到端的图像和视频编解码器在与传统压缩技术的竞争中越来越具有竞争力，后者经过了几十年的手工工程开发。这些可训练的编解码器相对于传统技术有许多优势，例如能够直接适应感知失真度量，并且由于其学习能力在特定领域表现出色。然而，当前最先进的神经编解码器并未充分利用向量量化的优势和解码器中存在的熵梯度。在本文中，我们提出利用这两个特性（向量量化和熵梯度）来改进现成编解码器的性能。首先，我们证明使用非均匀标量量化无法在性能上优于均匀量化。因此，我们建议使用预定义的最佳均匀向量量化以提高性能。其次，我们显示解码器中存在的熵梯度与重建误差梯度相关，而后者在解码器不可用。因此，我们使用前者作为代理以提升压缩性能。我们的实验结果表明，这些方法在各种预训练方法中节省了1%到3%的比特率，且在相同质量下。此外，基于熵梯度的方法还显著提升了传统编解码器的性能。	 | 本文提出了一种结合向量量化和解码器中存在的熵梯度来改进神经编解码器性能的方法，结果显示这些方法能够显著节省1%到3%的比特率，并且基于熵梯度的方法还显著提升了传统编解码器的性能。
2501.01223	 | Conditional Consistency Guided Image Translation and Enhancement	 | A. V. Subramanyam,Amil Bhagat,Milind Jain	 | Consistency models have emerged as a promising alternative to diffusion models, offering high-quality generative capabilities through single-step sample generation. However, their application to multi-domain image translation tasks, such as cross-modal translation and low-light image enhancement remains largely unexplored. In this paper, we introduce Conditional Consistency Models (CCMs) for multi-domain image translation by incorporating additional conditional inputs. We implement these modifications by introducing task-specific conditional inputs that guide the denoising process, ensuring that the generated outputs retain structural and contextual information from the corresponding input domain. We evaluate CCMs on 10 different datasets demonstrating their effectiveness in producing high-quality translated images across multiple domains. Code is available at https://github.com/amilbhagat/Conditional-Consistency-Models.	 | 一致性模型已经成为了扩散模型的一种有前途的替代方案，通过单步样本生成提供高质量的生成能力。然而，它们在多领域图像翻译任务中的应用，如跨模态翻译和低光照图像增强，仍然鲜有研究。本文中，我们通过引入附加的条件输入，引入条件一致性模型（CCMs）以实现多领域图像翻译。我们通过引入特定任务的条件输入来实施这些修改，这些条件输入指导去噪过程，确保生成的输出保留了相应输入域的结构和上下文信息。我们在10个不同的数据集上评估CCMs，展示了它们在多个领域生成高质量翻译图像的有效性。相关代码可在https://github.com/amilbhagat/Conditional-Consistency-Models获取。	 | 本文提出了条件一致性模型（CCMs），通过引入特定任务的条件输入来实现多领域图像翻译，从而在跨模态翻译和低光照图像增强等任务中生成高质量的翻译图像。实验结果展示了CCMs在多个数据集上的有效性。
2501.01212	 | Real-time Cross-modal Cybersickness Prediction in Virtual Reality	 | Yitong Zhu,Tangyao Li,Yuyang Wang	 | Cybersickness remains a significant barrier to the widespread adoption of immersive virtual reality (VR) experiences, as it can greatly disrupt user engagement and comfort. Research has shown that cybersickness can significantly be reflected in head and eye tracking data, along with other physiological data (e.g., TMP, EDA, and BMP). Despite the application of deep learning techniques such as CNNs and LSTMs, these models often struggle to capture the complex interactions between multiple data modalities and lack the capacity for real-time inference, limiting their practical application. Addressing this gap, we propose a lightweight model that leverages a transformer-based encoder with sparse self-attention to process bio-signal features and a PP-TSN network for video feature extraction. These features are then integrated via a cross-modal fusion module, creating a video-aware bio-signal representation that supports cybersickness prediction based on both visual and bio-signal inputs. Our model, trained with a lightweight framework, was validated on a public dataset containing eye and head tracking data, physiological data, and VR video, and demonstrated state-of-the-art performance in cybersickness prediction, achieving a high accuracy of 93.13\% using only VR video inputs. These findings suggest that our approach not only enables effective, real-time cybersickness prediction but also addresses the longstanding issue of modality interaction in VR environments. This advancement provides a foundation for future research on multimodal data integration in VR, potentially leading to more personalized, comfortable and widely accessible VR experiences.	 | 网络眩晕仍然是广泛采用沉浸式虚拟现实（VR）体验的一个重要障碍，因为它会严重影响用户的参与度和舒适度。研究表明，网络眩晕可以通过头部和眼部追踪数据以及其他生理数据（如体温、皮肤电导率和血压）显著反映出来。尽管应用了深度学习技术，如卷积神经网络（CNNs）和长短期记忆网络（LSTMs），但这些模型往往难以捕捉多种数据模态之间的复杂交互，并且缺乏实时推理的能力，限制了它们的实际应用。为解决这一问题，我们提出了一种轻量级模型，该模型结合了基于变压器的编码器与稀疏自我注意力机制来处理生物信号特征，并采用PP-TSN网络进行视频特征提取。然后，通过跨模态融合模块将这些特征融合，生成基于视觉和生物信号输入的视频感知生物信号表示，支持网络眩晕预测。我们的模型通过一个轻量级框架进行训练，并在包含眼动和头部追踪数据、生理数据和VR视频的公共数据集上进行了验证，仅使用VR视频输入就达到了93.13%的高准确率，展示了在网络眩晕预测方面的先进表现。这些发现表明，我们的方法不仅能够实现有效的实时网络眩晕预测，还能解决VR环境中长期存在的模态交互问题。这一进展为未来在VR中进行多模态数据整合的研究奠定了基础，可能有助于实现更加个性化、舒适和易于获取的VR体验。	 | 该研究提出了一种轻量级模型，结合基于变压器的编码器和稀疏自注意力机制处理生物信号，并通过PP-TSN网络提取视频特征，实现了高效准确的网络眩晕预测，验证结果显示其准确率达到93.13%，有效解决了多模态数据整合问题，提升了VR体验的舒适度和参与度。
2501.01197	 | LayeringDiff: Layered Image Synthesis via Generation, then Disassembly with Generative Knowledge	 | Kyoungkook Kang,Gyujin Sim,Geonung Kim,Donguk Kim,Seungho Nam,Sunghyun Cho	 | Layers have become indispensable tools for professional artists, allowing them to build a hierarchical structure that enables independent control over individual visual elements. In this paper, we propose LayeringDiff, a novel pipeline for the synthesis of layered images, which begins by generating a composite image using an off-the-shelf image generative model, followed by disassembling the image into its constituent foreground and background layers. By extracting layers from a composite image, rather than generating them from scratch, LayeringDiff bypasses the need for large-scale training to develop generative capabilities for individual layers. Furthermore, by utilizing a pretrained off-the-shelf generative model, our method can produce diverse contents and object scales in synthesized layers. For effective layer decomposition, we adapt a large-scale pretrained generative prior to estimate foreground and background layers. We also propose high-frequency alignment modules to refine the fine-details of the estimated layers. Our comprehensive experiments demonstrate that our approach effectively synthesizes layered images and supports various practical applications.	 | 分层已成为专业艺术家不可或缺的工具，它们能够构建层次结构，从而独立控制视觉元素。在本文中，我们提出了一种名为LayeringDiff的新颖管道，用于合成分层图像。该管道首先使用现成的图像生成模型生成合成图像，随后将其分解为前景和背景分层。通过从合成图像中提取分层，而不是从零开始生成它们，LayeringDiff避开了大规模训练以开发单个分层的生成能力的需要。此外，通过利用现成的预训练生成模型，我们的方法可以在合成分层中产生多样化的内容和对象尺度。为了有效进行分层分解，我们适应了一个大规模预训练生成先验模型，以估计前景和背景分层。我们还提出了高频对准模块以细化估计分层的细节。我们的综合实验表明，我们的方法能够有效合成分层图像并支持多种实际应用。	 | 本文提出了一种名为LayeringDiff的新管道，用于合成分层图像。该方法利用现成的生成模型生成合成图像并分解为前景和背景分层，避免了大规模训练，同时能够产生多样化的内容和对象尺度。
2501.01196	 | Sparis: Neural Implicit Surface Reconstruction of Indoor Scenes from Sparse Views	 | Yulun Wu,Han Huang,Wenyuan Zhang,Chao Deng,Ge Gao,Ming Gu,Yu-Shen Liu	 | In recent years, reconstructing indoor scene geometry from multi-view images has achieved encouraging accomplishments. Current methods incorporate monocular priors into neural implicit surface models to achieve high-quality reconstructions. However, these methods require hundreds of images for scene reconstruction. When only a limited number of views are available as input, the performance of monocular priors deteriorates due to scale ambiguity, leading to the collapse of the reconstructed scene geometry. In this paper, we propose a new method, named Sparis, for indoor surface reconstruction from sparse views. Specifically, we investigate the impact of monocular priors on sparse scene reconstruction, introducing a novel prior based on inter-image matching information. Our prior offers more accurate depth information while ensuring cross-view matching consistency. Additionally, we employ an angular filter strategy and an epipolar matching weight function, aiming to reduce errors due to view matching inaccuracies, thereby refining the inter-image prior for improved reconstruction accuracy. The experiments conducted on widely used benchmarks demonstrate superior performance in sparse-view scene reconstruction.	 | 近年来，从多视角图像中重建室内场景几何取得了令人鼓舞的进展。当前的方法将单目先验融入到神经隐式曲面模型中，以实现高质量的重建。然而，这些方法需要几百张图像来进行场景重建。当仅有有限数量的视图作为输入时，由于尺度模糊性，单目先验的表现会恶化，导致重建的场景几何结构崩溃。在本文中，我们提出了一种新的方法，称为Sparis，用于从稀疏视角重建室内表面。具体来说，我们研究了单目先验对稀疏场景重建的影响，并引入了一种基于图像间匹配信息的新先验。我们的先验提供了更准确的深度信息，同时确保跨视角匹配一致性。此外，我们采用角滤波策略和极线匹配权重函数，以减少视图匹配不准确性带来的误差，从而细化图像间先验以提高重建精度。在广泛使用的基准测试上进行的实验表明，该方法在稀疏视角场景重建中表现出色。	 | 本文提出了一种名为Sparis的新方法，用于从稀疏视角重建室内表面，该方法通过引入基于图像间匹配信息的新先验，并结合角滤波策略和极线匹配权重函数，提高了重建精度。实验结果表明，该方法在稀疏视角场景重建中表现优异。
2501.01184	 | Vulnerability-Aware Spatio-Temporal Learning for Generalizable and Interpretable Deepfake Video Detection	 | Dat Nguyen,Marcella Astrid,Anis Kacem,Enjie Ghorbel,Djamila Aouada	 | Detecting deepfake videos is highly challenging due to the complex intertwined spatial and temporal artifacts in forged sequences. Most recent approaches rely on binary classifiers trained on both real and fake data. However, such methods may struggle to focus on important artifacts, which can hinder their generalization capability. Additionally, these models often lack interpretability, making it difficult to understand how predictions are made. To address these issues, we propose FakeSTormer, offering two key contributions. First, we introduce a multi-task learning framework with additional spatial and temporal branches that enable the model to focus on subtle spatio-temporal artifacts. These branches also provide interpretability by highlighting video regions that may contain artifacts. Second, we propose a video-level data synthesis algorithm that generates pseudo-fake videos with subtle artifacts, providing the model with high-quality samples and ground truth data for our spatial and temporal branches. Extensive experiments on several challenging benchmarks demonstrate the competitiveness of our approach compared to recent state-of-the-art methods. The code is available at https://github.com/10Ring/FakeSTormer.	 | 检测深度伪造视频极具挑战性，因为伪造序列中存在复杂的空间和时间伪影。目前大多数方法依赖于在真实和伪造数据上训练的二元分类器。然而，这些方法可能会难以聚焦于重要的伪影，从而影响它们的泛化能力。此外，这些模型往往缺乏可解释性，使得理解预测过程变得困难。为解决这些问题，我们提出了一种名为FakeSTormer的方法，并做出了两项关键贡献。首先，我们引入了一种多任务学习框架，该框架包含空间和时间分支，使模型能够聚焦于微妙的空间和时间伪影。这些分支还通过高亮可能包含伪影的视频区域提供了可解释性。其次，我们提出了一种视频级别数据合成算法，该算法生成具有微妙伪影的伪伪造视频，为我们的空间和时间分支提供了高质量的样本和真实数据。在多个具有挑战性的基准测试上的广泛实验表明，我们的方法在与最近的先进方法相比时具有竞争力。代码可在https://github.com/10Ring/FakeSTormer 获取。	 | 该研究提出了名为FakeSTormer的方法，通过引入空间和时间分支的多任务学习框架和视频级别数据合成算法，专注于检测深度伪造视频中的微妙伪影，从而提高模型的泛化能力和可解释性。实验结果表明，该方法在多个基准测试中表现出竞争力。
2501.01174	 | L3D-Pose: Lifting Pose for 3D Avatars from a Single Camera in the Wild	 | Soumyaratna Debnath,Harish Katti,Shashikant Verma,Shanmuganathan Raman	 | While 2D pose estimation has advanced our ability to interpret body movements in animals and primates, it is limited by the lack of depth information, constraining its application range. 3D pose estimation provides a more comprehensive solution by incorporating spatial depth, yet creating extensive 3D pose datasets for animals is challenging due to their dynamic and unpredictable behaviours in natural settings. To address this, we propose a hybrid approach that utilizes rigged avatars and the pipeline to generate synthetic datasets to acquire the necessary 3D annotations for training. Our method introduces a simple attention-based MLP network for converting 2D poses to 3D, designed to be independent of the input image to ensure scalability for poses in natural environments. Additionally, we identify that existing anatomical keypoint detectors are insufficient for accurate pose retargeting onto arbitrary avatars. To overcome this, we present a lookup table based on a deep pose estimation method using a synthetic collection of diverse actions rigged avatars perform. Our experiments demonstrate the effectiveness and efficiency of this lookup table-based retargeting approach. Overall, we propose a comprehensive framework with systematically synthesized datasets for lifting poses from 2D to 3D and then utilize this to re-target motion from wild settings onto arbitrary avatars.	 | 尽管二维姿态估计已大幅提高了对动物和灵长类动物身体动作的解读能力，但它受限于缺乏深度信息，限制了其应用范围。三维姿态估计通过引入空间深度提供了更为全面的解决方案，但为动物创建广泛的三维姿态数据集却因它们在自然环境中的动态和不可预测行为而颇具挑战性。为解决这一问题，我们提出了一种混合方法，利用装备好的虚拟角色和流水线生成合成数据集，以获取训练所需的必要三维注释。我们的方法引入了一个基于注意力机制的简单MLP网络，用于将二维姿态转换为三维姿态，旨在使该网络独立于输入图像，以确保在自然环境中对姿态的可扩展性。此外，我们发现现有的解剖关键点检测器对于准确地将姿态重新映射到任意虚拟角色上是不够的。为此，我们提出了一种基于深度姿态估计方法的查找表，该方法使用了多种动作装备好的虚拟角色的合成集合。我们的实验表明，这种方法在查找表基础上的重新映射方法具有有效性与效率。总体而言，我们提出了一整套框架，利用系统合成的数据集将二维姿态提升到三维姿态，然后利用这一方法将野外动作重新映射到任意虚拟角色上。	 | 本文提出了一种混合方法，通过利用装备好的虚拟角色生成合成三维姿态数据集，并使用基于注意力机制的MLP网络将二维姿态转换为三维姿态，从而克服了直接在自然环境中创建三维姿态数据集的挑战。该方法结合了一种新的查找表技术，提高了在任意虚拟角色上重新映射姿态的准确性和效率。
2501.01166	 | Deep Learning in Palmprint Recognition-A Comprehensive Survey	 | Chengrui Gao,Ziyuan Yang,Wei Jia,Lu Leng,Bob Zhang,Andrew Beng Jin Teoh	 | Palmprint recognition has emerged as a prominent biometric technology, widely applied in diverse scenarios. Traditional handcrafted methods for palmprint recognition often fall short in representation capability, as they heavily depend on researchers' prior knowledge. Deep learning (DL) has been introduced to address this limitation, leveraging its remarkable successes across various domains. While existing surveys focus narrowly on specific tasks within palmprint recognition-often grounded in traditional methodologies-there remains a significant gap in comprehensive research exploring DL-based approaches across all facets of palmprint recognition. This paper bridges that gap by thoroughly reviewing recent advancements in DL-powered palmprint recognition. The paper systematically examines progress across key tasks, including region-of-interest segmentation, feature extraction, and security/privacy-oriented challenges. Beyond highlighting these advancements, the paper identifies current challenges and uncovers promising opportunities for future research. By consolidating state-of-the-art progress, this review serves as a valuable resource for researchers, enabling them to stay abreast of cutting-edge technologies and drive innovation in palmprint recognition.	 | 指纹识别已成为一种突出的生物识别技术，在多种应用场景中被广泛应用。传统的指纹识别方法在表示能力方面往往不足，因为它们高度依赖研究人员的先验知识。深度学习（DL）被引入以解决这一局限性，利用其在各个领域的显著成功。虽然现有的综述通常专注于指纹识别中的特定任务——通常基于传统方法——但在基于DL的各个方面研究中仍然存在显著的研究空白。本文通过全面回顾DL在指纹识别中的最新进展，填补了这一空白。文章系统地考察了关键任务的进步，包括感兴趣区域分割、特征提取以及安全/隐私相关的挑战。除了突出这些进展外，文章还指出了当前面临的挑战，并发现了未来研究的有希望的机会。通过汇集最新的进展，本文的综述为研究人员提供了一种宝贵资源，使他们能够跟踪最新技术并推动指纹识别领域的创新。	 | 本文全面回顾了深度学习在指纹识别中的最新进展，涵盖了感兴趣区域分割、特征提取以及安全/隐私相关挑战，并指出了当前研究的挑战和未来的研究机会，为研究人员提供了一种宝贵资源。
2501.01164	 | Towards Interactive Deepfake Analysis	 | Lixiong Qin,Ning Jiang,Yang Zhang,Yuhan Qiu,Dingheng Zeng,Jiani Hu,Weihong Deng	 | Existing deepfake analysis methods are primarily based on discriminative models, which significantly limit their application scenarios. This paper aims to explore interactive deepfake analysis by performing instruction tuning on multi-modal large language models (MLLMs). This will face challenges such as the lack of datasets and benchmarks, and low training efficiency. To address these issues, we introduce (1) a GPT-assisted data construction process resulting in an instruction-following dataset called DFA-Instruct, (2) a benchmark named DFA-Bench, designed to comprehensively evaluate the capabilities of MLLMs in deepfake detection, deepfake classification, and artifact description, and (3) construct an interactive deepfake analysis system called DFA-GPT, as a strong baseline for the community, with the Low-Rank Adaptation (LoRA) module. The dataset and code will be made available at https://github.com/lxq1000/DFA-Instruct to facilitate further research.	 | 现有的深伪分析方法主要基于判别模型，这极大地限制了其应用场景。本文旨在通过指令调优多模态大语言模型（MLLMs）来进行交互式深伪分析。这将面临数据集和基准缺乏以及训练效率低下的挑战。为了解决这些问题，我们提出了以下几方面的工作：（1）一个由GPT辅助的数据构建过程，生成了一种遵循指令的数据集，称为DFA-Instruct；（2）一个名为DFA-Bench的基准，旨在全面评估MLLMs在深伪检测、分类和特征描述方面的能力；（3）构建了一个交互式深伪分析系统DFA-GPT，作为社区中的强基准，并包含了一个低秩适应（LoRA）模块。该数据集和代码将在https://github.com/lxq1000/DFA-Instruct上发布，以便促进进一步的研究。	 | 本文通过指令调优多模态大语言模型来实现交互式深伪分析，并提出了一种新的数据集DFA-Instruct和基准DFA-Bench，以解决现有方法的限制和挑战；同时构建了交互式深伪分析系统DFA-GPT，包含低秩适应模块，以促进进一步研究。
2501.01163	 | 3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer	 | Jiajun Deng,Tianyu He,Li Jiang,Tianyu Wang,Feras Dayoub,Ian Reid	 | Current 3D Large Multimodal Models (3D LMMs) have shown tremendous potential in 3D-vision-based dialogue and reasoning. However, how to further enhance 3D LMMs to achieve fine-grained scene understanding and facilitate flexible human-agent interaction remains a challenging problem. In this work, we introduce 3D-LLaVA, a simple yet highly powerful 3D LMM designed to act as an intelligent assistant in comprehending, reasoning, and interacting with the 3D world. Unlike existing top-performing methods that rely on complicated pipelines-such as offline multi-view feature extraction or additional task-specific heads-3D-LLaVA adopts a minimalist design with integrated architecture and only takes point clouds as input. At the core of 3D-LLaVA is a new Omni Superpoint Transformer (OST), which integrates three functionalities: (1) a visual feature selector that converts and selects visual tokens, (2) a visual prompt encoder that embeds interactive visual prompts into the visual token space, and (3) a referring mask decoder that produces 3D masks based on text description. This versatile OST is empowered by the hybrid pretraining to obtain perception priors and leveraged as the visual connector that bridges the 3D data to the LLM. After performing unified instruction tuning, our 3D-LLaVA reports impressive results on various benchmarks. The code and model will be released to promote future exploration.	 | 当前的3D大规模多模态模型（3D LMMs）在基于3D视觉的对话和推理方面展现了巨大的潜力。然而，如何进一步增强3D LMMs以实现细粒度的场景理解，并促进灵活的人机交互仍然是一个具有挑战性的问题。在这项工作中，我们介绍了3D-LLaVA，这是一种简单而强大的3D LMM，旨在作为智能助手来理解和与3D世界进行交互和推理。与现有的依赖于复杂管道（如多视图特征的离线提取或附加的任务特定头部）的顶级方法不同，3D-LLaVA采用了一种简约的设计，并集成了架构，仅接受点云作为输入。3D-LLaVA的核心是一个新的全向超级点变换器（Omni Superpoint Transformer，OST），它整合了三种功能：（1）一种视觉特征选择器，将并转换视觉令牌，（2）一种视觉提示编码器，将交互式视觉提示嵌入到视觉令牌空间中，（3）一种引用掩码解码器，根据文本描述生成3D掩码。这种多功能的OST通过混合预训练获得了感知先验，并作为视觉连接器，将3D数据与LLM连接起来。经过统一指令调优后，我们的3D-LLaVA在各种基准测试中取得了令人印象深刻的结果。相关代码和模型将被公布，以促进未来的探索。	 | 3D-LLaVA是一种简化设计的3D大规模多模态模型，通过引入全向超级点变换器（OST）实现细粒度的场景理解和灵活的人机交互，仅接受点云作为输入，并在多个基准测试中取得了优异表现。
2501.01156	 | TexAVi: Generating Stereoscopic VR Video Clips from Text Descriptions	 | Vriksha Srihari,R. Bhavya,Shruti Jayaraman,V. Mary Anita Rajam	 | While generative models such as text-to-image, large language models and text-to-video have seen significant progress, the extension to text-to-virtual-reality remains largely unexplored, due to a deficit in training data and the complexity of achieving realistic depth and motion in virtual environments. This paper proposes an approach to coalesce existing generative systems to form a stereoscopic virtual reality video from text.Carried out in three main stages, we start with a base text-to-image model that captures context from an input text. We then employ Stable Diffusion on the rudimentary image produced, to generate frames with enhanced realism and overall quality. These frames are processed with depth estimation algorithms to create left-eye and right-eye views, which are stitched side-by-side to create an immersive viewing experience. Such systems would be highly beneficial in virtual reality production, since filming and scene building often require extensive hours of work and post-production effort.We utilize image evaluation techniques, specifically Fréchet Inception Distance and CLIP Score, to assess the visual quality of frames produced for the video. These quantitative measures establish the proficiency of the proposed method.Our work highlights the exciting possibilities of using natural language-driven graphics in fields like virtual reality simulations.	 | 虽然文本到图像、大型语言模型和文本到视频生成模型已经取得了显著进步，但将这些技术扩展到文本到虚拟现实领域仍然鲜有探索，这主要是由于缺乏训练数据以及在虚拟环境中实现逼真的深度和运动的复杂性。本文提出了一种方法，将现有的生成系统结合起来，从文本生成立体虚拟现实视频。该方法分为三个主要阶段。首先，我们使用基础的文本到图像模型从输入文本中捕捉上下文信息。接着，我们使用Stable Diffusion对初步生成的图像进行处理，以生成具有增强真实感和整体质量的帧。这些帧通过深度估计算法生成左眼和右眼视图，并将它们并排缝合在一起，以创造沉浸式的观看体验。在虚拟现实制作领域，这种系统将非常有益，因为拍摄和场景构建通常需要大量的时间和后期制作努力。我们利用图像评估技术，特别是Fréchet Inception Distance和CLIP Score，来评估视频中生成的帧的视觉质量。这些定量指标证实了提出方法的效能。我们的工作突显了自然语言驱动图形在虚拟现实模拟等领域中的令人兴奋的可能性。	 | 本文提出了一种结合现有生成系统的方法，从文本生成立体虚拟现实视频，分为三个阶段：文本到图像模型获取上下文信息，Stable Diffusion增强帧的真实感，深度估计算法生成左右眼视图并排缝合。该方法通过图像评估技术验证了其在提高虚拟现实视频视觉质量方面的有效性。
2501.01142	 | Adaptive Hardness-driven Augmentation and Alignment Strategies for Multi-Source Domain Adaptations	 | Yang Yuxiang,Zeng Xinyi,Zeng Pinxian,Zu Chen,Yan Binyu,Zhou Jiliu,Wang Yan	 | Multi-source Domain Adaptation (MDA) aims to transfer knowledge from multiple labeled source domains to an unlabeled target domain. Nevertheless, traditional methods primarily focus on achieving inter-domain alignment through sample-level constraints, such as Maximum Mean Discrepancy (MMD), neglecting three pivotal aspects: 1) the potential of data augmentation, 2) the significance of intra-domain alignment, and 3) the design of cluster-level constraints. In this paper, we introduce a novel hardness-driven strategy for MDA tasks, named "A3MDA" , which collectively considers these three aspects through Adaptive hardness quantification and utilization in both data Augmentation and domain Alignment.To achieve this, "A3MDA" progressively proposes three Adaptive Hardness Measurements (AHM), i.e., Basic, Smooth, and Comparative AHMs, each incorporating distinct mechanisms for diverse scenarios. Specifically, Basic AHM aims to gauge the instantaneous hardness for each source/target sample. Then, hardness values measured by Smooth AHM will adaptively adjust the intensity level of strong data augmentation to maintain compatibility with the model's generalization capacity.In contrast, Comparative AHM is designed to facilitate cluster-level constraints. By leveraging hardness values as sample-specific weights, the traditional MMD is enhanced into a weighted-clustered variant, strengthening the robustness and precision of inter-domain alignment. As for the often-neglected intra-domain alignment, we adaptively construct a pseudo-contrastive matrix by selecting harder samples based on the hardness rankings, enhancing the quality of pseudo-labels, and shaping a well-clustered target feature space. Experiments on multiple MDA benchmarks show that " A3MDA " outperforms other methods.	 | 多源领域适应（MDA）的目标是从多个带有标签的源领域向未标记的目标领域转移知识。然而，传统方法主要集中在通过样本级约束实现领域间的对齐，如最大均值偏差（MMD），忽视了三个关键方面：1）数据增强的潜力，2）领域内对齐的重要性，以及3）群落级约束的设计。本文介绍了一种针对MDA任务的新颖硬度驱动策略，称为“A3MDA”，该策略通过自适应硬度量化和在数据增强和领域对齐中的利用，综合考虑了这三个方面。为了实现这一点，“A3MDA”逐步提出了三种自适应硬度测量（AHM），即基础型、光滑型和比较型AHM，每种AHM都包含不同的机制以适应不同场景。具体而言，基础AHM旨在衡量每个源/目标样本的即时硬度。然后，光滑AHM测量的硬度值将自适应调整强数据增强的强度，以保持与模型泛化能力的兼容性。相反，比较AHM旨在促进群落级约束。通过利用硬度值作为样本特定的权重，传统的MMD被增强为加权聚类版本，从而增强领域间对齐的稳健性和精确度。至于常常被忽视的领域内对齐，我们通过选择根据硬度排名选取更难的样本来自适应构建伪对比矩阵，从而提高伪标签的质量，并塑造一个良好聚类的目标特征空间。在多个MDA基准测试上的实验表明，“A3MDA”优于其他方法。	 | A3MDA 是一种针对多源领域适应任务的新颖策略，通过自适应硬度量化和利用数据增强及领域对齐，综合考虑了数据增强潜力、领域内对齐和群体级约束，从而提升目标领域知识转移的效果。该方法通过三种不同类型的自适应硬度测量（基础型、光滑型和比较型 AHM）分别针对即时硬度、数据增强强度调整和群体级约束，结合加权聚类版本的 MMD 改进领域间对齐，同时通过伪标签优化提高领域内对齐质量。
2501.01127	 | InDeed: Interpretable image deep decomposition with guaranteed generalizability	 | Sihan Wang,Shangqi Gao,Fuping Wu,Xiahai Zhuang	 | Image decomposition aims to analyze an image into elementary components, which is essential for numerous downstream tasks and also by nature provides certain interpretability to the analysis. Deep learning can be powerful for such tasks, but surprisingly their combination with a focus on interpretability and generalizability is rarely explored. In this work, we introduce a novel framework for interpretable deep image decomposition, combining hierarchical Bayesian modeling and deep learning to create an architecture-modularized and model-generalizable deep neural network (DNN). The proposed framework includes three steps: (1) hierarchical Bayesian modeling of image decomposition, (2) transforming the inference problem into optimization tasks, and (3) deep inference via a modularized Bayesian DNN. We further establish a theoretical connection between the loss function and the generalization error bound, which inspires a new test-time adaptation approach for out-of-distribution scenarios. We instantiated the application using two downstream tasks, \textit{i.e.}, image denoising, and unsupervised anomaly detection, and the results demonstrated improved generalizability as well as interpretability of our methods. The source code will be released upon the acceptance of this paper.	 | 图像分解旨在将图像分解为其基本组件，这对于众多下游任务来说都是至关重要的，同时也自然地提供了分析的可解释性。深度学习在这方面可以非常强大，但令人惊讶的是，它们在专注于可解释性和泛化能力的结合方面很少被探索。在本文中，我们介绍了一种新的可解释深度图像分解框架，结合了层次贝叶斯建模和深度学习，创建了一个架构模块化和模型泛化的深度神经网络（DNN）。所提出的框架包括三个步骤：(1) 图像分解的层次贝叶斯建模，(2) 将推断问题转化为优化任务，(3) 通过模块化的贝叶斯DNN进行深度推断。我们进一步建立了损失函数与泛化误差界之间的理论联系，这启发了一种新的测试时适应方法，适用于分布外场景。我们通过两个下游任务实例化了应用，即图像去噪和无监督异常检测，结果表明我们的方法具有更好的泛化能力和可解释性。论文被接受后，我们将公开源代码。	 | 本文提出了一种结合层次贝叶斯建模和深度学习的可解释深度图像分解框架，该框架通过三个步骤实现图像分解、优化推断以及使用模块化贝叶斯深度神经网络进行深度推断，并展示了在图像去噪和无监督异常检测中的优越泛化能力和可解释性。
2501.01126	 | Source-free Semantic Regularization Learning for Semi-supervised Domain Adaptation	 | Xinyang Huang,Chuang Zhu,Ruiying Ren,Shengjie Liu,Tiejun Huang	 | Semi-supervised domain adaptation (SSDA) has been extensively researched due to its ability to improve classification performance and generalization ability of models by using a small amount of labeled data on the target domain. However, existing methods cannot effectively adapt to the target domain due to difficulty in fully learning rich and complex target semantic information and relationships. In this paper, we propose a novel SSDA learning framework called semantic regularization learning (SERL), which captures the target semantic information from multiple perspectives of regularization learning to achieve adaptive fine-tuning of the source pre-trained model on the target domain. SERL includes three robust semantic regularization techniques. Firstly, semantic probability contrastive regularization (SPCR) helps the model learn more discriminative feature representations from a probabilistic perspective, using semantic information on the target domain to understand the similarities and differences between samples. Additionally, adaptive weights in SPCR can help the model learn the semantic distribution correctly through the probabilities of different samples. To further comprehensively understand the target semantic distribution, we introduce hard-sample mixup regularization (HMR), which uses easy samples as guidance to mine the latent target knowledge contained in hard samples, thereby learning more complete and complex target semantic knowledge. Finally, target prediction regularization (TPR) regularizes the target predictions of the model by maximizing the correlation between the current prediction and the past learned objective, thereby mitigating the misleading of semantic information caused by erroneous pseudo-labels. Extensive experiments on three benchmark datasets demonstrate that our SERL method achieves state-of-the-art performance.	 | 半监督领域适应（SSDA）由于可以通过在目标域上使用少量标记数据提升模型的分类性能和泛化能力而得到了广泛研究。然而，现有方法由于难以充分学习目标域丰富的语义信息和关系，因此无法有效适应目标域。本文提出了一种新颖的SSDA学习框架——语义正则化学习（SERL），通过从正则化学习的多个角度来看捕获目标语义信息，以实现对预训练源模型在目标域上的自适应微调。SERL 包含三种稳健的语义正则化技术。首先，语义概率对比正则化（SPCR）从概率角度帮助模型学习更具区分性的特征表示，利用目标域的语义信息来理解样本之间的相似性和差异性。此外，SPCR中的自适应权重可以帮助模型通过不同样本的概率来正确学习语义分布。为了更全面地理解目标语义分布，我们引入了困难样本混叠正则化（HMR），该技术利用容易的样本作为指导，挖掘困难样本中隐含的目标知识，从而学习更完整和复杂的语义知识。最后，目标预测正则化（TPR）通过对当前预测和过去学习的目标之间相关性的最大化来调节模型的目标预测，从而减轻由错误伪标签引起的语义信息误导。在三个基准数据集上的广泛实验表明，我们的SERL方法达到了最先进的性能。	 | 本文提出了一种新颖的半监督领域适应（SSDA）学习框架——语义正则化学习（SERL），通过引入三种语义正则化技术（SPCR、HMR和TPR），有效提升了模型在目标域上的分类性能和泛化能力。实验结果表明，SERL在三个基准数据集上达到了最先进的性能。
2501.01125	 | DuMo: Dual Encoder Modulation Network for Precise Concept Erasure	 | Feng Han,Kai Chen,Chao Gong,Zhipeng Wei,Jingjing Chen,Yu-Gang Jiang	 | The exceptional generative capability of text-to-image models has raised substantial safety concerns regarding the generation of Not-Safe-For-Work (NSFW) content and potential copyright infringement. To address these concerns, previous methods safeguard the models by eliminating inappropriate concepts. Nonetheless, these models alter the parameters of the backbone network and exert considerable influences on the structural (low-frequency) components of the image, which undermines the model's ability to retain non-target concepts. In this work, we propose our Dual encoder Modulation network (DuMo), which achieves precise erasure of inappropriate target concepts with minimum impairment to non-target concepts. In contrast to previous methods, DuMo employs the Eraser with PRior Knowledge (EPR) module which modifies the skip connection features of the U-NET and primarily achieves concept erasure on details (high-frequency) components of the image. To minimize the damage to non-target concepts during erasure, the parameters of the backbone U-NET are frozen and the prior knowledge from the original skip connection features is introduced to the erasure process. Meanwhile, the phenomenon is observed that distinct erasing preferences for the image structure and details are demonstrated by the EPR at different timesteps and layers. Therefore, we adopt a novel Time-Layer MOdulation process (TLMO) that adjusts the erasure scale of EPR module's outputs across different layers and timesteps, automatically balancing the erasure effects and model's generative ability. Our method achieves state-of-the-art performance on Explicit Content Erasure, Cartoon Concept Removal and Artistic Style Erasure, clearly outperforming alternative methods. Code is available at https://github.com/Maplebb/DuMo	 | 文本到图像模型的卓越生成能力引发了关于生成不适合工作（NSFW）内容和潜在版权侵权的安全问题。为应对这些问题，先前的方法通过消除不适当的概念来保护模型。然而，这些方法会修改主网络的参数，对图像的结构（低频）成分产生显著影响，从而削弱了模型保留非目标概念的能力。在这项工作中，我们提出了双编码器调制网络（DuMo），该网络能够在尽量减少对非目标概念影响的情况下，精确地消除不适当的目标概念。与先前的方法不同，DuMo 使用 Eraser with Prior Knowledge (EPR) 模块，该模块修改了 U-NET 的跳跃连接特征，并主要在图像的细节（高频）成分上实现概念消除。为了在消除过程中尽量减少对非目标概念的损害，主 U-NET 的参数被冻结，并引入了原始跳跃连接特征的先验知识参与消除过程。同时，我们观察到在不同的时间步长和层中，EPR 对图像结构和细节的消除偏好表现出不同的现象。因此，我们采用了一种新的 Time-Layer Modulation 过程（TLMO），该过程调整 EPR 模块输出在不同层和时间步长上的消除规模，自动平衡消除效果和模型的生成能力。我们的方法在明确内容消除、卡通概念去除和艺术风格消除方面达到了最先进的性能，明显优于其他方法。代码可在 https://github.com/Maplebb/DuMo 获取。	 | 本文提出了一种名为DuMo的双编码器调制网络，该网络通过Eraser with Prior Knowledge (EPR)模块精确消除不适当的目标概念，同时利用先验知识和Time-Layer Modulation过程在不损害非目标概念的情况下优化图像生成。
2501.01121	 | PatchRefiner V2: Fast and Lightweight Real-Domain High-Resolution Metric Depth Estimation	 | Zhenyu Li,Wenqing Cui,Shariq Farooq Bhat,Peter Wonka	 | While current high-resolution depth estimation methods achieve strong results, they often suffer from computational inefficiencies due to reliance on heavyweight models and multiple inference steps, increasing inference time. To address this, we introduce PatchRefiner V2 (PRV2), which replaces heavy refiner models with lightweight encoders. This reduces model size and inference time but introduces noisy features. To overcome this, we propose a Coarse-to-Fine (C2F) module with a Guided Denoising Unit for refining and denoising the refiner features and a Noisy Pretraining strategy to pretrain the refiner branch to fully exploit the potential of the lightweight refiner branch. Additionally, we introduce a Scale-and-Shift Invariant Gradient Matching (SSIGM) loss to enhance synthetic-to-real domain transfer. PRV2 outperforms state-of-the-art depth estimation methods on UnrealStereo4K in both accuracy and speed, using fewer parameters and faster inference. It also shows improved depth boundary delineation on real-world datasets like CityScape, ScanNet++, and KITTI, demonstrating its versatility across domains.	 | 尽管当前的高分辨率深度估计方法取得了很好的结果，但由于依赖于重型模型和多个推理步骤，它们经常面临计算效率低下和推理时间增加的问题。为了解决这一问题，我们引入了PatchRefiner V2（PRV2），它用轻量级编码器替代了重型细化模型。这虽然减少了模型大小和推理时间，但也引入了噪声特征。为了克服这一问题，我们提出了一种粗到细（Coarse-to-Fine，C2F）模块以及引导去噪单元，用于细化和去噪细化特征，并引入了噪点预训练策略，以充分利用轻量级细化分支的潜力。此外，我们还引入了一种尺度和偏移不变梯度匹配（Scale-and-Shift Invariant Gradient Matching，SSIGM）损失，以增强从合成到现实域的转移。PRV2 在UnrealStereo4K 上在准确性和速度上都优于最先进的深度估计方法，使用更少的参数和更快的推理时间。它还在CityScape、ScanNet++和KITTI等真实世界数据集上展示了更好的深度边界划定效果，显示出其在不同领域的通用性。	 | PatchRefiner V2 (PRV2) 通过引入轻量级编码器替代重型细化模型，显著提升了高分辨率深度估计的计算效率和推理速度，同时使用粗到细模块和引导去噪单元来优化特征细化和去噪过程。PRV2 在多个数据集上表现出色，不仅提高了准确性，还减少了参数量和推理时间，显示了其在不同领域的通用性。
2501.01120	 | Retrieval-Augmented Dynamic Prompt Tuning for Incomplete Multimodal Learning	 | Jian Lang,Zhangtao Cheng,Ting Zhong,Fan Zhou	 | Multimodal learning with incomplete modality is practical and challenging. Recently, researchers have focused on enhancing the robustness of pre-trained MultiModal Transformers (MMTs) under missing modality conditions by applying learnable prompts. However, these prompt-based methods face several limitations: (1) incomplete modalities provide restricted modal cues for task-specific inference, (2) dummy imputation for missing content causes information loss and introduces noise, and (3) static prompts are instance-agnostic, offering limited knowledge for instances with various missing conditions. To address these issues, we propose RAGPT, a novel Retrieval-AuGmented dynamic Prompt Tuning framework. RAGPT comprises three modules: (I) the multi-channel retriever, which identifies similar instances through a within-modality retrieval strategy, (II) the missing modality generator, which recovers missing information using retrieved contexts, and (III) the context-aware prompter, which captures contextual knowledge from relevant instances and generates dynamic prompts to largely enhance the MMT's robustness. Extensive experiments conducted on three real-world datasets show that RAGPT consistently outperforms all competitive baselines in handling incomplete modality problems. The code of our work and prompt-based baselines is available at https://github.com/Jian-Lang/RAGPT.	 | 多模态学习在不完整模态条件下是实用但具有挑战性的。最近，研究人员专注于通过应用可学习提示来增强预训练多模态变压器（MMTs）在缺失模态条件下的鲁棒性。然而，基于提示的方法面临着一些限制：（1）不完整的模态为特定任务的推理提供了有限的模态线索，（2）缺失内容的填充会导致信息丢失并引入噪声，（3）静态提示对实例是不敏感的，无法为具有各种缺失条件的实例提供有限的知识。为了解决这些问题，我们提出了一种新的检索增强动态提示调优框架RAGPT。RAGPT 包含三个模块：（I）多频道检索器，通过同一模态内的检索策略识别相似实例，（II）缺失模态生成器，使用检索到的上下文恢复缺失的信息，（III）上下文感知提示生成器，从相关实例中捕捉上下文知识并生成动态提示，以大大增强MMT的鲁棒性。在三个实际数据集上进行的广泛实验表明，RAGPT 在处理不完整模态问题时始终优于所有竞争基线。我们的工作和基于提示的基线代码可在 https://github.com/Jian-Lang/RAGPT 获取。	 | RAGPT 提出了一种新的检索增强动态提示调优框架，通过多频道检索、缺失模态生成和上下文感知提示生成三个模块，增强多模态变压器在不完整模态条件下的鲁棒性；实验结果表明，RAGPT 在多个实际数据集上的表现始终优于其他竞争基线。
2501.01119	 | Leverage Cross-Attention for End-to-End Open-Vocabulary Panoptic Reconstruction	 | Xuan Yu,Yuxuan Xie,Yili Liu,Haojian Lu,Rong Xiong,Yiyi Liao,Yue Wang	 | Open-vocabulary panoptic reconstruction offers comprehensive scene understanding, enabling advances in embodied robotics and photorealistic simulation. In this paper, we propose PanopticRecon++, an end-to-end method that formulates panoptic reconstruction through a novel cross-attention perspective. This perspective models the relationship between 3D instances (as queries) and the scene's 3D embedding field (as keys) through their attention map. Unlike existing methods that separate the optimization of queries and keys or overlook spatial proximity, PanopticRecon++ introduces learnable 3D Gaussians as instance queries. This formulation injects 3D spatial priors to preserve proximity while maintaining end-to-end optimizability. Moreover, this query formulation facilitates the alignment of 2D open-vocabulary instance IDs across frames by leveraging optimal linear assignment with instance masks rendered from the queries. Additionally, we ensure semantic-instance segmentation consistency by fusing query-based instance segmentation probabilities with semantic probabilities in a novel panoptic head supervised by a panoptic loss. During training, the number of instance query tokens dynamically adapts to match the number of objects. PanopticRecon++ shows competitive performance in terms of 3D and 2D segmentation and reconstruction performance on both simulation and real-world datasets, and demonstrates a user case as a robot simulator. Our project website is at: https://yuxuan1206.github.io/panopticrecon_pp/	 | 开放词汇的全景重建提供了全面的场景理解，推动了实体机器人技术和照片现实度模拟的进步。在本文中，我们提出了一种名为PanopticRecon++的新颖端到端方法，该方法通过一种新颖的跨注意力视角来构建全景重建。这种视角通过注意力图来建模3D实例（作为查询）与场景的3D嵌入场（作为键）之间的关系。与现有方法将查询和键的优化分离或忽视空间邻近性不同，PanopticRecon++引入了可学习的3D高斯分布作为实例查询。这种表述通过注入3D空间先验来保留邻近性，同时保持端到端的优化能力。此外，这种查询表述通过利用实例掩码与查询渲染的最佳线性分配来实现跨帧2D开放词汇实例ID的对齐。另外，我们通过在新的全景头中融合基于查询的实例分割概率和语义概率来监督全景损失，以确保语义-实例分割一致性。在训练期间，实例查询令牌的数量动态适应匹配物体的数量。PanopticRecon++在模拟和真实世界数据集上的3D和2D分割与重建性能方面展示了竞争力，并且展示了作为机器人模拟器的用户案例。我们的项目网站为：https://yuxuan1206.github.io/panopticrecon_pp/	 | 本文提出了一种名为PanopticRecon++的新型端到端方法，通过引入可学习的3D高斯分布作为实例查询，利用跨注意力视角构建全景重建，从而提高3D和2D分割与重建性能，并展示了其在机器人模拟器中的应用案例。
2501.01116	 | HarmonyIQA: Pioneering Benchmark and Model for Image Harmonization Quality Assessment	 | Zitong Xu,Huiyu Duan,Guangji Ma,Liu Yang,Jiarui Wang,Qingbo Wu,Xiongkuo Min,Guangtao Zhai,Patrick Le Callet	 | Image composition involves extracting a foreground object from one image and pasting it into another image through Image harmonization algorithms (IHAs), which aim to adjust the appearance of the foreground object to better match the background. Existing image quality assessment (IQA) methods may fail to align with human visual preference on image harmonization due to the insensitivity to minor color or light inconsistency. To address the issue and facilitate the advancement of IHAs, we introduce the first Image Quality Assessment Database for image Harmony evaluation (HarmonyIQAD), which consists of 1,350 harmonized images generated by 9 different IHAs, and the corresponding human visual preference scores. Based on this database, we propose a Harmony Image Quality Assessment (HarmonyIQA), to predict human visual preference for harmonized images. Extensive experiments show that HarmonyIQA achieves state-of-the-art performance on human visual preference evaluation for harmonized images, and also achieves competing results on traditional IQA tasks. Furthermore, cross-dataset evaluation also shows that HarmonyIQA exhibits better generalization ability than self-supervised learning-based IQA methods. Both HarmonyIQAD and HarmonyIQA will be made publicly available upon paper publication.	 | 图像合成涉及从一个图像中提取前景对象，并通过图像和谐化算法（IHAs）将其粘贴到另一个图像中，这些算法旨在调整前景对象的外观，使其更好地与背景匹配。现有的图像质量评估（IQA）方法可能无法在图像和谐化方面与人类视觉偏好对齐，因为它们对细微的颜色或光的不一致性不敏感。为了解决这一问题并促进IHAs的发展，我们引入了首个用于图像和谐性评估的图像质量评估数据库（HarmonyIQAD），该数据库包含由9种不同IHAs生成的1,350张和谐化图像及其对应的人类视觉偏好评分。基于此数据库，我们提出了一种和谐图像质量评估（HarmonyIQA）方法，用于预测和谐化图像的人类视觉偏好。广泛实验表明，HarmonyIQA在和谐化图像的人类视觉偏好评估方面达到了最先进的性能，并且在传统IQA任务中也取得了竞争性结果。此外，跨数据集评估还表明，HarmonyIQA比基于自监督学习的IQA方法具有更好的泛化能力。在论文发表后，HarmonyIQAD和HarmonyIQA将公开提供。	 | 该研究介绍了首个用于图像和谐性评估的数据库HarmonyIQAD，包含1,350张由不同和谐化算法生成的图像及其人类视觉偏好评分，并提出了一种名为HarmonyIQA的新方法，用于预测和谐化图像的人类视觉偏好，该方法在性能上优于现有方法且具有良好泛化能力。
2501.01109	 | BatStyler: Advancing Multi-category Style Generation for Source-free Domain Generalization	 | Xiusheng Xu,Lei Qi,Jingyang Zhou,Xin Geng	 | Source-Free Domain Generalization (SFDG) aims to develop a model that performs on unseen domains without relying on any source domains. However, the implementation remains constrained due to the unavailability of training data. Research on SFDG focus on knowledge transfer of multi-modal models and style synthesis based on joint space of multiple modalities, thus eliminating the dependency on source domain images. However, existing works primarily work for multi-domain and less-category configuration, but performance on multi-domain and multi-category configuration is relatively poor. In addition, the efficiency of style synthesis also deteriorates in multi-category scenarios. How to efficiently synthesize sufficiently diverse data and apply it to multi-category configuration is a direction with greater practical value. In this paper, we propose a method called BatStyler, which is utilized to improve the capability of style synthesis in multi-category scenarios. BatStyler consists of two modules: Coarse Semantic Generation and Uniform Style Generation modules. The Coarse Semantic Generation module extracts coarse-grained semantics to prevent the compression of space for style diversity learning in multi-category configuration, while the Uniform Style Generation module provides a template of styles that are uniformly distributed in space and implements parallel training. Extensive experiments demonstrate that our method exhibits comparable performance on less-category datasets, while surpassing state-of-the-art methods on multi-category datasets.	 | 无源域泛化（Source-Free Domain Generalization, SFDG）的目标是开发一个在未见域中表现良好的模型，而不依赖于任何源域的数据。然而，由于缺乏训练数据，实现这一目标仍然受到限制。SFDG的研究主要集中在多模态模型的知识迁移和基于多种模态联合空间的风格合成上，以消除对源域图像的依赖。然而，现有的工作主要适用于多域和少类别配置，但在多域和多类别的配置下表现较差。此外，多类别场景下的风格合成效率也有所下降。因此，如何有效地合成足够多样化的数据，并将其应用于多类别配置，具有更大的实用价值。在本文中，我们提出了一种名为BatStyler的方法，以提高其在多类别场景下的风格合成能力。BatStyler由两个模块组成：粗粒度语义生成模块和均匀风格生成模块。粗粒度语义生成模块提取粗粒度语义，以防止在多类别配置中压缩风格多样性的学习空间，而均匀风格生成模块提供了一种均匀分布在空间中的风格模板，并实现了并行训练。广泛的实验表明，我们的方法在少类别数据集上表现出与现有方法相当的性能，而在多类别数据集上则超越了现有最先进的方法。	 | 无源域泛化（SFDG）旨在开发能够在未见域中表现良好的模型，主要通过多模态模型的知识迁移和风格合成来消除对源域数据的依赖，但现有方法在多类别配置下效果较差。为此，本文提出了一种名为BatStyler的新方法，通过粗粒度语义生成模块和均匀风格生成模块提高在多类别场景下的风格合成能力，实验结果表明该方法在多类别数据集上超过了现有最先进的方法。
2501.01106	 | AIM: Additional Image Guided Generation of Transferable Adversarial Attacks	 | Teng Li,Xingjun Ma,Yu-Gang Jiang	 | Transferable adversarial examples highlight the vulnerability of deep neural networks (DNNs) to imperceptible perturbations across various real-world applications. While there have been notable advancements in untargeted transferable attacks, targeted transferable attacks remain a significant challenge. In this work, we focus on generative approaches for targeted transferable attacks. Current generative attacks focus on reducing overfitting to surrogate models and the source data domain, but they often overlook the importance of enhancing transferability through additional semantics. To address this issue, we introduce a novel plug-and-play module into the general generator architecture to enhance adversarial transferability. Specifically, we propose a \emph{Semantic Injection Module} (SIM) that utilizes the semantics contained in an additional guiding image to improve transferability. The guiding image provides a simple yet effective method to incorporate target semantics from the target class to create targeted and highly transferable attacks. Additionally, we propose new loss formulations that can integrate the semantic injection module more effectively for both targeted and untargeted attacks. We conduct comprehensive experiments under both targeted and untargeted attack settings to demonstrate the efficacy of our proposed approach.	 | 可转移的对抗样本突显了深度神经网络（DNNs）在各种实际应用中对微不可见扰动的脆弱性。虽然在非目标可转移攻击方面已经取得了显著进展，但目标可转移攻击仍然是一个重大挑战。在本文中，我们重点关注目标可转移攻击的生成方法。当前的生成攻击主要集中在减少对代理模型和源数据域的过拟合上，但往往忽视了通过额外语义增强可转移性的的重要性。为解决这一问题，我们引入了一个新颖的即插即用模块，以增强对抗的可转移性。具体而言，我们提出了一个名为语义注入模块（Semantic Injection Module, SIM）的模块，它利用附加引导图像中的语义信息来提高可转移性。引导图像提供了一种简单而有效的方法，将目标类的语义信息融入到目标攻击中，从而创建出具有高度可转移性的攻击。此外，我们还提出了一种新的损失函数形式，使其能够更有效地整合语义注入模块，不仅适用于目标攻击，也适用于非目标攻击。我们在有目标和无目标攻击设置下进行了全面的实验，以展示我们提出的方法的有效性。	 | 本文提出了一种新的语义注入模块（SIM）来增强目标可转移对抗样本的可转移性，并引入了一种新的损失函数以更有效地整合该模块，适用于目标和非目标攻击场景。
2501.01101	 | Deformable Gaussian Splatting for Efficient and High-Fidelity Reconstruction of Surgical Scenes	 | Jiwei Shan,Zeyu Cai,Cheng-Tai Hsieh,Shing Shin Cheng,Hesheng Wang	 | Efficient and high-fidelity reconstruction of deformable surgical scenes is a critical yet challenging task. Building on recent advancements in 3D Gaussian splatting, current methods have seen significant improvements in both reconstruction quality and rendering speed. However, two major limitations remain: (1) difficulty in handling irreversible dynamic changes, such as tissue shearing, which are common in surgical scenes; and (2) the lack of hierarchical modeling for surgical scene deformation, which reduces rendering speed. To address these challenges, we introduce EH-SurGS, an efficient and high-fidelity reconstruction algorithm for deformable surgical scenes. We propose a deformation modeling approach that incorporates the life cycle of 3D Gaussians, effectively capturing both regular and irreversible deformations, thus enhancing reconstruction quality. Additionally, we present an adaptive motion hierarchy strategy that distinguishes between static and deformable regions within the surgical scene. This strategy reduces the number of 3D Gaussians passing through the deformation field, thereby improving rendering speed. Extensive experiments demonstrate that our method surpasses existing state-of-the-art approaches in both reconstruction quality and rendering speed. Ablation studies further validate the effectiveness and necessity of our proposed components. We will open-source our code upon acceptance of the paper.	 | 高效的高保真重建可变形手术场景是一项关键但具有挑战性的任务。基于近期在3D高斯点云技术上的进展，现有方法在重建质量和渲染速度方面已经取得了显著的进步。然而，仍存在两大主要局限：（1）难以处理不可逆的动态变化，如组织剪切等，这些在手术场景中非常常见；（2）缺乏对手术场景变形的层次建模，这降低了渲染速度。为解决这些挑战，我们提出了EH-SurGS，一种高效的高保真可变形手术场景重建算法。我们提出了一种变形建模方法，结合了3D高斯分布的生命周期，能够有效地捕捉到常规和不可逆变形，从而提高重建质量。此外，我们还提出了一种适应性运动层次化策略，区分手术场景中的静态和可变形区域。该策略减少了通过变形场的3D高斯分布的数量，从而提高了渲染速度。大量的实验表明，我们的方法在重建质量和渲染速度方面都超过了现有的最新方法。消融研究进一步验证了我们提出组件的有效性和必要性。论文被接受后，我们将开源我们的代码。	 | 该研究提出了EH-SurGS算法，一种高效处理不可逆动态变化和进行层次建模的高保真可变形手术场景重建方法，显著提高了重建质量和渲染速度；通过结合3D高斯分布的生命周期和适应性运动层次化策略，EH-SurGS有效解决了现有方法在处理动态变化和提高渲染速度方面的局限性。
2501.01097	 | EliGen: Entity-Level Controlled Image Generation with Regional Attention	 | Hong Zhang,Zhongjie Duan,Xingjun Wang,Yingda Chen,Yu Zhang	 | Recent advancements in diffusion models have significantly advanced text-to-image generation, yet global text prompts alone remain insufficient for achieving fine-grained control over individual entities within an image. To address this limitation, we present EliGen, a novel framework for Entity-Level controlled Image Generation. We introduce regional attention, a mechanism for diffusion transformers that requires no additional parameters, seamlessly integrating entity prompts and arbitrary-shaped spatial masks. By contributing a high-quality dataset with fine-grained spatial and semantic entity-level annotations, we train EliGen to achieve robust and accurate entity-level manipulation, surpassing existing methods in both positional control precision and image quality. Additionally, we propose an inpainting fusion pipeline, extending EliGen to multi-entity image inpainting tasks. We further demonstrate its flexibility by integrating it with community models such as IP-Adapter and MLLM, unlocking new creative possibilities. The source code, dataset, and model will be released publicly.	 | 近年来，扩散模型在文本到图像生成方面的进步显著，但在对图像中个体实体进行细微控制方面，仅使用全局文本提示仍然不够。为解决这一限制，我们提出了一种名为EliGen的新框架，用于实体级控制图像生成。我们引入了区域注意力机制，这是一种无需额外参数的机制，能够无缝整合实体提示和任意形状的空间掩码。通过提供一个带有精细空间和语义实体级别注释的高质量数据集，我们训练EliGen实现稳健且准确的实体级操控，其在位置控制精度和图像质量方面均超越现有方法。此外，我们提出了一种修复融合管道，将EliGen扩展到多实体图像修复任务。我们进一步展示了其灵活性，将其与IP-Adapter和MLLM等社区模型集成，解锁新的创作可能性。源代码、数据集和模型将公开发布。	 | 该研究提出了一种名为EliGen的新框架，通过引入区域注意力机制实现对图像中个体实体的精细控制，并通过高质量数据集训练，显著提升了位置控制精度和图像质量。此外，EliGen还被扩展应用于多实体图像修复任务，并与社区模型集成，展示了其灵活性和创作潜力。
2501.01072	 | Evidential Calibrated Uncertainty-Guided Interactive Segmentation paradigm for Ultrasound Images	 | Jiang Shang,Yuanmeng Wu,Xiaoxiang Han,Xi Chen,Qi Zhang	 | Accurate and robust ultrasound image segmentation is critical for computer-aided diagnostic systems. Nevertheless, the inherent challenges of ultrasound imaging, such as blurry boundaries and speckle noise, often cause traditional segmentation methods to struggle with performance. Despite recent advancements in universal image segmentation, such as the Segment Anything Model, existing interactive segmentation methods still suffer from inefficiency and lack of specialization. These methods rely heavily on extensive accurate manual or random sampling prompts for interaction, necessitating numerous prompts and iterations to reach satisfactory performance. In response to this challenge, we propose the Evidential Uncertainty-Guided Interactive Segmentation (EUGIS), an end-to-end, efficient tiered interactive segmentation paradigm based on evidential uncertainty estimation for ultrasound image segmentation. Specifically, EUGIS harnesses evidence-based uncertainty estimation, grounded in Dempster-Shafer theory and Subjective Logic, to gauge the level of uncertainty in the predictions of model for different regions. By prioritizing sampling the high-uncertainty region, our method can effectively simulate the interactive behavior of well-trained radiologists, enhancing the targeted of sampling while reducing the number of prompts and iterations required.Additionally, we propose a trainable calibration mechanism for uncertainty estimation, which can further optimize the boundary between certainty and uncertainty, thereby enhancing the confidence of uncertainty estimation.	 | 准确且稳健的超声图像分割对于计算机辅助诊断系统至关重要。然而，超声成像固有的挑战，如模糊边界和点状噪声，往往导致传统分割方法在性能上难以应对。尽管最近在通用图像分割方面取得了进展，如Segment Anything Model，现有的交互式分割方法仍然存在效率低下和缺乏专业化的不足。这些方法高度依赖于大量的准确手动或随机采样提示进行交互，需要多次提示和迭代才能达到满意的性能。为应对这一挑战，我们提出了证据不确定性指导的交互式分割（EUGIS）方法，这是一种基于证据不确定性估计的端到端、高效分层交互式分割范式，用于超声图像分割。具体而言，EUGIS 利用基于Dempster-Shafer理论和主观逻辑的证据基础不确定性估计，来衡量模型对不同区域预测的不确定性水平。通过优先采样高不确定性区域，我们的方法能够有效模拟训练有素的放射科医生的交互行为，从而提高采样的针对性，减少所需的提示和迭代次数。此外，我们还提出了一种可训练的校准机制，用于优化确定性和不确定性的边界，从而增强不确定性估计的信心。	 | EUGIS方法提出了一种基于证据不确定性估计的高效分层交互式分割范式，用于超声图像分割，通过优先处理高不确定性区域，减少所需的提示和迭代次数；同时，该方法还引入了可训练的校准机制以优化确定性和不确定性的边界，提高分割性能。
2501.01049	 | TS-SatMVSNet: Slope Aware Height Estimation for Large-Scale Earth Terrain Multi-view Stereo	 | Song Zhang,Zhiwei Wei,Wenjia Xu,Lili Zhang,Yang Wang,Jinming Zhang,Junyi Liu	 | 3D terrain reconstruction with remote sensing imagery achieves cost-effective and large-scale earth observation and is crucial for safeguarding natural disasters, monitoring ecological changes, and preserving the environment.Recently, learning-based multi-view stereo~(MVS) methods have shown promise in this task. However, these methods simply modify the general learning-based MVS framework for height estimation, which overlooks the terrain characteristics and results in insufficient accuracy. Considering that the Earth's surface generally undulates with no drastic changes and can be measured by slope, integrating slope considerations into MVS frameworks could enhance the accuracy of terrain reconstructions. To this end, we propose an end-to-end slope-aware height estimation network named TS-SatMVSNet for large-scale remote sensing terrain reconstruction.To effectively obtain the slope representation, drawing from mathematical gradient concepts, we innovatively proposed a height-based slope calculation strategy to first calculate a slope map from a height map to measure the terrain undulation. To fully integrate slope information into the MVS pipeline, we separately design two slope-guided modules to enhance reconstruction outcomes at both micro and macro levels. Specifically, at the micro level, we designed a slope-guided interval partition module for refined height estimation using slope values. At the macro level, a height correction module is proposed, using a learnable Gaussian smoothing operator to amend the inaccurate height values. Additionally, to enhance the efficacy of height estimation, we proposed a slope direction loss for implicitly optimizing height estimation results. Extensive experiments on the WHU-TLC dataset and MVS3D dataset show that our proposed method achieves state-of-the-art performance and demonstrates competitive generalization ability.	 | 基于遥感影像的三维地形重建实现了经济高效的大规模地球观测，并对于灾害防御、生态保护监测和环境保存至关重要。近年来，基于学习的多视图立体视觉（MVS）方法在这一任务中显示出巨大潜力。然而，这些方法只是简单地修改了一般的学习型MVS框架以进行高度估计，忽略了地形特性，导致精度不足。考虑到地球表面通常起伏变化不大，可以通过坡度进行测量，将坡度考虑集成到MVS框架中可以提升地形重建的精度。为此，我们提出了一种端到端的坡度意识高度估计网络TS-SatMVSNet，以实现大规模遥感地形重建。为了有效地获得坡度表示，借鉴数学梯度的概念，我们创新地提出了一种基于高度的坡度计算策略，首先从高度图计算一个坡度地图来测量地形起伏。为了充分将坡度信息集成到MVS处理流程中，我们分别设计了两个坡度引导模块，在微观和宏观两个层面提升重建结果。具体而言，在微观层面，我们设计了一个坡度引导的间隔分区模块，利用坡度值进行精细的高度估计。在宏观层面，我们提出了一种高度校正模块，利用可学习的高斯平滑操作符修正不准确的高度值。此外，为了增强高度估计的效果，我们引入了一种坡度方向损失，以隐式优化高度估计结果。在WHU-TLC数据集和MVS3D数据集上的广泛实验表明，我们提出的方法达到了最先进的性能，并展示了较强的泛化能力。	 | 本文提出了一种端到端的坡度意识高度估计网络TS-SatMVSNet，通过集成坡度信息提升大规模遥感地形重建的精度。该网络设计了两个坡度引导模块，在微观和宏观层面优化重建结果，并引入了坡度方向损失以进一步提高高度估计效果，实验结果表明其达到了最先进的性能。
2501.01045	 | ZeroFlow: Overcoming Catastrophic Forgetting is Easier than You Think	 | Tao Feng,Wei Li,DiDi Zhu,Hangjie Yuan,Wendi Zheng,Dan Zhang,Jie Tang	 | Backpropagation provides a generalized configuration for overcoming catastrophic forgetting. Like, SGD and Adam are commonly used for weight updates in continual learning and continual pre-training. In practice, permission to access gradient information is not always granted (the gradient ban), such as black-box APIs, hardware limitations, and non-differentiable systems. To bridge this gap, we introduce the first benchmark ZeroFlow to evaluate gradient-free optimization algorithms for overcoming forgetting. This benchmark examines a suite of forward pass methods across multiple methods, forgetting scenarios, and datasets. We find that forward passes alone are enough to overcome forgetting. Our findings reveal new optimization principles that highlight the potential of forward-pass in mitigating forgetting, managing task conflicts, and reducing memory demands, alongside novel enhancements that further mitigate forgetting with just one forward pass. This work provides essential insights and tools for advancing forward pass methods to overcome forgetting.	 | 反向传播提供了一种通用配置，用于克服灾难性遗忘。例如，SGD和Adam通常用于连续学习和连续预训练中的权重更新。在实践中，访问梯度信息的权利并不总是被授予（梯度禁令），这可能是因为黑盒API、硬件限制以及非可微系统等问题。为了解决这一问题，我们引入了第一个基准测试ZeroFlow，用于评估无梯度优化算法以克服遗忘。该基准测试评估了多种前向传播方法在不同遗忘场景和数据集上的表现。我们发现，仅前向传播就足以克服遗忘。我们的研究揭示了新的优化原则，突显了前向传播在减轻遗忘、管理任务冲突以及减少内存需求方面的潜力，并提出了新的改进措施，仅通过一次前向传播就能进一步减轻遗忘。这项工作提供了关于前向传播方法克服遗忘的宝贵见解和工具，有助于推动前向传播方法的发展。	 | 该研究引入了首个基准测试ZeroFlow，用于评估无梯度优化算法克服遗忘的能力，发现仅前向传播即可有效减轻遗忘，并揭示了前向传播在减轻遗忘、管理任务冲突和减少内存需求方面的潜力。
2501.01042	 | Image-based Multimodal Models as Intruders: Transferable Multimodal Attacks on Video-based MLLMs	 | Linhao Huang,Xue Jiang,Zhiqiang Wang,Wentao Mo,Xi Xiao,Bo Han,Yongjie Yin,Feng Zheng	 | Video-based multimodal large language models (V-MLLMs) have shown vulnerability to adversarial examples in video-text multimodal tasks. However, the transferability of adversarial videos to unseen models--a common and practical real world scenario--remains unexplored. In this paper, we pioneer an investigation into the transferability of adversarial video samples across V-MLLMs. We find that existing adversarial attack methods face significant limitations when applied in black-box settings for V-MLLMs, which we attribute to the following shortcomings: (1) lacking generalization in perturbing video features, (2) focusing only on sparse key-frames, and (3) failing to integrate multimodal information. To address these limitations and deepen the understanding of V-MLLM vulnerabilities in black-box scenarios, we introduce the Image-to-Video MLLM (I2V-MLLM) attack. In I2V-MLLM, we utilize an image-based multimodal model (IMM) as a surrogate model to craft adversarial video samples. Multimodal interactions and temporal information are integrated to disrupt video representations within the latent space, improving adversarial transferability. In addition, a perturbation propagation technique is introduced to handle different unknown frame sampling strategies. Experimental results demonstrate that our method can generate adversarial examples that exhibit strong transferability across different V-MLLMs on multiple video-text multimodal tasks. Compared to white-box attacks on these models, our black-box attacks (using BLIP-2 as surrogate model) achieve competitive performance, with average attack success rates of 55.48% on MSVD-QA and 58.26% on MSRVTT-QA for VideoQA tasks, respectively. Our code will be released upon acceptance.	 | 基于视频的多模态大型语言模型（V-MLLMs）在视频-文本多模态任务中显示出对对抗样本的脆弱性。然而，对抗视频在面对未见过的模型时的可转移性——这是常见的且实际的现实场景——尚未被探索。本文我们首次对V-MLLMs中的对抗视频样本在不同模型间可转移性展开了研究。我们发现，现有的对抗攻击方法在黑盒设置下应用于V-MLLMs时面临显著的局限性，我们将其归因于以下缺陷：（1）缺乏对视频特征扰动的一般化能力，（2）仅关注稀疏的关键帧，以及（3）未能整合多模态信息。为了解决这些局限性并加深对V-MLLMs在黑盒场景下脆弱性的理解，我们提出了图像到视频MILLM（I2V-MLLM）攻击。在I2V-MLLM中，我们利用图像为基础的多模态模型（IMM）作为代理模型来生成对抗视频样本。通过整合多模态交互和时间信息，破坏视频表示在潜空间中的表示，提高了对抗样本的可转移性。此外，我们还引入了一种扰动传播技术来处理不同的未知帧采样策略。实验结果表明，我们的方法能够生成在多个视频-文本多模态任务中不同V-MLLMs上表现出强可转移性的对抗样本。与这些模型上的白盒攻击相比，我们的黑盒攻击（使用BLIP-2作为代理模型）在VideoQA任务中分别在MSVD-QA和MSRVTT-QA上实现了55.48%和58.26%的平均攻击成功率。我们的代码将在文章被接受后发布。	 | 本文研究了对抗视频在不同模型间的可转移性，发现现有方法在黑盒设置下对基于视频的大规模多模态语言模型（V-MLLMs）效果不佳，并提出了一种新的I2V-MLLM攻击方法，通过利用图像为基础的多模态模型生成对抗视频样本，显著提高了对抗样本的可转移性。
2501.01040	 | Event Masked Autoencoder: Point-wise Action Recognition with Event-Based Cameras	 | Jingkai Sun,Qiang Zhang,Jiaxu Wang,Jiahang Cao,Renjing Xu	 | Dynamic vision sensors (DVS) are bio-inspired devices that capture visual information in the form of asynchronous events, which encode changes in pixel intensity with high temporal resolution and low latency. These events provide rich motion cues that can be exploited for various computer vision tasks, such as action recognition. However, most existing DVS-based action recognition methods lose temporal information during data transformation or suffer from noise and outliers caused by sensor imperfections or environmental factors. To address these challenges, we propose a novel framework that preserves and exploits the spatiotemporal structure of event data for action recognition. Our framework consists of two main components: 1) a point-wise event masked autoencoder (MAE) that learns a compact and discriminative representation of event patches by reconstructing them from masked raw event camera points data; 2) an improved event points patch generation algorithm that leverages an event data inlier model and point-wise data augmentation techniques to enhance the quality and diversity of event points patches. To the best of our knowledge, our approach introduces the pre-train method into event camera raw points data for the first time, and we propose a novel event points patch embedding to utilize transformer-based models on event cameras.	 | 动态视觉传感器（DVS）是灵感来自于生物的设备，它们以异步事件的形式捕获视觉信息，这些事件以高时间分辨率和低延迟的方式编码像素强度的变化。这些事件提供了丰富的运动线索，可以被利用来进行各种计算机视觉任务，如动作识别。然而，大多数现有的基于DVS的动作识别方法在数据转换过程中会丢失时间信息，或者由于传感器不完善或环境因素导致的噪声和离群值而受到影响。为了应对这些挑战，我们提出了一种新的框架，该框架能够保留并利用事件数据的空间-时间结构来进行动作识别。我们的框架包含两个主要组成部分：1）点事件掩码自编码器（MAE），通过从掩码原始事件摄像机点数据中重构事件块来学习事件片段的紧凑且具有区分性的表示；2）改进的事件点块生成算法，该算法利用事件数据的内点模型和点事件数据增强技术来提高事件点块的质量和多样性。据我们所知，我们的方法首次将预训练方法引入到事件摄像机原始点数据中，并且我们提出了一种新型的事件点块嵌入方法，以利用基于变压器的模型在事件摄像机上进行操作。	 | 该研究提出了一种新的框架，通过保留和利用事件数据的空间-时间结构来提高基于动态视觉传感器的动作识别性能，该框架包括一个掩码自编码器和增强的事件点块生成算法，并首次将预训练方法应用于原始点数据。
2501.01032	 | DynamicLip: Shape-Independent Continuous Authentication via Lip Articulator Dynamics	 | Huashan Chen,Yifan Xu,Yue Feng,Ming Jian,Feng Liu,Pengfei Hu,Kebin Peng,Sen He,Zi Wang	 | Biometrics authentication has become increasingly popular due to its security and convenience; however, traditional biometrics are becoming less desirable in scenarios such as new mobile devices, Virtual Reality, and Smart Vehicles. For example, while face authentication is widely used, it suffers from significant privacy concerns. The collection of complete facial data makes it less desirable for privacy-sensitive applications. Lip authentication, on the other hand, has emerged as a promising biometrics method. However, existing lip-based authentication methods heavily depend on static lip shape when the mouth is closed, which can be less robust due to lip shape dynamic motion and can barely work when the user is speaking. In this paper, we revisit the nature of lip biometrics and extract shape-independent features from the lips. We study the dynamic characteristics of lip biometrics based on articulator motion. Building on the knowledge, we propose a system for shape-independent continuous authentication via lip articulator dynamics. This system enables robust, shape-independent and continuous authentication, making it particularly suitable for scenarios with high security and privacy requirements. We conducted comprehensive experiments in different environments and attack scenarios and collected a dataset of 50 subjects. The results indicate that our system achieves an overall accuracy of 99.06% and demonstrates robustness under advanced mimic attacks and AI deepfake attacks, making it a viable solution for continuous biometric authentication in various applications.	 | 生物特征认证由于其安全性和便利性而变得越来越流行；然而，在新的移动设备、虚拟现实和智能车辆等场景中，传统的生物特征认证变得不再那么理想。例如，尽管面部认证被广泛应用，但它面临着显著的隐私问题。完整的面部数据采集使得它在对隐私敏感的应用中不太受欢迎。相比之下，唇部认证作为一种有前景的生物特征方法逐渐兴起。然而，现有的基于唇部的认证方法高度依赖于闭嘴时的静态唇形，这可能导致其不够稳健，尤其是在用户说话时几乎无法工作。在本文中，我们重新审视了唇部生物特征的本质，并从唇部中提取与形状无关的特征。基于唇部发音运动的动态特性，我们研究了唇部生物特征的动态特性。在此基础上，我们提出了一种基于唇部发音动态的形状无关连续认证系统。该系统能够实现稳健、与形状无关和连续的认证，特别适合对安全和隐私要求高的场景。我们在不同环境和攻击场景下进行了全面的实验，并收集了50名受试者的数据集。实验结果表明，我们的系统总体准确率为99.06%，并在高级模仿攻击和AI深度伪造攻击下显示出稳健性，使其成为各种应用中连续生物特征认证的可行解决方案。	 | 本文提出了一种基于唇部发音动态的形状无关连续认证系统，能够实现稳健、与形状无关和连续的认证，特别适合安全和隐私要求高的场景，并在实验中展示了99.06%的总体准确率和对高级模仿攻击和AI深度伪造攻击的稳健性。
2501.01023	 | Hadamard Attention Recurrent Transformer: A Strong Baseline for Stereo Matching Transformer	 | Ziyang Chen,Yongjun Zhang,Wenting Li,Bingshu Wang,Yabo Wu,Yong Zhao,C.L. Philip Chen	 | In light of the advancements in transformer technology, extant research posits the construction of stereo transformers as a potential solution to the binocular stereo matching challenge. However, constrained by the low-rank bottleneck and quadratic complexity of attention mechanisms, stereo transformers still fail to demonstrate sufficient nonlinear expressiveness within a reasonable inference time. The lack of focus on key homonymous points renders the representations of such methods vulnerable to challenging conditions, including reflections and weak textures. Furthermore, a slow computing speed is not conducive to the application. To overcome these difficulties, we present the \textbf{H}adamard \textbf{A}ttention \textbf{R}ecurrent Stereo \textbf{T}ransformer (HART) that incorporates the following components: 1) For faster inference, we present a Hadamard product paradigm for the attention mechanism, achieving linear computational complexity. 2) We designed a Dense Attention Kernel (DAK) to amplify the differences between relevant and irrelevant feature responses. This allows HART to focus on important details. DAK also converts zero elements to non-zero elements to mitigate the reduced expressiveness caused by the low-rank bottleneck. 3) To compensate for the spatial and channel interaction missing in the Hadamard product, we propose MKOI to capture both global and local information through the interleaving of large and small kernel convolutions. Experimental results demonstrate the effectiveness of our HART. In reflective area, HART ranked \textbf{1st} on the KITTI 2012 benchmark among all published methods at the time of submission. Code is available at \url{https://github.com/ZYangChen/HART}.	 | 鉴于变压器技术的进步，现有研究提出了构建立体变压器作为解决双眼立体匹配挑战的潜在解决方案。然而，由于注意力机制受到低秩瓶颈和二次复杂度的制约，立体变压器在合理推断时间内仍无法展现出足够的非线性表现力。此外，这些方法未能集中在关键同名点上，使得在反射和弱纹理等复杂条件下，其表示易受干扰。此外，计算速度慢也不利于实际应用。为克服这些难题，我们提出了一种 \textbf{Hadamard} \textbf{Attention} \textbf{Recurrent} Stereo \textbf{T}ransformer (HART)，并整合了以下组件：1) 为了实现更快的推理，我们提出了 Hadamard 乘积范式，实现了线性计算复杂度。2) 我们设计了一种密集注意力核 (DAK) 来放大相关和不相关特征响应之间的差异，这使 HART 能够专注于重要细节。DAK 还将零元素转换为非零元素，以缓解低秩瓶颈导致的表达力不足。3) 为弥补 Hadamard 乘积中缺失的空间和通道交互，我们提出了 MKOI，通过大核和小核卷积的交替使用来捕获全局和局部信息。实验结果证明了我们提出的 HART 的有效性。在反射区域，HART 在提交时在 KITTI 2012 基准测试中排名第一。代码可在 \url{https://github.com/ZYangChen/HART} 获取。	 | 本文提出了一种名为Hadamard Attention Recurrent Stereo Transformer (HART)的方法，以解决立体匹配中的非线性表示不足和计算效率低等问题。HART通过引入Hadamard乘积、密集注意力核（DAK）和多尺度卷积交互（MKOI）等组件，实现了线性计算复杂度、增强了特征响应差异，并提高了模型在复杂条件下的表现力。
2501.01022	 | Efficient Connectivity-Preserving Instance Segmentation with Supervoxel-Based Loss Function	 | Anna Grim,Jayaram Chandrashekar,Uygar Sumbul	 | Reconstructing the intricate local morphology of neurons and their long-range projecting axons can address many connectivity related questions in neuroscience. The main bottleneck in connectomics pipelines is correcting topological errors, as multiple entangled neuronal arbors is a challenging instance segmentation problem. More broadly, segmentation of curvilinear, filamentous structures continues to pose significant challenges. To address this problem, we extend the notion of simple points from digital topology to connected sets of voxels (i.e. supervoxels) and propose a topology-aware neural network segmentation method with minimal computational overhead. We demonstrate its effectiveness on a new public dataset of 3-d light microscopy images of mouse brains, along with the benchmark datasets DRIVE, ISBI12, and CrackTree.	 | 神经元及其长距离投射轴突的复杂局部形态重建可以解决神经科学中的许多连接相关问题。连接组学管道中的主要瓶颈在于修正拓扑错误，因为多个纠缠的神经树突是具有挑战性的实例分割问题。更广泛地说，曲线条状结构的分割仍然存在重大挑战。为了解决这个问题，我们将简单点的概念从数字拓扑扩展到连接的体素集合（即超级体素），并提出了一种具有最小计算开销的拓扑感知神经网络分割方法。我们通过新的公开的三维荧光显微镜图像小鼠大脑数据集以及基准数据集DRIVE、ISBI12和CrackTree，展示了其有效性。	 | 该研究针对神经科学中连接组学管道中的主要瓶颈——修正拓扑错误，提出了一种新的拓扑感知分割方法，利用连接的体素集合（超级体素）代替简单点，有效解决了神经元及其轴突的复杂形态重建问题，并在多个数据集上验证了其有效性。
2501.01015	 | Boosting Adversarial Transferability with Spatial Adversarial Alignment	 | Zhaoyu Chen,Haijing Guo,Kaixun Jiang,Jiyuan Fu,Xinyu Zhou,Dingkang Yang,Hao Tang,Bo Li,Wenqiang Zhang	 | Deep neural networks are vulnerable to adversarial examples that exhibit transferability across various models. Numerous approaches are proposed to enhance the transferability of adversarial examples, including advanced optimization, data augmentation, and model modifications. However, these methods still show limited transferability, particularly in cross-architecture scenarios, such as from CNN to ViT. To achieve high transferability, we propose a technique termed Spatial Adversarial Alignment (SAA), which employs an alignment loss and leverages a witness model to fine-tune the surrogate model. Specifically, SAA consists of two key parts: spatial-aware alignment and adversarial-aware alignment. First, we minimize the divergences of features between the two models in both global and local regions, facilitating spatial alignment. Second, we introduce a self-adversarial strategy that leverages adversarial examples to impose further constraints, aligning features from an adversarial perspective. Through this alignment, the surrogate model is trained to concentrate on the common features extracted by the witness model. This facilitates adversarial attacks on these shared features, thereby yielding perturbations that exhibit enhanced transferability. Extensive experiments on various architectures on ImageNet show that aligned surrogate models based on SAA can provide higher transferable adversarial examples, especially in cross-architecture attacks.	 | 深度神经网络对于跨不同模型具有转移性的对抗样本是脆弱的。提出了多种方法来增强对抗样本的转移性，包括高级优化、数据增强和模型修改。然而，这些方法在跨架构场景下仍然显示有限的转移性，例如从CNN到ViT的转移。为了实现高转移性，我们提出了一种称为空间对抗对齐（SAA，Spatial Adversarial Alignment）的技术，该技术利用对齐损失并利用见证模型来微调代理模型。具体来说，SAA 包含两个关键部分：空间感知对齐和对抗感知对齐。首先，我们通过在全局和局部区域最小化两个模型之间的特征差异，促进空间对齐。其次，我们引入了一种自我对抗策略，利用对抗样本进一步施加约束，从对抗的角度对齐特征。通过这种对齐，代理模型被训练以专注于见证模型提取的共同特征。这促进了对这些共享特征的对抗攻击，从而产生表现出增强转移性的扰动。在ImageNet上各种架构上的大量实验表明，基于SAA的对齐代理模型可以提供更高的可转移对抗样本，特别是在跨架构攻击中。	 | 该研究提出了一种称为空间对抗对齐（SAA）的技术，通过利用对齐损失和见证模型来微调代理模型，从而增强对抗样本在不同架构之间的转移性。实验结果表明，基于SAA的代理模型在ImageNet上能产生更高的可转移对抗样本，尤其在跨架构攻击中表现更佳。
2501.01003	 | EasySplat: View-Adaptive Learning makes 3D Gaussian Splatting Easy	 | Ao Gao,Luosong Guo,Tao Chen,Zhao Wang,Ying Tai,Jian Yang,Zhenyu Zhang	 | 3D Gaussian Splatting (3DGS) techniques have achieved satisfactory 3D scene representation. Despite their impressive performance, they confront challenges due to the limitation of structure-from-motion (SfM) methods on acquiring accurate scene initialization, or the inefficiency of densification strategy. In this paper, we introduce a novel framework EasySplat to achieve high-quality 3DGS modeling. Instead of using SfM for scene initialization, we employ a novel method to release the power of large-scale pointmap approaches. Specifically, we propose an efficient grouping strategy based on view similarity, and use robust pointmap priors to obtain high-quality point clouds and camera poses for 3D scene initialization. After obtaining a reliable scene structure, we propose a novel densification approach that adaptively splits Gaussian primitives based on the average shape of neighboring Gaussian ellipsoids, utilizing KNN scheme. In this way, the proposed method tackles the limitation on initialization and optimization, leading to an efficient and accurate 3DGS modeling. Extensive experiments demonstrate that EasySplat outperforms the current state-of-the-art (SOTA) in handling novel view synthesis.	 | 3D高斯点绘制（3DGS）技术已经实现了令人满意的3D场景表示。尽管它们的性能令人印象深刻，但由于结构从运动（SfM）方法在获取准确场景初始化方面的局限性，或者密集化策略的低效率，它们仍然面临挑战。本文中，我们介绍了一种名为EasySplat的新框架，以实现高质量的3DGS建模。我们不再使用SfM进行场景初始化，而是采用一种新颖的方法来释放大规模点云方法的潜力。具体来说，我们提出了一种基于视图相似性的高效分组策略，并利用鲁棒的点云先验来获取高质量的点云和相机姿态以进行3D场景初始化。在获得可靠的场景结构后，我们提出了一种新的密集化方法，该方法基于邻近高斯椭球的平均形状自适应分裂高斯原语，并利用KNN方案。通过这种方式，所提出的方法解决了初始化和优化的限制，从而实现了高效且准确的3DGS建模。广泛的实验表明，EasySplat在处理新颖视图合成方面优于当前最先进的技术（SOTA）。	 | EasySplat是一种新的框架，通过采用基于视图相似性的高效分组策略和利用点云先验来初始化场景结构，并提出一种自适应分裂高斯原语的新密集化方法，从而实现高效且准确的3D高斯点绘制建模，在处理新颖视图合成方面优于当前最先进的技术。
2501.00975	 | CoordFlow: Coordinate Flow for Pixel-wise Neural Video Representation	 | Daniel Silver,Ron Kimmel	 | In the field of video compression, the pursuit for better quality at lower bit rates remains a long-lasting goal. Recent developments have demonstrated the potential of Implicit Neural Representation (INR) as a promising alternative to traditional transform-based methodologies. Video INRs can be roughly divided into frame-wise and pixel-wise methods according to the structure the network outputs. While the pixel-based methods are better for upsampling and parallelization, frame-wise methods demonstrated better performance. We introduce CoordFlow, a novel pixel-wise INR for video compression. It yields state-of-the-art results compared to other pixel-wise INRs and on-par performance compared to leading frame-wise techniques. The method is based on the separation of the visual information into visually consistent layers, each represented by a dedicated network that compensates for the layer's motion. When integrated, a byproduct is an unsupervised segmentation of video sequence. Objects motion trajectories are implicitly utilized to compensate for visual-temporal redundancies. Additionally, the proposed method provides inherent video upsampling, stabilization, inpainting, and denoising capabilities.	 | 在视频压缩领域，追求在更低位率下获得更好的质量是一个长期的目标。近期的研究表明，显式神经表示（INR）作为一种替代传统变换方法的有前途的选择具有很大的潜力。视频INR可以大致分为框架级和像素级方法，根据网络输出的结构进行分类。尽管像素级方法在上采样和并行处理方面表现更佳，但框架级方法在性能上更为出色。我们提出了一种名为CoordFlow的新型像素级INR方法，它在与其他像素级INR方法相比时取得了最先进的结果，并在与其他领先框架级技术相比时提供了可比的性能。该方法基于视觉信息的分离，将视觉信息分成视觉上一致的层，每层由专门的网络表示并补偿该层的运动。当结合使用时，这会产生一个无监督的视频序列分割。对象运动轨迹被隐式利用来补偿视觉时序冗余。此外，该方法还提供了固有的视频上采样、稳定化、修补和去噪能力。	 | CoordFlow是一种新型的像素级显式神经表示方法，它在视频压缩中实现了最先进的结果，并提供了无监督的视频分割、对象运动补偿、上采样、稳定化、修补和去噪等多种功能。
2501.00962	 | OASIS Uncovers: High-Quality T2I Models, Same Old Stereotypes	 | Sepehr Dehdashtian,Gautam Sreekumar,Vishnu Naresh Boddeti	 | Images generated by text-to-image (T2I) models often exhibit visual biases and stereotypes of concepts such as culture and profession. Existing quantitative measures of stereotypes are based on statistical parity that does not align with the sociological definition of stereotypes and, therefore, incorrectly categorizes biases as stereotypes. Instead of oversimplifying stereotypes as biases, we propose a quantitative measure of stereotypes that aligns with its sociological definition. We then propose OASIS to measure the stereotypes in a generated dataset and understand their origins within the T2I model. OASIS includes two scores to measure stereotypes from a generated image dataset: (M1) Stereotype Score to measure the distributional violation of stereotypical attributes, and (M2) WALS to measure spectral variance in the images along a stereotypical attribute. OASIS also includes two methods to understand the origins of stereotypes in T2I models: (U1) StOP to discover attributes that the T2I model internally associates with a given concept, and (U2) SPI to quantify the emergence of stereotypical attributes in the latent space of the T2I model during image generation. Despite the considerable progress in image fidelity, using OASIS, we conclude that newer T2I models such as FLUX.1 and SDv3 contain strong stereotypical predispositions about concepts and still generate images with widespread stereotypical attributes. Additionally, the quantity of stereotypes worsens for nationalities with lower Internet footprints.	 | 由文本生成图像（T2I）模型生成的图像常常体现出文化和职业等概念上的视觉偏见和刻板印象。现有的刻板印象量化度量基于统计平等，这与社会学意义上的刻板印象定义不符，因此错误地将偏见归类为刻板印象。我们不简单地将刻板印象归结为偏见，而是提出了一种与社会学定义相一致的刻板印象量化度量方法。随后，我们提出了OASIS，用于测量生成数据集中的刻板印象，并理解这些刻板印象在T2I模型中的起源。OASIS 包括两个用于测量生成图像数据集中刻板印象的评分标准：(M1) 刻板印象分数，用于衡量刻板印象属性的分布偏差；(M2) WALS，用于衡量图像在某一刻板印象属性方向上的光谱方差。OASIS 还包括两种方法来理解T2I模型中刻板印象的起源：(U1) StOP，用于发现T2I模型内部与给定概念关联的属性；(U2) SPI，用于量化这些刻板印象属性在T2I模型的潜在空间中生成图像期间的出现。尽管在图像保真度方面取得了显著进展，使用OASIS，我们得出结论，诸如FLUX.1和SDv3等较新的T2I模型仍然含有强烈的概念刻板印象倾向，并且生成的图像中广泛存在刻板印象属性。此外，刻板印象的数量对于网络足迹较低的国家来说更为严重。	 | 该研究提出了一种新的量化方法OASIS，用于测量文本生成图像模型中刻板印象的分布，并分析这些刻板印象的来源，发现即使在图像保真度有所提升的情况下，当前的T2I模型仍然存在显著的刻板印象倾向，特别是对于网络足迹较低的国家更为严重。
2501.00958	 | 2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining	 | Wenqi Zhang,Hang Zhang,Xin Li,Jiashuo Sun,Yongliang Shen,Weiming Lu,Deli Zhao,Yueting Zhuang,Lidong Bing	 | Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality \textbf{multimodal textbook} corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving~\footnote{Our code are available at \url{https://github.com/DAMO-NLP-SG/multimodal_textbook}}.	 | 与图像-文本配对数据相比，交错语料库使视觉语言模型（VLMs）能够像人类一样更加自然地理解世界。然而，现有的这些数据集是从网页中抓取的，面临着知识密度低、图像-文本关系松散以及图像之间逻辑连贯性差等挑战。另一方面，互联网上充斥着大量的教学视频（例如，线上几何课程），这些视频被广泛用于人类学习基础学科，然而这些宝贵资源在VLM训练中却仍未得到充分探索。在本文中，我们介绍了一个高质量的多模态教科书语料库，以丰富VLM预训练的基础知识。该语料库收集了超过两年半的教学视频，总计22000课时。我们首先使用LLM提出的一套分类体系系统地收集教学视频。然后，我们逐步从视频中提取并精炼出视觉（关键帧）、音频（自动语音识别，ASR）和文本知识（光学字符识别，OCR），并根据时间顺序组织成图像-文本交错的语料库。与同类数据集相比，我们以视频为中心的教科书提供了更加连贯的上下文、更丰富的知识以及更好的图像-文本对齐。实验结果证明了其卓越的预训练性能，尤其是在科学问答（ScienceQA）和数学视野（MathVista）等知识密集型和推理密集型任务中表现出色。此外，基于我们教科书预训练的VLM在解决任务时能够充分利用视觉和文本线索，在少量示例中表现出出色的交错上下文感知能力。我们的代码可在以下链接获取：\url{https://github.com/DAMO-NLP-SG/multimodal_textbook}。	 | 本文介绍了一个高质量的多模态教科书语料库，该语料库收集并整理了来自教学视频的大量视觉、音频和文本数据，旨在提高视觉语言模型的预训练性能，特别是在知识密集型任务中表现出色。通过以视频为中心的方法，该数据集提供了更加连贯的上下文和更好的图像-文本对齐，从而提升了模型的交错上下文感知能力。
2501.00946	 | Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant Computation Elimination in Diffusion Model	 | Omid Saghatchian,Atiyeh Gh. Moghadam,Ahmad Nickabadi	 | Diffusion models have emerged as a promising approach for generating high-quality, high-dimensional images. Nevertheless, these models are hindered by their high computational cost and slow inference, partly due to the quadratic computational complexity of the self-attention mechanisms with respect to input size. Various approaches have been proposed to address this drawback. One such approach focuses on reducing the number of tokens fed into the self-attention, known as token merging (ToMe). In our method, which is called cached adaptive token merging(CA-ToMe), we calculate the similarity between tokens and then merge the r proportion of the most similar tokens. However, due to the repetitive patterns observed in adjacent steps and the variation in the frequency of similarities, we aim to enhance this approach by implementing an adaptive threshold for merging tokens and adding a caching mechanism that stores similar pairs across several adjacent steps. Empirical results demonstrate that our method operates as a training-free acceleration method, achieving a speedup factor of 1.24 in the denoising process while maintaining the same FID scores compared to existing approaches.	 | 扩散模型已经成为了生成高质量、高维度图像的一种有前景的方法。然而，这些模型受到了高计算成本和慢推理速度的阻碍，部分原因是自我注意力机制在输入大小上的二次计算复杂度。为了应对这一问题，已经提出了一些方法。其中一种方法侧重于减少输入自我注意力的令牌数量，这种方法被称为令牌合并（ToMe）。在我们提出的方法，称为缓存自适应令牌合并（CA-ToMe）中，我们首先计算令牌之间的相似性，然后合并最相似令牌的比例为r。然而，由于相邻步骤中观察到的重复模式以及相似性的频率变化，我们希望通过引入自适应合并阈值和缓存机制来改进这种方法。缓存机制可以存储多个相邻步骤中的相似对。实验结果表明，我们的方法可以作为一种无需训练的加速方法，在去噪过程中实现了1.24倍的加速因子，同时保持与现有方法相同的FID分数。	 | 扩散模型虽能生成高质量的高维度图像，但受限于高计算成本和慢推理速度，特别是自我注意力机制的二次复杂度。为此，提出了一种名为缓存自适应令牌合并（CA-ToMe）的方法，通过引入自适应合并阈值和缓存机制优化令牌合并过程，在保持与现有方法相同生成质量的前提下，实现了1.24倍的加速。
2501.00944	 | Diffusion Prism: Enhancing Diversity and Morphology Consistency in Mask-to-Image Diffusion	 | Hao Wang,Xiwen Chen,Ashish Bastola,Jiayou Qin,Abolfazl Razi	 | The emergence of generative AI and controllable diffusion has made image-to-image synthesis increasingly practical and efficient. However, when input images exhibit low entropy and sparse, the inherent characteristics of diffusion models often result in limited diversity. This constraint significantly interferes with data augmentation. To address this, we propose Diffusion Prism, a training-free framework that efficiently transforms binary masks into realistic and diverse samples while preserving morphological features. We explored that a small amount of artificial noise will significantly assist the image-denoising process. To prove this novel mask-to-image concept, we use nano-dendritic patterns as an example to demonstrate the merit of our method compared to existing controllable diffusion models. Furthermore, we extend the proposed framework to other biological patterns, highlighting its potential applications across various fields.	 | 生成式AI和可控扩散的出现使得从图像到图像的合成变得越来越实际和高效。然而，当输入图像表现出低熵和稀疏性时，扩散模型的固有特性往往会限制多样性的产生，从而显著干扰数据增强。为了解决这一问题，我们提出了一种无需训练的框架——扩散棱镜（Diffusion Prism），该框架能够高效地将二值掩码转化为现实且多样的样本，同时保留形态特征。我们发现，少量的人工噪声能显著帮助图像去噪过程。为了证明这一新颖的掩码到图像的概念，我们使用纳米树状图案作为例子，展示了我们方法相较于现有可控扩散模型的优势。此外，我们还将提出的框架扩展到其他生物模式，突显其在多个领域的潜在应用价值。	 | 扩散棱镜（Diffusion Prism）是一种无需训练的框架，能高效地将二值掩码转换为多样且现实的图像样本，同时保留形态特征；该框架通过引入少量人工噪声有效提升去噪效果，并在纳米树状图案等生物模式中展示了优于现有可控扩散模型的优势。
2501.00935	 | Multiscaled Multi-Head Attention-based Video Transformer Network for Hand Gesture Recognition	 | Mallika Garg,Debashis Ghosh,Pyari Mohan Pradhan	 | Dynamic gesture recognition is one of the challenging research areas due to variations in pose, size, and shape of the signer's hand. In this letter, Multiscaled Multi-Head Attention Video Transformer Network (MsMHA-VTN) for dynamic hand gesture recognition is proposed. A pyramidal hierarchy of multiscale features is extracted using the transformer multiscaled head attention model. The proposed model employs different attention dimensions for each head of the transformer which enables it to provide attention at the multiscale level. Further, in addition to single modality, recognition performance using multiple modalities is examined. Extensive experiments demonstrate the superior performance of the proposed MsMHA-VTN with an overall accuracy of 88.22\% and 99.10\% on NVGesture and Briareo datasets, respectively.	 | 动态手势识别是一个极具挑战性的研究领域，由于手语者的手部姿态、大小和形状的变化。在这封信中，我们提出了用于动态手势识别的多尺度多头注意力视频 transformer 网络（MsMHA-VTN）。该模型使用变压器多尺度头注意力模型提取多尺度特征的分层金字塔。所提出的模型在每个变压器头中使用不同的注意力维度，使其能够在多尺度级别提供注意力。此外，除了单模态外，还检查了使用多模态的识别性能。广泛的实验表明，提出的 MsMHA-VTN 在 NVGesture 数据集上的整体准确率为 88.22%，在 Briareo 数据集上的整体准确率为 99.10%，表现出优越的性能。	 | 本文提出了一种名为MsMHA-VTN的多尺度多头注意力视频Transformer网络，用于动态手势识别，该模型在NVGesture和Briareo数据集上分别取得了88.22%和99.10%的整体准确率，显示出优越的性能。
2501.00917	 | Hierarchical Vision-Language Alignment for Text-to-Image Generation via Diffusion Models	 | Emily Johnson,Noah Wilson	 | Text-to-image generation has witnessed significant advancements with the integration of Large Vision-Language Models (LVLMs), yet challenges remain in aligning complex textual descriptions with high-quality, visually coherent images. This paper introduces the Vision-Language Aligned Diffusion (VLAD) model, a generative framework that addresses these challenges through a dual-stream strategy combining semantic alignment and hierarchical diffusion. VLAD utilizes a Contextual Composition Module (CCM) to decompose textual prompts into global and local representations, ensuring precise alignment with visual features. Furthermore, it incorporates a multi-stage diffusion process with hierarchical guidance to generate high-fidelity images. Experiments conducted on MARIO-Eval and INNOVATOR-Eval benchmarks demonstrate that VLAD significantly outperforms state-of-the-art methods in terms of image quality, semantic alignment, and text rendering accuracy. Human evaluations further validate the superior performance of VLAD, making it a promising approach for text-to-image generation in complex scenarios.	 | 文本到图像生成在大型视觉-语言模型（LVLMs）集成后取得了显著进展，但仍面临将复杂文本描述与高质量、视觉一致性高的图像对齐的挑战。本文引入了一种名为Vision-Language Aligned Diffusion（VLAD）的生成框架，通过结合语义对齐和分层扩散的双流策略来应对这些挑战。VLAD 利用上下文组合模块（CCM）将文本提示分解为全局和局部表示，确保与视觉特征的精确对齐。此外，VLAD 还包含一个具有分层指导的多阶段扩散过程，用于生成高保真度图像。在 MARIO-Eval 和 INNOVATOR-Eval 基准测试中进行的实验表明，VLAD 在图像质量、语义对齐和文本渲染准确性方面显著优于现有最先进的方法。进一步的人类评估进一步验证了 VLAD 的优越性能，使其成为复杂场景下文本到图像生成的一个有前途的方法。文本到图像生成在大型视觉-语言模型（LVLMs）的集成后取得了显著进展，但仍面临将复杂文本描述与高质量、视觉一致性高的图像对齐的挑战。本文引入了Vision-Language Aligned Diffusion（VLAD）模型，这是一种生成框架，通过结合语义对齐和分层扩散的双流策略来应对这些挑战。VLAD 利用上下文组合模块（CCM）将文本提示分解为全局和局部表示，确保与视觉特征的精确对齐。此外，VLAD 还引入了一个多阶段的分层扩散过程，以生成高保真度图像。在 MARIO-Eval 和 INNOVATOR-Eval 基准测试中进行的实验表明，VLAD 在图像质量、语义对齐和文本渲染准确性方面显著优于现有最先进的方法。进一步的人类评估进一步验证了VLAD的优越性能，使其成为复杂场景下文本到图像生成的一个有前途的方法。	 | 本文提出了一种名为Vision-Language Aligned Diffusion（VLAD）的生成框架，通过结合语义对齐和分层扩散策略，有效解决了复杂文本描述与高质量、视觉一致图像对齐的挑战。实验结果显示，VLAD在多个基准测试中显著优于现有方法，并得到了人类评估的进一步验证。
2501.00912	 | AutoPresent: Designing Structured Visuals from Scratch	 | Jiaxin Ge,Zora Zhiruo Wang,Xuhui Zhou,Yi-Hao Peng,Sanjay Subramanian,Qinyue Tan,Maarten Sap,Alane Suhr,Daniel Fried,Graham Neubig,Trevor Darrell	 | Designing structured visuals such as presentation slides is essential for communicative needs, necessitating both content creation and visual planning skills. In this work, we tackle the challenge of automated slide generation, where models produce slide presentations from natural language (NL) instructions. We first introduce the SlidesBench benchmark, the first benchmark for slide generation with 7k training and 585 testing examples derived from 310 slide decks across 10 domains. SlidesBench supports evaluations that are (i)reference-based to measure similarity to a target slide, and (ii)reference-free to measure the design quality of generated slides alone. We benchmark end-to-end image generation and program generation methods with a variety of models, and find that programmatic methods produce higher-quality slides in user-interactable formats. Built on the success of program generation, we create AutoPresent, an 8B Llama-based model trained on 7k pairs of instructions paired with code for slide generation, and achieve results comparable to the closed-source model GPT-4o. We further explore iterative design refinement where the model is tasked to self-refine its own output, and we found that this process improves the slide's quality. We hope that our work will provide a basis for future work on generating structured visuals.	 | 设计结构化视觉元素，如演示文稿幻灯片，对于交流需求至关重要，这不仅需要内容创作能力，还需要视觉规划技能。在这项工作中，我们面对自动幻灯片生成的挑战，即模型根据自然语言（NL）指令生成幻灯片演示文稿。我们首先介绍了SlidesBench基准，这是首个用于幻灯片生成的基准，包含7000个训练样本和585个测试样本，这些样本来自涵盖10个领域的310个幻灯片集。SlidesBench 支持以下两种类型的评估：（i）基于参考的评估，衡量生成幻灯片与目标幻灯片的相似性；（ii）非基于参考的评估，单独衡量生成幻灯片的设计质量。我们使用多种模型对端到端的图像生成和程序生成方法进行了基准测试，并发现程序化方法能够生成更高质量的、可供用户交互的幻灯片。基于程序生成的成功，我们构建了AutoPresent，这是一个基于8B Llama模型的模型，并通过7000对指令与用于幻灯片生成的代码配对进行训练，其结果可与闭源模型GPT-4o相媲美。我们进一步探索了一个迭代设计细化的过程，即模型的任务是自我改进其输出，发现这一过程能够提高幻灯片的质量。我们希望我们的工作能够为未来的结构化视觉生成研究提供基础。	 | 该研究提出了一个名为SlidesBench的新基准，用于自动幻灯片生成，并使用多种模型测试了端到端的方法，发现程序化方法生成的幻灯片质量更高，随后开发了AutoPresent模型，展示了与闭源模型GPT-4o相当的能力，并通过迭代设计细化过程进一步提升了幻灯片质量。
2501.00895	 | Text2Earth: Unlocking Text-driven Remote Sensing Image Generation with a Global-Scale Dataset and a Foundation Model	 | Chenyang Liu,Keyan Chen,Rui Zhao,Zhengxia Zou,Zhenwei Shi	 | Generative foundation models have advanced large-scale text-driven natural image generation, becoming a prominent research trend across various vertical domains. However, in the remote sensing field, there is still a lack of research on large-scale text-to-image (text2image) generation technology. Existing remote sensing image-text datasets are small in scale and confined to specific geographic areas and scene types. Besides, existing text2image methods have struggled to achieve global-scale, multi-resolution controllable, and unbounded image generation. To address these challenges, this paper presents two key contributions: the Git-10M dataset and the Text2Earth foundation model. Git-10M is a global-scale image-text dataset comprising 10 million image-text pairs, 5 times larger than the previous largest one. The dataset covers a wide range of geographic scenes and contains resolution information, significantly surpassing existing datasets in both size and diversity. Building on Git-10M, we propose Text2Earth, a 1.3 billion parameter generative foundation model based on the diffusion framework to model global-scale remote sensing scenes. Text2Earth integrates a resolution guidance mechanism, enabling users to specify image resolutions. A dynamic condition adaptation strategy is proposed for training and inference to improve image quality. Text2Earth excels in zero-shot text2image generation and demonstrates robust generalization and flexibility across multiple tasks, including unbounded scene construction, image editing, and cross-modal image generation. This robust capability surpasses previous models restricted to the basic fixed size and limited scene types. On the previous benchmark dataset, Text2Earth outperforms previous models with an improvement of +26.23 FID and +20.95% Zero-shot Cls-OA metric.Our project page is \url{https://chen-yang-liu.github.io/Text2Earth}	 | 生成型基础模型在大规模文本驱动自然图像生成方面取得了显著进展，成为跨多个垂直领域的重要研究趋势。然而，在遥感领域，大规模的文本到图像（文本2图像）生成技术的研究仍然不足。现有的遥感图像-文本数据集规模较小，局限于特定的地理区域和场景类型。此外，现有的文本2图像方法难以实现全球规模、多分辨率可控和无限制的图像生成。为应对这些挑战，本文提出了两项主要贡献：Git-10M数据集和Text2Earth基础模型。Git-10M是一个包含1000万图像-文本对的全球规模图像-文本数据集，规模是之前最大的五倍。该数据集涵盖了广泛的地理场景，并包含分辨率信息，大大超过了现有数据集在规模和多样性上的表现。基于Git-10M，我们提出了Text2Earth，这是一个基于扩散框架的1.3亿参数生成型基础模型，用于建模全球规模的遥感场景。Text2Earth集成了分辨率指导机制，允许用户指定图像分辨率。我们提出了一种动态条件自适应策略，用于训练和推断以提高图像质量。Text2Earth在零样本文本2图像生成中表现出色，并且在多个任务（包括无限制场景构建、图像编辑和跨模态图像生成）中展现出强大的泛化能力和灵活性。这种稳健的能力超越了之前仅限于基本固定尺寸和有限场景类型的模型。在之前的基准数据集上，Text2Earth以+26.23 FID和+20.95% Zero-shot Cls-OA指标优于之前模型。我们的项目页面为 \url{https://chen-yang-liu.github.io/Text2Earth}。	 | 本文提出了一个名为Git-10M的全球规模图像-文本数据集和一个基于扩散框架的生成模型Text2Earth，以解决遥感领域文本到图像生成的不足，并展示了在图像质量和多样性方面的显著提升。Text2Earth能够生成多分辨率的遥感图像，并在多个任务中表现出色，优于现有模型。
2501.00882	 | FullTransNet: Full Transformer with Local-Global Attention for Video Summarization	 | Libin Lan,Lu Jiang,Tianshu Yu,Xiaojuan Liu,Zhongshi He	 | Video summarization mainly aims to produce a compact, short, informative, and representative synopsis of raw videos, which is of great importance for browsing, analyzing, and understanding video content. Dominant video summarization approaches are generally based on recurrent or convolutional neural networks, even recent encoder-only transformers. We propose using full transformer as an alternative architecture to perform video summarization. The full transformer with an encoder-decoder structure, specifically designed for handling sequence transduction problems, is naturally suitable for video summarization tasks. This work considers supervised video summarization and casts it as a sequence-to-sequence learning problem. Our key idea is to directly apply the full transformer to the video summarization task, which is intuitively sound and effective. Also, considering the efficiency problem, we replace full attention with the combination of local and global sparse attention, which enables modeling long-range dependencies while reducing computational costs. Based on this, we propose a transformer-like architecture, named FullTransNet, which has a full encoder-decoder structure with local-global sparse attention for video summarization. Specifically, both the encoder and decoder in FullTransNet are stacked the same way as ones in the vanilla transformer, and the local-global sparse attention is used only at the encoder side. Extensive experiments on two public multimedia benchmark datasets SumMe and TVSum demonstrate that our proposed model can outperform other video summarization approaches, achieving F-Measures of 54.4% on SumMe and 63.9% on TVSum with relatively lower compute and memory requirements, verifying its effectiveness and efficiency. The code and models are publicly available on GitHub.	 | 视频摘要的主要目标是生成一个紧凑、简短、信息丰富且具有代表性的视频原文摘要，这对于浏览、分析和理解视频内容至关重要。主导的视频摘要方法通常基于循环神经网络或卷积神经网络，即使是最近的仅编码器变换器。我们提出使用全变换器作为替代架构来进行视频摘要。全变换器具有编码器-解码器结构，专门设计用于处理序列转换问题，因此天然适合视频摘要任务。本文考虑监督视频摘要，并将其表述为序列到序列学习问题。我们的主要想法是直接将全变换器应用于视频摘要任务，这直观且有效。此外，考虑到效率问题，我们用局部和全局稀疏注意力的组合替代全注意力，这能够建模长距离依赖关系并减少计算成本。在此基础上，我们提出了一种类似变换器的架构，称为FullTransNet，它具有全编码器-解码器结构，并且使用局部-全局稀疏注意力来处理视频摘要。具体来说，FullTransNet的编码器和解码器都以与常规变换器相同的方式堆叠，并且仅在编码器侧使用局部-全局稀疏注意力。在两个公开的多媒体基准数据集SumMe和TVSum上的广泛实验表明，我们提出的方法可以优于其他视频摘要方法，在SumMe上实现54.4%的F-Measure，在TVSum上实现63.9%的F-Measure，同时计算和内存需求相对较低，验证了其有效性和效率。代码和模型已公开发布在GitHub上。	 | 本文提出了一种名为FullTransNet的全变换器架构用于视频摘要，通过局部和全局稀疏注意力机制解决长距离依赖问题，并在SumMe和TVSum数据集上取得了优于现有方法的性能。
2501.00880	 | Improving Autoregressive Visual Generation with Cluster-Oriented Token Prediction	 | Teng Hu,Jiangning Zhang,Ran Yi,Jieyu Weng,Yabiao Wang,Xianfang Zeng,Zhucun Xue,Lizhuang Ma	 | Employing LLMs for visual generation has recently become a research focus. However, the existing methods primarily transfer the LLM architecture to visual generation but rarely investigate the fundamental differences between language and vision. This oversight may lead to suboptimal utilization of visual generation capabilities within the LLM framework. In this paper, we explore the characteristics of visual embedding space under the LLM framework and discover that the correlation between visual embeddings can help achieve more stable and robust generation results. We present IAR, an Improved AutoRegressive Visual Generation Method that enhances the training efficiency and generation quality of LLM-based visual generation models. Firstly, we propose a Codebook Rearrangement strategy that uses balanced k-means clustering algorithm to rearrange the visual codebook into clusters, ensuring high similarity among visual features within each cluster. Leveraging the rearranged codebook, we propose a Cluster-oriented Cross-entropy Loss that guides the model to correctly predict the cluster where the token is located. This approach ensures that even if the model predicts the wrong token index, there is a high probability the predicted token is located in the correct cluster, which significantly enhances the generation quality and robustness. Extensive experiments demonstrate that our method consistently enhances the model training efficiency and performance from 100M to 1.4B, reducing the training time by half while achieving the same FID. Additionally, our approach can be applied to various LLM-based visual generation models and adheres to the scaling law, providing a promising direction for future research in LLM-based visual generation.	 | 近年来，使用大规模语言模型（LLM）进行视觉生成已成为研究热点。然而，现有方法主要将LLM架构转移到视觉生成领域，而很少探讨语言与视觉之间的根本差异。这种忽视可能导致在LLM框架中未能充分利用视觉生成能力。本文探讨了在LLM框架下视觉嵌入空间的特性，并发现视觉嵌入之间的相关性有助于实现更稳定和可靠的生成结果。我们提出了IAR（Improved AutoRegressive Visual Generation Method），一种改进的自回归视觉生成方法，旨在提高基于LLM的视觉生成模型的训练效率和生成质量。首先，我们提出了一种码本重整策略，利用平衡的k-means聚类算法重构视觉码本为集群，确保每个集群内部的视觉特征高度相似。利用重构的码本，我们提出了面向集群的交叉熵损失，引导模型正确预测标记所在的集群。这种方法确保即使模型预测了错误的标记索引，预测的标记也有很高的概率位于正确的集群中，从而显著提升生成质量和鲁棒性。大量实验表明，我们的方法能够从100M到1.4B的一致性地提高模型训练效率和性能，同时将训练时间减半，FID保持不变。此外，我们的方法可以应用于各种基于LLM的视觉生成模型，并遵循缩放定律，为未来LLM基视觉生成的研究提供了有前景的方向。	 | 本文探讨了在大规模语言模型框架下视觉生成的特性，提出了一种改进的自回归视觉生成方法IAR，通过重构视觉码本和引入面向集群的交叉熵损失，提高了生成质量和训练效率。实验结果表明，该方法能够显著提升基于LLM的视觉生成模型的性能，同时缩短训练时间。
2501.00877	 | FGAseg: Fine-Grained Pixel-Text Alignment for Open-Vocabulary Semantic Segmentation	 | Bingyu Li,Da Zhang,Zhiyuan Zhao,Junyu Gao,Xuelong Li	 | Open-vocabulary segmentation aims to identify and segment specific regions and objects based on text-based descriptions. A common solution is to leverage powerful vision-language models (VLMs), such as CLIP, to bridge the gap between vision and text information. However, VLMs are typically pretrained for image-level vision-text alignment, focusing on global semantic features. In contrast, segmentation tasks require fine-grained pixel-level alignment and detailed category boundary information, which VLMs alone cannot provide. As a result, information extracted directly from VLMs can't meet the requirements of segmentation tasks. To address this limitation, we propose FGAseg, a model designed for fine-grained pixel-text alignment and category boundary supplementation. The core of FGAseg is a Pixel-Level Alignment module that employs a cross-modal attention mechanism and a text-pixel alignment loss to refine the coarse-grained alignment from CLIP, achieving finer-grained pixel-text semantic alignment. Additionally, to enrich category boundary information, we introduce the alignment matrices as optimizable pseudo-masks during forward propagation and propose Category Information Supplementation module. These pseudo-masks, derived from cosine and convolutional similarity, provide essential global and local boundary information between different categories. By combining these two strategies, FGAseg effectively enhances pixel-level alignment and category boundary information, addressing key challenges in open-vocabulary segmentation. Extensive experiments demonstrate that FGAseg outperforms existing methods on open-vocabulary semantic segmentation benchmarks.	 | 开放词汇分割旨在根据文本描述识别并分割特定区域和物体。一种常见的解决方案是利用强大的多模态视觉-语言模型（VLMs），如CLIP，来弥合视觉与文本信息之间的差距。然而，VLMs通常是在图像级别的视觉-文本对齐上进行预训练，主要关注全局语义特征。相比之下，分割任务需要精细的像素级对齐以及详细的类别边界信息，仅靠VLMs是无法提供的。因此，从VLMs直接提取的信息无法满足分割任务的需求。为了解决这一限制，我们提出了FGAseg模型，专门用于精细的像素-文本对齐和类别边界补充。FGAseg的核心是一个像素级对齐模块，该模块采用跨模态注意力机制和文本-像素对齐损失，以改进CLIP的粗粒度对齐，实现更精细的像素-文本语义对齐。此外，为了丰富类别边界信息，我们在前向传播过程中引入可优化的伪掩码，并提出了类别信息补充模块。这些伪掩码来自余弦相似性和卷积相似性，提供了不同类别之间的关键全局和局部边界信息。通过结合这两种策略，FGAseg有效地增强了像素级对齐和类别边界信息，解决了开放词汇分割中的关键挑战。广泛的实验表明，FGAseg在开放词汇语义分割基准测试中优于现有方法。	 | FGAseg模型通过引入像素级对齐模块和类别信息补充模块，改进了多模态视觉-语言模型的粗粒度对齐，以实现更精细的像素-文本语义对齐和丰富的类别边界信息，从而在开放词汇语义分割任务中取得了优于现有方法的性能。
2501.00873	 | Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation	 | Mingjia Li,Shuang Li,Tongrui Su,Longhui Yuan,Jian Liang,Wei Li	 | Capitalizing on the complementary advantages of generative and discriminative models has always been a compelling vision in machine learning, backed by a growing body of research. This work discloses the hidden semantic structure within score-based generative models, unveiling their potential as effective discriminative priors. Inspired by our theoretical findings, we propose DUSA to exploit the structured semantic priors underlying diffusion score to facilitate the test-time adaptation of image classifiers or dense predictors. Notably, DUSA extracts knowledge from a single timestep of denoising diffusion, lifting the curse of Monte Carlo-based likelihood estimation over timesteps. We demonstrate the efficacy of our DUSA in adapting a wide variety of competitive pre-trained discriminative models on diverse test-time scenarios. Additionally, a thorough ablation study is conducted to dissect the pivotal elements in DUSA. Code is publicly available at https://github.com/BIT-DA/DUSA.	 | 利用生成模型和判别模型互补优势一直是机器学习中一个充满吸引力的理想。这一理念得到了越来越多研究的支持。本项工作揭示了基于评分的生成模型内部隐藏的语义结构，展示了其作为有效判别先验的潜力。受到我们理论发现的启发，我们提出了一种称为DUSA的方法，旨在利用扩散评分中的结构化语义先验来促进图像分类器或密集预测器在测试时的适应能力。值得注意的是，DUSA可以从去噪扩散的单个时间步中提取知识，克服基于蒙特卡洛方法的时间步长似然估计的局限性。我们在多种不同的测试场景中展示了DUSA在适应多种高性能先验判别模型方面的有效性。此外，我们还进行了详细的消融研究，以剖析DUSA中的关键组成部分。代码已在https://github.com/BIT-DA/DUSA公开可用。	 | 该研究揭示了基于评分的生成模型中的语义结构，并提出了一种称为DUSA的方法，利用扩散评分中的结构化语义先验来增强图像分类器或密集预测器的适应能力，特别是在测试时。
2501.00851	 | Scale-wise Bidirectional Alignment Network for Referring Remote Sensing Image Segmentation	 | Kun Li,George Vosselman,Michael Ying Yang	 | The goal of referring remote sensing image segmentation (RRSIS) is to extract specific pixel-level regions within an aerial image via a natural language expression. Recent advancements, particularly Transformer-based fusion designs, have demonstrated remarkable progress in this domain. However, existing methods primarily focus on refining visual features using language-aware guidance during the cross-modal fusion stage, neglecting the complementary vision-to-language flow. This limitation often leads to irrelevant or suboptimal representations. In addition, the diverse spatial scales of ground objects in aerial images pose significant challenges to the visual perception capabilities of existing models when conditioned on textual inputs. In this paper, we propose an innovative framework called Scale-wise Bidirectional Alignment Network (SBANet) to address these challenges for RRSIS. Specifically, we design a Bidirectional Alignment Module (BAM) with learnable query tokens to selectively and effectively represent visual and linguistic features, emphasizing regions associated with key tokens. BAM is further enhanced with a dynamic feature selection block, designed to provide both macro- and micro-level visual features, preserving global context and local details to facilitate more effective cross-modal interaction. Furthermore, SBANet incorporates a text-conditioned channel and spatial aggregator to bridge the gap between the encoder and decoder, enhancing cross-scale information exchange in complex aerial scenarios. Extensive experiments demonstrate that our proposed method achieves superior performance in comparison to previous state-of-the-art methods on the RRSIS-D and RefSegRS datasets, both quantitatively and qualitatively. The code will be released after publication.	 | 遥感图像分割的指针（RRSIS）的目标是通过自然语言表达从航空图像中提取特定的像素级区域。近年来，特别是基于Transformer的融合设计，已经在这一领域取得了显著的进展。然而，现有的方法主要集中在跨模态融合阶段利用语言感知引导来细化视觉特征上，忽视了视觉到语言的互补流程。这种局限性往往导致无关或次优的表示。此外，航空图像中地面物体的多样化空间尺度对现有模型在文本输入条件下进行视觉感知能力构成了重大挑战。在本文中，我们提出了一种创新框架，称为尺度级双向对齐网络（SBANet），以解决RRSIS中的这些挑战。具体而言，我们设计了一个带有可学习查询标记的双向对齐模块（BAM），以选择性地和有效地表示视觉和语言特征，强调与关键标记相关的区域。BAM 进一步通过一个动态特征选择块增强，旨在提供宏观和微观级别的视觉特征，保持全局上下文和局部细节，以促进更有效的跨模态交互。此外，SBANet引入了文本条件下的通道和空间聚合器，以缩小编码器和解码器之间的差距，增强在复杂航空场景中的多尺度信息交换。广泛的实验表明，我们的方法在RRSIS-D和RefSegRS数据集上，无论是定量还是定性地，都优于以前的最先进的方法。代码将在发表后发布。	 | 本文提出了一种名为尺度级双向对齐网络（SBANet）的新框架，以解决遥感图像分割中的挑战，该框架通过双向对齐模块和动态特征选择块增强了跨模态交互，提升了在复杂航空场景下的多尺度信息交换能力。实验结果表明，SBANet在RRSIS-D和RefSegRS数据集上显著优于现有方法。
2501.00848	 | IllusionBench: A Large-scale and Comprehensive Benchmark for Visual Illusion Understanding in Vision-Language Models	 | Yiming Zhang,Zicheng Zhang,Xinyi Wei,Xiaohong Liu,Guangtao Zhai,Xiongkuo Min	 | Current Visual Language Models (VLMs) show impressive image understanding but struggle with visual illusions, especially in real-world scenarios. Existing benchmarks focus on classical cognitive illusions, which have been learned by state-of-the-art (SOTA) VLMs, revealing issues such as hallucinations and limited perceptual abilities. To address this gap, we introduce IllusionBench, a comprehensive visual illusion dataset that encompasses not only classic cognitive illusions but also real-world scene illusions. This dataset features 1,051 images, 5,548 question-answer pairs, and 1,051 golden text descriptions that address the presence, causes, and content of the illusions. We evaluate ten SOTA VLMs on this dataset using true-or-false, multiple-choice, and open-ended tasks. In addition to real-world illusions, we design trap illusions that resemble classical patterns but differ in reality, highlighting hallucination issues in SOTA models. The top-performing model, GPT-4o, achieves 80.59% accuracy on true-or-false tasks and 76.75% on multiple-choice questions, but still lags behind human performance. In the semantic description task, GPT-4o's hallucinations on classical illusions result in low scores for trap illusions, even falling behind some open-source models. IllusionBench is, to the best of our knowledge, the largest and most comprehensive benchmark for visual illusions in VLMs to date.	 | 当前的视觉语言模型（VLMs）在图像理解方面表现出色，但在处理视觉错觉方面存在困难，尤其是在现实世界场景中。现有的基准主要集中在经典的认知错觉上，这些错觉已经被最先进的（SOTA）视觉语言模型所学习，揭示了诸如幻觉和感知能力有限等问题。为了弥补这一差距，我们提出了IllusionBench，这是一个全面的视觉错觉数据集，不仅包括经典的认知错觉，还包括现实世界的场景错觉。该数据集包含1,051张图像、5,548个问题-答案对以及1,051个金色文本描述，这些描述针对错觉的存在、原因和内容。我们在该数据集上评估了十个SOTA VLMs，使用的是真命题或假命题、多项选择和开放性任务。除了现实世界的错觉，我们还设计了陷阱错觉，这些错觉在表面上看起来类似于经典模式，但在现实中有所不同，突显了SOTA模型中的幻觉问题。表现最好的模型GPT-4o在真命题或假命题任务中的准确率为80.59%，在多项选择问题中的准确率为76.75%，但依然落后于人类的表现。在语义描述任务中，GPT-4o在经典错觉上的幻觉导致了陷阱错觉的低评分，甚至比一些开源模型还要差。据我们所知，IllusionBench是迄今为止规模最大、最全面的视觉错觉基准数据集。	 | IllusionBench 是一个全面的视觉错觉数据集，包含经典和现实世界场景中的错觉，评估了十个SOTA视觉语言模型在处理这些错觉时的表现，揭示了模型在理解和解释视觉错觉方面的局限性。
2501.00843	 | FusionSORT: Fusion Methods for Online Multi-object Visual Tracking	 | Nathanael L. Baisa	 | In this work, we investigate four different fusion methods for associating detections to tracklets in multi-object visual tracking. In addition to considering strong cues such as motion and appearance information, we also consider weak cues such as height intersection-over-union (height-IoU) and tracklet confidence information in the data association using different fusion methods. These fusion methods include minimum, weighted sum based on IoU, Kalman filter (KF) gating, and hadamard product of costs due to the different cues. We conduct extensive evaluations on validation sets of MOT17, MOT20 and DanceTrack datasets, and find out that the choice of a fusion method is key for data association in multi-object visual tracking. We hope that this investigative work helps the computer vision research community to use the right fusion method for data association in multi-object visual tracking.	 | 在这项工作中，我们研究了在多目标视觉跟踪中将检测与轨迹片段关联的四种不同的融合方法。除了考虑强烈的线索，如运动和外观信息外，我们还考虑了弱线索，如高度交并比（height-IoU）和轨迹片段置信度信息，并使用不同的融合方法进行数据关联。这些融合方法包括最小值、基于IOU的加权和、卡尔曼滤波（KF）门限以及不同线索的成本哈达玛积。我们在MOT17、MOT20和DanceTrack数据集的验证集上进行了广泛的评估，发现选择合适的融合方法对于多目标视觉跟踪中的数据关联至关重要。我们希望这项研究工作能够帮助计算机视觉研究社区选择正确的融合方法来进行多目标视觉跟踪中的数据关联。	 | 本文研究了四种不同的融合方法（最小值、基于IOU的加权和、卡尔曼滤波门限以及不同线索的成本哈达玛积）在多目标视觉跟踪中的轨迹片段关联，发现选择合适的融合方法对于数据关联至关重要，并在MOT17、MOT20和DanceTrack数据集上进行了验证。
2501.00838	 | Spatially-guided Temporal Aggregation for Robust Event-RGB Optical Flow Estimation	 | Qianang Zhou,Junhui Hou,Meiyi Yang,Yongjian Deng,Youfu Li,Junlin Xiong	 | Current optical flow methods exploit the stable appearance of frame (or RGB) data to establish robust correspondences across time. Event cameras, on the other hand, provide high-temporal-resolution motion cues and excel in challenging scenarios. These complementary characteristics underscore the potential of integrating frame and event data for optical flow estimation. However, most cross-modal approaches fail to fully utilize the complementary advantages, relying instead on simply stacking information. This study introduces a novel approach that uses a spatially dense modality to guide the aggregation of the temporally dense event modality, achieving effective cross-modal fusion. Specifically, we propose an event-enhanced frame representation that preserves the rich texture of frames and the basic structure of events. We use the enhanced representation as the guiding modality and employ events to capture temporally dense motion information. The robust motion features derived from the guiding modality direct the aggregation of motion information from events. To further enhance fusion, we propose a transformer-based module that complements sparse event motion features with spatially rich frame information and enhances global information propagation. Additionally, a mix-fusion encoder is designed to extract comprehensive spatiotemporal contextual features from both modalities. Extensive experiments on the MVSEC and DSEC-Flow datasets demonstrate the effectiveness of our framework. Leveraging the complementary strengths of frames and events, our method achieves leading performance on the DSEC-Flow dataset. Compared to the event-only model, frame guidance improves accuracy by 10\%. Furthermore, it outperforms the state-of-the-art fusion-based method with a 4\% accuracy gain and a 45\% reduction in inference time.	 | 当前的光学流方法依赖于帧（或RGB）数据的稳定外观，以在时间上建立稳健的对应关系。相比之下，事件相机提供高时间分辨率的运动线索，并在具有挑战性的场景中表现出色。这些互补特性强调了将帧数据和事件数据相结合以进行光学流估计的潜在价值。然而，大多数跨模态方法未能充分利用这些互补优势，而是简单地堆叠信息。本研究提出了一种新颖的方法，利用空间密集模态来引导时间密集事件模态的聚合，从而实现有效的跨模态融合。具体而言，我们提出了一种事件增强的帧表示，该表示保留了帧的丰富纹理和事件的基本结构。我们使用增强表示作为引导模态，并利用事件捕捉时间密集的运动信息。来自引导模态的稳健运动特征指导事件的运动信息聚合。为进一步增强融合，我们提出了一种基于Transformer的模块，该模块用空间丰富的帧信息补充稀疏的事件运动特征，并增强全局信息传播。此外，我们设计了一种混合融合编码器，从两种模态中提取综合的空间时间上下文特征。在MVSEC和DSEC-Flow数据集上的广泛实验表明，我们的框架是有效的。凭借帧和事件互补优势，我们的方法在DSEC-Flow数据集上实现了领先的性能。与仅使用事件的模型相比，帧引导提高了10%的准确性。此外，与最先进的融合方法相比，它在准确性上提高了4%，并将推断时间减少了45%。	 | 该研究提出了一种新颖的跨模态方法，利用帧数据的空间密集信息引导事件数据的时间密集运动信息聚合，从而实现高效的光学流估计。该方法在DSEC-Flow数据集上达到了领先的性能，并且与仅使用事件数据的方法相比，提高了10%的准确性，同时减少45%的推断时间。
2501.00836	 | Recognizing Artistic Style of Archaeological Image Fragments Using Deep Style Extrapolation	 | Gur Elkin,Ofir Itzhak Shahar,Yaniv Ohayon,Nadav Alali,Ohad Ben-Shahar	 | Ancient artworks obtained in archaeological excavations usually suffer from a certain degree of fragmentation and physical degradation. Often, fragments of multiple artifacts from different periods or artistic styles could be found on the same site. With each fragment containing only partial information about its source, and pieces from different objects being mixed, categorizing broken artifacts based on their visual cues could be a challenging task, even for professionals. As classification is a common function of many machine learning models, the power of modern architectures can be harnessed for efficient and accurate fragment classification. In this work, we present a generalized deep-learning framework for predicting the artistic style of image fragments, achieving state-of-the-art results for pieces with varying styles and geometries.	 | 考古发掘中获得的古代艺术品通常会遭受一定程度的破碎和物理退化。通常，在同一遗址上可以找到来自不同时期或艺术风格的多个碎片。由于每个碎片仅包含其来源的部分信息，并且来自不同物件的碎片被混合在一起，基于视觉特征对破碎艺术品进行分类可能是一项具有挑战性的任务，即使是专业人士也是如此。由于分类是许多机器学习模型的常见功能，因此可以利用现代架构的威力来实现高效且准确的碎片分类。在这项工作中，我们提出了一种通用的深度学习框架，用于预测图像碎片的艺术风格，该框架在具有不同风格和几何形状的碎片上达到了最先进的性能。	 | 古代艺术品碎片由于破碎和物理退化而难以分类，尤其对于不同时期或艺术风格的碎片而言更具挑战性。为此，研究提出了一种基于深度学习的通用框架，能够有效预测碎片的艺术风格，并在多样化的数据集上取得了最先进的性能。
2501.00818	 | SPARNet: Continual Test-Time Adaptation via Sample Partitioning Strategy and Anti-Forgetting Regularization	 | Xinru Meng,Han Sun,Jiamei Liu,Ningzhong Liu,Huiyu Zhou	 | Test-time Adaptation (TTA) aims to improve model performance when the model encounters domain changes after deployment. The standard TTA mainly considers the case where the target domain is static, while the continual TTA needs to undergo a sequence of domain changes. This encounters a significant challenge as the model needs to adapt for the long-term and is unaware of when the domain changes occur. The quality of pseudo-labels is hard to guarantee. Noisy pseudo-labels produced by simple self-training methods can cause error accumulation and catastrophic forgetting. In this work, we propose a new framework named SPARNet which consists of two parts, sample partitioning strategy and anti-forgetting regularization. The sample partition strategy divides samples into two groups, namely reliable samples and unreliable samples. According to the characteristics of each group of samples, we choose different strategies to deal with different groups of samples. This ensures that reliable samples contribute more to the model. At the same time, the negative impacts of unreliable samples are eliminated by the mean teacher's consistency learning. Finally, we introduce a regularization term to alleviate the catastrophic forgetting problem, which can limit important parameters from excessive changes. This term enables long-term adaptation of parameters in the network. The effectiveness of our method is demonstrated in continual TTA scenario by conducting a large number of experiments on CIFAR10-C, CIFAR100-C and ImageNet-C.	 | 测试时自适应（Test-time Adaptation, TTA）旨在提高模型性能，当模型在部署后遇到领域变化时。标准的TTA主要考虑目标领域是静态的情况，而连续的TTA则需要经历一系列的领域变化。这带来了显著的挑战，因为模型需要长期适应，且无法预知何时会发生领域变化。伪标签的质量难以保证。简单的自我训练方法产生的嘈杂伪标签会导致错误累积和灾难性遗忘。在本工作中，我们提出了一种新的框架，名为SPARNet，该框架包括两个部分：样本分区策略和反遗忘正则化。样本分区策略将样本分为两类，即可靠样本和不可靠样本。根据每组样本的特点，我们选择不同的策略来处理不同组的样本。这确保了可靠样本对模型的贡献更大。同时，通过均值教师的一致性学习来消除不可靠样本的负面影响。最后，我们引入了一个正则化项以缓解灾难性遗忘问题，该项能够限制重要参数的过度变化，从而在网络中实现长期适应。我们通过在CIFAR10-C、CIFAR100-C和ImageNet-C上进行大量实验，展示了我们方法在连续TTA场景中的有效性。	 | 本文提出了一种名为SPARNet的新框架，该框架包括样本分区策略和反遗忘正则化，旨在提高模型在连续领域适应场景下的表现，通过区分可靠和不可靠样本并限制关键参数的过度变化，从而实现长期适应。
2501.00816	 | MixSA: Training-free Reference-based Sketch Extraction via Mixture-of-Self-Attention	 | Rui Yang,Xiaojun Wu,Shengfeng He	 | Current sketch extraction methods either require extensive training or fail to capture a wide range of artistic styles, limiting their practical applicability and versatility. We introduce Mixture-of-Self-Attention (MixSA), a training-free sketch extraction method that leverages strong diffusion priors for enhanced sketch perception. At its core, MixSA employs a mixture-of-self-attention technique, which manipulates self-attention layers by substituting the keys and values with those from reference sketches. This allows for the seamless integration of brushstroke elements into initial outline images, offering precise control over texture density and enabling interpolation between styles to create novel, unseen styles. By aligning brushstroke styles with the texture and contours of colored images, particularly in late decoder layers handling local textures, MixSA addresses the common issue of color averaging by adjusting initial outlines. Evaluated with various perceptual metrics, MixSA demonstrates superior performance in sketch quality, flexibility, and applicability. This approach not only overcomes the limitations of existing methods but also empowers users to generate diverse, high-fidelity sketches that more accurately reflect a wide range of artistic expressions.	 | 当前的素描提取方法要么需要大量的训练，要么无法捕捉广泛的艺术风格，这限制了它们的实际适用性和灵活性。我们提出了Mixture-of-Self-Attention（MixSA），这是一种无需训练的素描提取方法，它利用强扩散先验以增强对素描的感知。MixSA的核心在于使用混合的自我注意技术，通过将关键和值替换为参考素描的那些来操作自我注意力层。这使得能够在初始轮廓图像中无缝地整合笔触元素，提供对纹理密度的精确控制，并能够通过插值来创建新型且未见过的风格。通过在后期解码层对彩色图像的纹理和轮廓进行笔触风格对齐，MixSA解决了常见的颜色平均问题，从而调整初始轮廓。采用各种感知度量进行评估，MixSA在素描质量、灵活性和适用性方面表现出优越的性能。这种方法不仅克服了现有方法的局限性，还使用户能够生成多样且高保真的素描，更准确地反映广泛的艺术表达。	 | Mixture-of-Self-Attention（MixSA）是一种无需训练的素描提取方法，通过混合自我注意技术利用强扩散先验来增强对素描的感知，从而提供精确的纹理控制和广泛的艺术风格生成能力，显著提高了素描提取的灵活性和适用性。
2501.00811	 | Regression Guided Strategy to Automated Facial Beauty Optimization through Image Synthesis	 | Erik Nguyen,Spencer Htin	 | The use of beauty filters on social media, which enhance the appearance of individuals in images, is a well-researched area, with existing methods proving to be highly effective. Traditionally, such enhancements are performed using rule-based approaches that leverage domain knowledge of facial features associated with attractiveness, applying very specific transformations to maximize these attributes. In this work, we present an alternative approach that projects facial images as points on the latent space of a pre-trained GAN, which are then optimized to produce beautiful faces. The movement of the latent points is guided by a newly developed facial beauty evaluation regression network, which learns to distinguish attractive facial features, outperforming many existing facial beauty evaluation models in this domain. By using this data-driven approach, our method can automatically capture holistic patterns in beauty directly from data rather than relying on predefined rules, enabling more dynamic and potentially broader applications of facial beauty editing. This work demonstrates a potential new direction for automated aesthetic enhancement, offering a complementary alternative to existing methods.	 | 社交媒体上使用的美化滤镜能够增强图像中个人的外观，这是一个研究广泛的领域，现有的方法证明非常有效。传统上，这些增强是通过利用与吸引力相关的面部特征领域的知识来进行的，采用特定的变换以最大化这些特征。在本研究中，我们提出了一种替代的方法，即将面部图像投影到预训练的生成对抗网络（GAN）的潜在空间中，然后对其进行优化以生成美丽的面部图像。潜在点的移动由一种新开发的面部美丽评估回归网络引导，该网络能够学习区分吸引力特征，并在这一领域中表现出色，超越了许多现有面部美丽评估模型。通过使用这种数据驱动的方法，我们的方法可以直接从数据中自动捕获整体的美丽模式，而不是依赖预设的规则，从而使面部美丽编辑的应用更加动态且可能更广泛。本研究展示了自动化美学增强的一个潜在新方向，提供了对现有方法的一种补充替代方案。	 | 本研究提出了一种新的面部美化方法，通过将面部图像投影到预训练的生成对抗网络的潜在空间中并优化以生成美丽图像，这种方法由一个新开发的面部美丽评估回归网络引导，并优于现有面部美丽评估模型。该研究展示了自动化美学增强的一个潜在新方向，提供了对现有方法的补充替代方案。
2501.00795	 | Multimodal Large Models Are Effective Action Anticipators	 | Binglu Wang,Yao Tian,Shunzhou Wang,Le Yang	 | The task of long-term action anticipation demands solutions that can effectively model temporal dynamics over extended periods while deeply understanding the inherent semantics of actions. Traditional approaches, which primarily rely on recurrent units or Transformer layers to capture long-term dependencies, often fall short in addressing these challenges. Large Language Models (LLMs), with their robust sequential modeling capabilities and extensive commonsense knowledge, present new opportunities for long-term action anticipation. In this work, we introduce the ActionLLM framework, a novel approach that treats video sequences as successive tokens, leveraging LLMs to anticipate future actions. Our baseline model simplifies the LLM architecture by setting future tokens, incorporating an action tuning module, and reducing the textual decoder layer to a linear layer, enabling straightforward action prediction without the need for complex instructions or redundant descriptions. To further harness the commonsense reasoning of LLMs, we predict action categories for observed frames and use sequential textual clues to guide semantic understanding. In addition, we introduce a Cross-Modality Interaction Block, designed to explore the specificity within each modality and capture interactions between vision and textual modalities, thereby enhancing multimodal tuning. Extensive experiments on benchmark datasets demonstrate the superiority of the proposed ActionLLM framework, encouraging a promising direction to explore LLMs in the context of action anticipation. Code is available at https://github.com/2tianyao1/ActionLLM.git.	 | 长期动作预测的任务要求解决方案能够有效地建模较长时期的时序动态，并深刻理解动作的内在语义。传统的解决方案，主要依赖于循环单元或Transformer层来捕捉长期依赖关系，往往难以应对这些挑战。大语言模型（LLMs）因其强大的序列建模能力和丰富的常识知识，为长期动作预测提供了新的机会。在本工作中，我们引入了ActionLLM框架，这是一种新颖的方法，将视频序列视为连续的标记，利用LLMs来预测未来动作。我们的基础模型简化了LLM架构，通过设定未来的标记、引入动作调整模块，并将文本解码层减少为线性层，从而实现简单的动作预测，无需复杂的指令或冗余描述。为了进一步利用LLMs的常识推理能力，我们预测观察帧的动作类别，并使用序列的文本线索来引导语义理解。此外，我们还引入了一种跨模态交互块，设计用于探索每种模态的特定性，并捕捉视觉和文本模态之间的交互，从而增强多模态调整。在基准数据集上的广泛实验表明，所提出的ActionLLM框架具有优越性，为探索LLMs在动作预测中的应用提供了充满希望的方向。代码可在https://github.com/2tianyao1/ActionLLM.git 获取。	 | 本文提出了一种名为ActionLLM的新框架，利用大语言模型预测视频中的未来动作，通过简化LLM架构并引入动作调整模块和跨模态交互块，实现了有效的长期动作预测。实验结果表明，该框架在多个基准数据集上表现优异，展示了大语言模型在动作预测任务中的潜力。
2501.00765	 | Beyond Words: AuralLLM and SignMST-C for Precise Sign Language Production and Bidirectional Accessibility	 | Yulong Li,Yuxuan Zhang,Feilong Tang,Mian Zhou,Zhixiang Lu,Haochen Xue,Yifang Wang,Kang Dang,Jionglong Su	 | Although sign language recognition aids non-hearing-impaired understanding, many hearing-impaired individuals still rely on sign language alone due to limited literacy, underscoring the need for advanced sign language production and translation (SLP and SLT) systems. In the field of sign language production, the lack of adequate models and datasets restricts practical applications. Existing models face challenges in production accuracy and pose control, making it difficult to provide fluent sign language expressions across diverse scenarios. Additionally, data resources are scarce, particularly high-quality datasets with complete sign vocabulary and pose annotations. To address these issues, we introduce CNText2Sign and CNSign, comprehensive datasets to benchmark SLP and SLT, respectively, with CNText2Sign covering gloss and landmark mappings for SLP, and CNSign providing extensive video-to-text data for SLT. To improve the accuracy and applicability of sign language systems, we propose the AuraLLM and SignMST-C models. AuraLLM, incorporating LoRA and RAG techniques, achieves a BLEU-4 score of 50.41 on the CNText2Sign dataset, enabling precise control over gesture semantics and motion. SignMST-C employs self-supervised rapid motion video pretraining, achieving a BLEU-4 score of 31.03/32.08 on the PHOENIX2014-T benchmark, setting a new state-of-the-art. These models establish robust baselines for the datasets released for their respective tasks.	 | 尽管手语识别有助于非听力障碍人士的理解，但许多听力障碍人士仍然依赖单一的手语表达，这凸显了先进手语生产与翻译（SLP 和 SLT）系统的需求。在手语生产领域，缺乏足够的模型和数据集限制了其实用应用。现有的模型在生产准确性和动作控制方面面临挑战，难以在各种场景中提供流畅的手语表达。此外，数据资源稀缺，特别是包含完整手语词汇和姿态标注的高质量数据集。为了解决这些问题，我们介绍了 CNText2Sign 和 CNSign 两个全面的数据集，分别用于评估 SLP 和 SLT。CNText2Sign 覆盖了 SLP 的手语释义和关键点映射，而 CNSign 提供了大量视频到文本的数据用于 SLT。为了提高手语系统的准确性和适用性，我们提出了 AuraLLM 和 SignMST-C 模型。AuraLLM 结合了 LoRA 和 RAG 技术，在 CNText2Sign 数据集上实现了 BLEU-4得分为 50.41，实现了对手势语义和动作的精确控制。SignMST-C 使用自监督快速动作视频预训练，在 PHOENIX2014-T 验证集上实现了 BLEU-4 得分为 31.03/32.08，创造了新的最先进的技术水平。这些模型为各自任务发布的数据集建立了稳健的基础。	 | 该研究介绍了两个新的数据集CNText2Sign和CNSign，用于评估手语生产（SLP）和手语翻译（SLT）系统，并提出了AuraLLM和SignMST-C两个模型，分别在SLP和SLT任务上取得了最先进的性能，从而推动了手语识别技术的实用应用。
2501.00758	 | Less is More: Token Context-aware Learning for Object Tracking	 | Chenlong Xu,Bineng Zhong,Qihua Liang,Yaozong Zheng,Guorong Li,Shuxiang Song	 | Recently, several studies have shown that utilizing contextual information to perceive target states is crucial for object tracking. They typically capture context by incorporating multiple video frames. However, these naive frame-context methods fail to consider the importance of each patch within a reference frame, making them susceptible to noise and redundant tokens, which deteriorates tracking performance. To address this challenge, we propose a new token context-aware tracking pipeline named LMTrack, designed to automatically learn high-quality reference tokens for efficient visual tracking. Embracing the principle of Less is More, the core idea of LMTrack is to analyze the importance distribution of all reference tokens, where important tokens are collected, continually attended to, and updated. Specifically, a novel Token Context Memory module is designed to dynamically collect high-quality spatio-temporal information of a target in an autoregressive manner, eliminating redundant background tokens from the reference frames. Furthermore, an effective Unidirectional Token Attention mechanism is designed to establish dependencies between reference tokens and search frame, enabling robust cross-frame association and target localization. Extensive experiments demonstrate the superiority of our tracker, achieving state-of-the-art results on tracking benchmarks such as GOT-10K, TrackingNet, and LaSOT.	 | 最近，多项研究显示，利用上下文信息感知目标状态对于对象追踪至关重要。它们通常通过整合多帧视频来捕捉上下文。然而，这些简单的帧上下文方法没有考虑到参考帧内各块的重要性，从而容易受到噪声和冗余令牌的影响，降低了追踪性能。为了解决这一挑战，我们提出了一种名为LMTrack的新令牌上下文感知追踪管道，旨在自动学习高质量的参考令牌以实现高效的视觉追踪。LMTrack的核心思想是Less is More，即分析所有参考令牌的重要性分布，收集重要令牌，持续关注并更新它们。具体而言，设计了一个新颖的令牌上下文记忆模块，以自回归的方式动态收集目标的高质量时空信息，消除参考帧中的冗余背景令牌。此外，设计了一种有效的单向令牌注意力机制，用于建立参考令牌与搜索帧之间的依赖关系，从而实现稳健的跨帧关联和目标定位。广泛的实验结果表明，我们的追踪器在GOT-10K、TrackingNet和LaSOT等追踪基准测试上的表现优于现有方法。	 | LMTrack是一种新的令牌上下文感知追踪管道，通过自动学习高质量的参考令牌来提升视觉追踪性能，该方法通过分析参考帧中令牌的重要性分布，消除冗余背景令牌，并建立有效的单向令牌注意力机制来实现稳健的跨帧关联和目标定位。
2501.00752	 | Foreground-Covering Prototype Generation and Matching for SAM-Aided Few-Shot Segmentation	 | Suho Park,SuBeen Lee,Hyun Seok Seong,Jaejoon Yoo,Jae-Pil Heo	 | We propose Foreground-Covering Prototype Generation and Matching to resolve Few-Shot Segmentation (FSS), which aims to segment target regions in unlabeled query images based on labeled support images. Unlike previous research, which typically estimates target regions in the query using support prototypes and query pixels, we utilize the relationship between support and query prototypes. To achieve this, we utilize two complementary features: SAM Image Encoder features for pixel aggregation and ResNet features for class consistency. Specifically, we construct support and query prototypes with SAM features and distinguish query prototypes of target regions based on ResNet features. For the query prototype construction, we begin by roughly guiding foreground regions within SAM features using the conventional pseudo-mask, then employ iterative cross-attention to aggregate foreground features into learnable tokens. Here, we discover that the cross-attention weights can effectively alternate the conventional pseudo-mask. Therefore, we use the attention-based pseudo-mask to guide ResNet features to focus on the foreground, then infuse the guided ResNet feature into the learnable tokens to generate class-consistent query prototypes. The generation of the support prototype is conducted symmetrically to that of the query one, with the pseudo-mask replaced by the ground-truth mask. Finally, we compare these query prototypes with support ones to generate prompts, which subsequently produce object masks through the SAM Mask Decoder. Our state-of-the-art performances on various datasets validate the effectiveness of the proposed method for FSS. Our official code is available at https://github.com/SuhoPark0706/FCP	 | 我们提出了一种前景覆盖原型生成与匹配方法，以解决少量样本分割（Few-Shot Segmentation, FSS）问题，该问题旨在基于标注的支持图像对未标注查询图像中的目标区域进行分割。与以往研究通常使用支持原型和查询像素来估计查询图像中的目标区域不同，我们利用了支持原型和查询原型之间的关系。为此，我们利用了两种互补的功能：SAM图像编码器功能用于像素聚合，ResNet功能用于类一致性。具体地，我们使用SAM特性构建支持原型和查询原型，并基于ResNet特性区分查询原型中的目标区域。对于查询原型的构建，我们首先使用传统的伪掩码粗略地引导SAM特征中的前景区域，然后采用迭代交叉注意力来将前景特征聚合到可学习的令牌中。在此过程中，我们发现交叉注意力权重可以有效地替代传统的伪掩码。因此，我们使用基于注意力的伪掩码来引导ResNet特征关注前景，并将引导后的ResNet特征注入到可学习的令牌中，以生成类一致的查询原型。支持原型的生成是对称进行的，伪掩码被真实的掩码所替代。最后，我们比较这些查询原型和支持原型以生成提示，然后通过SAM掩码解码器产生对象掩码。我们在多个数据集上的最新性能验证了所提出方法的有效性。我们的官方代码可在https://github.com/SuhoPark0706/FCP获取。	 | 该研究提出了一种前景覆盖原型生成与匹配方法，通过利用支持原型和查询原型之间的关系，结合SAM图像编码器和ResNet功能，以改善少量样本分割问题，特别是在未标注查询图像的目标区域分割上。这种方法通过迭代交叉注意力机制生成类一致的查询原型，并通过SAM掩码解码器产生准确的对象掩码，表现出在多个数据集上的优越性能。
2501.00741	 | Towards End-to-End Neuromorphic Voxel-based 3D Object Reconstruction Without Physical Priors	 | Chuanzhi Xu,Langyi Chen,Vincent Qu,Haodong Chen,Vera Chung	 | Neuromorphic cameras, also known as event cameras, are asynchronous brightness-change sensors that can capture extremely fast motion without suffering from motion blur, making them particularly promising for 3D reconstruction in extreme environments. However, existing research on 3D reconstruction using monocular neuromorphic cameras is limited, and most of the methods rely on estimating physical priors and employ complex multi-step pipelines. In this work, we propose an end-to-end method for dense voxel 3D reconstruction using neuromorphic cameras that eliminates the need to estimate physical priors. Our method incorporates a novel event representation to enhance edge features, enabling the proposed feature-enhancement model to learn more effectively. Additionally, we introduced Optimal Binarization Threshold Selection Principle as a guideline for future related work, using the optimal reconstruction results achieved with threshold optimization as the benchmark. Our method achieves a 54.6% improvement in reconstruction accuracy compared to the baseline method.	 | 神经形态相机，也被称为事件相机，是一种异步亮度变化传感器，能够捕捉极快的运动而不产生运动模糊，使其在极端环境下进行三维重建方面极具潜力。然而，目前使用单目神经形态相机进行三维重建的研究有限，大多数方法依赖于估计物理先验，并采用复杂的多步骤管道。在本工作中，我们提出了一种端到端的方法，用于使用神经形态相机进行密集体素三维重建，从而消除了估计物理先验的需求。我们的方法结合了一种新颖的事件表示，以增强边缘特征，使提出的特征增强模型能够更有效地学习。此外，我们引入了最优二值化阈值选择原则作为未来相关工作的指导准则，使用阈值优化后获得的最佳重建结果作为基准。与基线方法相比，我们的方法在重建精度上提高了54.6%。	 | 该研究提出了一种端到端的方法，利用神经形态相机进行密集体素三维重建，通过一种新颖的事件表示增强边缘特征，提高了重建精度54.6%，并建议了最优二值化阈值选择原则作为未来工作的指导。
2501.00740	 | RORem: Training a Robust Object Remover with Human-in-the-Loop	 | Ruibin Li,Tao Yang,Song Guo,Lei Zhang	 | Despite the significant advancements, existing object removal methods struggle with incomplete removal, incorrect content synthesis and blurry synthesized regions, resulting in low success rates. Such issues are mainly caused by the lack of high-quality paired training data, as well as the self-supervised training paradigm adopted in these methods, which forces the model to in-paint the masked regions, leading to ambiguity between synthesizing the masked objects and restoring the background. To address these issues, we propose a semi-supervised learning strategy with human-in-the-loop to create high-quality paired training data, aiming to train a Robust Object Remover (RORem). We first collect 60K training pairs from open-source datasets to train an initial object removal model for generating removal samples, and then utilize human feedback to select a set of high-quality object removal pairs, with which we train a discriminator to automate the following training data generation process. By iterating this process for several rounds, we finally obtain a substantial object removal dataset with over 200K pairs. Fine-tuning the pre-trained stable diffusion model with this dataset, we obtain our RORem, which demonstrates state-of-the-art object removal performance in terms of both reliability and image quality. Particularly, RORem improves the object removal success rate over previous methods by more than 18\%. The dataset, source code and trained model are available at https://github.com/leeruibin/RORem.	 | 尽管取得了显著进展，现有的物体去除方法仍面临不完全去除、合成内容不准确以及合成区域模糊的问题，导致成功率较低。这些问题主要是由于缺乏高质量的配对训练数据，以及这些方法采用的半监督训练范式，迫使模型对遮罩区域进行填充，导致在合成遮罩对象和恢复背景之间产生模糊性。为解决这些问题，我们提出了一种带有人工干预的半监督学习策略，以创建高质量的配对训练数据，旨在训练一个鲁棒的物体去除器（RORem）。首先，我们从开源数据集中收集了60,000个训练配对，用于训练初步的物体去除模型生成去除样本，然后利用人类反馈选择一组高质量的物体去除配对，使用这组配对训练一个鉴别器，以自动实现后续训练数据生成过程。通过多次迭代这一过程，最终我们获得了包含超过200,000个配对的大量物体去除数据集。使用此数据集微调预先训练的稳定扩散模型，我们获得了RORem，其在可靠性和图像质量方面均展示了最先进的物体去除性能。特别是，RORem相较于先前的方法，提升了超过18%的物体去除成功率。数据集、源代码和训练模型可在https://github.com/leeruibin/RORem 获取。	 | 本文提出了一种带有人工干预的半监督学习策略，通过创建高质量的配对训练数据来提升物体去除效果，最终 developed a robust object removal model named RORem that significantly improves removal success rate by over 18% and demonstrates state-of-the-art performance in reliability and image quality.
2501.00734	 | DDD: Discriminative Difficulty Distance for plant disease diagnosis	 | Yuji Arima,Satoshi Kagiwada,Hitoshi Iyatomi	 | Recent studies on plant disease diagnosis using machine learning (ML) have highlighted concerns about the overestimated diagnostic performance due to inappropriate data partitioning, where training and test datasets are derived from the same source (domain). Plant disease diagnosis presents a challenging classification task, characterized by its fine-grained nature, vague symptoms, and the extensive variability of image features within each domain. In this study, we propose the concept of Discriminative Difficulty Distance (DDD), a novel metric designed to quantify the domain gap between training and test datasets while assessing the classification difficulty of test data. DDD provides a valuable tool for identifying insufficient diversity in training data, thus supporting the development of more diverse and robust datasets. We investigated multiple image encoders trained on different datasets and examined whether the distances between datasets, measured using low-dimensional representations generated by the encoders, are suitable as a DDD metric. The study utilized 244,063 plant disease images spanning four crops and 34 disease classes collected from 27 domains. As a result, we demonstrated that even if the test images are from different crops or diseases than those used to train the encoder, incorporating them allows the construction of a distance measure for a dataset that strongly correlates with the difficulty of diagnosis indicated by the disease classifier developed independently. Compared to the base encoder, pre-trained only on ImageNet21K, the correlation higher by 0.106 to 0.485, reaching a maximum of 0.909.	 | 近年来，关于使用机器学习（ML）进行植物病害诊断的研究强调了由于不恰当的数据分割导致的诊断性能高估问题，其中训练集和测试集数据来源于相同的领域（domain）。植物病害诊断是一个具有挑战性的分类任务，其特点是细粒度的特征、模糊的症状以及每个领域内图像特征的广泛变异。在本研究中，我们提出了区分困难距离（Discriminative Difficulty Distance, DDD）的概念，这是一种新型的度量标准，旨在量化训练集和测试集之间的领域差异，并评估测试数据的分类难度。DDD 提供了一个有价值的工具，用于识别训练数据的不足多样性，从而支持更多样化和稳健的数据集的发展。我们调查了在不同数据集上训练的多种图像编码器，并检查了使用编码器生成的低维表示计算的领域间距离是否适合作为 DDD 度量标准。研究使用了从 27 个领域收集的涵盖四种作物和 34 种病害类别的 244,063 张植物病害图像。结果表明，即使测试图像与用于训练编码器的图像属于不同的作物或病害，将这些测试图像纳入可以构建与植物病害分类器独立开发的诊断难度高度相关（相关系数为 0.106 至 0.485，最高可达 0.909）的距离度量。与仅在 ImageNet21K 上预训练的基本编码器相比，相关性提高了 0.106 至 0.485，达到最大值 0.909。	 | 本文提出了一种新的度量标准——区分困难距离（DDD），用于量化训练集和测试集之间的领域差异，并评估测试数据的分类难度，从而支持更稳健的数据集开发。研究发现，即使测试图像与训练图像来自不同的作物或病害类别，DDD 仍能高度相关地反映诊断难度，相关系数可达 0.909。
2501.00707	 | Everywhere Attack: Attacking Locally and Globally to Boost Targeted Transferability	 | Hui Zeng,Sanshuai Cui,Biwei Chen,Anjie Peng	 | Adversarial examples' (AE) transferability refers to the phenomenon that AEs crafted with one surrogate model can also fool other models. Notwithstanding remarkable progress in untargeted transferability, its targeted counterpart remains challenging. This paper proposes an everywhere scheme to boost targeted transferability. Our idea is to attack a victim image both globally and locally. We aim to optimize 'an army of targets' in every local image region instead of the previous works that optimize a high-confidence target in the image. Specifically, we split a victim image into non-overlap blocks and jointly mount a targeted attack on each block. Such a strategy mitigates transfer failures caused by attention inconsistency between surrogate and victim models and thus results in stronger transferability. Our approach is method-agnostic, which means it can be easily combined with existing transferable attacks for even higher transferability. Extensive experiments on ImageNet demonstrate that the proposed approach universally improves the state-of-the-art targeted attacks by a clear margin, e.g., the transferability of the widely adopted Logit attack can be improved by 28.8%-300%.We also evaluate the crafted AEs on a real-world platform: Google Cloud Vision. Results further support the superiority of the proposed method.	 |  adversarial 例子（AE）的可转移性指的是，用一个代理模型生成的 AE 也可以欺骗其他模型的现象。尽管在未目标化的可转移性方面取得了显著进展，其目标化对应物仍然具有挑战性。本文提出了一种通用方案以提升目标化可转移性。我们的想法是同时从全局和局部攻击受害者图像。我们的目标是优化每个局部图像区域中的“一支军队”的目标，而不是之前的工作中优化图像中的高置信度目标。具体来说，我们将受害者图像划分为不重叠的块，并在每个块上联合实施目标化攻击。这种策略减轻了代理模型和受害者模型之间注意力不一致导致的转移失败问题，从而增强了可转移性。我们的方法是无偏的方法，这意味着它可以很容易地与其他现有的可转移攻击结合使用，以进一步提高可转移性。在 ImageNet 上进行的大量实验表明，所提出的方法比现有最先进的目标化攻击方法普遍提高了清晰度的幅度，例如，广泛采用的逻辑攻击的可转移性可以提高 28.8% 至 300%。我们还评估了生成的 AE 在一个实际平台（Google Cloud Vision）上。结果进一步支持了所提方法的优越性。	 | 本文提出了一种通用方案，通过同时攻击受害者图像的全局和局部区域来提升目标化对抗样本的转移性，实验结果表明该方法显著提高了现有目标化对抗样本的转移性能，特别是在ImageNet上相较于逻辑攻击提高了28.8%至300%。
2501.00700	 | Knowledge-Guided Prompt Learning for Deepfake Facial Image Detection	 | Hao Wang,Cheng Deng,Zhidong Zhao	 | Recent generative models demonstrate impressive performance on synthesizing photographic images, which makes humans hardly to distinguish them from pristine ones, especially on realistic-looking synthetic facial images. Previous works mostly focus on mining discriminative artifacts from vast amount of visual data. However, they usually lack the exploration of prior knowledge and rarely pay attention to the domain shift between training categories (e.g., natural and indoor objects) and testing ones (e.g., fine-grained human facial images), resulting in unsatisfactory detection performance. To address these issues, we propose a novel knowledge-guided prompt learning method for deepfake facial image detection. Specifically, we retrieve forgery-related prompts from large language models as expert knowledge to guide the optimization of learnable prompts. Besides, we elaborate test-time prompt tuning to alleviate the domain shift, achieving significant performance improvement and boosting the application in real-world scenarios. Extensive experiments on DeepFakeFaceForensics dataset show that our proposed approach notably outperforms state-of-the-art methods.	 | 近年来，生成模型在合成照片级图像方面的表现极为出色，以至于人类难以将它们与真实的图像区分开来，尤其是在合成的人脸图像方面更为明显。以往的研究主要集中在从大量的视觉数据中挖掘区分性的特征。然而，这些研究通常忽略了先验知识的探索，并很少关注训练类别（例如，自然和室内物体）与测试类别（例如，精细的人脸图像）之间的领域转移，导致检测性能不佳。为了解决这些问题，我们提出了一种新的知识引导的提示学习方法，用于深度伪造人脸图像的检测。具体来说，我们从大型语言模型中检索伪造相关的提示作为专家知识，以指导可学习提示的优化。此外，我们还详细阐述了测试时提示调整的方法，以缓解领域转移问题，从而实现显著的性能提升，并推动其实用化。在 DeepFakeFaceForensics 数据集上的广泛实验表明，我们提出的方法明显优于当前最先进的方法。	 | 本文提出了一种知识引导的提示学习方法，用于检测深度伪造的人脸图像，通过从大型语言模型中获取伪造相关的提示并优化可学习提示，同时调整测试时的提示以缓解领域转移问题，从而显著提高了检测性能。实验结果表明，该方法在DeepFakeFaceForensics数据集上明显优于现有最先进的方法。
2501.00654	 | ICONS: Influence Consensus for Vision-Language Data Selection	 | Xindi Wu,Mengzhou Xia,Rulin Shao,Zhiwei Deng,Pang Wei Koh,Olga Russakovsky	 | Visual Instruction Tuning typically requires a large amount of vision-language training data. This data often containing redundant information that increases computational costs without proportional performance gains. In this work, we introduce ICONS, a gradient-driven Influence CONsensus approach for vision-language data Selection that selects a compact training dataset for efficient multi-task training. The key element of our approach is cross-task influence consensus, which uses majority voting across task-specific influence matrices to identify samples that are consistently valuable across multiple tasks, allowing us to effectively prioritize data that optimizes for overall performance. Experiments show that models trained on our selected data (20% of LLaVA-665K) achieve 98.6% of the relative performance obtained using the full dataset. Additionally, we release this subset, LLaVA-ICONS-133K, a compact yet highly informative subset of LLaVA-665K visual instruction tuning data, preserving high impact training data for efficient vision-language model development.	 | 视觉指令调优通常需要大量的视觉-语言训练数据。这些数据往往包含冗余信息，增加了计算成本而没有相应的性能提升。在本工作中，我们引入了ICONS（基于梯度的影响力共识方法），这是一种用于选择高效多任务训练的紧凑训练数据集的视觉-语言数据选择方法。我们方法的关键要素是跨任务影响力共识，它通过在任务特定影响力矩阵中进行多数投票来识别在多个任务中始终具有价值的样本，从而能够有效地优先选择优化整体性能的数据。实验结果表明，基于我们选择的数据（LLaVA-665K数据的20%）训练的模型，可以达到使用完整数据集时相对性能的98.6%。此外，我们还发布了这个子集，即LLaVA-ICONS-133K，它是LLaVA-665K视觉指令调优数据的一个紧凑而高信息量的子集，保留了高效视觉-语言模型开发中的高影响训练数据。	 | 该研究提出了ICONS方法，一种基于梯度的影响力共识方法，用于选择高效的多任务训练数据集以优化视觉-语言模型的性能，实验表明使用选择的数据集训练的模型达到了接近使用完整数据集训练的模型的性能，同时显著减少了数据量。
2501.00651	 | Taming Feed-forward Reconstruction Models as Latent Encoders for 3D Generative Models	 | Suttisak Wizadwongsa,Jinfan Zhou,Edward Li,Jeong Joon Park	 | Recent AI-based 3D content creation has largely evolved along two paths: feed-forward image-to-3D reconstruction approaches and 3D generative models trained with 2D or 3D supervision. In this work, we show that existing feed-forward reconstruction methods can serve as effective latent encoders for training 3D generative models, thereby bridging these two paradigms. By reusing powerful pre-trained reconstruction models, we avoid computationally expensive encoder network training and obtain rich 3D latent features for generative modeling for free. However, the latent spaces of reconstruction models are not well-suited for generative modeling due to their unstructured nature. To enable flow-based model training on these latent features, we develop post-processing pipelines, including protocols to standardize the features and spatial weighting to concentrate on important regions. We further incorporate a 2D image space perceptual rendering loss to handle the high-dimensional latent spaces. Finally, we propose a multi-stream transformer-based rectified flow architecture to achieve linear scaling and high-quality text-conditioned 3D generation. Our framework leverages the advancements of feed-forward reconstruction models to enhance the scalability of 3D generative modeling, achieving both high computational efficiency and state-of-the-art performance in text-to-3D generation.	 | 近年来，基于AI的3D内容生成主要沿着两条路径发展：前向的图像到3D重建方法和通过2D或3D监督训练的3D生成模型。在本工作中，我们展示了现有的前向重建方法可以作为有效的潜在编码器，用于训练3D生成模型，从而将这两条路径结合起来。通过重用预先训练的强大的重建模型，我们可以避免昂贵的编码器网络训练，并且可以免费获得丰富的3D潜在特征进行生成建模。然而，重建模型的潜在空间由于其非结构化性质，不适合生成建模。为了在这些潜在特征上进行基于流的模型训练，我们开发了后处理管道，包括标准化特征的协议和空间加权，以集中于重要区域。我们还引入了感知渲染损失，以处理高维度的潜在空间。最后，我们提出了一种基于多流变压器的修正流架构，以实现线性扩展并生成高质量的文本条件3D内容。我们的框架利用了前向重建模型的进步，增强了3D生成建模的可扩展性，实现了高计算效率和业界领先的表现。	 | 该工作将现有的图像到3D重建方法作为潜在编码器，结合前向重建和3D生成模型路径，通过开发后处理管道和引入感知渲染损失，提出了一种基于多流变压器的生成架构，实现了高质量的文本条件3D内容生成，提高了计算效率和性能。
2501.00645	 | SoundBrush: Sound as a Brush for Visual Scene Editing	 | Kim Sung-Bin,Kim Jun-Seong,Junseok Ko,Yewon Kim,Tae-Hyun Oh	 | We propose SoundBrush, a model that uses sound as a brush to edit and manipulate visual scenes. We extend the generative capabilities of the Latent Diffusion Model (LDM) to incorporate audio information for editing visual scenes. Inspired by existing image-editing works, we frame this task as a supervised learning problem and leverage various off-the-shelf models to construct a sound-paired visual scene dataset for training. This richly generated dataset enables SoundBrush to learn to map audio features into the textual space of the LDM, allowing for visual scene editing guided by diverse in-the-wild sound. Unlike existing methods, SoundBrush can accurately manipulate the overall scenery or even insert sounding objects to best match the audio inputs while preserving the original content. Furthermore, by integrating with novel view synthesis techniques, our framework can be extended to edit 3D scenes, facilitating sound-driven 3D scene manipulation. Demos are available at https://soundbrush.github.io/.	 | 我们提出了SoundBrush模型，该模型使用声音作为画笔来编辑和操控视觉场景。我们扩展了潜在扩散模型（LDM）的生成能力，使其能够结合音频信息来编辑视觉场景。受到现有图像编辑工作的启发，我们将此任务视为一个监督学习问题，并利用各种现成的模型构建了一个带有声音配对的视觉场景数据集用于训练。这个丰富生成的数据集使SoundBrush能够学习将音频特征映射到LDM的文本空间中，从而实现由多样化的野生声音引导的视觉场景编辑。与现有方法不同，SoundBrush可以准确地操控整体场景，甚至插入声音对象以最佳匹配音频输入，同时保留原始内容。此外，通过结合新型视图合成技术，我们的框架可以扩展以编辑3D场景，从而实现由声音驱动的3D场景操控。演示可以在https://soundbrush.github.io/ 中找到。	 | SoundBrush模型通过将声音作为画笔来编辑和操控视觉场景，扩展了潜在扩散模型的生成能力，能够将音频特征映射到视觉场景中，实现由声音引导的视觉编辑。该模型不仅可以在2D场景中准确操控整体内容，还能通过结合视图合成技术编辑3D场景，支持由声音驱动的三维场景操控。
2501.00637	 | Flash-Split: 2D Reflection Removal with Flash Cues and Latent Diffusion Separation	 | Tianfu Wang,Mingyang Xie,Haoming Cai,Sachin Shah,Christopher A. Metzler	 | Transparent surfaces, such as glass, create complex reflections that obscure images and challenge downstream computer vision applications. We introduce Flash-Split, a robust framework for separating transmitted and reflected light using a single (potentially misaligned) pair of flash/no-flash images. Our core idea is to perform latent-space reflection separation while leveraging the flash cues. Specifically, Flash-Split consists of two stages. Stage 1 separates apart the reflection latent and transmission latent via a dual-branch diffusion model conditioned on an encoded flash/no-flash latent pair, effectively mitigating the flash/no-flash misalignment issue. Stage 2 restores high-resolution, faithful details to the separated latents, via a cross-latent decoding process conditioned on the original images before separation. By validating Flash-Split on challenging real-world scenes, we demonstrate state-of-the-art reflection separation performance and significantly outperform the baseline methods.	 | 透明表面，如玻璃，会产生复杂的反射，从而模糊图像并挑战下游计算机视觉应用。我们提出了一种名为 Flash-Split 的稳健框架，用于利用单张（可能对齐不准确的）闪光/非闪光图像对来分离透射光和反射光。我们的核心思想是在利用闪光提示的同时，在潜在空间中进行反射分离。具体来说，Flash-Split 包含两个阶段。第一阶段通过编码的闪光/非闪光潜在对使用双重分支扩散模型来分离反射潜在和透射潜在，有效地解决了闪光/非闪光对齐不准确的问题。第二阶段通过交叉潜在解码过程恢复高分辨率、忠实的细节，该过程基于分离前的原始图像。通过在具有挑战性的真实场景上验证 Flash-Split，我们展示了最先进的反射分离性能，并显著优于基准方法。	 | Flash-Split 是一个稳健的框架，利用单张对齐不准确的闪光/非闪光图像对来分离透射光和反射光，通过双重分支扩散模型和交叉潜在解码过程优化反射分离，展示了在真实场景中的优越性能。
2501.00625	 | Gaussian Building Mesh (GBM): Extract a Building's 3D Mesh with Google Earth and Gaussian Splatting	 | Kyle Gao,Liangzhi Li,Hongjie He,Dening Lu,Linlin Xu,Jonathan Li	 | Recently released open-source pre-trained foundational image segmentation and object detection models (SAM2+GroundingDINO) allow for geometrically consistent segmentation of objects of interest in multi-view 2D images. Users can use text-based or click-based prompts to segment objects of interest without requiring labeled training datasets. Gaussian Splatting allows for the learning of the 3D representation of a scene's geometry and radiance based on 2D images. Combining Google Earth Studio, SAM2+GroundingDINO, 2D Gaussian Splatting, and our improvements in mask refinement based on morphological operations and contour simplification, we created a pipeline to extract the 3D mesh of any building based on its name, address, or geographic coordinates.	 | 最近发布的开源预训练基础图像分割和对象检测模型（SAM2+GroundingDINO）允许用户在多视角2D图像中对感兴趣的物体进行几何一致的分割，而无需使用标注的训练数据集。Gaussian Splatting技术使得基于2D图像学习场景几何和辐射的3D表示成为可能。结合Google Earth Studio、SAM2+GroundingDINO、2D Gaussian Splatting以及我们基于形态学操作和轮廓简化改进的掩膜细化技术，我们建立了一个提取建筑物3D网格的管道，仅需提供建筑物名称、地址或地理坐标。	 | 该研究利用SAM2+GroundingDINO和2D Gaussian Splatting技术，结合形态学操作和轮廓简化，开发了一套无需标注数据即可从多视角2D图像中提取建筑物3D网格的管道。
2501.00603	 | DiC: Rethinking Conv3x3 Designs in Diffusion Models	 | Yuchuan Tian,Jing Han,Chengcheng Wang,Yuchen Liang,Chao Xu,Hanting Chen	 | Diffusion models have shown exceptional performance in visual generation tasks. Recently, these models have shifted from traditional U-Shaped CNN-Attention hybrid structures to fully transformer-based isotropic architectures. While these transformers exhibit strong scalability and performance, their reliance on complicated self-attention operation results in slow inference speeds. Contrary to these works, we rethink one of the simplest yet fastest module in deep learning, 3x3 Convolution, to construct a scaled-up purely convolutional diffusion model. We first discover that an Encoder-Decoder Hourglass design outperforms scalable isotropic architectures for Conv3x3, but still under-performing our expectation. Further improving the architecture, we introduce sparse skip connections to reduce redundancy and improve scalability. Based on the architecture, we introduce conditioning improvements including stage-specific embeddings, mid-block condition injection, and conditional gating. These improvements lead to our proposed Diffusion CNN (DiC), which serves as a swift yet competitive diffusion architecture baseline. Experiments on various scales and settings show that DiC surpasses existing diffusion transformers by considerable margins in terms of performance while keeping a good speed advantage. Project page: https://github.com/YuchuanTian/DiC	 | 扩散模型在视觉生成任务中展现了出色的性能。最近，这些模型已经从传统的 U 型 CNN-注意力混合结构转向了完全基于Transformer的各向同性架构。虽然这些Transformer具有很强的可扩展性和性能，但由于依赖复杂的自我注意力操作，导致推理速度较慢。与这些工作不同，我们重新思考了深度学习中简单且快速的一个模块—3x3卷积，构建了一个扩大规模的纯卷积扩散模型。我们首先发现，编码器-解码器钟摆设计（Hourglass）在Conv3x3中优于可扩展的各向同性架构，但仍未能达到我们的预期。为进一步改进架构，我们引入了稀疏跳连接以减少冗余并提高可扩展性。基于该架构，我们引入了包括阶段特定嵌入、中间块条件注入和条件门控在内的条件改进措施。这些改进措施导致我们提出了扩散卷积网络（DiC），这是一种快速而具有竞争力的扩散架构基准。实验表明，无论在何种规模和设置下，DiC 在性能上都显著优于现有的扩散Transformer，同时保持了良好的速度优势。项目页面：https://github.com/YuchuanTian/DiC	 | 该研究提出了扩散卷积网络（DiC），这是一种基于纯卷积的扩散模型，通过引入稀疏跳连接和条件改进措施，提高了性能和速度，显著优于现有的扩散Transformer模型。
2501.00602	 | STORM: Spatio-Temporal Reconstruction Model for Large-Scale Outdoor Scenes	 | Jiawei Yang,Jiahui Huang,Yuxiao Chen,Yan Wang,Boyi Li,Yurong You,Apoorva Sharma,Maximilian Igl,Peter Karkus,Danfei Xu,Boris Ivanovic,Yue Wang,Marco Pavone	 | We present STORM, a spatio-temporal reconstruction model designed for reconstructing dynamic outdoor scenes from sparse observations. Existing dynamic reconstruction methods often rely on per-scene optimization, dense observations across space and time, and strong motion supervision, resulting in lengthy optimization times, limited generalization to novel views or scenes, and degenerated quality caused by noisy pseudo-labels for dynamics. To address these challenges, STORM leverages a data-driven Transformer architecture that directly infers dynamic 3D scene representations--parameterized by 3D Gaussians and their velocities--in a single forward pass. Our key design is to aggregate 3D Gaussians from all frames using self-supervised scene flows, transforming them to the target timestep to enable complete (i.e., "amodal") reconstructions from arbitrary viewpoints at any moment in time. As an emergent property, STORM automatically captures dynamic instances and generates high-quality masks using only reconstruction losses. Extensive experiments on public datasets show that STORM achieves precise dynamic scene reconstruction, surpassing state-of-the-art per-scene optimization methods (+4.3 to 6.6 PSNR) and existing feed-forward approaches (+2.1 to 4.7 PSNR) in dynamic regions. STORM reconstructs large-scale outdoor scenes in 200ms, supports real-time rendering, and outperforms competitors in scene flow estimation, improving 3D EPE by 0.422m and Acc5 by 28.02%. Beyond reconstruction, we showcase four additional applications of our model, illustrating the potential of self-supervised learning for broader dynamic scene understanding.	 | 我们提出了STORM，这是一种针对从稀疏观测重构动态户外场景而设计的时空重建模型。现有的动态重构方法通常依赖于场景特定优化、时空密集观测以及强烈的运动监督，这导致了优化时间过长、对新视角或新场景的泛化能力有限，并且由于动态伪标签的噪声导致质量下降。为了解决这些挑战，STORM 利用了一个数据驱动的 Transformer 架构，能够在单次前向传播中直接推断动态的 3D 场景表示——这些表示由 3D 高斯分布及其速度参数化。我们设计的关键点在于，通过自监督场景流聚集所有帧的 3D 高斯分布，并将它们转化为目标时间步，从而可以在任意时间点从任意视角生成完整（即“全视图”）重构。作为这一性质的衍生，STORM 可以仅使用重建损失自动捕捉动态实例并生成高质量的掩码。在公共数据集上的广泛实验表明，STORM 在动态区域实现了精确的动态场景重构，相比当前最先进的场景特定优化方法（PSNR 增益 4.3 到 6.6）和现有的前馈方法（PSNR 增益 2.1 到 4.7）表现更佳。STORM 可以在 200 毫秒内重构大规模户外场景，支持实时渲染，并在场景流估计方面超越竞争对手，提高了 3D EPE（端到端点误差）0.422 米和 Acc5（准确率）28.02%。除此之外，我们展示了该模型的四个额外应用场景，展示了自监督学习在更广泛动态场景理解中的潜力。	 | STORM 是一种基于 Transformer 的时空重建模型，能够在单次前向传播中直接推断动态 3D 场景表示，有效解决了现有方法中优化时间长、泛化能力弱和质量下降的问题，实现精确的动态场景重构并支持实时渲染。
2501.00601	 | DreamDrive: Generative 4D Scene Modeling from Street View Images	 | Jiageng Mao,Boyi Li,Boris Ivanovic,Yuxiao Chen,Yan Wang,Yurong You,Chaowei Xiao,Danfei Xu,Marco Pavone,Yue Wang	 | Synthesizing photo-realistic visual observations from an ego vehicle's driving trajectory is a critical step towards scalable training of self-driving models. Reconstruction-based methods create 3D scenes from driving logs and synthesize geometry-consistent driving videos through neural rendering, but their dependence on costly object annotations limits their ability to generalize to in-the-wild driving scenarios. On the other hand, generative models can synthesize action-conditioned driving videos in a more generalizable way but often struggle with maintaining 3D visual consistency. In this paper, we present DreamDrive, a 4D spatial-temporal scene generation approach that combines the merits of generation and reconstruction, to synthesize generalizable 4D driving scenes and dynamic driving videos with 3D consistency. Specifically, we leverage the generative power of video diffusion models to synthesize a sequence of visual references and further elevate them to 4D with a novel hybrid Gaussian representation. Given a driving trajectory, we then render 3D-consistent driving videos via Gaussian splatting. The use of generative priors allows our method to produce high-quality 4D scenes from in-the-wild driving data, while neural rendering ensures 3D-consistent video generation from the 4D scenes. Extensive experiments on nuScenes and street view images demonstrate that DreamDrive can generate controllable and generalizable 4D driving scenes, synthesize novel views of driving videos with high fidelity and 3D consistency, decompose static and dynamic elements in a self-supervised manner, and enhance perception and planning tasks for autonomous driving.	 | 将自动驾驶模型的大规模训练所需的真实视觉观察从自我车辆的行驶轨迹中合成出来，是一个关键步骤。基于重建的方法能够从驾驶日志中创建3D场景，并通过神经渲染合成几何一致的驾驶视频，但它们对昂贵的目标注解的依赖限制了其在野外驾驶场景中的泛化能力。另一方面，生成模型能够以更泛化的方式合成条件下的驾驶视频，但常常难以保持三维视觉一致性。在本文中，我们提出了DreamDrive，这是一种结合了生成和重建优点的4D空间-时间场景生成方法，旨在合成可泛化的4D驾驶场景和具有三维一致性的动态驾驶视频。具体而言，我们利用视频扩散模型的生成能力来合成一系列视觉参考，并通过一种新颖的混合高斯表示将其提升到4D。给定一个驾驶轨迹，我们通过高斯点绘技术渲染三维一致性的驾驶视频。使用生成先验可使我们的方法能够从野外驾驶数据生成高质量的4D场景，而神经渲染则确保从4D场景生成三维一致性的视频。在nuScenes和街景图像上的广泛实验表明，DreamDrive能够生成可控且可泛化的4D驾驶场景，合成具有高保真度和三维一致性的新视角的驾驶视频，以自监督的方式分解静态和动态元素，并增强自动驾驶中的感知和规划任务。	 | DreamDrive 是一种结合生成模型和重建方法的 4D 空间-时间场景生成技术，能够从自动驾驶日志中合成高质量、三维一致的驾驶视频，并支持自监督的静态和动态元素分解，从而增强自动驾驶中的感知和规划任务。
2501.00599	 | VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM	 | Yuqian Yuan,Hang Zhang,Wentong Li,Zesen Cheng,Boqiang Zhang,Long Li,Xin Li,Deli Zhao,Wenqiao Zhang,Yueting Zhuang,Jianke Zhu,Lidong Bing	 | Video Large Language Models (Video LLMs) have recently exhibited remarkable capabilities in general video understanding. However, they mainly focus on holistic comprehension and struggle with capturing fine-grained spatial and temporal details. Besides, the lack of high-quality object-level video instruction data and a comprehensive benchmark further hinders their advancements. To tackle these challenges, we introduce the VideoRefer Suite to empower Video LLM for finer-level spatial-temporal video understanding, i.e., enabling perception and reasoning on any objects throughout the video. Specially, we thoroughly develop VideoRefer Suite across three essential aspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent data engine to meticulously curate a large-scale, high-quality object-level video instruction dataset, termed VideoRefer-700K. Next, we present the VideoRefer model, which equips a versatile spatial-temporal object encoder to capture precise regional and sequential representations. Finally, we meticulously create a VideoRefer-Bench to comprehensively assess the spatial-temporal understanding capability of a Video LLM, evaluating it across various aspects. Extensive experiments and analyses demonstrate that our VideoRefer model not only achieves promising performance on video referring benchmarks but also facilitates general video understanding capabilities.	 | 视频大语言模型（Video LLMs）最近在通用视频理解方面表现出色。然而，它们主要侧重于整体理解，难以捕捉到精细的空间和时间细节。此外，缺乏高质量的对象级视频指令数据和全面的基准进一步阻碍了它们的进步。为应对这些挑战，我们引入了VideoRefer Suite，以增强Video LLM在精细化空间-时间视频理解方面的能力，即在视频中的任何对象上实现感知和推理。特别地，我们从三个关键方面全面开发了VideoRefer Suite：数据集、模型和基准。首先，我们引入了一个多智能体数据引擎，精心构建了一组大规模、高质量的对象级视频指令数据集，称为VideoRefer-700K。其次，我们提出了VideoRefer模型，该模型配备了多功能的空间-时间对象编码器，以捕捉精确的区域和序列表示。最后，我们精心构建了VideoRefer-Bench，以全面评估Video LLM的空间-时间理解能力，并从多个方面进行评估。广泛的实验和分析表明，我们的VideoRefer模型不仅在视频引用基准测试中取得了令人瞩目的性能，还促进了通用视频理解能力。	 | VideoRefer Suite通过引入多智能体数据引擎生成大规模高质量对象级视频指令数据集（VideoRefer-700K）、开发配备多功能空间-时间对象编码器的VideoRefer模型以及构建全面评估基准VideoRefer-Bench，显著提升了视频大语言模型在精细化空间-时间视频理解方面的性能。
2501.00585	 | Sidewalk Hazard Detection Using Variational Autoencoder and One-Class SVM	 | Edgar Guzman,Robert D. Howe	 | The unpredictable nature of outdoor settings introduces numerous safety concerns, making hazard detection crucial for safe navigation. This paper introduces a novel system for sidewalk safety navigation utilizing a hybrid approach that combines a Variational Autoencoder (VAE) with a One-Class Support Vector Machine (OCSVM). The system is designed to detect anomalies on sidewalks that could potentially pose walking hazards. A dataset comprising over 15,000 training frames and 5,000 testing frames was collected using video recordings, capturing various sidewalk scenarios, including normal and hazardous conditions. During deployment, the VAE utilizes its reconstruction mechanism to detect anomalies within a frame. Poor reconstruction by the VAE implies the presence of an anomaly, after which the OCSVM is used to confirm whether the anomaly is hazardous or non-hazardous. The proposed VAE model demonstrated strong performance, with a high Area Under the Curve (AUC) of 0.94, effectively distinguishing anomalies that could be potential hazards. The OCSVM is employed to reduce the detection of false hazard anomalies, such as manhole or water valve covers. This approach achieves an accuracy of 91.4%, providing a highly reliable system for distinguishing between hazardous and non-hazardous scenarios. These results suggest that the proposed system offers a robust solution for hazard detection in uncertain environments.	 | 户外环境的不确定性带来了诸多安全问题，因此危害检测对于安全导航至关重要。本文介绍了一种结合变分自编码器（VAE）和一类支持向量机（OCSVM）的混合方法的新系统，用于人行道安全导航。该系统旨在检测可能成为行走危害的异常情况。该系统利用超过15,000个训练帧和5,000个测试帧的数据集进行训练，这些数据是从视频录制中收集的，涵盖了各种人行道场景，包括正常和危险条件。在部署过程中，VAE 使用其重构机制来检测帧内的异常。VAE 的重构效果不佳意味着存在异常，随后使用OCSVM来确认该异常是否为真正的危害或非危害。所提出的VAE模型表现出色，其曲线下面积（AUC）高达0.94，有效地区分了可能构成潜在危害的异常。OCSVM用于减少误报为危害的异常，例如井盖或水阀盖等问题。该方法实现了91.4%的准确性，提供了一个高度可靠的系统来区分危险和非危险场景。这些结果表明，所提出系统为不确定环境中的危害检测提供了一个稳健的解决方案。	 | 本文提出了一种结合变分自编码器（VAE）和一类支持向量机（OCSVM）的混合系统，用于人行道安全导航中的危害检测，该系统能够有效地区分潜在的危害和非危害情况，实现91.4%的准确性。
2501.00584	 | Online Video Understanding: A Comprehensive Benchmark and Memory-Augmented Method	 | Zhenpeng Huang,Xinhao Li,Jiaqi Li,Jing Wang,Xiangyu Zeng,Cheng Liang,Tao Wu,Xi Chen,Liang Li,Limin Wang	 | Multimodal Large Language Models (MLLMs) have shown significant progress in offline video understanding. However, applying these models to real-world scenarios, such as autonomous driving and human-computer interaction, presents unique challenges due to the need for real-time processing of continuous online video streams. To this end, this paper presents systematic efforts from three perspectives: evaluation benchmark, model architecture, and training strategy. First, we introduce OVBench, a comprehensive question-answering benchmark specifically designed to evaluate models' ability to perceive, memorize, and reason within online video contexts. It features six core task types across three temporal contexts-past, present, and future-forming 16 subtasks from diverse datasets. Second, we propose a new Pyramid Memory Bank (PMB) that effectively retains key spatiotemporal information in video streams. Third, we proposed an offline-to-online learning paradigm, designing an interleaved dialogue format for online video data and constructing an instruction-tuning dataset tailored for online video training. This framework led to the development of VideoChat-Online, a robust and efficient model for online video understanding. Despite the lower computational cost and higher efficiency, VideoChat-Online outperforms existing state-of-the-art offline and online models across popular offline video benchmarks and OVBench, demonstrating the effectiveness of our model architecture and training strategy.	 | 多模态大语言模型（MLLMs）在离线视频理解方面取得了显著进展。然而，在将这些模型应用于真实场景，如自动驾驶和人机交互时，由于需要实时处理连续的在线视频流，因此面临独特的挑战。为此，本文从三个角度进行了系统性研究：评估基准、模型架构和训练策略。首先，我们介绍了OVBench，这是一个面向在线视频环境的综合问答基准，旨在评估模型在感知、记忆和推理方面的能力。OVBench涵盖了三个时间维度（过去、现在和未来）下的六种核心任务类型，共形成了16个子任务，来源于多种数据集。其次，我们提出了一种新的金字塔记忆库（PMB），能够有效保留视频流中的关键时空信息。第三，我们提出了离线到在线学习范式，设计了一种交错对话格式以适配在线视频数据，并构建了一个专为在线视频训练设计的指令调优数据集。在此框架下，我们开发了VideoChat-Online，这是一种强大且高效的在线视频理解模型。尽管具有较低的计算成本和更高的效率，VideoChat-Online在流行的离线视频基准和OVBench上均优于现有最先进的离线和在线模型，这证明了我们模型架构和训练策略的有效性。	 | 本文系统性地研究了多模态大语言模型在在线视频理解中的应用挑战，并提出了OVBench作为评估基准，PMB金字塔记忆库作为模型架构改进，以及离线到在线学习范式作为训练策略。在此基础上，开发了VideoChat-Online模型，它在计算成本较低的情况下，优于现有离线和在线模型。
2501.00574	 | VideoChat-Flash: Hierarchical Compression for Long-Context Video Modeling	 | Xinhao Li,Yi Wang,Jiashuo Yu,Xiangyu Zeng,Yuhan Zhu,Haian Huang,Jianfei Gao,Kunchang Li,Yinan He,Chenting Wang,Yu Qiao,Yali Wang,Limin Wang	 | Long-context modeling is a critical capability for multimodal large language models (MLLMs), enabling them to process long-form contents with implicit memorization. Despite its advances, handling extremely long videos remains challenging due to the difficulty in maintaining crucial features over extended sequences. This paper introduces a Hierarchical visual token Compression (HiCo) method designed for high-fidelity representation and a practical context modeling system VideoChat-Flash tailored for multimodal long-sequence processing. HiCo capitalizes on the redundancy of visual information in long videos to compress long video context from the clip-level to the video-level, reducing the compute significantly while preserving essential details. VideoChat-Flash features a multi-stage short-to-long learning scheme, a rich dataset of real-world long videos named LongVid, and an upgraded "Needle-In-A-video-Haystack" (NIAH) for evaluating context capacities. In extensive experiments, VideoChat-Flash shows the leading performance on both mainstream long and short video benchmarks at the 7B model scale. It firstly gets 99.1% accuracy over 10,000 frames in NIAH among open-source models.	 | 长上下文建模是多模态大型语言模型（MLLMs）的一项关键能力，使其能够处理长格式内容并进行隐式记忆。尽管取得了进展，但在处理极长的视频时仍然面临挑战，因为难以在长时间序列中保持关键特征。本文介绍了一种名为HiCo的分层视觉标记压缩方法，用于高保真表示，并提出了一种名为VideoChat-Flash的实用上下文建模系统，专门针对多模态长序列处理。HiCo利用长视频中的冗余视觉信息，将视频片段级别的上下文压缩到视频级别的上下文，显著减少计算量的同时保留了关键细节。VideoChat-Flash具备多阶段短到长学习方案，包括一个名为LongVid的现实世界长视频丰富数据集，以及一个升级的“视频堆中的绣花针”（NIAH）评估上下文容量。在广泛的实验中，VideoChat-Flash在7B模型规模上分别在主流的长视频和短视频基准测试中表现出领先性能。它首次在NIAH中实现了超过10,000帧的99.1%的准确率，这是开源模型中的最佳表现。	 | 本文介绍了一种名为HiCo的分层视觉标记压缩方法和VideoChat-Flash系统，后者专门针对多模态长序列处理，显著提高了多模态大型语言模型在处理极长视频时的性能，实现了在NIAH评估中超过10,000帧的99.1%准确率。
2501.00569	 | Probing Visual Language Priors in VLMs	 | Tiange Luo,Ang Cao,Gunhee Lee,Justin Johnson,Honglak Lee	 | Despite recent advances in Vision-Language Models (VLMs), many still over-rely on visual language priors present in their training data rather than true visual reasoning. To examine the situation, we introduce ViLP, a visual question answering (VQA) benchmark that pairs each question with three potential answers and three corresponding images: one image whose answer can be inferred from text alone, and two images that demand visual reasoning. By leveraging image generative models, we ensure significant variation in texture, shape, conceptual combinations, hallucinated elements, and proverb-based contexts, making our benchmark images distinctly out-of-distribution. While humans achieve near-perfect accuracy, modern VLMs falter; for instance, GPT-4 achieves only 66.17% on ViLP. To alleviate this, we propose a self-improving framework in which models generate new VQA pairs and images, then apply pixel-level and semantic corruptions to form "good-bad" image pairs for self-training. Our training objectives compel VLMs to focus more on actual visual inputs and have demonstrated their effectiveness in enhancing the performance of open-source VLMs, including LLaVA-v1.5 and Cambrian.	 | 尽管视觉语言模型（VLMs）取得了近期的进步，但许多模型仍然过度依赖于训练数据中存在的视觉语言先验，而不是真正进行视觉推理。为了解这一情况，我们引入了ViLP，这是一个视觉问答（VQA）基准，每个问题配对了三个潜在答案和三个相应的图像：一个图像的答案仅凭文本信息即可推断，而另外两个图像则需要进行视觉推理。通过利用图像生成模型，我们确保在纹理、形状、概念组合、幻象元素以及成语背景方面具有高度的变异性，从而使基准图像明显处于分布外区域。尽管人类几乎达到了完美准确度，但现代VLMs表现不佳；例如，GPT-4在ViLP上的准确度仅为66.17%。为了解决这一问题，我们提出了一种自我改进框架，在该框架中，模型生成新的VQA配对和图像，然后通过像素级和语义损坏来形成“好-坏”图像配对进行自我训练。我们的训练目标促使VLMs更多地关注实际的视觉输入，并已证明在增强开源VLMs（如LLaVA-v1.5和Cambrian）的性能方面具有有效性。	 | 该研究引入了ViLP基准，通过利用图像生成模型生成高度变异性图像，使现代视觉语言模型在视觉推理上表现不佳，特别是对于仅凭文本难以推断答案的图像；为此，研究提出了一种自我改进框架，使模型在自我训练中更关注实际视觉输入，从而提升模型性能。
2501.00527	 | Exploiting Boundary Loss for the Hierarchical Panoptic Segmentation of Plants and Leaves	 | Madeleine Darbyshire,Elizabeth Sklar,Simon Parsons	 | Precision agriculture leverages data and machine learning so that farmers can monitor their crops and target interventions precisely. This enables the precision application of herbicide only to weeds, or the precision application of fertilizer only to undernourished crops, rather than to the entire field. The approach promises to maximize yields while minimizing resource use and harm to the surrounding environment. To this end, we propose a hierarchical panoptic segmentation method that simultaneously determines leaf count (as an identifier of plant growth)and locates weeds within an image. In particular, our approach aims to improve the segmentation of smaller instances like the leaves and weeds by incorporating focal loss and boundary loss. Not only does this result in competitive performance, achieving a PQ+ of 81.89 on the standard training set, but we also demonstrate we can improve leaf-counting accuracy with our method. The code is available at https://github.com/madeleinedarbyshire/HierarchicalMask2Former.	 | 精准农业利用数据和机器学习技术，使农民能够精确监测作物并精准实施干预措施。这种方法使得农民可以仅对杂草喷洒除草剂，或仅对营养不良的作物施用肥料，而不是对整个农田进行处理。这种方法承诺在最大限度地提高产量的同时，减少资源使用和对周围环境的损害。为此，我们提出了一种分层全景分割方法，该方法同时确定叶片数量（作为植物生长的标识符）并在图像中定位杂草。特别地，我们的方法旨在通过引入焦点损失和边界损失来改善较小实例（如叶片和杂草）的分割，从而实现竞争性的性能，标准训练集上的PQ+达到81.89，我们还展示了通过这种方法可以提高叶片计数的准确性。相关代码可在https://github.com/madeleinedarbyshire/HierarchicalMask2Former获取。	 | 该研究提出了一种分层全景分割方法，结合焦点损失和边界损失，以精准农业为目标，同时计数叶片和定位杂草，提高作物管理的精确度和效率，相关代码已公开。
2501.00525	 | Is Segment Anything Model 2 All You Need for Surgery Video Segmentation? A Systematic Evaluation	 | Cheng Yuan,Jian Jiang,Kunyi Yang,Lv Wu,Rui Wang,Zi Meng,Haonan Ping,Ziyu Xu,Yifan Zhou,Wanli Song,Hesheng Wang,Qi Dou,Yutong Ban	 | Surgery video segmentation is an important topic in the surgical AI field. It allows the AI model to understand the spatial information of a surgical scene. Meanwhile, due to the lack of annotated surgical data, surgery segmentation models suffer from limited performance. With the emergence of SAM2 model, a large foundation model for video segmentation trained on natural videos, zero-shot surgical video segmentation became more realistic but meanwhile remains to be explored. In this paper, we systematically evaluate the performance of SAM2 model in zero-shot surgery video segmentation task. We conducted experiments under different configurations, including different prompting strategies, robustness, etc. Moreover, we conducted an empirical evaluation over the performance, including 9 datasets with 17 different types of surgeries.	 | 手术视频分割是手术人工智能领域的重要课题。它使AI模型能够理解手术场景的空间信息。同时，由于缺乏标注的手术数据，手术分割模型的性能受到限制。随着SAM2模型的出现，这是一种在自然视频上训练的大规模基础模型，零样本手术视频分割变得更加现实，但仍需进一步探索。在本文中，我们系统地评估了SAM2模型在零样本手术视频分割任务中的性能。我们进行了不同配置的实验，包括不同的提示策略、鲁棒性等。此外，我们进行了实证评估，包括9个数据集和17种不同类型的手术。	 | 本文系统评估了SAM2模型在零样本手术视频分割任务中的性能，通过不同配置实验探讨了其在多种手术类型上的适用性。
2501.00520	 | Innovative Silicosis and Pneumonia Classification: Leveraging Graph Transformer Post-hoc Modeling and Ensemble Techniques	 | Bao Q. Bui,Tien T.T. Nguyen,Duy M. Le,Cong Tran,Cuong Pham	 | This paper presents a comprehensive study on the classification and detection of Silicosis-related lung inflammation. Our main contributions include 1) the creation of a newly curated chest X-ray (CXR) image dataset named SVBCX that is tailored to the nuances of lung inflammation caused by distinct agents, providing a valuable resource for silicosis and pneumonia research community; and 2) we propose a novel deep-learning architecture that integrates graph transformer networks alongside a traditional deep neural network module for the effective classification of silicosis and pneumonia. Additionally, we employ the Balanced Cross-Entropy (BalCE) as a loss function to ensure more uniform learning across different classes, enhancing the model's ability to discern subtle differences in lung conditions. The proposed model architecture and loss function selection aim to improve the accuracy and reliability of inflammation detection, particularly in the context of Silicosis. Furthermore, our research explores the efficacy of an ensemble approach that combines the strengths of diverse model architectures. Experimental results on the constructed dataset demonstrate promising outcomes, showcasing substantial enhancements compared to baseline models. The ensemble of models achieves a macro-F1 score of 0.9749 and AUC ROC scores exceeding 0.99 for each class, underscoring the effectiveness of our approach in accurate and robust lung inflammation classification.	 | 本文对矽肺相关肺炎症的分类与检测进行了全面研究。我们的主要贡献包括：1) 创建了一个新编目的胸部X光（CXR）图像数据集，名为SVBCX，该数据集针对不同病原体引起的肺炎症的细微差别进行了定制，为矽肺和肺炎研究社区提供了宝贵的资源；以及2) 我们提出了一种新的深度学习架构，该架构将图变换器网络与传统的深度神经网络模块相结合，以有效分类矽肺和肺炎。此外，我们采用平衡交叉熵（BalCE）作为损失函数，以确保不同类别之间的更均匀学习，从而增强模型区分肺部状况细微差异的能力。提出的模型架构和损失函数选择旨在提高炎症检测的准确性和可靠性，特别是在矽肺的背景下。此外，我们的研究探讨了组合多种模型架构优势的集成方法的有效性。在所构建的数据集上的实验结果显示出令人鼓舞的结果，展示了与基线模型相比的显著改进。模型集成实现了宏F1分数为0.9749，AUC ROC分数超过0.99，突显了我们方法在准确且稳健的肺部炎症分类方面的有效性。	 | 本文通过对新编目的矽肺相关肺炎症胸部X光图像数据集（SVBCX）的研究，提出了一种结合图变换器网络与传统深度神经网络的新型架构，并采用平衡交叉熵损失函数，显著提高了矽肺和肺炎的分类准确性，实验结果显示宏F1分数为0.9749，AUC ROC分数超过0.99。
2501.00513	 | Fine-grained Video-Text Retrieval: A New Benchmark and Method	 | Yifan Xu,Xinhao Li,Yichun Yang,Rui Huang,Limin Wang	 | The ability of perceiving fine-grained spatial and temporal information is crucial for video-language retrieval. However, the existing video retrieval benchmarks, such as MSRVTT and MSVD, fail to efficiently evaluate the fine-grained retrieval ability of video-language models (VLMs) due to a lack of detailed annotations. To address this problem, we present FIBER, a FIne-grained BEnchmark for text to video Retrieval, containing 1,000 videos sourced from the FineAction dataset. Uniquely, our FIBER benchmark provides detailed human-annotated spatial annotations and temporal annotations for each video, making it possible to independently evaluate the spatial and temporal bias of VLMs on video retrieval task. Besides, we employ a text embedding method to unlock the capability of fine-grained video-language understanding of Multimodal Large Language Models (MLLMs). Surprisingly, the experiment results show that our Video Large Language Encoder (VLLE) performs comparably to CLIP-based models on traditional benchmarks and has a stronger capability of fine-grained representation with lower spatial-temporal bias. Project page: https://fiber-bench.github.io.	 | 感知精细空间和时间信息的能力对于视频-语言检索至关重要。然而，现有的视频检索基准，如MSRVTT和MSVD，由于缺乏详细的注释，无法有效地评估视频-语言模型（VLMs）的精细粒度检索能力。为解决这一问题，我们提出了FIBER，这是一个细粒度视频到文本检索基准，包含了来自FineAction数据集的1000个视频。独特之处在于，我们的FIBER基准为每个视频提供了详细的由人类标注的空间注释和时间注释，使其能够独立评估VLMs在视频检索任务中的空间和时间偏差。此外，我们采用了一种文本嵌入方法，解锁了多模态大型语言模型（MLLMs）对精细粒度视频-语言理解的能力。令人惊讶的是，实验结果表明，我们的视频大型语言编码器（VLLE）在传统基准上的表现与基于CLIP的模型相当，并且具有较低的空间-时间偏差且更强的精细粒度表示能力。项目页面：https://fiber-bench.github.io。	 | FIBER是一个细粒度视频到文本检索基准，包含了来自FineAction数据集的1000个视频，并提供了详尽的人类标注的空间和时间注释，以独立评估视频-语言模型的空间和时间检索能力。实验表明，FIBER基准上的新型视频大型语言编码器在传统基准上表现与CLIP模型相当，且具有更低的空间-时间偏差和更强的精细粒度表示能力。
2501.00437	 | Unleashing Text-to-Image Diffusion Prior for Zero-Shot Image Captioning	 | Jianjie Luo,Jingwen Chen,Yehao Li,Yingwei Pan,Jianlin Feng,Hongyang Chao,Ting Yao	 | Recently, zero-shot image captioning has gained increasing attention, where only text data is available for training. The remarkable progress in text-to-image diffusion model presents the potential to resolve this task by employing synthetic image-caption pairs generated by this pre-trained prior. Nonetheless, the defective details in the salient regions of the synthetic images introduce semantic misalignment between the synthetic image and text, leading to compromised results. To address this challenge, we propose a novel Patch-wise Cross-modal feature Mix-up (PCM) mechanism to adaptively mitigate the unfaithful contents in a fine-grained manner during training, which can be integrated into most of encoder-decoder frameworks, introducing our PCM-Net. Specifically, for each input image, salient visual concepts in the image are first detected considering the image-text similarity in CLIP space. Next, the patch-wise visual features of the input image are selectively fused with the textual features of the salient visual concepts, leading to a mixed-up feature map with less defective content. Finally, a visual-semantic encoder is exploited to refine the derived feature map, which is further incorporated into the sentence decoder for caption generation. Additionally, to facilitate the model training with synthetic data, a novel CLIP-weighted cross-entropy loss is devised to prioritize the high-quality image-text pairs over the low-quality counterparts. Extensive experiments on MSCOCO and Flickr30k datasets demonstrate the superiority of our PCM-Net compared with state-of-the-art VLMs-based approaches. It is noteworthy that our PCM-Net ranks first in both in-domain and cross-domain zero-shot image captioning. The synthetic dataset SynthImgCap and code are available at https://jianjieluo.github.io/SynthImgCap.	 | 最近，零样本图像生成描述（captioning）问题得到了越来越多的关注，其中仅可用文本数据进行训练。文本到图像的扩散模型取得了显著进展，为通过此预训练先验生成的合成图像描述对解决该问题提供了潜力。然而，合成图像中突出区域的缺陷细节导致合成图像与文本之间存在语义对齐问题，从而影响结果质量。为了解决这一挑战，我们提出了一种新的局部跨模态特征Mix-up（Patch-wise Cross-modal feature Mix-up，简称PCM）机制，以细粒度的方式适应性地减少训练过程中的不忠实内容，该机制可以集成到大多数编码器-解码器框架中，引入我们的PCM-Net。具体而言，对于每个输入图像，首先在CLIP空间中考虑图像与文本的相似性，检测出图像中的重要视觉概念。接着，根据重要视觉概念的文本特征，选择性地将输入图像的局部视觉特征与文本特征融合，生成一个含有较少缺陷特征图。最后，利用视觉-语义编码器进一步细化提取出的特征图，并将其整合到句子解码器中用于生成描述。为了使模型在使用合成数据训练时更加容易，我们还设计了一种新的CLIP加权交叉熵损失函数，优先考虑高质量的图像-文本对。在MSCOCO和Flickr30k数据集上的大量实验表明，与基于视觉语言模型（VL-M）的先进方法相比，我们的PCM-Net表现出显著的优势。值得注意的是，我们的PCM-Net在领域内和跨领域的零样本图像生成描述中均排名首位。合成数据集SynthImgCap及代码可在https://jianjieluo.github.io/SynthImgCap获取。	 | 该研究提出了一种新的局部跨模态特征Mix-up机制（PCM），用于提高零样本图像生成描述的质量，通过减少训练过程中的不忠实内容，使合成图像与文本更好地对齐。实验结果表明，基于该机制的PCM-Net在多项指标上优于现有方法，尤其在MSCOCO和Flickr30k数据集上的表现显著优于其他视觉语言模型（VL-M）方法。
2501.00432	 | OV-HHIR: Open Vocabulary Human Interaction Recognition Using Cross-modal Integration of Large Language Models	 | Lala Shakti Swarup Ray,Bo Zhou,Sungho Suh,Paul Lukowicz	 | Understanding human-to-human interactions, especially in contexts like public security surveillance, is critical for monitoring and maintaining safety. Traditional activity recognition systems are limited by fixed vocabularies, predefined labels, and rigid interaction categories that often rely on choreographed videos and overlook concurrent interactive groups. These limitations make such systems less adaptable to real-world scenarios, where interactions are diverse and unpredictable. In this paper, we propose an open vocabulary human-to-human interaction recognition (OV-HHIR) framework that leverages large language models to generate open-ended textual descriptions of both seen and unseen human interactions in open-world settings without being confined to a fixed vocabulary. Additionally, we create a comprehensive, large-scale human-to-human interaction dataset by standardizing and combining existing public human interaction datasets into a unified benchmark. Extensive experiments demonstrate that our method outperforms traditional fixed-vocabulary classification systems and existing cross-modal language models for video understanding, setting the stage for more intelligent and adaptable visual understanding systems in surveillance and beyond.	 | 理解人类之间的互动，特别是在公共安全监控等情境中，对于监测和维护安全至关重要。传统的活动识别系统受到固定词汇表、预定义标签和刚性互动类别等限制，这些类别通常依赖于编排好的视频，忽略了并发互动群体。这些限制使得这些系统在应对真实世界中的多样性且不可预测的互动时显得不够灵活。在本文中，我们提出了一种基于开放词汇表的人类间互动识别（OV-HHIR）框架，该框架利用大规模语言模型生成开放式的文本描述，无论是在已知还是未知的人类互动中，都不受固定词汇表的限制。此外，我们还创建了一个全面的大规模人类互动数据集，通过标准化和合并现有的公共人类互动数据集，形成一个统一的基准。广泛的实验表明，我们的方法在视频理解方面优于传统的固定词汇表分类系统和现有的跨模态语言模型，为更智能和适应性强的视觉理解系统在监控等领域的发展奠定了基础。	 | 本文提出了一种基于开放词汇表的人类互动识别（OV-HHIR）框架，利用大规模语言模型生成开放式文本描述，以应对真实世界中多样且不可预测的互动，该框架在视频理解方面优于传统系统和现有跨模态语言模型，为智能监控等领域的发展奠定基础。
2501.00426	 | B2Net: Camouflaged Object Detection via Boundary Aware and Boundary Fusion	 | Junmin Cai,Han Sun,Ningzhong Liu	 | Camouflaged object detection (COD) aims to identify objects in images that are well hidden in the environment due to their high similarity to the background in terms of texture and color. However, existing most boundary-guided camouflage object detection algorithms tend to generate object boundaries early in the network, and inaccurate edge priors often introduce noises in object detection. Address on this issue, we propose a novel network named B2Net aiming to enhance the accuracy of obtained boundaries by reusing boundary-aware modules at different stages of the network. Specifically, we present a Residual Feature Enhanced Module (RFEM) with the goal of integrating more discriminative feature representations to enhance detection accuracy and reliability. After that, the Boundary Aware Module (BAM) is introduced to explore edge cues twice by integrating spatial information from low-level features and semantic information from high-level features. Finally, we design the Cross-scale Boundary Fusion Module(CBFM) that integrate information across different scales in a top-down manner, merging boundary features with object features to obtain a comprehensive feature representation incorporating boundary information. Extensive experimental results on three challenging benchmark datasets demonstrate that our proposed method B2Net outperforms 15 state-of-art methods under widely used evaluation metrics. Code will be made publicly available.	 | 伪装目标检测（COD）旨在识别那些由于在纹理和颜色方面与背景高度相似而在图像中隐藏较好的目标。然而，现有的大多数基于边界引导的伪装目标检测算法倾向于在网络早期生成目标边界，不准确的边缘先验往往会引入检测中的噪声。为解决这一问题，我们提出了一种名为B2Net的新网络，旨在通过在不同网络阶段重新使用边界感知模块来增强获得边界的准确性。具体来说，我们提出了一个残差特征增强模块（RFEM），其目标是整合更多的区分性特征表示，以提高检测的准确性和可靠性。随后，我们引入了边界感知模块（BAM），通过结合低级特征的空间信息和高级特征的语义信息，两次探索边缘线索。最后，我们设计了一种跨尺度边界融合模块（CBFM），以自上而下的方式整合不同尺度的信息，将边界特征与对象特征相结合，获得包含边界信息的全面特征表示。在三个具有挑战性的基准数据集上的广泛实验结果表明，我们提出的B2Net方法在常用评价指标下优于15种最先进的方法。代码将公开发布。	 | B2Net是一种旨在通过在不同网络阶段重新使用边界感知模块来提高伪装目标检测准确性的新型网络，通过残差特征增强模块、边界感知模块和跨尺度边界融合模块，显著提高了边界检测的准确性和可靠性。实验结果表明，B2Net在三个具有挑战性的基准数据集中均优于现有的15种先进方法。
2501.00375	 | Token Pruning for Caching Better: 9 Times Acceleration on Stable Diffusion for Free	 | Evelyn Zhang,Bang Xiao,Jiayi Tang,Qianli Ma,Chang Zou,Xuefei Ning,Xuming Hu,Linfeng Zhang	 | Stable Diffusion has achieved remarkable success in the field of text-to-image generation, with its powerful generative capabilities and diverse generation results making a lasting impact. However, its iterative denoising introduces high computational costs and slows generation speed, limiting broader adoption. The community has made numerous efforts to reduce this computational burden, with methods like feature caching attracting attention due to their effectiveness and simplicity. Nonetheless, simply reusing features computed at previous timesteps causes the features across adjacent timesteps to become similar, reducing the dynamics of features over time and ultimately compromising the quality of generated images. In this paper, we introduce a dynamics-aware token pruning (DaTo) approach that addresses the limitations of feature caching. DaTo selectively prunes tokens with lower dynamics, allowing only high-dynamic tokens to participate in self-attention layers, thereby extending feature dynamics across timesteps. DaTo combines feature caching with token pruning in a training-free manner, achieving both temporal and token-wise information reuse. Applied to Stable Diffusion on the ImageNet, our approach delivered a 9$\times$ speedup while reducing FID by 0.33, indicating enhanced image quality. On the COCO-30k, we observed a 7$\times$ acceleration coupled with a notable FID reduction of 2.17.	 | 稳定扩散（Stable Diffusion）在文本到图像生成领域取得了显著的成功，其强大的生成能力和多样的生成结果产生了深远的影响。然而，其迭代去噪过程引入了高昂的计算成本，并减慢了生成速度，这限制了其更广泛的采用。社区已经做出了多项努力来减轻这种计算负担，其中像特征缓存这样的方法因其有效性和简单性而吸引了大量关注。然而，简单地在先前时间步长中重新使用特征会导致相邻时间步长中的特征变得相似，从而减少了特征随时间的动态性，并最终损害了生成图像的质量。本文介绍了一种关注动态的标记剪枝（DaTo）方法，以解决特征缓存的局限性。DaTo 选择性地修剪动态较低的标记，仅允许高动态标记参与自注意力层，从而在时间步长之间延长特征动态性。DaTo 以无训练的方式结合了特征缓存与标记剪枝，实现了时间和标记级别的信息重用。我们将这种方法应用于ImageNet中的稳定扩散，结果实现了9倍的速度提升，并将FID降低了0.33，表明提高了图像质量。在COCO-30k上，我们观察到7倍的加速，并且FID显著降低了2.17。	 | 稳定扩散在文本到图像生成中表现出色，但其迭代去噪过程导致了计算成本高、生成速度慢的问题。为此，本文提出了一种动态标记剪枝（DaTo）方法，通过选择性修剪动态较低的标记，提升了特征的动态性，实现了9倍的速度提升并提高了图像质量。
2501.00360	 | A Novel Shape Guided Transformer Network for Instance Segmentation in Remote Sensing Images	 | Dawen Yu,Shunping Ji	 | Instance segmentation performance in remote sensing images (RSIs) is significantly affected by two issues: how to extract accurate boundaries of objects from remote imaging through the dynamic atmosphere, and how to integrate the mutual information of related object instances scattered over a vast spatial region. In this study, we propose a novel Shape Guided Transformer Network (SGTN) to accurately extract objects at the instance level. Inspired by the global contextual modeling capacity of the self-attention mechanism, we propose an effective transformer encoder termed LSwin, which incorporates vertical and horizontal 1D global self-attention mechanisms to obtain better global-perception capacity for RSIs than the popular local-shifted-window based Swin Transformer. To achieve accurate instance mask segmentation, we introduce a shape guidance module (SGM) to emphasize the object boundary and shape information. The combination of SGM, which emphasizes the local detail information, and LSwin, which focuses on the global context relationships, achieve excellent RSI instance segmentation. Their effectiveness was validated through comprehensive ablation experiments. Especially, LSwin is proved better than the popular ResNet and Swin transformer encoder at the same level of efficiency. Compared to other instance segmentation methods, our SGTN achieves the highest average precision (AP) scores on two single-class public datasets (WHU dataset and BITCC dataset) and a multi-class public dataset (NWPU VHR-10 dataset). Code will be available at http://gpcv.whu.edu.cn/data/.	 | 遥感图像（RSIs）中的实例分割性能受到两个问题的显著影响：如何通过动态大气从遥感成像中准确提取物体边界，以及如何整合分布在广阔空间区域中的相关物体实例的互信息。在本研究中，我们提出了一种新颖的形状引导变换器网络（SGTN）以准确在实例级别提取物体。受自我注意力机制的全局上下文建模能力的启发，我们提出了一种有效的变换器编码器LSwin，该编码器结合了垂直和水平的1D全局自我注意力机制，从而在遥感图像上获得了比基于局部移动窗口的Swin变换器更好的全局感知能力。为了实现准确的实例掩模分割，我们引入了一种形状引导模块（SGM），以强调物体边界和形状信息。SGM强调局部细节信息，而LSwin注重全局上下文关系，这两种方法的结合在遥感图像实例分割上表现出色。其有效性通过全面的消融实验得到了验证。特别是，LSwin在效率相同的情况下证明优于流行的ResNet和Swin变换器编码器。与其它实例分割方法相比，我们的SGTN在两个单类公开数据集（WHU数据集和BITCC数据集）和一个多类公开数据集（NWPU VHR-10数据集）上实现了最高的平均精度（AP）分数。代码将在 http://gpcv.whu.edu.cn/data/ 获取。	 | 本文提出了一种新颖的形状引导变换器网络（SGTN），结合了形状引导模块（SGM）和新型的LSwin编码器，以提高遥感图像实例分割的精度。实验结果表明，SGTN在多个公开数据集上实现了最高的平均精度，并且LSwin编码器在全局感知能力方面优于传统的ResNet和Swin变换器编码器。
2501.00358	 | Embodied VideoAgent: Persistent Memory from Egocentric Videos and Embodied Sensors Enables Dynamic Scene Understanding	 | Yue Fan,Xiaojian Ma,Rongpeng Su,Jun Guo,Rujie Wu,Xi Chen,Qing Li	 | This paper investigates the problem of understanding dynamic 3D scenes from egocentric observations, a key challenge in robotics and embodied AI. Unlike prior studies that explored this as long-form video understanding and utilized egocentric video only, we instead propose an LLM-based agent, Embodied VideoAgent, which constructs scene memory from both egocentric video and embodied sensory inputs (e.g. depth and pose sensing). We further introduce a VLM-based approach to automatically update the memory when actions or activities over objects are perceived. Embodied VideoAgent attains significant advantages over counterparts in challenging reasoning and planning tasks in 3D scenes, achieving gains of 4.9% on Ego4D-VQ3D, 5.8% on OpenEQA, and 11.7% on EnvQA. We have also demonstrated its potential in various embodied AI tasks including generating embodied interactions and perception for robot manipulation. The code and demo will be made public.	 | 本文探讨了从第一人称观察中理解动态3D场景的问题，这是机器人技术和具身人工智能中的一个关键挑战。与先前研究将此问题视为长视频理解，并仅利用第一人称视频的做法不同，我们提出了一种基于LLM的代理——具身视频代理（Embodied VideoAgent），它从第一人称视频和具身感官输入（例如深度和姿态感应）中构建场景记忆。我们进一步提出了一种基于VLM的方法，可以在感知到动作或物体活动时自动更新记忆。在3D场景中的复杂推理和规划任务中，具身视频代理取得了显著优势，在Ego4D-VQ3D上取得了4.9%的改进，在OpenEQA上取得了5.8%的改进，在EnvQA上取得了11.7%的改进。我们还展示了它在各种具身人工智能任务中的潜力，包括生成机器人操作的具身交互和感知。代码和演示将公开发布。	 | 本文提出了一个基于LLM的具身视频代理，能够从第一人称视频和具身感官输入中构建场景记忆，并在3D场景的推理和规划任务中表现出色，相比现有方法在多个基准测试上取得了显著改进。此外，该代理在多种具身人工智能任务中也显示了广泛应用的潜力。
2501.00352	 | PanoSLAM: Panoptic 3D Scene Reconstruction via Gaussian SLAM	 | Runnan Chen,Zhaoqing Wang,Jiepeng Wang,Yuexin Ma,Mingming Gong,Wenping Wang,Tongliang Liu	 | Understanding geometric, semantic, and instance information in 3D scenes from sequential video data is essential for applications in robotics and augmented reality. However, existing Simultaneous Localization and Mapping (SLAM) methods generally focus on either geometric or semantic reconstruction. In this paper, we introduce PanoSLAM, the first SLAM system to integrate geometric reconstruction, 3D semantic segmentation, and 3D instance segmentation within a unified framework. Our approach builds upon 3D Gaussian Splatting, modified with several critical components to enable efficient rendering of depth, color, semantic, and instance information from arbitrary viewpoints. To achieve panoptic 3D scene reconstruction from sequential RGB-D videos, we propose an online Spatial-Temporal Lifting (STL) module that transfers 2D panoptic predictions from vision models into 3D Gaussian representations. This STL module addresses the challenges of label noise and inconsistencies in 2D predictions by refining the pseudo labels across multi-view inputs, creating a coherent 3D representation that enhances segmentation accuracy. Our experiments show that PanoSLAM outperforms recent semantic SLAM methods in both mapping and tracking accuracy. For the first time, it achieves panoptic 3D reconstruction of open-world environments directly from the RGB-D video. (https://github.com/runnanchen/PanoSLAM)	 | 从序贯视频数据中理解3D场景中的几何、语义和实例信息对于机器人技术和增强现实应用至关重要。然而，现有的同时定位与建图（SLAM）方法通常侧重于几何或语义重构之一。本文中，我们引入了PanoSLAM，这是首个在统一框架中结合几何重构、3D语义分割和3D实例分割的SLAM系统。我们的方法基于3D高斯点积，并通过几个关键组件进行了修改，以实现从任意视角高效地渲染深度、颜色、语义和实例信息。为实现从序贯RGB-D视频中进行全景3D场景重构，我们提出了一种在线空间-时间提升（STL）模块，该模块将视觉模型中的2D全景预测转换为3D高斯表示。STL模块通过在多视图输入中细化伪标签，解决2D预测中的标签噪声和不一致性问题，从而创建一个连贯的3D表示，提高分割精度。我们的实验表明，PanoSLAM在建图和跟踪准确性方面优于近期的语义SLAM方法。它是首次能够直接从RGB-D视频中进行开放世界环境的全景3D重构。（https://github.com/runnanchen/PanoSLAM）	 | PanoSLAM是一种创新的SLAM系统，能够在统一框架中实现几何、语义和实例信息的三维重建，通过在线空间-时间提升模块有效解决标签噪声和不一致性问题，从而提高分割精度，并在地图构建和跟踪准确性上优于现有方法。
2501.00346	 | CNC: Cross-modal Normality Constraint for Unsupervised Multi-class Anomaly Detection	 | Xiaolei Wang,Xiaoyang Wang,Huihui Bai,Eng Gee Lim,Jimin Xiao	 | Existing unsupervised distillation-based methods rely on the differences between encoded and decoded features to locate abnormal regions in test images. However, the decoder trained only on normal samples still reconstructs abnormal patch features well, degrading performance. This issue is particularly pronounced in unsupervised multi-class anomaly detection tasks. We attribute this behavior to over-generalization(OG) of decoder: the significantly increasing diversity of patch patterns in multi-class training enhances the model generalization on normal patches, but also inadvertently broadens its generalization to abnormal patches. To mitigate OG, we propose a novel approach that leverages class-agnostic learnable prompts to capture common textual normality across various visual patterns, and then apply them to guide the decoded features towards a normal textual representation, suppressing over-generalization of the decoder on abnormal patterns. To further improve performance, we also introduce a gated mixture-of-experts module to specialize in handling diverse patch patterns and reduce mutual interference between them in multi-class training. Our method achieves competitive performance on the MVTec AD and VisA datasets, demonstrating its effectiveness.	 | 现有的无监督蒸馏方法依赖于编码和解码特征之间的差异来定位测试图像中的异常区域。然而，仅在正常样本上训练的解码器仍然能够很好地重建异常区域的特征，这会降低性能。这一问题在无监督多类别异常检测任务中尤为明显。我们认为这种行为是由于解码器的过度泛化（OG）：多类别训练中补丁模式的显著多样化增强了模型对正常补丁的泛化能力，但也无意中扩大了其对异常补丁的泛化能力。为减轻OG，我们提出了一种新颖的方法，该方法利用通用可学习提示来捕捉各种视觉模式下的共同文本正常性，并将其应用于引导解码特征向正常的文本表示转化，从而抑制解码器对异常补丁模式的过度泛化。为了进一步提高性能，我们还引入了一个门控混合专家模块，专门处理多类别训练中多样化的补丁模式，并减少它们之间的相互干扰。我们的方法在MVTec AD和VisA数据集上取得了竞争力的表现，证明了其有效性。	 | 该研究提出了一个新颖的无监督蒸馏方法，通过引入通用可学习提示和门控混合专家模块，来减轻解码器在多类别异常检测中的过度泛化问题，从而提升异常检测性能。该方法在MVTec AD和VisA数据集上表现出色，验证了其有效性和竞争力。
2501.00342	 | SG-Splatting: Accelerating 3D Gaussian Splatting with Spherical Gaussians	 | Yiwen Wang,Siyuan Chen,Ran Yi	 | 3D Gaussian Splatting is emerging as a state-of-the-art technique in novel view synthesis, recognized for its impressive balance between visual quality, speed, and rendering efficiency. However, reliance on third-degree spherical harmonics for color representation introduces significant storage demands and computational overhead, resulting in a large memory footprint and slower rendering speed. We introduce SG-Splatting with Spherical Gaussians based color representation, a novel approach to enhance rendering speed and quality in novel view synthesis. Our method first represents view-dependent color using Spherical Gaussians, instead of three degree spherical harmonics, which largely reduces the number of parameters used for color representation, and significantly accelerates the rendering process. We then develop an efficient strategy for organizing multiple Spherical Gaussians, optimizing their arrangement to achieve a balanced and accurate scene representation. To further improve rendering quality, we propose a mixed representation that combines Spherical Gaussians with low-degree spherical harmonics, capturing both high- and low-frequency color information effectively. SG-Splatting also has plug-and-play capability, allowing it to be easily integrated into existing systems. This approach improves computational efficiency and overall visual fidelity, making it a practical solution for real-time applications.	 | 3D高斯点渲染正逐渐成为新颖视图合成领域的前沿技术，因其在视觉质量、速度和渲染效率之间的出色平衡而受到认可。然而，依赖三次球谐函数来表示颜色会带来显著的存储需求和计算开销，导致较大的内存占用和较慢的渲染速度。我们提出了基于球形高斯函数的颜色表示的SG-Splatting方法，这是一种增强新颖视图合成渲染速度和质量的新颖方法。我们的方法首先使用球形高斯函数来表示视角依赖的颜色，而不是三次球谐函数，这大大减少了用于颜色表示的参数数量，并显著加速了渲染过程。我们随后开发了一种高效的策略，用于组织多个球形高斯函数，优化它们的布局以实现场景表示的平衡和准确。为了进一步提高渲染质量，我们提出了一种混合表示方法，将球形高斯函数与低次球谐函数结合使用，有效捕捉高和低频率的颜色信息。SG-Splatting具备即插即用功能，可以轻松集成到现有系统中。这种方法提高了计算效率和整体视觉保真度，使其成为实时应用的一个实用解决方案。	 | 该研究提出了一种基于球形高斯函数的SG-Splatting方法，通过减少颜色表示的参数数量和优化布局，显著提高了新颖视图合成的渲染速度和质量，同时保持了视觉保真度。该方法结合了球形高斯函数和低次球谐函数，有效降低了存储需求和计算开销，适用于实时应用的集成。
2501.00340	 | Dynamic Prompt Adjustment for Multi-Label Class-Incremental Learning	 | Haifeng Zhao,Yuguang Jin,Leilei Ma	 | Significant advancements have been made in single label incremental learning (SLCIL),yet the more practical and challenging multi label class incremental learning (MLCIL) remains understudied. Recently,visual language models such as CLIP have achieved good results in classification tasks. However,directly using CLIP to solve MLCIL issue can lead to catastrophic forgetting. To tackle this issue, we integrate an improved data replay mechanism and prompt loss to curb knowledge forgetting. Specifically,our model enhances the prompt information to better adapt to multi-label classification tasks and employs confidence-based replay strategy to select representative samples. Moreover, the prompt loss significantly reduces the model's forgetting of previous knowledge. Experimental results demonstrate that our method has substantially improved the performance of MLCIL tasks across multiple benchmark datasets,validating its effectiveness.	 | 在单标签增量学习（SLCIL）方面已经取得了显著的进步，但更为实际和具有挑战性的多标签类别增量学习（MLCIL）仍较少被研究。最近，视觉语言模型如CLIP在分类任务中取得了很好的成绩。然而，直接使用CLIP解决MLCIL问题可能会导致灾难性遗忘。为了解决这个问题，我们整合了一种改进的数据重播机制和提示损失，以遏制知识遗忘。具体而言，我们的模型增强提示信息以更好地适应多标签分类任务，并采用基于置信度的重播策略来选择代表性样本。此外，提示损失显著降低了模型对之前知识的遗忘。实验结果表明，我们的方法在多个基准数据集上的MLCIL任务性能有了显著提升，验证了其有效性。	 | 该研究针对多标签类别增量学习（MLCIL）进行了探索，提出了一种结合改进数据重播机制和提示损失的方法，有效防止了模型的知识遗忘，并在多个基准数据集上显著提高了MLCIL任务的性能。
2501.00326	 | OVGaussian: Generalizable 3D Gaussian Segmentation with Open Vocabularies	 | Runnan Chen,Xiangyu Sun,Zhaoqing Wang,Youquan Liu,Jiepeng Wang,Lingdong Kong,Jiankang Deng,Mingming Gong,Liang Pan,Wenping Wang,Tongliang Liu	 | Open-vocabulary scene understanding using 3D Gaussian (3DGS) representations has garnered considerable attention. However, existing methods mostly lift knowledge from large 2D vision models into 3DGS on a scene-by-scene basis, restricting the capabilities of open-vocabulary querying within their training scenes so that lacking the generalizability to novel scenes. In this work, we propose \textbf{OVGaussian}, a generalizable \textbf{O}pen-\textbf{V}ocabulary 3D semantic segmentation framework based on the 3D \textbf{Gaussian} representation. We first construct a large-scale 3D scene dataset based on 3DGS, dubbed \textbf{SegGaussian}, which provides detailed semantic and instance annotations for both Gaussian points and multi-view images. To promote semantic generalization across scenes, we introduce Generalizable Semantic Rasterization (GSR), which leverages a 3D neural network to learn and predict the semantic property for each 3D Gaussian point, where the semantic property can be rendered as multi-view consistent 2D semantic maps. In the next, we propose a Cross-modal Consistency Learning (CCL) framework that utilizes open-vocabulary annotations of 2D images and 3D Gaussians within SegGaussian to train the 3D neural network capable of open-vocabulary semantic segmentation across Gaussian-based 3D scenes. Experimental results demonstrate that OVGaussian significantly outperforms baseline methods, exhibiting robust cross-scene, cross-domain, and novel-view generalization capabilities. Code and the SegGaussian dataset will be released. (https://github.com/runnanchen/OVGaussian).	 | 使用3D高斯（3DGS）表示进行开放词汇场景理解受到了广泛关注。然而，现有的方法主要从大型2D视觉模型中提取知识，并在场景的基础上将其提升到3DGS上，这限制了开放词汇查询的能力，仅在训练场景内有效，无法泛化到新的场景中。在本文中，我们提出了一种基于3D高斯表示的泛化开放词汇3D语义分割框架，命名为**OVGaussian**。首先，我们基于3DGS构建了一个大规模的3D场景数据集，命名为**SegGaussian**，该数据集提供了高斯点和多视图图像的详细语义和实例注释。为了促进场景间的语义泛化，我们引入了泛化语义稀疏化（GSR），这是一种利用3D神经网络学习和预测每个3D高斯点的语义属性的方法，其中语义属性可以渲染为多视图一致的2D语义图。随后，我们提出了跨模态一致性学习（CCL）框架，该框架利用SegGaussian中的2D图像和3D高斯点的开放词汇注释来训练能够在基于高斯点的3D场景中进行开放词汇语义分割的3D神经网络。实验结果表明，OVGaussian显著优于基线方法，展示出跨场景、跨领域和新颖视角的泛化能力。代码和SegGaussian数据集将被发布。（https://github.com/runnanchen/OVGaussian）。	 | 本文提出了一种名为OVGaussian的框架，用于泛化开放词汇3D语义分割，它通过构建大规模3D场景数据集SegGaussian和引入泛化语义稀疏化以及跨模态一致性学习方法，增强了在不同场景和视角下的语义分割能力。
2501.00321	 | OCRBench v2: An Improved Benchmark for Evaluating Large Multimodal Models on Visual Text Localization and Reasoning	 | Ling Fu,Biao Yang,Zhebin Kuang,Jiajun Song,Yuzhe Li,Linghao Zhu,Qidi Luo,Xinyu Wang,Hao Lu,Mingxin Huang,Zhang Li,Guozhi Tang,Bin Shan,Chunhui Lin,Qi Liu,Binghong Wu,Hao Feng,Hao Liu,Can Huang,Jingqun Tang,Wei Chen,Lianwen Jin,Yuliang Liu,Xiang Bai	 | Scoring the Optical Character Recognition (OCR) capabilities of Large Multimodal Models (LMMs) has witnessed growing interest recently. Existing benchmarks have highlighted the impressive performance of LMMs in text recognition; however, their abilities on certain challenging tasks, such as text localization, handwritten content extraction, and logical reasoning, remain underexplored. To bridge this gap, we introduce OCRBench v2, a large-scale bilingual text-centric benchmark with currently the most comprehensive set of tasks (4x more tasks than the previous multi-scene benchmark OCRBench), the widest coverage of scenarios (31 diverse scenarios including street scene, receipt, formula, diagram, and so on), and thorough evaluation metrics, with a total of 10,000 human-verified question-answering pairs and a high proportion of difficult samples. After carefully benchmarking state-of-the-art LMMs on OCRBench v2, we find that 20 out of 22 LMMs score below 50 (100 in total) and suffer from five-type limitations, including less frequently encountered text recognition, fine-grained perception, layout perception, complex element parsing, and logical reasoning. The benchmark and evaluation scripts are available at https://github.com/Yuliang-liu/MultimodalOCR.	 | 近年来，评估大型多模态模型（LMMs）的光学字符识别（OCR）能力引起了越来越多的关注。现有的基准已经突显了LMMs在文本识别方面的出色表现；然而，它们在某些具有挑战性的任务上的能力，如文本定位、手写内容提取和逻辑推理，仍然没有得到充分探索。为了解决这一差距，我们引入了OCR Bench v2，这是一个大规模的双语文本中心基准，目前包含的任务种类最多（比之前多四倍）、场景覆盖面最广（包括31种不同的场景，如街道场景、收据、公式、图表等），并且具有详尽的评估指标，总共包括10,000个由人工验证的问题-答案对，且包含大量的困难样本。经过仔细测试OCR Bench v2上的最新LMMs后，我们发现有20个中的22个LMMs得分低于50（满分100分），并且遭受五种类型的限制，包括不常见的文本识别、细粒度感知、布局感知、复杂元素解析和逻辑推理。基准数据集和评估脚本可以在https://github.com/Yuliang-liu/MultimodalOCR获取。	 | OCR Bench v2 是一个大规模的双语文本中心基准，包含31种不同场景和10,000个由人工验证的问题-答案对，旨在评估大型多模态模型在文本识别、手写内容提取和逻辑推理等任务上的能力。研究表明，当前最先进的大型多模态模型在这项基准测试中表现不佳，普遍得分低于50分，并且面临多种限制。
2501.00318	 | Improving Text-based Person Search via Part-level Cross-modal Correspondence	 | Jicheol Park,Boseung Jeong,Dongwon Kim,Suha Kwak	 | Text-based person search is the task of finding person images that are the most relevant to the natural language text description given as query. The main challenge of this task is a large gap between the target images and text queries, which makes it difficult to establish correspondence and distinguish subtle differences across people. To address this challenge, we introduce an efficient encoder-decoder model that extracts coarse-to-fine embedding vectors which are semantically aligned across the two modalities without supervision for the alignment. There is another challenge of learning to capture fine-grained information with only person IDs as supervision, where similar body parts of different individuals are considered different due to the lack of part-level supervision. To tackle this, we propose a novel ranking loss, dubbed commonality-based margin ranking loss, which quantifies the degree of commonality of each body part and reflects it during the learning of fine-grained body part details. As a consequence, it enables our method to achieve the best records on three public benchmarks.	 | 基于文本的人像搜索是指根据给定的自然语言文本描述查找最相关的图像。这项任务的主要挑战在于目标图像和文本查询之间存在巨大的差距，这使得建立对应关系和区分不同人的细微差异变得困难。为了解决这一挑战，我们引入了一个高效的编码-解码模型，该模型提取从粗到细的嵌入向量，并在没有监督的情况下使两种模态之间的嵌入向量语义对齐。另一个挑战是如何仅使用人员ID作为监督信息来学习捕捉细微的信息，因为在缺乏部件级监督的情况下，不同个体的相似身体部位被视为不同。为了解决这一问题，我们提出了一种新颖的排名损失，称为基于共性度的边际排名损失，该损失量化每个身体部位的共性度，并在学习细微的身体部位细节时反映这种共性度。因此，我们的方法在三个公开基准上取得了最佳记录。	 | 该研究提出了一种高效的编码-解码模型，通过从粗到细提取嵌入向量并在无监督的情况下使图像和文本嵌入对齐，以解决基于文本的人像搜索挑战；同时引入了一种基于共性度的边际排名损失，以利用人员ID监督学习捕捉细微的个体差异，从而在多个公开基准上取得了最优性能。
2501.00317	 | Spatio-Temporal Multi-Subgraph GCN for 3D Human Motion Prediction	 | Jiexin Wang,Yiju Guo,Bing Su	 | Human motion prediction (HMP) involves forecasting future human motion based on historical data. Graph Convolutional Networks (GCNs) have garnered widespread attention in this field for their proficiency in capturing relationships among joints in human motion. However, existing GCN-based methods tend to focus on either temporal-domain or spatial-domain features, or they combine spatio-temporal features without fully leveraging the complementarity and cross-dependency of these two features. In this paper, we propose the Spatial-Temporal Multi-Subgraph Graph Convolutional Network (STMS-GCN) to capture complex spatio-temporal dependencies in human motion. Specifically, we decouple the modeling of temporal and spatial dependencies, enabling cross-domain knowledge transfer at multiple scales through a spatio-temporal information consistency constraint mechanism. Besides, we utilize multiple subgraphs to extract richer motion information and enhance the learning associations of diverse subgraphs through a homogeneous information constraint mechanism. Extensive experiments on the standard HMP benchmarks demonstrate the superiority of our method.	 | 人体运动预测（HMP）涉及根据历史数据预测未来的运动。图卷积网络（GCNs）在这一领域受到了广泛关注，因为它们擅长捕捉人体运动中关节之间的关系。然而，现有的基于GCN的方法往往侧重于时间域或空间域特征，或者在结合时空特征时未能充分利用这些两种特征之间的互补性和交叉依赖性。本文中，我们提出了时空多子图图卷积网络（STMS-GCN），以捕捉人体运动中的复杂时空依赖关系。具体而言，我们分解了时间域和空间域依赖性的建模，通过时空信息一致性约束机制在多个尺度上实现跨域知识转移。此外，我们利用多个子图来提取更丰富的运动信息，并通过同质信息约束机制增强不同子图的学习关联。在标准HMP基准数据集上的广泛实验表明了我们方法的优势。	 | 本文提出了一种名为STMS-GCN的方法，用于捕捉人体运动中的复杂时空依赖关系，通过在多个尺度上实现跨域知识转移和利用多个子图提取更丰富的运动信息来改进现有基于GCN的方法。实验结果表明，STMS-GCN在标准HMP基准数据集上表现优越。
2501.00315	 | Temporal Dynamics Decoupling with Inverse Processing for Enhancing Human Motion Prediction	 | Jiexin Wang,Yiju Guo,Bing Su	 | Exploring the bridge between historical and future motion behaviors remains a central challenge in human motion prediction. While most existing methods incorporate a reconstruction task as an auxiliary task into the decoder, thereby improving the modeling of spatio-temporal dependencies, they overlook the potential conflicts between reconstruction and prediction tasks. In this paper, we propose a novel approach: Temporal Decoupling Decoding with Inverse Processing (\textbf{$TD^2IP$}). Our method strategically separates reconstruction and prediction decoding processes, employing distinct decoders to decode the shared motion features into historical or future sequences. Additionally, inverse processing reverses motion information in the temporal dimension and reintroduces it into the model, leveraging the bidirectional temporal correlation of human motion behaviors. By alleviating the conflicts between reconstruction and prediction tasks and enhancing the association of historical and future information, \textbf{$TD^2IP$} fosters a deeper understanding of motion patterns. Extensive experiments demonstrate the adaptability of our method within existing methods.	 | 探索历史和未来运动行为之间的桥梁仍然是人类运动预测中的一个核心挑战。虽然大多数现有方法将重建任务作为辅助任务集成到解码器中，从而提高了时空依赖关系的建模，但它们忽视了重建任务和预测任务之间的潜在冲突。在本文中，我们提出了一种新颖的方法：时间解耦解码与逆向处理（\textbf{$TD^2IP$}）。我们的方法有策略性地分离了重建和预测的解码过程，使用不同的解码器将共享的运动特征解码为历史或未来序列。此外，逆向处理在时间维度上反转运动信息并将其重新引入模型中，利用人类运动行为的双向时间相关性。通过缓解重建和预测任务之间的冲突，并增强历史和未来信息的关联，\textbf{$TD^2IP$} 促进了对运动模式的更深层次理解。大量实验表明，我们的方法在现有方法中具有很高的适应性。	 | 本文提出了一种新颖的方法$TD^2IP$，通过时间解耦解码与逆向处理策略，分别处理重建和预测任务，利用运动行为的双向时间相关性，从而提高运动预测的准确性。该方法有效缓解了重建和预测任务之间的冲突，增强了历史和未来信息的关联，展现了在多种实验中的优越性能。
2501.00303	 | SAM-Aware Graph Prompt Reasoning Network for Cross-Domain Few-Shot Segmentation	 | Shi-Feng Peng,Guolei Sun,Yong Li,Hongsong Wang,Guo-Sen Xie	 | The primary challenge of cross-domain few-shot segmentation (CD-FSS) is the domain disparity between the training and inference phases, which can exist in either the input data or the target classes. Previous models struggle to learn feature representations that generalize to various unknown domains from limited training domain samples. In contrast, the large-scale visual model SAM, pre-trained on tens of millions of images from various domains and classes, possesses excellent generalizability. In this work, we propose a SAM-aware graph prompt reasoning network (GPRN) that fully leverages SAM to guide CD-FSS feature representation learning and improve prediction accuracy. Specifically, we propose a SAM-aware prompt initialization module (SPI) to transform the masks generated by SAM into visual prompts enriched with high-level semantic information. Since SAM tends to divide an object into many sub-regions, this may lead to visual prompts representing the same semantic object having inconsistent or fragmented features. We further propose a graph prompt reasoning (GPR) module that constructs a graph among visual prompts to reason about their interrelationships and enable each visual prompt to aggregate information from similar prompts, thus achieving global semantic consistency. Subsequently, each visual prompt embeds its semantic information into the corresponding mask region to assist in feature representation learning. To refine the segmentation mask during testing, we also design a non-parameter adaptive point selection module (APS) to select representative point prompts from query predictions and feed them back to SAM to refine inaccurate segmentation results. Experiments on four standard CD-FSS datasets demonstrate that our method establishes new state-of-the-art results. Code: https://github.com/CVL-hub/GPRN.	 | 跨领域少量标注分割（CD-FSS）的主要挑战在于训练阶段与推理阶段之间的领域差异，这种差异可能存在于输入数据或目标类别中。以往的模型难以从有限的训练领域样本中学习到能在各种未知领域中泛化的特征表示。相比之下，大规模视觉模型SAM在从数百万张来自各种领域和类别的图像中预训练后，具有出色的泛化能力。在本文中，我们提出了一个SAM感知的图提示推理网络（GPRN），该网络充分利用SAM来指导CD-FSS特征表示学习，从而提高预测准确性。具体而言，我们提出了一种SAM感知的提示初始化模块（SPI），将SAM生成的掩码转换为包含高层语义信息的视觉提示。由于SAM倾向于将一个对象分割成许多子区域，这可能导致表示同一语义对象的视觉提示具有不一致或碎片化的特征。我们进一步提出了一种图提示推理（GPR）模块，该模块在视觉提示之间构建一个图来推理它们之间的关系，并使每个视觉提示能够从相似的提示中聚合信息，从而实现全局语义一致性。随后，每个视觉提示将其语义信息嵌入到相应的掩码区域中，以协助特征表示学习。为了在测试阶段细化分割掩码，我们还设计了一种非参数自适应点选择模块（APS），从查询预测中选择代表性点提示，并将其反馈给SAM以改进不准确的分割结果。在四个标准CD-FSS数据集上的实验表明，我们的方法建立了新的最佳性能。代码：https://github.com/CVL-hub/GPRN。	 | 本文提出了一种SAM感知的图提示推理网络（GPRN），通过利用大规模预训练模型SAM的泛化能力，提出SAM感知的提示初始化模块和图提示推理模块来解决跨领域少量标注分割中的领域差异问题，并在多个标准数据集上取得了最优性能。
2501.00300	 | Research on vehicle detection based on improved YOLOv8 network	 | Haocheng Guo,Yaqiong Zhang,Lieyang Chen,Arfat Ahmad Khan	 | The key to ensuring the safe obstacle avoidance function of autonomous driving systems lies in the use of extremely accurate vehicle recognition techniques. However, the variability of the actual road environment and the diverse characteristics of vehicles and pedestrians together constitute a huge obstacle to improving detection accuracy, posing a serious challenge to the realization of this goal. To address the above issues, this paper proposes an improved YOLOv8 vehicle detection method. Specifically, taking the YOLOv8n-seg model as the base model, firstly, the FasterNet network is used to replace the backbone network to achieve the purpose of reducing the computational complexity and memory while improving the detection accuracy and speed; secondly, the feature enhancement is achieved by adding the attention mechanism CBAM to the Neck; and lastly, the loss function CIoU is modified to WIoU, which optimizes the detection box localization while improving the segmentation accuracy. The results show that the improved model achieves 98.3%, 89.1% and 88.4% detection accuracy for car, Person and Motorcycle. Compared with the pre-improvement and YOLOv9 models in six metrics such as Precision.	 | 确保自动驾驶系统安全避障功能的关键在于使用极其精确的车辆识别技术。然而，实际道路环境的多样性以及车辆和行人的多种特性共同构成了提高检测精度的巨大障碍，对实现这一目标构成了严重挑战。为了解决上述问题，本文提出了一种改进的YOLOv8车辆检测方法。具体而言，以YOLOv8n-seg模型为基础模型，首先，使用FasterNet网络替换骨干网络，以达到在减少计算复杂性和内存消耗的同时提高检测准确性和速度的目的；其次，通过在Neck中添加注意力机制CBAM来实现特征增强；最后，将损失函数CIoU修改为WIoU，优化检测框定位的同时提高分割精度。实验结果显示，改进后的模型在车辆、行人和摩托车的检测准确率分别达到了98.3%、89.1%和88.4%。与改进前以及YOLOv9模型在精度、召回率等六个指标上的表现相比，改进后的模型具有明显的优势。	 | 本文提出了一种改进的YOLOv8车辆检测方法，通过使用FasterNet替换骨干网络、添加注意力机制CBAM以及修改损失函数为WIoU，显著提高了对车辆、行人和摩托车的检测精度，实验显示其在多个指标上优于原有模型。
2501.00289	 | Dual Diffusion for Unified Image Generation and Understanding	 | Zijie Li,Henry Li,Yichun Shi,Amir Barati Farimani,Yuval Kluger,Linjie Yang,Peng Wang	 | Diffusion models have gained tremendous success in text-to-image generation, yet still lag behind with visual understanding tasks, an area dominated by autoregressive vision-language models. We propose a large-scale and fully end-to-end diffusion model for multi-modal understanding and generation that significantly improves on existing diffusion-based multimodal models, and is the first of its kind to support the full suite of vision-language modeling capabilities. Inspired by the multimodal diffusion transformer (MM-DiT) and recent advances in discrete diffusion language modeling, we leverage a cross-modal maximum likelihood estimation framework that simultaneously trains the conditional likelihoods of both images and text jointly under a single loss function, which is back-propagated through both branches of the diffusion transformer. The resulting model is highly flexible and capable of a wide range of tasks including image generation, captioning, and visual question answering. Our model attained competitive performance compared to recent unified image understanding and generation models, demonstrating the potential of multimodal diffusion modeling as a promising alternative to autoregressive next-token prediction models.	 | 扩散模型在文本到图像生成任务中取得了巨大的成功，但在视觉理解任务上仍然落后于由自回归视觉语言模型主导的领域。我们提出了一种大规模且完全端到端的多模态扩散模型，显著改进了现有的基于扩散的多模态模型，并且是首个能够支持全系列视觉语言建模能力的模型。受到多模态扩散变换器（MM-DiT）和离散扩散语言建模最近进展的启发，我们利用跨模态最大似然估计框架，在单一损失函数下同时训练图像和文本的条件似然，该框架通过扩散变换器的两条支路反向传播。由此产生的模型非常灵活，能够应用于包括图像生成、字幕生成和视觉问答等一系列任务。与近期统一的图像理解和生成模型相比，我们的模型取得了竞争力的表现，这表明多模态扩散建模作为一种替代自回归下一词预测模型的有前途的方法的潜力。	 | 本文提出了一种大规模且端到端的多模态扩散模型，该模型能够在视觉语言建模任务上显著超越现有方法，并且能够支持全系列视觉语言能力。通过跨模态最大似然估计框架，该模型在图像生成、字幕生成和视觉问答等任务上取得了竞争力的表现，展示了多模态扩散建模作为一种替代自回归模型的有效方法的潜力。
2501.00243	 | Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained Image Recognition	 | Edwin Arkel Rios,Jansen Christopher Yuanda,Vincent Leon Ghanz,Cheng-Wei Yu,Bo-Cheng Lai,Min-Chun Hu	 | Ultra-fine-grained image recognition (UFGIR) is a challenging task that involves classifying images within a macro-category. While traditional FGIR deals with classifying different species, UFGIR goes beyond by classifying sub-categories within a species such as cultivars of a plant. In recent times the usage of Vision Transformer-based backbones has allowed methods to obtain outstanding recognition performances in this task but this comes at a significant cost in terms of computation specially since this task significantly benefits from incorporating higher resolution images. Therefore, techniques such as token reduction have emerged to reduce the computational cost. However, dropping tokens leads to loss of essential information for fine-grained categories, specially as the token keep rate is reduced. Therefore, to counteract the loss of information brought by the usage of token reduction we propose a novel Cross-Layer Aggregation Classification Head and a Cross-Layer Cache mechanism to recover and access information from previous layers in later locations. Extensive experiments covering more than 2000 runs across diverse settings including 5 datasets, 9 backbones, 7 token reduction methods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the proposed plug-and-play modules and allow us to push the boundaries of accuracy vs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to 10\% while maintaining a competitive accuracy to state-of-the-art models. Code is available at: \url{https://github.com/arkel23/CLCA}	 | 超细粒度图像识别（Ultra-fine-grained Image Recognition，UFGIR）是一项具有挑战性的任务，涉及在宏观类别内对图像进行分类。传统的一般细粒度图像识别（Fine-grained Image Recognition，FGIR）聚焦于不同物种的分类，而UFGIR则更进一步，针对同一物种内的亚类别进行分类，例如植物的品种。近年来，基于视觉变换器（Vision Transformer）的骨干网络的应用使得这些方法在这种任务中获得了出色的表现，但这也带来了一个显著的计算成本问题，尤其是在高分辨率图像特别有益的情况下。因此，为了减少计算成本，已经出现了诸如标记缩减（token reduction）等技术。然而，标记缩减会导致细粒度类别中重要信息的丢失，尤其是在标记保留率降低的情况下。因此，为了抵消标记缩减带来的信息丢失，我们提出了一种新颖的跨层聚合分类头（Cross-Layer Aggregation Classification Head）和跨层缓存机制（Cross-Layer Cache mechanism），用于从先前层中恢复和访问信息。通过涵盖超过2000次运行，并包括5个数据集、9个骨干网络、7种标记缩减方法、5种标记保留率以及2种图像大小的多种设置，我们进行了广泛的实验。结果表明，所提出的即插即用模块具有较高的有效性，并且通过将保留的标记比例降低到极低的比率（最高可达10%），同时维持与最先进的模型相当的准确率，从而能够推动UFGIR在准确性和成本之间的边界。代码已发布在：\url{https://github.com/arkel23/CLCA}	 | 超细粒度图像识别任务涉及在同一物种内部类别的分类，近年来通过使用视觉变换器取得了显著进展，但面临高计算成本问题。为此，本文提出了一种新的跨层聚合分类头和跨层缓存机制，能够在大幅降低标记保留率的情况下保持高准确率，从而有效降低成本。
2501.00237	 | Make Domain Shift a Catastrophic Forgetting Alleviator in Class-Incremental Learning	 | Wei Chen,Yi Zhou	 | In the realm of class-incremental learning (CIL), alleviating the catastrophic forgetting problem is a pivotal challenge. This paper discovers a counter-intuitive observation: by incorporating domain shift into CIL tasks, the forgetting rate is significantly reduced. Our comprehensive studies demonstrate that incorporating domain shift leads to a clearer separation in the feature distribution across tasks and helps reduce parameter interference during the learning process. Inspired by this observation, we propose a simple yet effective method named DisCo to deal with CIL tasks. DisCo introduces a lightweight prototype pool that utilizes contrastive learning to promote distinct feature distributions for the current task relative to previous ones, effectively mitigating interference across tasks. DisCo can be easily integrated into existing state-of-the-art class-incremental learning methods. Experimental results show that incorporating our method into various CIL methods achieves substantial performance improvements, validating the benefits of our approach in enhancing class-incremental learning by separating feature representation and reducing interference. These findings illustrate that DisCo can serve as a robust fashion for future research in class-incremental learning.	 | 在类增量学习（CIL）领域，缓解灾难性遗忘问题是一个关键挑战。本文发现了一个令人意外的观察：通过将领域偏移引入CIL任务中，可以显著降低遗忘率。我们的全面研究显示，引入领域偏移有助于在不同任务之间更清晰地分离特征分布，并在学习过程中减少参数干扰。受到这一观察的启发，我们提出了一种简单而有效的名为DisCo的方法来处理CIL任务。DisCo引入了一个轻量级的原型池，利用对比学习促进当前任务与以往任务之间的特征分布差异，从而有效减少任务之间的干扰。DisCo可以轻松整合到现有的先进类增量学习方法中。实验结果表明，将我们的方法整合到各种CIL方法中可以实现显著的性能提升，证实了我们方法在通过分离特征表示和减少干扰来增强类增量学习方面的优势。这些发现表明，DisCo可以作为未来类增量学习研究的稳健工具。	 | 本文通过引入领域偏移来缓解类增量学习中的灾难性遗忘问题，并提出了一种名为DisCo的方法，利用轻量级原型池和对比学习促进任务间的特征分布差异，从而减少参数干扰，提升学习性能。实验结果显示DisCo能有效增强类增量学习的效果。
2501.00220	 | DecoratingFusion: A LiDAR-Camera Fusion Network with the Combination of Point-level and Feature-level Fusion	 | Zixuan Yin,Han Sun,Ningzhong Liu,Huiyu Zhou,Jiaquan Shen	 | Lidars and cameras play essential roles in autonomous driving, offering complementary information for 3D detection. The state-of-the-art fusion methods integrate them at the feature level, but they mostly rely on the learned soft association between point clouds and images, which lacks interpretability and neglects the hard association between them. In this paper, we combine feature-level fusion with point-level fusion, using hard association established by the calibration matrices to guide the generation of object queries. Specifically, in the early fusion stage, we use the 2D CNN features of images to decorate the point cloud data, and employ two independent sparse convolutions to extract the decorated point cloud features. In the mid-level fusion stage, we initialize the queries with a center heatmap and embed the predicted class labels as auxiliary information into the queries, making the initial positions closer to the actual centers of the targets. Extensive experiments conducted on two popular datasets, i.e. KITTI, Waymo, demonstrate the superiority of DecoratingFusion.	 | 激光雷达和摄像头在自动驾驶中扮演着至关重要的角色，它们提供了互补的三维检测信息。最先进的融合方法在特征层面进行融合，但主要依赖于学习的点云和图像之间的软关联，这缺乏可解释性并且忽略了它们之间的硬关联。本文中，我们将特征层面的融合与点层面的融合相结合，利用校准矩阵建立的硬关联来指导生成目标查询。具体来说，在早期融合阶段，我们使用图像的2D CNN特征来装饰点云数据，并采用两个独立的稀疏卷积来提取装饰后的点云特征。在中期融合阶段，我们用中心热图初始化查询，并将预测的类别标签嵌入到查询中作为辅助信息，使初始位置更接近目标的实际中心。在KITTI和Waymo两个流行数据集上进行的大量实验表明，DecoratingFusion具有优越性。	 | 本文提出了一种名为DecoratingFusion的新方法，该方法结合了特征层面和点层面的融合，利用校准矩阵建立的硬关联生成目标查询，从而在自动驾驶的三维检测中取得优越性能。
2501.00192	 | MLLM-as-a-Judge for Image Safety without Human Labeling	 | Zhenting Wang,Shuming Hu,Shiyu Zhao,Xiaowen Lin,Felix Juefei-Xu,Zhuowei Li,Ligong Han,Harihar Subramanyam,Li Chen,Jianfa Chen,Nan Jiang,Lingjuan Lyu,Shiqing Ma,Dimitris N. Metaxas,Ankit Jain	 | Image content safety has become a significant challenge with the rise of visual media on online platforms. Meanwhile, in the age of AI-generated content (AIGC), many image generation models are capable of producing harmful content, such as images containing sexual or violent material. Thus, it becomes crucial to identify such unsafe images based on established safety rules. Pre-trained Multimodal Large Language Models (MLLMs) offer potential in this regard, given their strong pattern recognition abilities. Existing approaches typically fine-tune MLLMs with human-labeled datasets, which however brings a series of drawbacks. First, relying on human annotators to label data following intricate and detailed guidelines is both expensive and labor-intensive. Furthermore, users of safety judgment systems may need to frequently update safety rules, making fine-tuning on human-based annotation more challenging. This raises the research question: Can we detect unsafe images by querying MLLMs in a zero-shot setting using a predefined safety constitution (a set of safety rules)? Our research showed that simply querying pre-trained MLLMs does not yield satisfactory results. This lack of effectiveness stems from factors such as the subjectivity of safety rules, the complexity of lengthy constitutions, and the inherent biases in the models. To address these challenges, we propose a MLLM-based method includes objectifying safety rules, assessing the relevance between rules and images, making quick judgments based on debiased token probabilities with logically complete yet simplified precondition chains for safety rules, and conducting more in-depth reasoning with cascaded chain-of-thought processes if necessary. Experiment results demonstrate that our method is highly effective for zero-shot image safety judgment tasks.	 | 随着在线平台上传播视觉媒体的增加，图像内容的安全性已成为一个重要挑战。同时，在AI生成内容（AIGC）的时代，许多图像生成模型能够生成有害内容，如包含色情或暴力的图像。因此，基于已有的安全规则识别这些不安全的图像变得至关重要。预训练的多模态大语言模型（MLLMs）在这方面显示出潜在的应用价值，因为它们具有很强的模式识别能力。现有的方法通常通过使用人工标注的数据集对MLLMs进行微调，但这种方法也带来了一系列问题。首先，依赖人工注释员根据复杂而详细的指南标注数据既昂贵又劳动密集。此外，使用安全判断系统的用户可能需要频繁更新安全规则，使得基于人工注释的微调更加困难。这提出了一个问题：我们能否通过查询预训练的MLLMs在零样本（zero-shot）设置下使用预先定义的安全宪法（一组安全规则）来检测不安全的图像？我们的研究显示，仅通过查询预训练的MLLMs并不能获得满意的结果。这种不有效性主要是由于安全规则的主观性、复杂的宪法文本以及模型固有的偏见等因素。为了解决这些挑战，我们提出了一种基于MLLM的方法，该方法包括客观化安全规则、评估规则与图像的相关性、基于去偏见的token概率进行快速判断，并使用逻辑完整而简化的预设条件链来进行安全规则的判断；如果必要，还可以使用逐层推理过程进行更深入的推理。实验结果表明，我们的方法对于零样本图像安全性判断任务具有很高的有效性。	 | 随着在线平台上传播视觉媒体的增加，图像内容的安全性成为一个重要挑战，特别是在AI生成内容时代，许多图像生成模型能够生成有害内容。为解决这一问题，研究提出了一种基于预训练多模态大语言模型的方法，通过客观化安全规则、评估规则与图像的相关性等步骤，有效提高了零样本图像安全性的判断能力。
2501.00142	 | Minimalist Vision with Freeform Pixels	 | Jeremy Klotz,Shree K. Nayar	 | A minimalist vision system uses the smallest number of pixels needed to solve a vision task. While traditional cameras use a large grid of square pixels, a minimalist camera uses freeform pixels that can take on arbitrary shapes to increase their information content. We show that the hardware of a minimalist camera can be modeled as the first layer of a neural network, where the subsequent layers are used for inference. Training the network for any given task yields the shapes of the camera's freeform pixels, each of which is implemented using a photodetector and an optical mask. We have designed minimalist cameras for monitoring indoor spaces (with 8 pixels), measuring room lighting (with 8 pixels), and estimating traffic flow (with 8 pixels). The performance demonstrated by these systems is on par with a traditional camera with orders of magnitude more pixels. Minimalist vision has two major advantages. First, it naturally tends to preserve the privacy of individuals in the scene since the captured information is inadequate for extracting visual details. Second, since the number of measurements made by a minimalist camera is very small, we show that it can be fully self-powered, i.e., function without an external power supply or a battery.	 | 最小主义的视觉系统使用完成特定视觉任务所需的最小数量的像素。传统摄像头使用大量的方形像素网格，而最小主义摄像头则使用自由形式的像素，这些像素可以采用任意形状以增加信息量。我们展示了最小主义摄像头的硬件可以被建模为神经网络的第一层，后续层用于进行推断。为任意给定任务训练网络会产出摄像头自由形状像素的具体形状，每个像素都由光电探测器和光学遮罩实现。我们设计了用于监测室内空间（8个像素）、测量房间照明（8个像素）和估算交通流量（8个像素）的最小主义摄像头。这些系统的性能与具有数万倍像素的传统摄像头相当。最小主义视觉具有两大显著优势。首先，由于捕捉的信息不足以提取视觉细节，它自然倾向于保护场景中个人的隐私。其次，由于最小主义摄像头进行的测量非常少，我们展示了它可以完全自我供电，即无需外部电源或电池即可运行。	 | 该研究介绍了一种使用自由形式像素的最小主义视觉系统，通过减少像素数量并采用特定形状来优化信息捕获，适用于隐私保护和自我供电等应用场景，其性能与传统摄像头具有数万倍像素的设备相当。
2501.00136	 | Detection-Fusion for Knowledge Graph Extraction from Videos	 | Taniya Das,Louis Mahon,Thomas Lukasiewicz	 | One of the challenging tasks in the field of video understanding is extracting semantic content from video inputs. Most existing systems use language models to describe videos in natural language sentences, but this has several major shortcomings. Such systems can rely too heavily on the language model component and base their output on statistical regularities in natural language text rather than on the visual contents of the video. Additionally, natural language annotations cannot be readily processed by a computer, are difficult to evaluate with performance metrics and cannot be easily translated into a different natural language. In this paper, we propose a method to annotate videos with knowledge graphs, and so avoid these problems. Specifically, we propose a deep-learning-based model for this task that first predicts pairs of individuals and then the relations between them. Additionally, we propose an extension of our model for the inclusion of background knowledge in the construction of knowledge graphs.	 | 视频理解领域的其中一个挑战性任务是从视频输入中提取语义内容。目前大多数现有的系统使用语言模型来用自然语言句子描述视频，但这种方法存在几个主要的缺点。这类系统可能过于依赖语言模型组件，且其输出基于自然语言文本中的统计规律，而不是视频的视觉内容。此外，自然语言注释无法被计算机方便地处理，难以用性能指标进行评估，并且难以翻译成不同的自然语言。在本文中，我们提出了一种使用知识图谱标注视频的方法，以避免这些问题。具体来说，我们提出了一种基于深度学习的模型，该模型首先预测个体对，然后预测这些个体之间的关系。此外，我们还提出了一种扩展模型，使知识图谱的构建能够包含背景知识。	 | 当前视频理解系统主要通过语言模型生成自然语言描述，但这种方法依赖性强且难以评估和翻译。本文提出使用知识图谱标注视频，并提出一种基于深度学习的模型来预测个体及其关系，同时扩展模型以包含背景知识，以解决这些缺点。
2501.00124	 | PQD: Post-training Quantization for Efficient Diffusion Models	 | Jiaojiao Ye,Zhen Wang,Linnan Jiang	 | Diffusionmodels(DMs)havedemonstratedremarkableachievements in synthesizing images of high fidelity and diversity. However, the extensive computational requirements and slow generative speed of diffusion models have limited their widespread adoption. In this paper, we propose a novel post-training quantization for diffusion models (PQD), which is a time-aware optimization framework for diffusion models based on post-training quantization. The proposed framework optimizes the inference process by selecting representative samples and conducting time-aware calibration. Experimental results show that our proposed method is able to directly quantize full-precision diffusion models into 8-bit or 4-bit models while maintaining comparable performance in a training-free manner, achieving a few FID change on ImageNet for unconditional image generation. Our approach demonstrates compatibility and can also be applied to 512x512 text-guided image generation for the first time.	 | 扩散模型（DMs）在生成高保真度和多样性的图像方面已经取得了显著成就。然而，扩散模型的计算需求广泛且生成速度较慢，这限制了它们的广泛应用。在本文中，我们提出了一种新型的后训练量化方法（PQD），这是一种基于后训练量化的时间感知优化框架。该提出的框架通过选择代表性样本和进行时间感知校准来优化推断过程。实验结果表明，我们的方法能够在无需训练的情况下直接将全精度扩散模型量化为8位或4位模型，同时在无条件图像生成方面保持可比的性能，图像网（ImageNet）上的FID几乎没有变化。我们的方法具有兼容性，并且首次能够应用于512x512文本引导的图像生成。	 | 本文提出了一种新型后训练量化方法（PQD），通过选择代表性样本和时间感知校准优化扩散模型的推断过程，能够在保持图像生成性能的同时将全精度模型量化为8位或4位模型，并首次实现了该方法在512x512文本引导图像生成任务中的应用。
2501.00116	 | Text-to-Image GAN with Pretrained Representations	 | Xiaozhou You,Jian Zhang	 | Generating desired images conditioned on given text descriptions has received lots of attention. Recently, diffusion models and autoregressive models have demonstrated their outstanding expressivity and gradually replaced GAN as the favored architectures for text-to-image synthesis. However, they still face some obstacles: slow inference speed and expensive training costs. To achieve more powerful and faster text-to-image synthesis under complex scenes, we propose TIGER, a text-to-image GAN with pretrained representations. To be specific, we propose a vision-empowered discriminator and a high-capacity generator. (i) The vision-empowered discriminator absorbs the complex scene understanding ability and the domain generalization ability from pretrained vision models to enhance model performance. Unlike previous works, we explore stacking multiple pretrained models in our discriminator to collect multiple different representations. (ii) The high-capacity generator aims to achieve effective text-image fusion while increasing the model capacity. The high-capacity generator consists of multiple novel high-capacity fusion blocks (HFBlock). And the HFBlock contains several deep fusion modules and a global fusion module, which play different roles to benefit our model. Extensive experiments demonstrate the outstanding performance of our proposed TIGER both on standard and zero-shot text-to-image synthesis tasks. On the standard text-to-image synthesis task, TIGER achieves state-of-the-art performance on two challenging datasets, which obtain a new FID 5.48 (COCO) and 9.38 (CUB). On the zero-shot text-to-image synthesis task, we achieve comparable performance with fewer model parameters, smaller training data size and faster inference speed. Additionally, more experiments and analyses are conducted in the Supplementary Material.	 | 基于给定文本描述生成所需图像的研究受到了广泛关注。近年来，扩散模型和自回归模型展示了其出色的表达能力，并逐渐取代了GAN，成为了文本到图像合成的首选架构。然而，它们仍然面临着一些挑战：推理速度缓慢和训练成本高昂。为了在复杂场景下实现更强大的、更快的文本到图像合成，我们提出了一种名为TIGER的预训练表示驱动的文本到图像GAN。具体而言，我们提出了视觉增强的判别器和高容量生成器。（i）视觉增强的判别器吸收了预训练视觉模型的复杂场景理解和领域泛化能力，以增强模型性能。与之前的工作不同，我们在判别器中探索堆叠多个预训练模型，以收集多种不同的表示。（ii）高容量生成器旨在在增加模型容量的同时实现有效的文本-图像融合。高容量生成器由多个新颖的高容量融合块（HFBlock）组成，HFBlock包含多个深层融合模块和全局融合模块，它们在不同的角色方面发挥作用，以使我们的模型受益。大量的实验表明，我们在标准和零样本文本到图像合成任务上都取得了出色的性能。在标准文本到图像合成任务中，TIGER在两个具有挑战性的数据集上达到了最先进的性能，分别获得了新的FID得分5.48（COCO）和9.38（CUB）。在零样本文本到图像合成任务中，我们使用更少的模型参数、更小的训练数据集和更快的推理速度取得了可比的性能。此外，还在补充材料中进行了更多实验和分析。	 | 该研究提出了TIGER，一种预训练表示驱动的文本到图像生成对抗网络，通过视觉增强的判别器和高容量生成器来提高复杂场景下的生成质量和速度。TIGER在标准和零样本文本到图像合成任务中表现出色，特别是在FID得分上超越了现有方法。
2501.00103	 | LTX-Video: Realtime Video Latent Diffusion	 | Yoav HaCohen,Nisan Chiprut,Benny Brazowski,Daniel Shalem,Dudu Moshe,Eitan Richardson,Eran Levin,Guy Shiran,Nir Zabari,Ori Gordon,Poriya Panet,Sapir Weissbuch,Victor Kulikov,Yaki Bitterman,Zeev Melumian,Ofir Bibi	 | We introduce LTX-Video, a transformer-based latent diffusion model that adopts a holistic approach to video generation by seamlessly integrating the responsibilities of the Video-VAE and the denoising transformer. Unlike existing methods, which treat these components as independent, LTX-Video aims to optimize their interaction for improved efficiency and quality. At its core is a carefully designed Video-VAE that achieves a high compression ratio of 1:192, with spatiotemporal downscaling of 32 x 32 x 8 pixels per token, enabled by relocating the patchifying operation from the transformer's input to the VAE's input. Operating in this highly compressed latent space enables the transformer to efficiently perform full spatiotemporal self-attention, which is essential for generating high-resolution videos with temporal consistency. However, the high compression inherently limits the representation of fine details. To address this, our VAE decoder is tasked with both latent-to-pixel conversion and the final denoising step, producing the clean result directly in pixel space. This approach preserves the ability to generate fine details without incurring the runtime cost of a separate upsampling module. Our model supports diverse use cases, including text-to-video and image-to-video generation, with both capabilities trained simultaneously. It achieves faster-than-real-time generation, producing 5 seconds of 24 fps video at 768x512 resolution in just 2 seconds on an Nvidia H100 GPU, outperforming all existing models of similar scale. The source code and pre-trained models are publicly available, setting a new benchmark for accessible and scalable video generation.	 | 我们介绍了LTX-Video，这是一种基于变压器的潜变换模型，它采用了一种整体方法来生成视频，无缝地整合了视频VAE和去噪变压器的功能。与现有方法将这些组件视为独立部分的做法不同，LTX-Video 的目标是优化它们之间的交互，从而提高效率和质量。其核心是一个精心设计的视频VAE，实现了高达1:192的压缩比，并通过将补丁化操作从变压器的输入转移到VAE的输入，实现了每令牌32x32x8像素的时空下采样。在这种高度压缩的潜空间中操作，使变压器能够高效地执行全时空自我注意，这是生成具有时间一致性的高分辨率视频所必需的。然而，高度压缩本身就固有限制了微小细节的表示能力。为了解决这个问题，我们的VAE解码器不仅要执行从潜空间到像素空间的转换，还要执行最终的去噪步骤，直接在像素空间中生成干净的结果。这种方法保留了生成微小细节的能力，而无需引入额外的上采样模块的时间开销。我们的模型支持多种应用场景，包括文本到视频和图像到视频的生成，这两种能力都是同时训练的。它实现了比现实时间更快的生成，仅在Nvidia H100 GPU上2秒内就能生成768x512分辨率的24帧每秒的5秒视频，优于所有现有规模相似的模型。源代码和预训练模型均已开源，为可访问性和可扩展性设定了新的基准。	 | LTX-Video 是一种基于变压器的视频生成模型，通过优化视频VAE和去噪变压器之间的交互，实现了高效且高质量的视频生成，特别适合文本到视频和图像到视频的生成任务，在Nvidia H100 GPU上仅需2秒即可生成高分辨率的视频。
2501.01392	 | ProjectedEx: Enhancing Generation in Explainable AI for Prostate Cancer	 | Xuyin Qi,Zeyu Zhang,Aaron Berliano Handoko,Huazhan Zheng,Mingxi Chen,Ta Duc Huy,Vu Minh Hieu Phan,Lei Zhang,Linqi Cheng,Shiyu Jiang,Zhiwei Zhang,Zhibin Liao,Yang Zhao,Minh-Son To	 | Prostate cancer, a growing global health concern, necessitates precise diagnostic tools, with Magnetic Resonance Imaging (MRI) offering high-resolution soft tissue imaging that significantly enhances diagnostic accuracy. Recent advancements in explainable AI and representation learning have significantly improved prostate cancer diagnosis by enabling automated and precise lesion classification. However, existing explainable AI methods, particularly those based on frameworks like generative adversarial networks (GANs), are predominantly developed for natural image generation, and their application to medical imaging often leads to suboptimal performance due to the unique characteristics and complexity of medical image. To address these challenges, our paper introduces three key contributions. First, we propose ProjectedEx, a generative framework that provides interpretable, multi-attribute explanations, effectively linking medical image features to classifier decisions. Second, we enhance the encoder module by incorporating feature pyramids, which enables multiscale feedback to refine the latent space and improves the quality of generated explanations. Additionally, we conduct comprehensive experiments on both the generator and classifier, demonstrating the clinical relevance and effectiveness of ProjectedEx in enhancing interpretability and supporting the adoption of AI in medical settings. Code will be released at https://github.com/Richardqiyi/ProjectedEx	 | 前列腺癌是日益增长的全球健康问题，需要精确的诊断工具。磁共振成像（MRI）能够提供高分辨率的软组织图像，显著提高了诊断准确性。近年来，可解释的人工智能和表示学习的进展极大地提高了前列腺癌的诊断能力，使其能够实现自动且精确的病灶分类。然而，现有的可解释人工智能方法，尤其是基于生成对抗网络（GANs）等框架的方法，主要针对自然图像生成，将其应用于医学影像时往往由于医学图像的独特特性和复杂性而表现不佳。为解决这些挑战，我们论文提出了三个关键贡献。首先，我们提出了ProjectEx，一种生成框架，提供了可解释的、多属性的解释，有效地将医学图像特征与分类器决策联系起来。其次，我们通过引入特征金字塔增强编码模块，使其能够实现多尺度反馈，从而细化潜在空间并提高生成解释的质量。此外，我们在生成器和分类器上进行了全面的实验，证明了ProjectEx在增强解释性和支持医学环境中使用AI方面的临床相关性和有效性。代码将在https://github.com/Richardqiyi/ProjectedEx 释放。	 | 该研究提出了ProjectEx，一种生成框架，通过提供可解释的多属性解释，将医学图像特征与分类器决策联系起来，并通过特征金字塔增强编码模块提高生成解释的质量，从而改善前列腺癌的诊断准确性。
2501.01372	 | ScarNet: A Novel Foundation Model for Automated Myocardial Scar Quantification from LGE in Cardiac MRI	 | Neda Tavakoli,Amir Ali Rahsepar,Brandon C. Benefield,Daming Shen,Santiago López-Tapia,Florian Schiffers,Jeffrey J. Goldberger,Christine M. Albert,Edwin Wu,Aggelos K. Katsaggelos,Daniel C. Lee,Daniel Kim	 | Background: Late Gadolinium Enhancement (LGE) imaging is the gold standard for assessing myocardial fibrosis and scarring, with left ventricular (LV) LGE extent predicting major adverse cardiac events (MACE). Despite its importance, routine LGE-based LV scar quantification is hindered by labor-intensive manual segmentation and inter-observer variability. Methods: We propose ScarNet, a hybrid model combining a transformer-based encoder from the Medical Segment Anything Model (MedSAM) with a convolution-based U-Net decoder, enhanced by tailored attention blocks. ScarNet was trained on 552 ischemic cardiomyopathy patients with expert segmentations of myocardial and scar boundaries and tested on 184 separate patients. Results: ScarNet achieved robust scar segmentation in 184 test patients, yielding a median Dice score of 0.912 (IQR: 0.863--0.944), significantly outperforming MedSAM (median Dice = 0.046, IQR: 0.043--0.047) and nnU-Net (median Dice = 0.638, IQR: 0.604--0.661). ScarNet demonstrated lower bias (-0.63%) and coefficient of variation (4.3%) compared to MedSAM (bias: -13.31%, CoV: 130.3%) and nnU-Net (bias: -2.46%, CoV: 20.3%). In Monte Carlo simulations with noise perturbations, ScarNet achieved significantly higher scar Dice (0.892 \pm 0.053, CoV = 5.9%) than MedSAM (0.048 \pm 0.112, CoV = 233.3%) and nnU-Net (0.615 \pm 0.537, CoV = 28.7%). Conclusion: ScarNet outperformed MedSAM and nnU-Net in accurately segmenting myocardial and scar boundaries in LGE images. The model exhibited robust performance across diverse image qualities and scar patterns.	 | 背景：晚期钆增强（LGE）成像是评估心肌纤维化和瘢痕的金标准，左心室（LV）LGE范围预测主要不良心脏事件（MACE）。尽管非常重要，但常规基于LGE的LV瘢痕量化受到手工分割劳动密集型和观察者间变异性的阻碍。方法：我们提出了一种名为ScarNet的混合模型，该模型结合了来自Medical Segment Anything Model（MedSAM）的基于变换器的编码器和基于卷积的U-Net解码器，并通过自定义注意模块进行增强。ScarNet在552例缺血性心肌病患者的数据上进行了训练，这些患者拥有专家标记的心肌和瘢痕边界，并在184名单独患者的数据上进行了测试。结果：在184名测试患者中，ScarNet实现了稳健的瘢痕分割，中位Dice得分为0.912（四分位数：0.863-0.944），显著优于MedSAM（中位Dice = 0.046，四分位数：0.043-0.047）和nnU-Net（中位Dice = 0.638，四分位数：0.604-0.661）。与MedSAM相比，ScarNet的偏差为-0.63%，变异系数为4.3%，而MedSAM的偏差为-13.31%，变异系数为130.3%，nnU-Net的偏差为-2.46%，变异系数为20.3%。在有噪声扰动的蒙特卡洛模拟中，ScarNet的瘢痕Dice得分为0.892 ± 0.053，变异系数为5.9%，显著高于MedSAM的0.048 ± 0.112（变异系数为233.3%）和nnU-Net的0.615 ± 0.537（变异系数为28.7%）。结论：ScarNet在LGE图像中准确分割心肌和瘢痕边界方面优于MedSAM和nnU-Net。该模型在不同图像质量和瘢痕模式下表现出稳健的性能。	 | ScarNet是一种混合模型，结合了基于变换器的编码器和卷积的U-Net解码器，并通过自定义注意模块增强，能够准确分割LGE图像中的心肌和瘢痕边界，优于MedSAM和nnU-Net，表现出稳健的性能。
2501.01326	 | Domain-invariant feature learning in brain MR imaging for content-based image retrieval	 | Shuya Tobari,Shuhei Tomoshige,Hayato Muraki,Kenichi Oishi,Hitoshi Iyatomi	 | When conducting large-scale studies that collect brain MR images from multiple facilities, the impact of differences in imaging equipment and protocols at each site cannot be ignored, and this domain gap has become a significant issue in recent years. In this study, we propose a new low-dimensional representation (LDR) acquisition method called style encoder adversarial domain adaptation (SE-ADA) to realize content-based image retrieval (CBIR) of brain MR images. SE-ADA reduces domain differences while preserving pathological features by separating domain-specific information from LDR and minimizing domain differences using adversarial learning.In evaluation experiments comparing SE-ADA with recent domain harmonization methods on eight public brain MR datasets (ADNI1/2/3, OASIS1/2/3/4, PPMI), SE-ADA effectively removed domain information while preserving key aspects of the original brain structure and demonstrated the highest disease search accuracy.	 | 在从多个设施收集大脑MRI图像的大规模研究中，每个站点的成像设备和协议差异不能被忽视，这种领域差距近年来已经成为一个重要问题。在这项研究中，我们提出了一种新的低维表示（LDR）获取方法，称为风格编码对抗领域适应（SE-ADA），以实现大脑MRI图像的内容基于图像检索（CBIR）。SE-ADA通过将领域特定信息从LDR中分离，并通过对抗学习减少领域差异，从而在保留病理特征的同时减小领域差异。在使用八个公共大脑MRI数据集（ADNI1/2/3、OASIS1/2/3/4、PPMI）与近期的领域谐波方法进行评估实验时，SE-ADA有效地去除了领域信息，同时保留了原始大脑结构的关键方面，并表现出最高的疾病查找准确性。	 | 该研究提出了一种名为SE-ADA的新方法，通过将领域特定信息从低维表示中分离并减少领域差异，实现大脑MRI图像的领域适应，以提高基于图像检索的疾病识别准确性。实验结果表明，SE-ADA在多个公开的大脑MRI数据集中表现最优，有效去除了领域信息并保留了关键的大脑结构特征。
2501.01282	 | CultureVLM: Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries	 | Shudong Liu,Yiqiao Jin,Cheng Li,Derek F. Wong,Qingsong Wen,Lichao Sun,Haipeng Chen,Xing Xie,Jindong Wang	 | Vision-language models (VLMs) have advanced human-AI interaction but struggle with cultural understanding, often misinterpreting symbols, gestures, and artifacts due to biases in predominantly Western-centric training data. In this paper, we construct CultureVerse, a large-scale multimodal benchmark covering 19, 682 cultural concepts, 188 countries/regions, 15 cultural concepts, and 3 question types, with the aim of characterizing and improving VLMs' multicultural understanding capabilities. Then, we propose CultureVLM, a series of VLMs fine-tuned on our dataset to achieve significant performance improvement in cultural understanding. Our evaluation of 16 models reveals significant disparities, with a stronger performance in Western concepts and weaker results in African and Asian contexts. Fine-tuning on our CultureVerse enhances cultural perception, demonstrating cross-cultural, cross-continent, and cross-dataset generalization without sacrificing performance on models' general VLM benchmarks. We further present insights on cultural generalization and forgetting. We hope that this work could lay the foundation for more equitable and culturally aware multimodal AI systems.	 | 视觉语言模型（Vision-Language Models, VLMs）在促进人机交互方面取得了进展，但它们在文化理解方面面临挑战，经常由于训练数据以西方为中心而产生偏见，导致错误地解读符号、手势和文物。在本文中，我们构建了CultureVerse，这是一个大规模的多模态基准，涵盖了19,682种文化概念、188个国家/地区、15种文化概念和3种问题类型，旨在刻画和提升VLMs的多元文化理解能力。然后，我们提出了CultureVLM系列，该系列模型在我们的数据集上进行了微调，以显著提高文化理解性能。我们对16个模型的评估揭示了显著的差异，表现为对西方概念的较强表现和对非洲和亚洲地区的较弱结果。在我们的CultureVerse上进行微调可以增强文化感知，展示出跨文化、跨大陆和跨数据集的一般化能力，而不会牺牲模型在一般VLM基准上的性能。我们还进一步阐述了文化一般化和遗忘的见解。我们希望这项工作能够为更公平和文化意识更强的多模态AI系统的构建奠定基础。	 | 本文构建了CultureVerse，一个大规模多模态基准，旨在提升视觉语言模型的多元文化理解能力，并提出了CultureVLM系列模型，通过在CultureVerse上的微调显著改善了模型的文化理解性能，特别是在非西方文化方面的表现。
2501.01132	 | Missing Data as Augmentation in the Earth Observation Domain: A Multi-View Learning Approach	 | Francisco Mena,Diego Arenas,Andreas Dengel	 | Multi-view learning (MVL) leverages multiple sources or views of data to enhance machine learning model performance and robustness. This approach has been successfully used in the Earth Observation (EO) domain, where views have a heterogeneous nature and can be affected by missing data. Despite the negative effect that missing data has on model predictions, the ML literature has used it as an augmentation technique to improve model generalization, like masking the input data. Inspired by this, we introduce novel methods for EO applications tailored to MVL with missing views. Our methods integrate the combination of a set to simulate all combinations of missing views as different training samples. Instead of replacing missing data with a numerical value, we use dynamic merge functions, like average, and more complex ones like Transformer. This allows the MVL model to entirely ignore the missing views, enhancing its predictive robustness. We experiment on four EO datasets with temporal and static views, including state-of-the-art methods from the EO domain. The results indicate that our methods improve model robustness under conditions of moderate missingness, and improve the predictive performance when all views are present. The proposed methods offer a single adaptive solution to operate effectively with any combination of available views.	 | 多视图学习（MVL）利用多种数据源或视图来增强机器学习模型的性能和鲁棒性。这种方法在地球观测（EO）领域已经得到了成功应用，其中不同视图具有异质性，并且可能会受到缺失数据的影响。尽管缺失数据会对模型预测产生负面影响，但机器学习文献中已经将其用作一种增强模型泛化能力的技术，例如通过屏蔽输入数据。受此启发，我们为EO应用引入了针对MVL的具有新颖性的方法，专门处理缺失视图情况。我们的方法结合了一组视图的组合，以模拟所有可能的缺失视图组合作为不同的训练样本。我们不使用数值填充缺失数据，而是使用动态合并函数，如平均值，以及更复杂的函数如Transformer。这使得MVL模型可以完全忽略缺失视图，从而增强其预测鲁棒性。我们在四个包含时间动态和静态视图的EO数据集上进行了实验，包括来自EO领域的最新方法。实验结果表明，我们的方法在中等缺失条件下提高了模型的鲁棒性，并且在所有视图都存在时提高了预测性能。所提出的方法提供了一个可适应的解决方案，能够有效地处理任何可用视图的组合。	 | 该研究提出了一种新颖的多视图学习（MVL）方法，专门处理地球观测（EO）数据中的缺失视图问题，通过动态合并函数而不是数值填充来增强模型的鲁棒性和预测性能。实验结果表明，该方法在多种EO数据集上提高了模型的鲁棒性和预测准确性，特别是在存在中等缺失数据的情况下。
2501.01090	 | HoneypotNet: Backdoor Attacks Against Model Extraction	 | Yixu Wang,Tianle Gu,Yan Teng,Yingchun Wang,Xingjun Ma	 | Model extraction attacks are one type of inference-time attacks that approximate the functionality and performance of a black-box victim model by launching a certain number of queries to the model and then leveraging the model's predictions to train a substitute model. These attacks pose severe security threats to production models and MLaaS platforms and could cause significant monetary losses to the model owners. A body of work has proposed to defend machine learning models against model extraction attacks, including both active defense methods that modify the model's outputs or increase the query overhead to avoid extraction and passive defense methods that detect malicious queries or leverage watermarks to perform post-verification. In this work, we introduce a new defense paradigm called attack as defense which modifies the model's output to be poisonous such that any malicious users that attempt to use the output to train a substitute model will be poisoned. To this end, we propose a novel lightweight backdoor attack method dubbed HoneypotNet that replaces the classification layer of the victim model with a honeypot layer and then fine-tunes the honeypot layer with a shadow model (to simulate model extraction) via bi-level optimization to modify its output to be poisonous while remaining the original performance. We empirically demonstrate on four commonly used benchmark datasets that HoneypotNet can inject backdoors into substitute models with a high success rate. The injected backdoor not only facilitates ownership verification but also disrupts the functionality of substitute models, serving as a significant deterrent to model extraction attacks.	 | 模型提取攻击是反向推理攻击的一种，通过向黑盒受害模型发起一定数量的查询，并利用模型的预测来训练替代模型，从而近似受害模型的功能和性能。这些攻击对生产模型和MLaaS平台构成严重安全威胁，可能导致模型所有者遭受重大经济损失。已有大量研究提出了防御机器学习模型免受模型提取攻击的方法，包括：主动防御方法，通过修改模型输出或增加查询开销来避免提取；以及被动防御方法，通过检测恶意查询或利用水印进行后验证来识别恶意行为。在此项工作中，我们引入了一种新的防御范式——“攻击作为防御”，通过修改模型输出使其具有毒性，从而防止任何试图利用输出训练替代模型的恶意用户。为此，我们提出了一种名为HoneypotNet的新型轻量级后门攻击方法，该方法将受害模型的分类层替换为诱饵层，并通过双层优化与影子模型共同调整诱饵层的输出，使其保持原始性能的同时被污染。我们通过四个常用的基准数据集实验证明，HoneypotNet可以在高成功率下将后门注入替代模型。注入的后门不仅可以促进所有权验证，还能破坏替代模型的功能，从而对模型提取攻击起到显著的威慑作用。	 | 该研究提出了一种新的防御方法“攻击作为防御”，通过引入HoneypotNet在受害模型中植入有毒诱饵层，防止模型提取攻击，并通过实验验证了其有效性和威慑作用。
2501.01087	 | Bridging Simplicity and Sophistication using GLinear: A Novel Architecture for Enhanced Time Series Prediction	 | Syed Tahir Hussain Rizvi,Neel Kanwal,Muddasar Naeem,Alfredo Cuzzocrea,Antonio Coronato	 | Time Series Forecasting (TSF) is an important application across many fields. There is a debate about whether Transformers, despite being good at understanding long sequences, struggle with preserving temporal relationships in time series data. Recent research suggests that simpler linear models might outperform or at least provide competitive performance compared to complex Transformer-based models for TSF tasks. In this paper, we propose a novel data-efficient architecture, GLinear, for multivariate TSF that exploits periodic patterns to provide better accuracy. It also provides better prediction accuracy by using a smaller amount of historical data compared to other state-of-the-art linear predictors. Four different datasets (ETTh1, Electricity, Traffic, and Weather) are used to evaluate the performance of the proposed predictor. A performance comparison with state-of-the-art linear architectures (such as NLinear, DLinear, and RLinear) and transformer-based time series predictor (Autoformer) shows that the GLinear, despite being parametrically efficient, significantly outperforms the existing architectures in most cases of multivariate TSF. We hope that the proposed GLinear opens new fronts of research and development of simpler and more sophisticated architectures for data and computationally efficient time-series analysis. The source code is publicly available on GitHub.	 | 时间序列预测（TSF）在许多领域中都是一项重要的应用。关于Transformer是否适合时间序列数据，一个争论点是，尽管它们擅长理解长序列，但在保留时间关系方面可能存在困难。最近的研究表明，更简单的线性模型可能在TSF任务中表现更好，甚至能够与复杂的Transformer基线模型提供竞争力。在本文中，我们提出了一种新颖的数据高效架构GLinear，用于多变量TSF，该架构利用周期性模式以提高准确性。此外，它在使用较少历史数据的情况下提供了更高的预测准确性，与其他最先进的线性预测器相比。本研究使用了四个不同的数据集（ETTh1、电力、交通和天气）来评估所提出的预测器的性能。与最先进的线性架构（如NLinear、DLinear和RLinear）和基于Transformer的时间序列预测器（Autoformer）的性能进行比较显示，尽管GLinear参数效率高，但在大多数情况下，它在多变量TSF方面显著优于现有架构。我们希望提出的GLinear能够开启更简单且更复杂架构的研究和开发新领域，以实现数据和计算上的高效时间序列分析。源代码已公开发布在GitHub上。	 | 本文提出了一种名为GLinear的新数据高效架构，用于多变量时间序列预测，通过利用周期性模式提高预测准确性，并在多个数据集上展示了优于当前领先线性和基于Transformer模型的性能。
2501.01037	 | MSC-Bench: Benchmarking and Analyzing Multi-Sensor Corruption for Driving Perception	 | Xiaoshuai Hao,Guanqun Liu,Yuting Zhao,Yuheng Ji,Mengchuan Wei,Haimei Zhao,Lingdong Kong,Rong Yin,Yu Liu	 | Multi-sensor fusion models play a crucial role in autonomous driving perception, particularly in tasks like 3D object detection and HD map construction. These models provide essential and comprehensive static environmental information for autonomous driving systems. While camera-LiDAR fusion methods have shown promising results by integrating data from both modalities, they often depend on complete sensor inputs. This reliance can lead to low robustness and potential failures when sensors are corrupted or missing, raising significant safety concerns. To tackle this challenge, we introduce the Multi-Sensor Corruption Benchmark (MSC-Bench), the first comprehensive benchmark aimed at evaluating the robustness of multi-sensor autonomous driving perception models against various sensor corruptions. Our benchmark includes 16 combinations of corruption types that disrupt both camera and LiDAR inputs, either individually or concurrently. Extensive evaluations of six 3D object detection models and four HD map construction models reveal substantial performance degradation under adverse weather conditions and sensor failures, underscoring critical safety issues. The benchmark toolkit and affiliated code and model checkpoints have been made publicly accessible.	 | 多传感器融合模型在自动驾驶感知中扮演着重要角色，特别是在3D物体检测和高精度地图构建等任务中。这些模型提供了自动驾驶系统所需的详尽和必要的静态环境信息。虽然相机-LiDAR融合方法通过结合两种模态的数据表现出令人鼓舞的结果，但它们通常依赖于完整的传感器输入。这种依赖可能导致在传感器损坏或缺失时出现低鲁棒性和潜在故障，从而引发重大安全关切。为了应对这一挑战，我们引入了多传感器腐蚀基准（MSC-Bench），这是首个旨在评估多传感器自动驾驶感知模型在各种传感器腐蚀情况下的鲁棒性的全面基准。我们的基准包括16种不同类型的腐蚀类型组合，这些类型会同时或分别干扰相机和LiDAR输入。对六种3D物体检测模型和四种高精度地图构建模型的广泛评估表明，在恶劣天气条件和传感器故障下，性能会有显著下降，突出了关键的安全问题。基准工具套件及其相关的代码和模型检查点已经公开提供。	 | 多传感器融合模型在自动驾驶中至关重要，但现有相机-LiDAR融合方法依赖完整传感器输入，导致在传感器损坏或缺失时存在低鲁棒性。为此，研究人员提出了多传感器腐蚀基准（MSC-Bench），评估模型在不同传感器腐蚀情况下的表现，并公开了相关代码和模型检查点。
2501.00961	 | The Silent Majority: Demystifying Memorization Effect in the Presence of Spurious Correlations	 | Chenyu You,Haocheng Dai,Yifei Min,Jasjeet S. Sekhon,Sarang Joshi,James S. Duncan	 | Machine learning models often rely on simple spurious features -- patterns in training data that correlate with targets but are not causally related to them, like image backgrounds in foreground classification. This reliance typically leads to imbalanced test performance across minority and majority groups. In this work, we take a closer look at the fundamental cause of such imbalanced performance through the lens of memorization, which refers to the ability to predict accurately on \textit{atypical} examples (minority groups) in the training set but failing in achieving the same accuracy in the testing set. This paper systematically shows the ubiquitous existence of spurious features in a small set of neurons within the network, providing the first-ever evidence that memorization may contribute to imbalanced group performance. Through three experimental sources of converging empirical evidence, we find the property of a small subset of neurons or channels in memorizing minority group information. Inspired by these findings, we articulate the hypothesis: the imbalanced group performance is a byproduct of ``noisy'' spurious memorization confined to a small set of neurons. To further substantiate this hypothesis, we show that eliminating these unnecessary spurious memorization patterns via a novel framework during training can significantly affect the model performance on minority groups. Our experimental results across various architectures and benchmarks offer new insights on how neural networks encode core and spurious knowledge, laying the groundwork for future research in demystifying robustness to spurious correlation.	 | 机器学习模型常常依赖于简单的伪特征——训练数据中的模式，这些模式与目标值相关但不具有因果关系，例如前景分类中的图像背景。这种依赖通常会导致少数群体和多数群体在测试性能上的不平衡。在本研究中，我们通过记忆化的视角更仔细地探讨了这种不平衡性能的根本原因，记忆化指的是能够准确预测训练集中异常（少数群体）样本的能力，但在测试集中却无法达到相同的准确度。本文系统地展示了在神经网络中一小部分神经元中普遍存在伪特征的现象，这是首次有证据表明，记忆化可能对少数群体的性能不平衡有所贡献。通过三项实验来源的综合实验证据，我们发现一小部分神经元或通道在记忆少数群体信息方面的特性。受到这些发现的启发，我们提出了假设：少数群体性能不平衡是“嘈杂”的伪特征记忆局限于一小部分神经元的产物。为了进一步验证这一假设，我们展示了通过一个新颖的框架在训练过程中消除这些不必要的伪特征记忆模式，可以显著影响少数群体的模型性能。我们跨越各种架构和基准的实验结果提供了关于神经网络如何编码核心和伪特征知识的新见解，奠定了未来研究如何在伪相关特征下提高模型鲁棒性的基础。	 | 该研究通过记忆化视角探讨了机器学习模型在处理少数群体样本时性能不平衡的原因，发现一小部分神经元在记忆少数群体信息方面存在“嘈杂”的伪特征记忆，并提出通过消除这些伪特征记忆可以显著改善少数群体的模型性能。
2501.00954	 | Enhancing Early Diabetic Retinopathy Detection through Synthetic DR1 Image Generation: A StyleGAN3 Approach	 | Sagarnil Das,Pradeep Walia	 | Diabetic Retinopathy (DR) is a leading cause of preventable blindness. Early detection at the DR1 stage is critical but is hindered by a scarcity of high-quality fundus images. This study uses StyleGAN3 to generate synthetic DR1 images characterized by microaneurysms with high fidelity and diversity. The aim is to address data scarcity and enhance the performance of supervised classifiers. A dataset of 2,602 DR1 images was used to train the model, followed by a comprehensive evaluation using quantitative metrics, including Frechet Inception Distance (FID), Kernel Inception Distance (KID), and Equivariance with respect to translation (EQ-T) and rotation (EQ-R). Qualitative assessments included Human Turing tests, where trained ophthalmologists evaluated the realism of synthetic images. Spectral analysis further validated image quality. The model achieved a final FID score of 17.29, outperforming the mean FID of 21.18 (95 percent confidence interval - 20.83 to 21.56) derived from bootstrap resampling. Human Turing tests demonstrated the model's ability to produce highly realistic images, though minor artifacts near the borders were noted. These findings suggest that StyleGAN3-generated synthetic DR1 images hold significant promise for augmenting training datasets, enabling more accurate early detection of Diabetic Retinopathy. This methodology highlights the potential of synthetic data in advancing medical imaging and AI-driven diagnostics.	 | 糖尿病视网膜病变（DR）是可预防失明的主要原因之一。在DR1阶段早期检测至关重要，但由于高质量的眼底图像稀缺，这一目标难以实现。本研究使用StyleGAN3生成具有高保真度和多样性的DR1阶段图像，这些图像特征为微动脉瘤。本研究旨在解决数据稀缺问题，提高监督分类器的性能。研究使用了2,602张DR1图像来训练模型，并通过Frechet inception距离（FID）、Kernel inception距离（KID）以及平移（EQ-T）和旋转（EQ-R）的等变性等定量指标进行了全面评估。定性评估包括由训练有素的眼科医生进行的人工图灵测试，他们评估了合成图像的逼真度。光谱分析进一步验证了图像质量。该模型最终获得了17.29的FID分数，超过了Bootstrap重采样得到的平均FID分数21.18（95%置信区间-20.83至21.56）。人工图灵测试表明，该模型能够生成高度逼真的图像，尽管边缘附近有一些细微的伪影。这些发现表明，StyleGAN3生成的DR1阶段合成图像有望增强训练数据集，有助于更准确地早期检测糖尿病视网膜病变。该方法还展示了合成数据在推动医学成像和AI驱动诊断方面的潜力。	 | 该研究利用StyleGAN3生成高保真且多样化的糖尿病视网膜病变（DR）第1阶段图像，以解决高质量眼底图像稀缺的问题，并提高DR早期检测的准确性，结果显示生成的图像在图灵测试中表现逼真，提升了监督分类器的性能。
2501.00942	 | Efficient Unsupervised Shortcut Learning Detection and Mitigation in Transformers	 | Lukas Kuhn,Sari Sadiya,Jorg Schlotterer,Christin Seifert,Gemma Roig	 | Shortcut learning, i.e., a model's reliance on undesired features not directly relevant to the task, is a major challenge that severely limits the applications of machine learning algorithms, particularly when deploying them to assist in making sensitive decisions, such as in medical diagnostics. In this work, we leverage recent advancements in machine learning to create an unsupervised framework that is capable of both detecting and mitigating shortcut learning in transformers. We validate our method on multiple datasets. Results demonstrate that our framework significantly improves both worst-group accuracy (samples misclassified due to shortcuts) and average accuracy, while minimizing human annotation effort. Moreover, we demonstrate that the detected shortcuts are meaningful and informative to human experts, and that our framework is computationally efficient, allowing it to be run on consumer hardware.	 | 捷径学习，即模型对与任务无关的非直接特征的依赖，是机器学习算法应用中的一个重大挑战，尤其是在将其部署用于辅助作出敏感决策（如医疗诊断）时。在本工作中，我们利用近期机器学习的进步，开发了一个无监督框架，该框架能够检测和缓解变换器中的捷径学习。我们在多个数据集上验证了该方法。结果表明，我们的框架显著提高了最坏群体准确率（由于捷径而导致的误分类样本）和平均准确率，同时减少了人工标注的工作量。此外，我们证明了检测到的捷径对人类专家具有意义和信息价值，并且我们的框架计算效率高，能够在消费级硬件上运行。	 | 该研究开发了一个无监督框架来检测和缓解变换器中的捷径学习问题，特别适用于敏感决策任务，该框架提高了最坏群体准确率和平均准确率，并且计算效率高，可在消费级硬件上运行。
2501.00941	 | A Novel Diffusion Model for Pairwise Geoscience Data Generation with Unbalanced Training Dataset	 | Junhuan Yang,Yuzhou Zhang,Yi Sheng,Youzuo Lin,Lei Yang	 | Recently, the advent of generative AI technologies has made transformational impacts on our daily lives, yet its application in scientific applications remains in its early stages. Data scarcity is a major, well-known barrier in data-driven scientific computing, so physics-guided generative AI holds significant promise. In scientific computing, most tasks study the conversion of multiple data modalities to describe physical phenomena, for example, spatial and waveform in seismic imaging, time and frequency in signal processing, and temporal and spectral in climate modeling; as such, multi-modal pairwise data generation is highly required instead of single-modal data generation, which is usually used in natural images (e.g., faces, scenery). Moreover, in real-world applications, the unbalance of available data in terms of modalities commonly exists; for example, the spatial data (i.e., velocity maps) in seismic imaging can be easily simulated, but real-world seismic waveform is largely lacking. While the most recent efforts enable the powerful diffusion model to generate multi-modal data, how to leverage the unbalanced available data is still unclear. In this work, we use seismic imaging in subsurface geophysics as a vehicle to present ``UB-Diff'', a novel diffusion model for multi-modal paired scientific data generation. One major innovation is a one-in-two-out encoder-decoder network structure, which can ensure pairwise data is obtained from a co-latent representation. Then, the co-latent representation will be used by the diffusion process for pairwise data generation. Experimental results on the OpenFWI dataset show that UB-Diff significantly outperforms existing techniques in terms of Fréchet Inception Distance (FID) score and pairwise evaluation, indicating the generation of reliable and useful multi-modal pairwise data.	 | 最近，生成式AI技术的兴起对我们的日常生活产生了颠覆性的影响，但在科学应用中的应用仍处于初级阶段。数据不足是数据驱动科学计算中的一个主要障碍，因此，基于物理的生成式AI具有重大潜力。在科学计算中，大多数任务涉及将多种数据模态转换以描述物理现象，例如地震成像中的空间和波形、信号处理中的时间和频率、气候建模中的时间和频谱；因此，需要多模态配对数据生成，而不是单模态数据生成，这在自然图像（如人脸、风景）中通常使用。此外，在实际应用中，模态间数据的不平衡通常普遍存在；例如，在地震成像中，空间数据（即速度图）可以容易地模拟，但实际地震波形则严重缺乏。尽管最新的努力使强大的扩散模型能够生成多模态数据，但如何利用不平衡的数据仍然不清楚。在这项工作中，我们以地下地质物理中的地震成像为例，展示了一种新的多模态配对科学数据生成的扩散模型“UB-Diff”。一个主要创新是使用了一种一进二出的编码器-解码器网络结构，可以确保从一个共同的潜在表示中获得配对数据。然后，共同的潜在表示将由扩散过程用于配对数据生成。在OpenFWI数据集上的实验结果表明，UB-Diff在弗雷肖特引入距离（FID）分数和配对评估方面显著优于现有技术，表明生成了可靠且有用多模态配对数据。	 | 本文介绍了一种新的多模态配对科学数据生成模型UB-Diff，该模型以地下地质物理中的地震成像为例，通过使用一进二出的编码器-解码器网络结构，有效生成可靠且有用的空间和地震波形配对数据，并在OpenFWI数据集上显著优于现有技术。
2501.00876	 | A Novel Approach using CapsNet and Deep Belief Network for Detection and Identification of Oral Leukopenia	 | Hirthik Mathesh GV,Kavin Chakravarthy M,Sentil Pandi S	 | Oral cancer constitutes a significant global health concern, resulting in 277,484 fatalities in 2023, with the highest prevalence observed in low- and middle-income nations. Facilitating automation in the detection of possibly malignant and malignant lesions in the oral cavity could result in cost-effective and early disease diagnosis. Establishing an extensive repository of meticulously annotated oral lesions is essential. In this research photos are being collected from global clinical experts, who have been equipped with an annotation tool to generate comprehensive labelling. This research presents a novel approach for integrating bounding box annotations from various doctors. Additionally, Deep Belief Network combined with CAPSNET is employed to develop automated systems that extracted intricate patterns to address this challenging problem. This study evaluated two deep learning-based computer vision methodologies for the automated detection and classification of oral lesions to facilitate the early detection of oral cancer: image classification utilizing CAPSNET. Image classification attained an F1 score of 94.23% for detecting photos with lesions 93.46% for identifying images necessitating referral. Object detection attained an F1 score of 89.34% for identifying lesions for referral. Subsequent performances are documented about classification based on the sort of referral decision. Our preliminary findings indicate that deep learning possesses the capability to address this complex problem.	 | 口腔癌构成了全球重要的公共卫生问题，2023年导致了277,484例死亡，其中低收入和中等收入国家的发病率最高。在口腔中自动化检测可能恶性或恶性病变可以实现经济有效且早期的疾病诊断。建立一个包含详尽标注的口腔病变数据库至关重要。在本研究中，来自全球临床专家的照片被收集，并配备了标注工具以生成全面的标注。本研究提出了一种将多位医生的边界框标注整合的新方法。此外，采用深度信念网络与CAPSNET相结合的方法来开发自动提取复杂模式的系统，以解决这一挑战性问题。本研究评估了两种基于深度学习的计算机视觉方法，用于自动检测和分类口腔病变，以促进口腔癌的早期发现：利用CAPSNET进行图像分类。图像分类在检测带有病变的照片方面取得了94.23%的F1分数，在识别需要转诊的图像方面达到了93.46%的F1分数。目标检测在识别需要转诊的病变方面取得了89.34%的F1分数。后续性能则根据转诊决策的类型进行了记录。初步研究结果表明，深度学习具备解决这一复杂问题的能力。	 | 本研究通过收集全球临床专家的照片并使用深度信念网络与CAPSNET结合的方法，开发了一种自动检测和分类口腔病变的系统，提高了早期发现口腔癌的能力，结果显示图像分类和目标检测分别取得了较高准确率。
2501.00751	 | HCMA-UNet: A Hybrid CNN-Mamba UNet with Inter-Slice Self-Attention for Efficient Breast Cancer Segmentation	 | Haoxuan Li,Wei song,Peiwu Qin,Xi Yuan,Zhenglin Chen	 | Breast cancer lesion segmentation in DCE-MRI remains challenging due to heterogeneous tumor morphology and indistinct boundaries. To address these challenges, this study proposes a novel hybrid segmentation network, HCMA-UNet, for lesion segmentation of breast cancer. Our network consists of a lightweight CNN backbone and a Multi-view Inter-Slice Self-Attention Mamba (MISM) module. The MISM module integrates Visual State Space Block (VSSB) and Inter-Slice Self-Attention (ISSA) mechanism, effectively reducing parameters through Asymmetric Split Channel (ASC) strategy to achieve efficient tri-directional feature extraction. Our lightweight model achieves superior performance with 2.87M parameters and 126.44 GFLOPs. A Feature-guided Region-aware loss function (FRLoss) is proposed to enhance segmentation accuracy. Extensive experiments on one private and two public DCE-MRI breast cancer datasets demonstrate that our approach achieves state-of-the-art performance while maintaining computational efficiency. FRLoss also exhibits good cross-architecture generalization capabilities. The source code and dataset is available on this link.	 | 由于乳腺癌病变形态异质性和边界不清晰，基于动态对比增强磁共振成像（DCE-MRI）的病变分割仍然具有挑战性。为了解决这些挑战，本研究提出了一种新的混合分割网络HCMA-UNet，用于乳腺癌病变分割。该网络由轻量级的CNN主干和多视图跨层自我注意Mamba（MISM）模块组成。MISM模块通过视觉状态空间块（VSSB）和跨层自我注意（ISSA）机制结合，并利用非对称分通道（ASC）策略有效减少参数，实现高效三维特征提取。我们提出的轻量级模型参数量仅为2.87M，计算量为126.44 GFLOPs，性能卓越。我们还提出了一种导向特征区段损失函数（FRLoss）以增强分割准确性。在针对一个私人和两个公开的DCE-MRI乳腺癌数据集进行的广泛实验中，我们的方法在保持计算效率的同时达到了最先进的性能。FRLoss还表现出良好的跨架构泛化能力。源代码和数据集可在该链接处获取。	 | 本文提出了一种新的混合分割网络HCMA-UNet，用于乳腺癌病变分割，该网络结合了轻量级CNN主干和多视图跨层自我注意模块Mamba，同时引入了一种导向特征区段损失函数FRLoss以提高分割准确性。实验结果表明，该方法在保持高效计算的同时达到了最先进的性能，并且FRLoss具有良好的跨架构泛化能力。
2501.00725	 | Automatic Construction of Pattern Classifiers Capable of Continuous Incremental Learning and Unlearning Tasks Based on Compact-Sized Probabilistic Neural Network	 | Tetsuya Hoya,Shunpei Morita	 | This paper proposes a novel approach to pattern classification using a probabilistic neural network model. The strategy is based on a compact-sized probabilistic neural network capable of continuous incremental learning and unlearning tasks. The network is constructed/reconstructed using a simple, one-pass network-growing algorithm with no hyperparameter tuning. Then, given the training dataset, its structure and parameters are automatically determined and can be dynamically varied in continual incremental and decremental learning situations. The algorithm proposed in this work involves no iterative or arduous matrix-based parameter approximations but a simple data-driven updating scheme. Simulation results using nine publicly available databases demonstrate the effectiveness of this approach, showing that compact-sized probabilistic neural networks constructed have a much smaller number of hidden units compared to the original probabilistic neural network model and yet can achieve a similar classification performance to that of multilayer perceptron neural networks in standard classification tasks, while also exhibiting sufficient capability in continuous class incremental learning and unlearning tasks.	 | 本文提出了一种基于概率神经网络模型的新型模式分类方法。该策略基于一个紧凑型的概率神经网络，能够实现连续的增量学习和遗忘任务。该网络使用一个简单的、一次通过的网络生长算法构建/重构，无需进行超参数调优。然后，给定训练数据集后，其结构和参数会自动确定，并且可以在连续的增量和减少学习情况下动态变化。本文提出的方法的算法不涉及迭代或基于矩阵的参数近似，而是使用一个简单的数据驱动更新方案。使用九个公开可用的数据集进行的仿真结果表明了该方法的有效性，展示了紧凑型的概率神经网络相比原始的概率神经网络模型具有更少的隐藏单元，但在标准分类任务中的分类性能却与多层感知器神经网络相当，同时在连续的类别增量学习和遗忘任务中也展现出足够的能力。	 | 本文提出了一种基于紧凑型概率神经网络的新型模式分类方法，该方法能够实现连续的增量学习和遗忘任务，并在标准分类任务中表现出与多层感知器神经网络相当的性能，同时在连续类别增量学习和遗忘任务中也表现出色。
2501.00677	 | Deeply Learned Robust Matrix Completion for Large-scale Low-rank Data Recovery	 | HanQin Cai,Chandra Kundu,Jialin Liu,Wotao Yin	 | Robust matrix completion (RMC) is a widely used machine learning tool that simultaneously tackles two critical issues in low-rank data analysis: missing data entries and extreme outliers. This paper proposes a novel scalable and learnable non-convex approach, coined Learned Robust Matrix Completion (LRMC), for large-scale RMC problems. LRMC enjoys low computational complexity with linear convergence. Motivated by the proposed theorem, the free parameters of LRMC can be effectively learned via deep unfolding to achieve optimum performance. Furthermore, this paper proposes a flexible feedforward-recurrent-mixed neural network framework that extends deep unfolding from fix-number iterations to infinite iterations. The superior empirical performance of LRMC is verified with extensive experiments against state-of-the-art on synthetic datasets and real applications, including video background subtraction, ultrasound imaging, face modeling, and cloud removal from satellite imagery.	 | 稳健矩阵完成（RMC）是一种广泛使用的机器学习工具，用于解决低秩数据分析中的两个关键问题：缺失数据项和极端异常值。本文提出了一种新的可扩展且可学习的非凸方法——学习稳健矩阵完成（LRMC），以解决大规模RMC问题。LRMC具有较低的计算复杂性并享有线性收敛性。受所提出的定理的启发，LRMC中的自由参数可通过深度层叠有效地学习以实现最佳性能。此外，本文提出了一种灵活的前馈-递归-混合神经网络框架，该框架将深度层叠从固定迭代次数扩展到了无限迭代次数。通过在合成数据集和实际应用（包括视频背景减法、超声成像、面部建模和卫星图像中的云去除）中进行广泛的实验，验证了LRMC的优越实证性能，其表现优于最先进的方法。	 | 本文提出了一种新的可扩展且可学习的非凸方法——学习稳健矩阵完成（LRMC），通过结合高效的学习机制和灵活的前馈-递归-混合神经网络框架，LRMC能够有效解决大规模的稳健矩阵完成问题，并在多种实际应用中表现出优越的性能。
2501.00669	 | Leaf diseases detection using deep learning methods	 | El Houcine El Fatimi	 | This study, our main topic is to devlop a new deep-learning approachs for plant leaf disease identification and detection using leaf image datasets. We also discussed the challenges facing current methods of leaf disease detection and how deep learning may be used to overcome these challenges and enhance the accuracy of disease detection. Therefore, we have proposed a novel method for the detection of various leaf diseases in crops, along with the identification and description of an efficient network architecture that encompasses hyperparameters and optimization methods. The effectiveness of different architectures was compared and evaluated to see the best architecture configuration and to create an effective model that can quickly detect leaf disease. In addition to the work done on pre-trained models, we proposed a new model based on CNN, which provides an efficient method for identifying and detecting plant leaf disease. Furthermore, we evaluated the efficacy of our model and compared the results to those of some pre-trained state-of-the-art architectures.	 | 本研究的主要主题是开发一种新的深度学习方法，用于通过叶图像数据集识别和检测植物叶片疾病。我们还讨论了当前叶片疾病检测方法面临的挑战，并探讨了如何使用深度学习来克服这些挑战，从而提高疾病检测的准确性。因此，我们提出了一种新的方法，用于检测农作物中的各种叶片疾病，并描述了一个高效网络架构，该架构涵盖超参数和优化方法。对比和评估了不同架构的有效性，以确定最佳的架构配置，并创建一个能够快速检测叶片疾病的高效模型。除了对预训练模型的工作外，我们还提出了一种基于CNN的新模型，该模型提供了一种有效的方法来识别和检测植物叶片疾病。此外，我们评估了我们模型的有效性，并将其结果与一些最先进的预训练架构的结果进行了比较。	 | 本研究开发了一种基于深度学习的方法，通过分析叶图像数据集来识别和检测植物叶片疾病，并提出了一种高效网络架构，以提高疾病检测的准确性；研究还评估了不同架构的有效性，并与一些最先进的预训练模型进行了比较。
2501.00647	 | Lightweight G-YOLOv11: Advancing Efficient Fracture Detection in Pediatric Wrist X-rays	 | Abdesselam Ferdi	 | Computer-aided diagnosis (CAD) systems have greatly improved the interpretation of medical images by radiologists and surgeons. However, current CAD systems for fracture detection in X-ray images primarily rely on large, resource-intensive detectors, which limits their practicality in clinical settings. To address this limitation, we propose a novel lightweight CAD system based on the YOLO detector for fracture detection. This system, named ghost convolution-based YOLOv11 (G-YOLOv11), builds on the latest version of the YOLO detector family and incorporates the ghost convolution operation for feature extraction. The ghost convolution operation generates the same number of feature maps as traditional convolution but requires fewer linear operations, thereby reducing the detector's computational resource requirements. We evaluated the performance of the proposed G-YOLOv11 detector on the GRAZPEDWRI-DX dataset, achieving an mAP@0.5 of 0.535 with an inference time of 2.4 ms on an NVIDIA A10 GPU. Compared to the standard YOLOv11l, G-YOLOv11l achieved reductions of 13.6% in mAP@0.5 and 68.7% in size. These results establish a new state-of-the-art benchmark in terms of efficiency, outperforming existing detectors. Code and models are available at https://github.com/AbdesselamFerdi/G-YOLOv11.	 | 计算机辅助诊断（CAD）系统极大地提高了放射科医生和外科医生对医学图像的解释能力。然而，当前用于X光图像骨折检测的CAD系统主要依赖于大型、资源密集型检测器，这限制了其在临床环境中的实用性。为了解决这一局限性，我们提出了一种基于YOLO检测器的新型轻量级CAD系统，用于骨折检测。该系统名为基于ghost卷积的YOLOv11（G-YOLOv11），它基于YOLO检测器家族的最新版本，并结合了ghost卷积操作以实现特征提取。ghost卷积操作生成与传统卷积相同数量的特征图，但只需要较少的线性操作，从而减少了检测器的计算资源需求。我们利用GRAZPEDWRI-DX数据集评估了所提出的G-YOLOv11检测器的性能，在NVIDIA A10 GPU上的推理时间为2.4毫秒，mAP@0.5为0.535。与标准的YOLOv11相比，G-YOLOv11在mAP@0.5上的性能降低了13.6%，在大小上减少了68.7%。这些结果在效率方面确立了新的领先基准，超越了现有检测器。相关代码和模型可在https://github.com/AbdesselamFerdi/G-YOLOv11获得。	 | 本文提出了一种名为G-YOLOv11的轻量级骨折检测系统，该系统结合了ghost卷积操作以减少计算资源需求，实现了高效的X光骨折检测，在NVIDIA A10 GPU上的推理时间仅为2.4毫秒，且在大小上减少了68.7%，性能接近标准YOLOv11。
2501.00636	 | Applying Graph Explanation to Operator Fusion	 | Keith G. Mills,Muhammad Fetrat Qharabagh,Weichen Qiu,Fred X. Han,Mohammad Salameh,Wei Lu,Shangling Jui,Di Niu	 | Layer fusion techniques are critical to improving the inference efficiency of deep neural networks (DNN) for deployment. Fusion aims to lower inference costs by reducing data transactions between an accelerator's on-chip buffer and DRAM. This is accomplished by grouped execution of multiple operations like convolution and activations together into single execution units - fusion groups. However, on-chip buffer capacity limits fusion group size and optimizing fusion on whole DNNs requires partitioning into multiple fusion groups. Finding the optimal groups is a complex problem where the presence of invalid solutions hampers traditional search algorithms and demands robust approaches. In this paper we incorporate Explainable AI, specifically Graph Explanation Techniques (GET), into layer fusion. Given an invalid fusion group, we identify the operations most responsible for group invalidity, then use this knowledge to recursively split the original fusion group via a greedy tree-based algorithm to minimize DRAM access. We pair our scheme with common algorithms and optimize DNNs on two types of layer fusion: Line-Buffer Depth First (LBDF) and Branch Requirement Reduction (BRR). Experiments demonstrate the efficacy of our scheme on several popular and classical convolutional neural networks like ResNets and MobileNets. Our scheme achieves over 20% DRAM Access reduction on EfficientNet-B3.	 | 层融合技术对于提高部署中深度神经网络（DNN）的推理效率至关重要。融合旨在通过减少加速器片上缓存与DRAM之间的数据传输来降低推理成本。这通过将多个操作（如卷积和激活）组合成单一执行单元——融合组来实现。然而，片上缓存容量限制了融合组的大小，而要在整个DNN中优化融合则需要将DNN划分为多个融合组。找到最优的融合组是一个复杂的问题，其中无效解的存在阻碍了传统搜索算法，并要求更加稳健的方法。在本文中，我们将可解释人工智能，特别是图解释技术（GET）引入层融合。给定一个无效的融合组，我们识别出最负责该组无效性的操作，然后利用这些知识通过贪婪树形算法递归地将原始融合组划分为多个子组，从而最小化对DRAM的访问。我们将我们的方案与常见算法相结合，并在两种类型的层融合：行缓冲深度优先（LBDF）和分支需求减少（BRR）上优化DNN。实验表明，我们的方案在ResNets和MobileNets等流行的卷积神经网络中有效，能够实现超过20%的DRAM访问减少，特别是在EfficientNet-B3中。	 | 本文引入了图解释技术（GET）来优化深度神经网络中的层融合，通过识别并解决无效融合组，实现减少深度学习模型对DRAM的访问次数，从而提高推理效率，实验证明该方法在多种网络结构中有效，能显著降低DRAM访问次数超过20%。
2501.00586	 | Advanced Lung Nodule Segmentation and Classification for Early Detection of Lung Cancer using SAM and Transfer Learning	 | Asha V,Bhavanishankar K	 | Lung cancer is an extremely lethal disease primarily due to its late-stage diagnosis and significant mortality rate, making it the major cause of cancer-related demises globally. Machine Learning (ML) and Convolution Neural network (CNN) based Deep Learning (DL) techniques are primarily used for precise segmentation and classification of cancerous nodules in the CT (Computed Tomography) or MRI images. This study introduces an innovative approach to lung nodule segmentation by utilizing the Segment Anything Model (SAM) combined with transfer learning techniques. Precise segmentation of lung nodules is crucial for the early detection of lung cancer. The proposed method leverages Bounding Box prompts and a vision transformer model to enhance segmentation performance, achieving high accuracy, Dice Similarity Coefficient (DSC) and Intersection over Union (IoU) metrics. The integration of SAM and Transfer Learning significantly improves Computer-Aided Detection (CAD) systems in medical imaging, particularly for lung cancer diagnosis. The findings demonstrate the proposed model effectiveness in precisely segmenting lung nodules from CT scans, underscoring its potential to advance early detection and improve patient care outcomes in lung cancer diagnosis. The results show SAM Model with transfer learning achieving a DSC of 97.08% and an IoU of 95.6%, for segmentation and accuracy of 96.71% for classification indicates that ,its performance is noteworthy compared to existing techniques.	 | 肺癌是一种由于晚期诊断和高死亡率而极其致命的疾病，使其成为全球癌症相关死亡的主要原因。基于机器学习（ML）和卷积神经网络（CNN）的深度学习（DL）技术主要用于精确分割和分类CT（计算机断层扫描）或MRI图像中的癌性结节。本文提出了一种创新的方法，通过结合分割一切模型（SAM）和迁移学习技术来实现肺结节分割。精确分割肺结节对于早期发现肺癌至关重要。所提出的方法利用边界框提示和视觉变换器模型来增强分割性能，实现了高精度、Dice相似系数（DSC）和交并比（IoU）指标。SAM与迁移学习的结合显著提高了医学影像中的计算机辅助检测（CAD）系统，特别是在肺癌诊断方面。研究结果表明，所提出的模型在精确分割CT扫描中的肺结节方面表现出色，突显了它在肺癌诊断中早期检测和改善患者护理结果的潜力。结果显示，结合迁移学习的SAM模型在分割上的DSC为97.08%，IoU为95.6%，分类准确率为96.71%，表明其性能优于现有技术。	 | 本文提出了一种结合分割一切模型（SAM）和迁移学习的创新方法，用于肺结节分割，显著提高了计算机辅助检测系统的性能，特别是在肺癌诊断方面，结果显示其Dice相似系数和交并比分别达到了97.08%和95.6%。
2501.00514	 | H-Net: A Multitask Architecture for Simultaneous 3D Force Estimation and Stereo Semantic Segmentation in Intracardiac Catheters	 | Pedram Fekri,Mehrdad Zadeh,Javad Dargahi	 | The success rate of catheterization procedures is closely linked to the sensory data provided to the surgeon. Vision-based deep learning models can deliver both tactile and visual information in a sensor-free manner, while also being cost-effective to produce. Given the complexity of these models for devices with limited computational resources, research has focused on force estimation and catheter segmentation separately. However, there is a lack of a comprehensive architecture capable of simultaneously segmenting the catheter from two different angles and estimating the applied forces in 3D. To bridge this gap, this work proposes a novel, lightweight, multi-input, multi-output encoder-decoder-based architecture. It is designed to segment the catheter from two points of view and concurrently measure the applied forces in the x, y, and z directions. This network processes two simultaneous X-Ray images, intended to be fed by a biplane fluoroscopy system, showing a catheter's deflection from different angles. It uses two parallel sub-networks with shared parameters to output two segmentation maps corresponding to the inputs. Additionally, it leverages stereo vision to estimate the applied forces at the catheter's tip in 3D. The architecture features two input channels, two classification heads for segmentation, and a regression head for force estimation through a single end-to-end architecture. The output of all heads was assessed and compared with the literature, demonstrating state-of-the-art performance in both segmentation and force estimation. To the best of the authors' knowledge, this is the first time such a model has been proposed	 | 该穿刺操作的成功率与外科医生获得的感官数据密切相关。基于视觉的深度学习模型可以在无需传感器的情况下提供触觉和视觉信息，同时生产成本相对较低。然而，鉴于这些模型在计算资源有限的设备上的复杂性，研究主要集中在分别估计力和进行导管分割上。然而，缺乏一种能够同时从两个角度分割导管并在三维空间中估计所施加力的全面架构。为弥补这一空白，本文提出了一种新颖、轻量级、多输入多输出的编码器-解码器架构。该架构旨在从两个视角分割导管，并同时测量沿 x、y 和 z 方向的施加力。该网络处理两个同时的 X 射线图像，旨在由双平面透视系统提供，显示出导管从不同角度的偏转。该网络利用两个具有共享参数的并行子网络输出与输入对应的两个分割图。此外，该网络利用立体视觉在三维空间中估计导管尖端所施加的力。该架构具有两个输入通道、两个用于分割的分类头和一个用于通过单一端到端架构进行力估计的回归头。所有头的输出都被评估并与其他文献进行了比较，展示了在分割和力估计方面的先进性能。据作者所知，这是首次提出这样的模型。	 | 本文提出了一种新颖的轻量级多输入多输出编码器-解码器架构，该架构能够在双平面X射线图像中同时分割导管并估计沿三个方向的施加力，展示了在分割和力估计方面的优越性能。
2501.00463	 | SAT-LDM: Provably Generalizable Image Watermarking for Latent Diffusion Models with Self-Augmented Training	 | Lu Zhang,Liang Zeng	 | The proliferation of AI-generated images necessitates effective watermarking to protect intellectual property and identify fake content. While existing training-based watermarking methods show promise, they often struggle with generalization across diverse prompts and tend to produce noticeable artifacts. To this end, we introduce a provably generalizable image watermarking method for Latent Diffusion Models with Self-Augmented Training (SAT-LDM), which aligns the training and testing phases by a free generation distribution to bolster the watermarking module's generalization capabilities. We theoretically consolidate our method by proving that the free generation distribution contributes to its tight generalization bound without the need to collect new data. Extensive experimental results show that SAT-LDM achieves robust watermarking while significantly improving the quality of watermarked images across diverse prompts. Furthermore, we conduct experimental analyses to demonstrate the strong generalization abilities of SAT-LDM. We hope our method offers a practical and convenient solution for securing high-fidelity AI-generated content.	 | 随着AI生成图像的增加，需要有效的水印技术来保护知识产权并识别虚假内容。虽然现有的基于训练的水印方法显示出潜力，但它们往往难以在多样化的提示下进行泛化，并且往往会生成明显的伪影。为此，我们提出了一种用于潜扩散模型（Latent Diffusion Models，LDM）的自增强训练（Self-Augmented Training，SAT）的可证明泛化的图像水印方法，通过自由生成分布对训练和测试阶段进行对齐，从而增强水印模块的泛化能力。我们通过理论上的证明，展示了自由生成分布有助于其泛化边界的紧缩，无需收集新数据即可实现。广泛的实验结果表明，SAT-LDM 能够实现稳健的水印，并且在多样化的提示下显著提高了水印图像的质量。此外，我们进行了实验分析以展示 SAT-LDM 强大的泛化能力。我们希望该方法能提供一种实用和方便的解决方案，以保护高保真的AI生成内容。	 | 该研究提出了一种用于潜扩散模型的自增强训练图像水印方法，通过对齐自由生成分布来增强泛化能力，从而在多样化的提示下提高水印质量并实现稳健的水印嵌入。实验表明，该方法能够有效保护AI生成内容的知识产权，并显著提高水印的鲁棒性和质量。
2501.00457	 | Differentiable Prompt Learning for Vision Language Models	 | Zhenhan Huang,Tejaswini Pedapati,Pin-Yu Chen,Jianxi Gao	 | Prompt learning is an effective way to exploit the potential of large-scale pre-trained foundational models. Continuous prompts parameterize context tokens in prompts by turning them into differentiable vectors. Deep continuous prompts insert prompts not only in the input but also in the intermediate hidden representations. Manually designed deep continuous prompts exhibit a remarkable improvement compared to the zero-shot pre-trained model on downstream tasks. How to automate the continuous prompt design is an underexplored area, and a fundamental question arises, is manually designed deep prompt strategy optimal? To answer this question, we propose a method dubbed differentiable prompt learning (DPL). The DPL method is formulated as an optimization problem to automatically determine the optimal context length of the prompt to be added to each layer, where the objective is to maximize the performance. We test the DPL method on the pre-trained CLIP. We empirically find that by using only limited data, our DPL method can find deep continuous prompt configuration with high confidence. The performance on the downstream tasks exhibits the superiority of the automatic design: our method boosts the average test accuracy by 2.60% on 11 datasets compared to baseline methods. Besides, our method focuses only on the prompt configuration (i.e. context length for each layer), which means that our method is compatible with the baseline methods that have sophisticated designs to boost the performance. The DPL method can be deployed to large language models or computer vision models at no cost.	 | 大型预训练基础模型潜在能力的有效利用方法之一是提示学习。连续提示通过将提示中的上下文标记转换为可微矢量来参数化上下文标记。深层连续提示不仅在输入中插入提示，还在中间隐藏表示中插入提示。人工设计的深层连续提示在下游任务中比零样本预训练模型表现出了显著的改进。如何自动化设计连续提示是一个尚未探索的领域，从而引发了一个基本问题：人工设计的深层提示策略是否是最优的？为了回答这个问题，我们提出了一种名为可微提示学习（DPL）的方法。DPL方法被形式化为一个优化问题，以自动确定要添加到每一层的提示的最优上下文长度，目标是最大化性能。我们在预训练的CLIP上测试了DPL方法。实验证明，仅使用有限的数据，我们的DPL方法能够以高置信度找到深层连续提示配置。下游任务中的表现展示了自动化设计的优势：与基线方法相比，我们的方法在11个数据集上提高了平均测试准确率2.60%。此外，我们的方法仅关注提示配置（即每一层的上下文长度），这意味着我们的方法与那些为了提高性能而进行了复杂设计的基线方法兼容。DPL方法可以零成本部署到大型语言模型或计算机视觉模型中。	 | 该研究提出了一种名为可微提示学习（DPL）的方法，以自动化设计深层连续提示，从而在多个下游任务中提高了性能，平均测试准确率提高了2.60%，展示了自动化设计的优势并且兼容复杂设计的基线方法。
2501.00378	 | STARFormer: A Novel Spatio-Temporal Aggregation Reorganization Transformer of FMRI for Brain Disorder Diagnosis	 | Wenhao Dong,Yueyang Li,Weiming Zeng,Lei Chen,Hongjie Yan,Wai Ting Siok,Nizhuan Wang	 | Many existing methods that use functional magnetic resonance imaging (fMRI) classify brain disorders, such as autism spectrum disorder (ASD) and attention deficit hyperactivity disorder (ADHD), often overlook the integration of spatial and temporal dependencies of the blood oxygen level-dependent (BOLD) signals, which may lead to inaccurate or imprecise classification results. To solve this problem, we propose a Spatio-Temporal Aggregation eorganization ransformer (STARFormer) that effectively captures both spatial and temporal features of BOLD signals by incorporating three key modules. The region of interest (ROI) spatial structure analysis module uses eigenvector centrality (EC) to reorganize brain regions based on effective connectivity, highlighting critical spatial relationships relevant to the brain disorder. The temporal feature reorganization module systematically segments the time series into equal-dimensional window tokens and captures multiscale features through variable window and cross-window attention. The spatio-temporal feature fusion module employs a parallel transformer architecture with dedicated temporal and spatial branches to extract integrated features. The proposed STARFormer has been rigorously evaluated on two publicly available datasets for the classification of ASD and ADHD. The experimental results confirm that the STARFormer achieves state-of-the-art performance across multiple evaluation metrics, providing a more accurate and reliable tool for the diagnosis of brain disorders and biomedical research. The codes will be available at: https://github.com/NZWANG/STARFormer.	 | 许多现有的使用功能性磁共振成像（fMRI）的方法在分类自闭症谱系障碍（ASD）和注意力缺陷多动障碍（ADHD）等脑疾病时，往往会忽略血氧水平依赖（BOLD）信号的空间和时间依赖性的整合，这可能导致不准确或不够精确的分类结果。为了解决这个问题，我们提出了一个时空聚合变压器（STARFormer），该方法通过结合三个关键模块有效地捕捉BOLD信号的空间和时间特征。兴趣区域（ROI）空间结构分析模块使用特征向量中心性（EC）重新组织脑区，基于有效连接强调与脑疾病相关的关键空间关系。时间特征重新组织模块系统地将时间序列分割成等维度窗口标记，并通过可变窗口和跨窗口注意力捕获多尺度特征。时空特征融合模块采用具有专门时间支路和空间支路的并行变压器结构来提取集成特征。STARFormer已经在两个公开可用的数据集上针对ASD和ADHD的分类进行了严格的评估。实验结果证实，STARFormer在多个评估指标上实现了最先进的性能，为脑疾病诊断和生物医学研究提供了更准确和可靠的工具。相关代码可在以下链接获取：https://github.com/NZWANG/STARFormer。	 | 该研究提出了一种时空聚合变压器（STARFormer）方法，通过整合三个关键模块有效捕捉BOLD信号的空间和时间特征，以提高自闭症谱系障碍（ASD）和注意力缺陷多动障碍（ADHD）等脑疾病的分类准确性，并在公开数据集上取得了最先进的性能。
2501.00296	 | Predicate Invention from Pixels via Pretrained Vision-Language Models	 | Ashay Athalye,Nishanth Kumar,Tom Silver,Yichao Liang,Tomás Lozano-Pérez,Leslie Pack Kaelbling	 | Our aim is to learn to solve long-horizon decision-making problems in highly-variable, combinatorially-complex robotics domains given raw sensor input in the form of images. Previous work has shown that one way to achieve this aim is to learn a structured abstract transition model in the form of symbolic predicates and operators, and then plan within this model to solve novel tasks at test time. However, these learned models do not ground directly into pixels from just a handful of demonstrations. In this work, we propose to invent predicates that operate directly over input images by leveraging the capabilities of pretrained vision-language models (VLMs). Our key idea is that, given a set of demonstrations, a VLM can be used to propose a set of predicates that are potentially relevant for decision-making and then to determine the truth values of these predicates in both the given demonstrations and new image inputs. We build upon an existing framework for predicate invention, which generates feature-based predicates operating on object-centric states, to also generate visual predicates that operate on images. Experimentally, we show that our approach -- pix2pred -- is able to invent semantically meaningful predicates that enable generalization to novel, complex, and long-horizon tasks across two simulated robotic environments.	 | 我们的目标是在高度变化且组合复杂性高的机器人领域中，通过原始传感器输入（图像形式）来学习解决长期决策问题。以前的工作已经表明，实现这一目标的一种方法是学习一种结构化的抽象转换模型，该模型以符号谓词和操作符的形式存在，然后在这个模型中进行规划，以在测试时解决新的任务。然而，这些学习到的模型不能直接从少量演示中将这些谓词映射到像素上。在这项工作中，我们建议通过利用预训练的视觉-语言模型（VLMs）的能力来直接在输入图像上发明谓词。我们的主要思路是，给定一组演示，可以使用VLM来提出一组可能对决策有用的谓词，然后确定这些谓词在给定的演示和新的图像输入中的真值。我们在此前基于谓词发明的框架基础上，进一步生成基于视觉的谓词，该框架生成操作于对象中心状态的特征谓词。实验中，我们展示了我们的方法——pix2pred——能够发明具有语义意义的谓词，并在两个模拟的机器人环境中使模型能够泛化到新的、复杂的和长期的任务中。	 | 该研究旨在通过原始图像输入来学习解决长期决策问题，并提出了一种利用预训练视觉-语言模型直接在图像上发明谓词的方法，从而使得机器人能够在新的、复杂的任务中进行泛化。
2501.00265	 | Outlier-Robust Training of Machine Learning Models	 | Rajat Talak,Charis Georgiou,Jingnan Shi,Luca Carlone	 | Robust training of machine learning models in the presence of outliers has garnered attention across various domains. The use of robust losses is a popular approach and is known to mitigate the impact of outliers. We bring to light two literatures that have diverged in their ways of designing robust losses: one using M-estimation, which is popular in robotics and computer vision, and another using a risk-minimization framework, which is popular in deep learning. We first show that a simple modification of the Black-Rangarajan duality provides a unifying view. The modified duality brings out a definition of a robust loss kernel $σ$ that is satisfied by robust losses in both the literatures. Secondly, using the modified duality, we propose an Adaptive Alternation Algorithm (AAA) for training machine learning models with outliers. The algorithm iteratively trains the model by using a weighted version of the non-robust loss, while updating the weights at each iteration. The algorithm is augmented with a novel parameter update rule by interpreting the weights as inlier probabilities, and obviates the need for complex parameter tuning. Thirdly, we investigate convergence of the adaptive alternation algorithm to outlier-free optima. Considering arbitrary outliers (i.e., with no distributional assumption on the outliers), we show that the use of robust loss kernels σ increases the region of convergence. We experimentally show the efficacy of our algorithm on regression, classification, and neural scene reconstruction problems. We release our implementation code: https://github.com/MIT-SPARK/ORT.	 | 在数据中存在异常值的情况下，机器学习模型的健壮训练已经引起了各个领域的关注。使用健壮损失是一种流行的方法，已被证明可以减轻异常值的影响。我们揭示了两种路径设计健壮损失的方式已经出现了分歧：一种是利用M-估计的方式，这种方法在机器人技术和计算机视觉领域很受欢迎；另一种是基于风险最小化框架的方法，这种方法在深度学习领域很受欢迎。首先，我们通过对Black-Rangarajan对偶进行简单的修改，提供了一个统一的观点。修改后的对偶揭示了一种健壮损失核$σ$的定义，这种核同时适用于两种文献中的健壮损失。其次，利用修改后的对偶，我们提出了一种自适应交替算法（AAA），用于在存在异常值的情况下训练机器学习模型。该算法通过使用非健壮损失的加权版本迭代地训练模型，并在每次迭代中更新权重。算法中引入了一个新的参数更新规则，将权重解释为内点概率，从而避免了复杂的参数调节。第三，我们研究了自适应交替算法收敛到无异常值最优解的情况。我们假设异常值具有任意分布（即，不作分布假设），证明使用健壮损失核$σ$可以扩大收敛区域。我们在回归、分类和神经场景重建问题上实验展示了我们算法的有效性。我们发布了我们的实现代码：https://github.com/MIT-SPARK/ORT。	 | 该研究统一了两种设计稳健损失的方法，并提出了一种自适应交替算法（AAA），该算法利用非稳健损失的加权版本迭代训练模型，同时证明了在存在异常值的情况下仍能收敛到无异常值的最优解，通过实验证明了算法的有效性。
2501.00184	 | TrajLearn: Trajectory Prediction Learning using Deep Generative Models	 | Amirhossein Nadiri,Jing Li,Ali Faraji,Ghadeer Abuoda,Manos Papagelis	 | Trajectory prediction aims to estimate an entity's future path using its current position and historical movement data, benefiting fields like autonomous navigation, robotics, and human movement analytics. Deep learning approaches have become key in this area, utilizing large-scale trajectory datasets to model movement patterns, but face challenges in managing complex spatial dependencies and adapting to dynamic environments. To address these challenges, we introduce TrajLearn, a novel model for trajectory prediction that leverages generative modeling of higher-order mobility flows based on hexagonal spatial representation. TrajLearn predicts the next $k$ steps by integrating a customized beam search for exploring multiple potential paths while maintaining spatial continuity. We conducted a rigorous evaluation of TrajLearn, benchmarking it against leading state-of-the-art approaches and meaningful baselines. The results indicate that TrajLearn achieves significant performance gains, with improvements of up to ~40% across multiple real-world trajectory datasets. In addition, we evaluated different prediction horizons (i.e., various values of $k$), conducted resolution sensitivity analysis, and performed ablation studies to assess the impact of key model components. Furthermore, we developed a novel algorithm to generate mixed-resolution maps by hierarchically subdividing hexagonal regions into finer segments within a specified observation area. This approach supports selective detailing, applying finer resolution to areas of interest or high activity (e.g., urban centers) while using coarser resolution for less significant regions (e.g., rural areas), effectively reducing data storage requirements and computational overhead. We promote reproducibility and adaptability by offering complete code, data, and detailed documentation with flexible configuration options for various applications.	 | 轨迹预测旨在利用实体的当前位置和历史运动数据来估计其未来的路径，这一技术在自主导航、机器人技术和人类运动分析等领域中具有广泛应用。深度学习方法在这一领域中变得尤为重要，通过利用大规模轨迹数据集来建模运动模式，但仍然面临管理复杂空间依赖性和适应动态环境的挑战。为解决这些挑战，我们引入了TrajLearn，这是一种基于六边形空间表示的高级流动生成建模的新颖轨迹预测模型。TrajLearn通过结合定制的束搜索方法来预测接下来的k步，以探索多种可能的路径并保持空间连续性。我们对TrajLearn进行了严格的评估，将其与领先的状态-of-the-art方法和有意义的基线进行对比。结果表明，TrajLearn在多个实际轨迹数据集上实现了显著的性能提升，性能提升高达约40%。此外，我们还评估了不同的预测窗口（即各种k值），进行了分辨率敏感性分析，并进行了消融研究以评估关键模型组件的影响。我们还开发了一种新颖的算法，通过在指定的观测区域内逐级细分六边形区域来生成混合分辨率的地图。这种方法支持选择性细化，将更精细的分辨率应用于感兴趣区域或高活动区域（例如城市中心），而对于不重要区域（例如农村地区）则使用较粗的分辨率，从而有效减少了数据存储需求和计算开销。为了促进可重复性和适应性，我们提供了完整的代码、数据和详细的文档，并提供了灵活的配置选项以适应各种应用需求。	 | TrajLearn是一种基于六边形空间表示的轨迹预测模型，通过结合定制的束搜索方法来预测实体未来的路径，显著提高了在多个实际轨迹数据集上的预测性能，相比现有方法提高了约40%。该模型还支持不同预测窗口的分辨率敏感性分析，并通过生成混合分辨率的地图有效减少了数据存储需求和计算开销。
2501.00057	 | VisTabNet: Adapting Vision Transformers for Tabular Data	 | Witold Wydmański,Ulvi Movsum-zada,Jacek Tabor,Marek Śmieja	 | Although deep learning models have had great success in natural language processing and computer vision, we do not observe comparable improvements in the case of tabular data, which is still the most common data type used in biological, industrial and financial applications. In particular, it is challenging to transfer large-scale pre-trained models to downstream tasks defined on small tabular datasets. To address this, we propose VisTabNet -- a cross-modal transfer learning method, which allows for adapting Vision Transformer (ViT) with pre-trained weights to process tabular data. By projecting tabular inputs to patch embeddings acceptable by ViT, we can directly apply a pre-trained Transformer Encoder to tabular inputs. This approach eliminates the conceptual cost of designing a suitable architecture for processing tabular data, while reducing the computational cost of training the model from scratch. Experimental results on multiple small tabular datasets (less than 1k samples) demonstrate VisTabNet's superiority, outperforming both traditional ensemble methods and recent deep learning models. The proposed method goes beyond conventional transfer learning practice and shows that pre-trained image models can be transferred to solve tabular problems, extending the boundaries of transfer learning.	 | 尽管深度学习模型在自然语言处理和计算机视觉领域取得了巨大成功，但在表格数据上的表现并没有达到同样的高度。表格数据依然是生物学、工业和金融应用中最常见的数据类型。特别是，将大规模预训练模型迁移到小规模的表格数据定义的下游任务上极具挑战性。为了解决这个问题，我们提出了一种跨模态迁移学习方法VisTabNet，该方法允许使用预训练权重的视觉变换器（ViT）处理表格数据。通过将表格输入投影到ViT可接受的patch嵌入中，可以直接将预训练的Transformer编码器应用于表格输入。这种方法消除了为处理表格数据设计合适架构的概念性成本，并减少了从头训练模型的计算成本。在多个小规模表格数据集（少于1000个样本）上的实验结果表明，VisTabNet具有优越性，优于传统的集成方法和最近的深度学习模型。所提出的方法超越了传统的迁移学习实践，展示了预训练的图像模型可以迁移到解决表格问题上，扩展了迁移学习的边界。	 | 本文提出了一种跨模态迁移学习方法VisTabNet，该方法使用预训练的视觉变换器（ViT）处理表格数据，并在多个小规模表格数据集上取得了优于传统方法和最近深度学习模型的性能，展示了图像模型可以迁移应用于表格问题。
