| ID | Title | Authors | Summary (zh) | Abstract (en) | Abstract (zh) | 
| --- | --- | --- | --- | --- | --- |
2501.01428	 | GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models	 | Zhangyang Qi,Zhixiong Zhang,Ye Fang,Jiaqi Wang,Hengshuang Zhao	 | 近年来，尽管2D视觉-语言模型在图像-文本任务中取得显著进展，但在三维空间理解方面仍存在局限性。为此，本文提出了一种新的视觉提示范式GPT4Scene，通过构建三维鸟瞰图和标记视频帧中的对象ID，显著提高了室内场景的三维空间理解能力，并展示了在零样本评估中优于现有模型的效果。	 | In recent years, 2D Vision-Language Models (VLMs) have made significant strides in image-text understanding tasks. However, their performance in 3D spatial comprehension, which is critical for embodied intelligence, remains limited. Recent advances have leveraged 3D point clouds and multi-view images as inputs, yielding promising results. However, we propose exploring a purely vision-based solution inspired by human perception, which merely relies on visual cues for 3D spatial understanding. This paper empirically investigates the limitations of VLMs in 3D spatial knowledge, revealing that their primary shortcoming lies in the lack of global-local correspondence between the scene and individual frames. To address this, we introduce GPT4Scene, a novel visual prompting paradigm in VLM training and inference that helps build the global-local relationship, significantly improving the 3D spatial understanding of indoor scenes. Specifically, GPT4Scene constructs a 3D Bird's Eye View (BEV) image from the video and marks consistent object IDs across both frames and the BEV image. The model then inputs the concatenated BEV image and video frames with markers. In zero-shot evaluations, GPT4Scene improves performance over closed-source VLMs like GPT-4o. Additionally, we prepare a processed video dataset consisting of 165K text annotation to fine-tune open-source VLMs, achieving state-of-the-art performance on all 3D understanding tasks. Surprisingly, after training with the GPT4Scene paradigm, VLMs consistently improve during inference, even without visual prompting and BEV image as explicit correspondence. It demonstrates that the proposed paradigm helps VLMs develop an intrinsic ability to understand 3D scenes, which paves the way for a noninvasive approach to extending pre-trained VLMs for 3D scene understanding.	 | 近年来，2D 视觉-语言模型（VLMs）在图像-文本理解任务中取得了显著进展。然而，这些模型在三维空间理解方面的表现仍然有限，而三维空间理解对于具身智能至关重要。最近的研究利用3D点云和多视角图像作为输入，取得了令人鼓舞的结果。然而，我们提出了一种基于纯视觉的方法，这种方法借鉴了人类感知，仅依赖视觉线索进行三维空间理解。本文通过实验证明了VLMs在三维空间知识方面的局限性，揭示了它们的主要问题是场景和单个帧之间的全局-局部对应关系不足。为解决这一问题，我们引入了一种新的视觉提示范式GPT4Scene，用于VLM的训练和推理，有助于建立全局-局部关系，显著提高了室内场景的三维空间理解能力。具体而言，GPT4Scene从视频中构建了一个三维顶视图（BEV）图像，并在视频帧和BEV图像中标记一致的对象ID。然后，模型将连接起来的BEV图像和带有标记的视频帧作为输入。在零样本评估中，GPT4Scene的表现优于封闭源的VLMs如GPT-4o。此外，我们准备了一个包含16.5万个文本注释的处理后的视频数据集，用于微调开源的VLMs，使其在所有三维理解任务上均实现最佳性能。令人惊讶的是，在使用GPT4Scene范式训练后，VLMs在推理过程中即使没有视觉提示和BEV图像的明确对应关系，也表现出了持续的改进。这表明所提出的范式有助于VLMs发展内在理解三维场景的能力，从而为非侵入性地扩展预训练VLMs进行三维场景理解奠定了基础。
2501.01427	 | VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control	 | Yuanpeng Tu,Hao Luo,Xi Chen,Sihui Ji,Xiang Bai,Hengshuang Zhao	 | 本文提出了一种名为VideoAnydoor的零样本视频对象插入框架，它能够高保真地保留细节并精确控制运动，通过像素变形器和重权重构损失实现，并且无需特定任务微调即可支持多种下游应用。	 | Despite significant advancements in video generation, inserting a given object into videos remains a challenging task. The difficulty lies in preserving the appearance details of the reference object and accurately modeling coherent motions at the same time. In this paper, we propose VideoAnydoor, a zero-shot video object insertion framework with high-fidelity detail preservation and precise motion control. Starting from a text-to-video model, we utilize an ID extractor to inject the global identity and leverage a box sequence to control the overall motion. To preserve the detailed appearance and meanwhile support fine-grained motion control, we design a pixel warper. It takes the reference image with arbitrary key-points and the corresponding key-point trajectories as inputs. It warps the pixel details according to the trajectories and fuses the warped features with the diffusion U-Net, thus improving detail preservation and supporting users in manipulating the motion trajectories. In addition, we propose a training strategy involving both videos and static images with a reweight reconstruction loss to enhance insertion quality. VideoAnydoor demonstrates significant superiority over existing methods and naturally supports various downstream applications (e.g., talking head generation, video virtual try-on, multi-region editing) without task-specific fine-tuning.	 | 尽管在视频生成方面取得了显著进展，但在视频中插入给定对象仍是一项具有挑战性的任务。困难在于同时保持参考对象的外观细节和准确建模连贯运动。在本文中，我们提出了一种高保真细节保留和精确运动控制的零样本视频对象插入框架——VideoAnydoor。从文本到视频模型开始，我们使用一个ID提取器注入全局身份，并利用一个框序列来控制整体运动。为了同时保留详细的外观和支持精细的运动控制，我们设计了一个像素变形器。它以带有任意关键点的参考图像及其对应的关键点轨迹作为输入。根据轨迹对像素细节进行变形，并将变形特征与扩散U-Net融合，从而提高细节保留度并支持用户操纵运动轨迹。此外，我们提出了一种包含视频和静态图像的训练策略，并采用重权重构损失来增强插入质量。VideoAnydoor 在现有方法上表现出显著的优势，并且无需特定任务的微调即可自然支持各种下游应用（例如，生成有声头像、视频虚拟试穿、多区域编辑）。
2501.01426	 | Unifying Specialized Visual Encoders for Video Language Models	 | Jihoon Chung,Tyler Zhu,Max Gonzalez Saez-Diez,Juan Carlos Niebles,Honglu Zhou,Olga Russakovsky	 | MERV（Multi-Encoder Representation of Videos）通过使用多个冻结的视觉编码器为视频构建统一表示，增强了视频大型语言模型（VideoLLMs）的视觉处理能力，显著提高了视频理解的准确性和效率，特别是在开放性和多项选择性视频理解任务上超越了现有方法。	 | The recent advent of Large Language Models (LLMs) has ushered sophisticated reasoning capabilities into the realm of video through Video Large Language Models (VideoLLMs). However, VideoLLMs currently rely on a single vision encoder for all of their visual processing, which limits the amount and type of visual information that can be conveyed to the LLM. Our method, MERV, Multi-Encoder Representation of Videos, instead leverages multiple frozen visual encoders to create a unified representation of a video, providing the VideoLLM with a comprehensive set of specialized visual knowledge. Spatio-temporally aligning the features from each encoder allows us to tackle a wider range of open-ended and multiple-choice video understanding questions and outperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy than Video-LLaVA across the standard suite video understanding benchmarks, while also having a better Video-ChatGPT score. We also improve upon SeViLA, the previous best on zero-shot Perception Test accuracy, by 2.2%. MERV introduces minimal extra parameters and trains faster than equivalent single-encoder methods while parallelizing the visual processing. Finally, we provide qualitative evidence that MERV successfully captures domain knowledge from each of its encoders. Our results offer promising directions in utilizing multiple vision encoders for comprehensive video understanding.	 | 近年来，大型语言模型（LLMs）引入了复杂的推理能力到视频领域，形成了视频大型语言模型（VideoLLMs）。然而，目前的VideoLLMs依赖单一的视觉编码器进行所有视觉处理，这限制了可以传达给LLM的视觉信息的数量和类型。我们提出的方法，即MERV（Multi-Encoder Representation of Videos），利用多个冻结的视觉编码器来创建视频的统一表示，为VideoLLM提供全面的专门视觉知识。通过空间-时间对齐每个编码器的特征，我们能够应对更广泛范围的开放性和多项选择性视频理解问题，并超越了先前的最先进工作。MERV在标准视频理解基准测试中的准确率比Video-LLaVA高出3.7%，同时具有更好的Video-ChatGPT评分。我们还在零样本感知测试中将SeViLA的准确性提高了2.2%。MERV引入了最小的额外参数，并且比相应的单编码器方法训练速度快，同时实现了视觉处理的并行化。最后，我们提供了定性的证据，证明MERV成功地从其每个编码器中捕捉到了领域知识。我们的结果提供了利用多个视觉编码器进行全面视频理解的有前景的方向。
2501.01425	 | Free-Form Motion Control: A Synthetic Video Generation Dataset with Controllable Camera and Object Motions	 | Xincheng Shuai,Henghui Ding,Zhenyuan Qin,Hao Luo,Xingjun Ma,Dacheng Tao	 | 该研究引入了一个新的合成数据集SynFMC，旨在控制生成视频中动态物体和相机的运动，并提出了一种相应的方法FMC，能够独立或同时控制物体和相机运动，生成高质量的视频内容。	 | Controlling the movements of dynamic objects and the camera within generated videos is a meaningful yet challenging task. Due to the lack of datasets with comprehensive motion annotations, existing algorithms can not simultaneously control the motions of both camera and objects, resulting in limited controllability over generated contents. To address this issue and facilitate the research in this field, we introduce a Synthetic Dataset for Free-Form Motion Control (SynFMC). The proposed SynFMC dataset includes diverse objects and environments and covers various motion patterns according to specific rules, simulating common and complex real-world scenarios. The complete 6D pose information facilitates models learning to disentangle the motion effects from objects and the camera in a video. To validate the effectiveness and generalization of SynFMC, we further propose a method, Free-Form Motion Control (FMC). FMC enables independent or simultaneous control of object and camera movements, producing high-fidelity videos. Moreover, it is compatible with various personalized text-to-image (T2I) models for different content styles. Extensive experiments demonstrate that the proposed FMC outperforms previous methods across multiple scenarios.	 | 在生成的视频中控制动态物体和相机的运动是一项具有重要意义但极具挑战的任务。由于缺乏包含全面运动注释的数据集，现有的算法无法同时控制相机和物体的运动，从而限制了生成内容的可控性。为解决这一问题并促进该领域的研究，我们引入了一个合成数据集用于自由形式运动控制（SynFMC）。提出的SynFMC数据集包括多样化的物体和环境，并根据特定规则涵盖了各种运动模式，模拟常见的复杂现实场景。完整的6D姿态信息有助于模型学习在视频中从物体和相机分离运动效果。为了验证SynFMC的有效性和泛化能力，我们进一步提出了一种方法，自由形式运动控制（FMC）。FMC允许独立或同时控制物体和相机的运动，生成高保真度的视频。此外，它与多种个性化的文本到图像（T2I）模型兼容，适用于不同内容风格。广泛的实验表明，提出的FMC在多种场景中优于先前的方法。
2501.01424	 | Object-level Visual Prompts for Compositional Image Generation	 | Gaurav Parmar,Or Patashnik,Kuan-Chieh Wang,Daniil Ostashev,Srinivasa Narasimhan,Jun-Yan Zhu,Daniel Cohen-Or,Kfir Aberman	 | 本文提出了一种新的KV混合交叉注意力机制，用于在文本到图像扩散模型中组合对象级视觉提示，生成跨多种场景和风格的语义连贯组合，同时保留输入提示中物体的身份并支持其灵活变化。通过这种方法，模型能够在多样化的图像中灵活排列对象的姿态和构成，从而扩展了文本到图像生成的创意潜力。	 | We introduce a method for composing object-level visual prompts within a text-to-image diffusion model. Our approach addresses the task of generating semantically coherent compositions across diverse scenes and styles, similar to the versatility and expressiveness offered by text prompts. A key challenge in this task is to preserve the identity of the objects depicted in the input visual prompts, while also generating diverse compositions across different images. To address this challenge, we introduce a new KV-mixed cross-attention mechanism, in which keys and values are learned from distinct visual representations. The keys are derived from an encoder with a small bottleneck for layout control, whereas the values come from a larger bottleneck encoder that captures fine-grained appearance details. By mixing keys and values from these complementary sources, our model preserves the identity of the visual prompts while supporting flexible variations in object arrangement, pose, and composition. During inference, we further propose object-level compositional guidance to improve the method's identity preservation and layout correctness. Results show that our technique produces diverse scene compositions that preserve the unique characteristics of each visual prompt, expanding the creative potential of text-to-image generation.	 | 我们提出了一种在文本到图像扩散模型中组合对象级视觉提示的方法。我们的方法旨在生成跨多种场景和风格的语义连贯组合，类似于文本提示提供的多功能性和表达力。在这个任务中面临的一个关键挑战是如何在保留输入视觉提示中所描绘的物体身份的同时，生成不同图像中的多样化组合。为了应对这一挑战，我们引入了一种新的 KV 混合交叉注意力机制，在该机制中，键和值分别从不同的视觉表示中学习。键由具有小型瓶颈结构以控制布局的编码器生成，而值则由一个较大的瓶颈编码器生成，该编码器捕捉到了精细的外观细节。通过将这些互补来源的键和值混合使用，我们的模型在保留视觉提示的身份的同时，支持对象排列、姿态和构成的灵活变化。在推理过程中，我们进一步提出了一种对象级构图指导，以提高方法的身份保留能力和布局准确性。实验结果表明，我们的技术生成了多样化的场景组合，同时保留了每个视觉提示的独特特征，从而扩展了文本到图像生成的创意潜力。
2501.01423	 | Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models	 | Jingfeng Yao,Xinggang Wang	 | 该研究解决了潜在扩散模型在提高重建质量和生成性能之间的优化难题，通过将潜在空间与预训练的视觉基础模型对齐，提出了一种新的方法VA-VAE，并构建了更高效的LightningDiT系统，显著提升了ImageNet 256x256图像生成的性能和训练效率，达到最先进的FID得分为1.35。	 | Latent diffusion models with Transformer architectures excel at generating high-fidelity images. However, recent studies reveal an optimization dilemma in this two-stage design: while increasing the per-token feature dimension in visual tokenizers improves reconstruction quality, it requires substantially larger diffusion models and more training iterations to achieve comparable generation performance. Consequently, existing systems often settle for sub-optimal solutions, either producing visual artifacts due to information loss within tokenizers or failing to converge fully due to expensive computation costs. We argue that this dilemma stems from the inherent difficulty in learning unconstrained high-dimensional latent spaces. To address this, we propose aligning the latent space with pre-trained vision foundation models when training the visual tokenizers. Our proposed VA-VAE (Vision foundation model Aligned Variational AutoEncoder) significantly expands the reconstruction-generation frontier of latent diffusion models, enabling faster convergence of Diffusion Transformers (DiT) in high-dimensional latent spaces. To exploit the full potential of VA-VAE, we build an enhanced DiT baseline with improved training strategies and architecture designs, termed LightningDiT. The integrated system achieves state-of-the-art (SOTA) performance on ImageNet 256x256 generation with an FID score of 1.35 while demonstrating remarkable training efficiency by reaching an FID score of 2.11 in just 64 epochs--representing an over 21 times convergence speedup compared to the original DiT. Models and codes are available at: https://github.com/hustvl/LightningDiT.	 | 基于Transformer架构的潜在扩散模型在生成高保真图像方面表现出色。然而，最近的研究揭示了这种两阶段设计中的一个优化难题：虽然在视觉标记器中增加每个标记的特征维度可以提高重建质量，但这需要更大的扩散模型和更多的训练迭代次数才能达到相当的生成性能。结果，现有系统往往选择次优解决方案，要么由于标记器中的信息丢失而产生视觉伪影，要么由于昂贵的计算成本未能完全收敛。我们认为这一难题源于学习不受约束的高维潜在空间的固有难度。为解决这一问题，我们建议在训练视觉标记器时将潜在空间与预训练的视觉基础模型对齐。我们提出的VA-VAE（视觉基础模型对齐的变分自编码器）显著扩展了潜在扩散模型的重建-生成边界，使扩散变换器（DiT）在高维潜在空间中的收敛速度加快。为了充分发挥VA-VAE的潜力，我们构建了一个增强的DiT基线，采用了改进的训练策略和架构设计，称为LightningDiT。该集成系统在ImageNet 256x256生成上达到了最先进的（SOTA）性能，FID得分为1.35，并通过在仅64个周期内达到FID得分为2.11，展示了显著的训练效率，相比于原始DiT的收敛速度提高了超过21倍。模型和代码可在以下链接获取：https://github.com/hustvl/LightningDiT。
2501.01422	 | Multi-Modal Video Feature Extraction for Popularity Prediction	 | Haixu Liu,Wenning Wang,Haoxiang Zheng,Penghao Jiang,Qirui Wang,Ruiqing Yan,Qiuzhuang Sun	 | 本文利用视频及其相关特征预测短视频的流行度，通过多种视频分类模型提取视频模态特征，并结合清理后的字幕和数据挖掘得到的实用特征，最终使用神经网络和XGBoost模型进行预测，结果综合平均以提高准确性。	 | This work aims to predict the popularity of short videos using the videos themselves and their related features. Popularity is measured by four key engagement metrics: view count, like count, comment count, and share count. This study employs video classification models with different architectures and training methods as backbone networks to extract video modality features. Meanwhile, the cleaned video captions are incorporated into a carefully designed prompt framework, along with the video, as input for video-to-text generation models, which generate detailed text-based video content understanding. These texts are then encoded into vectors using a pre-trained BERT model. Based on the six sets of vectors mentioned above, a neural network is trained for each of the four prediction metrics. Moreover, the study conducts data mining and feature engineering based on the video and tabular data, constructing practical features such as the total frequency of hashtag appearances, the total frequency of mention appearances, video duration, frame count, frame rate, and total time online. Multiple machine learning models are trained, and the most stable model, XGBoost, is selected. Finally, the predictions from the neural network and XGBoost models are averaged to obtain the final result.	 | 本文旨在使用视频本身及其相关特征来预测短视频的流行度。流行度通过四个关键的互动指标进行衡量：播放次数、点赞次数、评论次数和分享次数。本研究采用具有不同架构和训练方法的视频分类模型作为主干网络，以提取视频模态特征。同时，清理后的视频字幕被整合到一个精心设计的提示框架中，并与视频一起作为输入提供给视频到文本生成模型，生成详细的基于文本的视频内容理解。这些文本随后使用预训练的BERT模型编码成向量。基于上述六组向量，分别对四个预测指标训练了一个神经网络。此外，基于视频和表格式数据进行了数据挖掘和特征工程，构建了实用特征，如哈希标签出现的总频率、被提及的总频率、视频时长、帧数、帧率以及总的在线时间。多种机器学习模型被训练，最稳定的模型XGBoost被选择。最后，神经网络和XGBoost模型的预测结果被平均，以获得最终结果。
2501.01421	 | R-SCoRe: Revisiting Scene Coordinate Regression for Robust Large-Scale Visual Localization	 | Xudong Jiang,Fangjinhua Wang,Silvano Galliani,Christoph Vogel,Marc Pollefeys	 | 本文提出了一种新的基于学习的视觉定位方法，通过图基全局编码、数据增强策略和深度调整的重投影损失来改善特征匹配的稳健性。该方法在复杂光照和图像级别歧义的数据集上取得了最先进的性能，且无需依赖网络集成或3D监督，在Aachen Day-Night数据集上的表现尤其显著。	 | Learning-based visual localization methods that use scene coordinate regression (SCR) offer the advantage of smaller map sizes. However, on datasets with complex illumination changes or image-level ambiguities, it remains a less robust alternative to feature matching methods. This work aims to close the gap. We introduce a covisibility graph-based global encoding learning and data augmentation strategy, along with a depth-adjusted reprojection loss to facilitate implicit triangulation. Additionally, we revisit the network architecture and local feature extraction module. Our method achieves state-of-the-art on challenging large-scale datasets without relying on network ensembles or 3D supervision. On Aachen Day-Night, we are 10$\times$ more accurate than previous SCR methods with similar map sizes and require at least 5$\times$ smaller map sizes than any other SCR method while still delivering superior accuracy. Code will be available at: https://github.com/cvg/scrstudio .	 | 基于学习的视觉定位方法使用场景坐标回归（SCR）具有地图尺寸较小的优势。然而，在复杂光照变化或图像级别歧义的数据集上，它仍然是特征匹配方法不太稳健的替代方案。本工作旨在缩小这一差距。我们提出了一种共视图图基全局编码学习和数据增强策略，以及一种深度调整的重投影损失，以促进隐式三角测量。此外，我们还重新审视了网络架构和局部特征提取模块。我们的方法在具有挑战性的大规模数据集上达到了最先进的性能，而无需依赖网络集成或3D监督。在Aachen Day-Night数据集上，我们的方法在类似地图尺寸的情况下比之前的SCR方法准确度高10倍，并且所需的地图尺寸至少是其他任何SCR方法的5倍，同时仍能提供更优的准确度。代码将在以下链接中提供：https://github.com/cvg/scrstudio 。
2501.01420	 | A Multi-task Supervised Compression Model for Split Computing	 | Yoshitomo Matsubara,Matteo Mendula,Marco Levorato	 | Ladon 是首个用于多任务分片计算的多任务头监督压缩模型，能够在保持或提高预测性能的同时，显著降低端到端延迟和移动设备能耗。	 | Split computing ($\neq$ split learning) is a promising approach to deep learning models for resource-constrained edge computing systems, where weak sensor (mobile) devices are wirelessly connected to stronger edge servers through channels with limited communication capacity. State-of-theart work on split computing presents methods for single tasks such as image classification, object detection, or semantic segmentation. The application of existing methods to multitask problems degrades model accuracy and/or significantly increase runtime latency. In this study, we propose Ladon, the first multi-task-head supervised compression model for multi-task split computing. Experimental results show that the multi-task supervised compression model either outperformed or rivaled strong lightweight baseline models in terms of predictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets while learning compressed representations at its early layers. Furthermore, our models reduced end-to-end latency (by up to 95.4%) and energy consumption of mobile devices (by up to 88.2%) in multi-task split computing scenarios.	 | 分片计算（不同于分片学习）是一种有前景的方法，适用于资源受限的边缘计算系统中的深度学习模型，其中弱传感器（移动）设备通过具有有限通信能力的无线通道连接到更强的边缘服务器。关于分片计算的最新研究主要介绍了单一任务方法，例如图像分类、目标检测或语义分割。将现有方法应用到多任务问题中会降低模型准确性并/或显著增加运行时延迟。在本研究中，我们提出了Ladon，这是第一个用于多任务分片计算的多任务头监督压缩模型。实验结果表明，在ILSVRC 2012、COCO 2017 和 PASCAL VOC 2012 数据集上，Ladon 在预测性能方面要么优于或与强大的轻量级基线模型持平，同时在早期层学习压缩表示。此外，我们的模型在多任务分片计算场景中将端到端延迟降低了高达95.4%，并将移动设备的能耗降低了高达88.2%。
2501.01416	 | Hierarchical Alignment-enhanced Adaptive Grounding Network for Generalized Referring Expression Comprehension	 | Yaxian Wang,Henghui Ding,Shuting He,Xudong Jiang,Bifan Wei,Jun Liu	 | 本文提出了HieA2G模型，一种用于解决泛化参照表达理解任务的层次化对齐增强自适应定位网络，该模型在多个任务上取得了最佳性能，证明了其在复杂场景下的优越性和泛化能力。	 | In this work, we address the challenging task of Generalized Referring Expression Comprehension (GREC). Compared to the classic Referring Expression Comprehension (REC) that focuses on single-target expressions, GREC extends the scope to a more practical setting by further encompassing no-target and multi-target expressions. Existing REC methods face challenges in handling the complex cases encountered in GREC, primarily due to their fixed output and limitations in multi-modal representations. To address these issues, we propose a Hierarchical Alignment-enhanced Adaptive Grounding Network (HieA2G) for GREC, which can flexibly deal with various types of referring expressions. First, a Hierarchical Multi-modal Semantic Alignment (HMSA) module is proposed to incorporate three levels of alignments, including word-object, phrase-object, and text-image alignment. It enables hierarchical cross-modal interactions across multiple levels to achieve comprehensive and robust multi-modal understanding, greatly enhancing grounding ability for complex cases. Then, to address the varying number of target objects in GREC, we introduce an Adaptive Grounding Counter (AGC) to dynamically determine the number of output targets. Additionally, an auxiliary contrastive loss is employed in AGC to enhance object-counting ability by pulling in multi-modal features with the same counting and pushing away those with different counting. Extensive experimental results show that HieA2G achieves new state-of-the-art performance on the challenging GREC task and also the other 4 tasks, including REC, Phrase Grounding, Referring Expression Segmentation (RES), and Generalized Referring Expression Segmentation (GRES), demonstrating the remarkable superiority and generalizability of the proposed HieA2G.	 | 在本文中，我们着重解决了一项具有挑战性的任务：泛化参照表达理解（Generalized Referring Expression Comprehension, GREC）。与传统参照表达理解（Classic Referring Expression Comprehension, REC）仅关注单一目标表达不同，GREC进一步扩展到更多实际应用场景，涵盖了无目标和多目标表达。现有的REC方法在处理GREC中遇到的复杂情况时面临挑战，主要原因是它们输出固定且多模态表示能力有限。为了解决这些问题，我们提出了一种层次化对齐增强的自适应定位网络（Hierarchical Alignment-enhanced Adaptive Grounding Network, HieA2G），可以灵活处理各种类型的参照表达。首先，我们提出了层次化多模态语义对齐（Hierarchical Multi-modal Semantic Alignment, HMSA）模块，引入了三个层次对齐，包括词-物体、短语-物体和文本-图像对齐。它实现了多层次的跨模态交互，从而实现全面且稳健的多模态理解，极大地增强了在复杂情况下的定位能力。其次，为了应对GREC中目标物体数量的变化，我们引入了一个自适应定位计数器（Adaptive Grounding Counter, AGC），以动态确定输出目标的数量。此外，在AGC中还使用了辅助对比损失，通过拉近具有相同计数的多模态特征并推开具有不同计数的特征来增强物体计数能力。广泛的实验结果表明，HieA2G 在具有挑战性的GREC任务以及其他4个任务（包括REC、短语定位、参照表达分割（Referring Expression Segmentation, RES）和泛化参照表达分割（Generalized Referring Expression Segmentation, GRES））上均取得了新的最佳性能，证明了所提出的HieA2G的显著优越性和泛化能力。
2501.01409	 | On Unifying Video Generation and Camera Pose Estimation	 | Chun-Hao Paul Huang,Jae Shin Yoon,Hyeonho Jeong,Niloy Mitra,Duygu Ceylan	 | 研究发现，尽管视频生成器OpenSora的中间特征原始3D意识有限，但通过专门任务监督微调，可以显著提高其在摄像机姿态估计中的准确性，从而生成具有竞争力质量的3D视频。	 | Inspired by the emergent 3D capabilities in image generators, we explore whether video generators similarly exhibit 3D awareness. Using structure-from-motion (SfM) as a benchmark for 3D tasks, we investigate if intermediate features from OpenSora, a video generation model, can support camera pose estimation. We first examine native 3D awareness in video generation features by routing raw intermediate outputs to SfM-prediction modules like DUSt3R. Then, we explore the impact of fine-tuning on camera pose estimation to enhance 3D awareness. Results indicate that while video generator features have limited inherent 3D awareness, task-specific supervision significantly boosts their accuracy for camera pose estimation, resulting in competitive performance. The proposed unified model, named JOG3R, produces camera pose estimates with competitive quality without degrading video generation quality.	 | 受图像生成器中新兴的3D能力启发，我们研究视频生成器是否也表现出3D意识。利用结构从运动（SfM）作为3D任务的基准，我们调查OpenSora（一种视频生成模型）的中间特征是否能够支持摄像机姿态估计。我们首先通过将原始中间输出路由到DUSt3R等SfM预测模块来检查视频生成特征的原生3D意识。然后，我们探索微调对摄像机姿态估计的影响，以增强3D意识。结果显示，尽管视频生成器特征的固有3D意识有限，但专门的任务监督显著提高了它们在摄像机姿态估计中的准确性，最终获得了竞争力的表现。所提出的统一模型JOG3R能够生成具有竞争力质量的摄像机姿态估计，同时不降低视频生成质量。
2501.01407	 | Nested Attention: Semantic-aware Attention Values for Concept Personalization	 | Or Patashnik,Rinon Gal,Daniil Ostashev,Sergey Tulyakov,Kfir Aberman,Daniel Cohen-Or	 | 这项研究提出了一种名为嵌套注意力的新机制，通过将丰富的图像表示注入到现有模型的交叉注意力层中，以实现高度的身份保持和良好的文本对齐性，从而在保持一致性和灵活性之间取得平衡。该方法不仅适用于各种领域，还能在单张图像中结合来自不同领域的多个个性化主题。	 | Personalizing text-to-image models to generate images of specific subjects across diverse scenes and styles is a rapidly advancing field. Current approaches often face challenges in maintaining a balance between identity preservation and alignment with the input text prompt. Some methods rely on a single textual token to represent a subject, which limits expressiveness, while others employ richer representations but disrupt the model's prior, diminishing prompt alignment. In this work, we introduce Nested Attention, a novel mechanism that injects a rich and expressive image representation into the model's existing cross-attention layers. Our key idea is to generate query-dependent subject values, derived from nested attention layers that learn to select relevant subject features for each region in the generated image. We integrate these nested layers into an encoder-based personalization method, and show that they enable high identity preservation while adhering to input text prompts. Our approach is general and can be trained on various domains. Additionally, its prior preservation allows us to combine multiple personalized subjects from different domains in a single image.	 | 将特定文本转换成图像的模型进行个性化调整，以生成特定主题在多种场景和风格下的图像，这是一个迅速发展的领域。当前的方法往往在保持身份一致性和与输入文本提示的对齐之间面临挑战。一些方法依赖于单一的文本标记来表示主题，这限制了表达能力，而另一些方法则使用更丰富的表示，但会破坏模型的先验知识，降低提示对齐性。在此项工作中，我们引入了一种名为嵌套注意力的新机制，该机制将丰富的图像表示注入模型现有的交叉注意力层。我们的核心思想是生成查询依赖的主题值，这些值来自嵌套注意力层，该层能够学习选择生成图像中每个区域的相关主题特征。我们将这些嵌套层整合到基于编码器的个性化方法中，并展示了它们能够实现高度的身份保持，同时遵循输入文本提示。我们的方法具有通用性，可以在各种领域进行训练。此外，其先验知识的保持使我们能够在单张图像中结合来自不同领域的多个个性化主题。
2501.01393	 | Learning 3D Garment Animation from Trajectories of A Piece of Cloth	 | Yidi Shao,Chen Change Loy,Bo Dai	 | 本文提出了一种解耦方案，利用能量单元网络（EUNet）从观察到的布料中学习本构行为，并在学习到的约束下动态动画化各种服装，从而减轻了对大量服装数据的需求，提高了动画性能的稳定性和物理一致性。	 | Garment animation is ubiquitous in various applications, such as virtual reality, gaming, and film producing. Recently, learning-based approaches obtain compelling performance in animating diverse garments under versatile scenarios. Nevertheless, to mimic the deformations of the observed garments, data-driven methods require large scale of garment data, which are both resource-wise expensive and time-consuming. In addition, forcing models to match the dynamics of observed garment animation may hinder the potentials to generalize to unseen cases. In this paper, instead of using garment-wise supervised-learning we adopt a disentangled scheme to learn how to animate observed garments: 1). learning constitutive behaviors from the observed cloth; 2). dynamically animate various garments constrained by the learned constitutive laws. Specifically, we propose Energy Unit network (EUNet) to model the constitutive relations in the format of energy. Without the priors from analytical physics models and differentiable simulation engines, EUNet is able to directly capture the constitutive behaviors from the observed piece of cloth and uniformly describes the change of energy caused by deformations, such as stretching and bending. We further apply the pre-trained EUNet to animate various garments based on energy optimizations. The disentangled scheme alleviates the need of garment data and enables us to utilize the dynamics of a piece of cloth for animating garments. Experiments show that while EUNet effectively delivers the energy gradients due to the deformations, models constrained by EUNet achieve more stable and physically plausible performance comparing with those trained in garment-wise supervised manner. Code is available at https://github.com/ftbabi/EUNet_NeurIPS2024.git .	 | 服装动画在虚拟现实、游戏和电影制作等众多应用中无处不在。最近，基于学习的方法在模拟多样化的服装在不同场景下的变形方面表现出色。然而，为了模拟观察到的服装的变形，数据驱动的方法需要大量的服装数据，这在资源和时间上都十分昂贵。此外，强制模型匹配观察到的服装动画的动力学可能妨碍其泛化到未见过的情况的能力。在本文中，我们没有采用基于服装的监督学习方法，而是采用了一个解耦方案来学习如何动画化观察到的服装：1）从观察到的布料中学习本构行为；2）在学习到的本构法则约束下动态动画化各种服装。具体来说，我们提出了能量单元网络（EUNet）来以能量的形式建模本构关系。在没有解析物理模型和可微模拟引擎先验的情况下，EUNet能够直接从观察到的布料中捕捉本构行为，并统一描述由于变形（如拉伸和弯曲）引起的能量变化。我们进一步利用预训练的EUNet基于能量优化来动画化各种服装。解耦方案减轻了对服装数据的需求，使我们能够利用一块布的动力学来动画化服装。实验表明，虽然EUNet有效地传递了由于变形引起的能量梯度，但受EUNet约束的模型在动画性能上更稳定且更符合物理原理，相比那些以服装为单位进行监督训练的模型。代码可从https://github.com/ftbabi/EUNet_NeurIPS2024.git 获取。
2501.01375	 | Iris Recognition for Infants	 | Rasel Ahmed Bhuiyan,Mateusz Trokielewicz,Piotr Maciejewicz,Sherri Bucher,Adam Czajka	 | 本文探讨了利用虹膜识别技术为4-6周龄婴儿构建生物特征标识符的可行性，并开发了一个完全运行的婴儿虹膜识别系统，该系统在等错误率和受试者操作特征曲线下面积方面显著优于现有的成人虹膜识别系统。	 | Non-invasive, efficient, physical token-less, accurate and stable identification methods for newborns may prevent baby swapping at birth, limit baby abductions and improve post-natal health monitoring across geographies, within the context of both the formal (i.e., hospitals) and informal (i.e., humanitarian and fragile settings) health sectors. This paper explores the feasibility of application iris recognition to build biometric identifiers for 4-6 week old infants. We (a) collected near infrared (NIR) iris images from 17 infants using a specially-designed NIR iris sensor; (b) evaluated six iris recognition methods to assess readiness of the state-of-the-art iris recognition to be applied to newborns and infants; (c) proposed a new segmentation model that correctly detects iris texture within infants iris images, and coupled it with several iris texture encoding approaches to offer, to the first of our knowledge, a fully-operational infant iris recognition system; and, (d) trained a StyleGAN-based model to synthesize iris images mimicking samples acquired from infants to deliver to the research community privacy-safe infant iris images. The proposed system, incorporating the specially-designed iris sensor and segmenter, and applied to the collected infant iris samples, achieved Equal Error Rate (EER) of 3\% and Area Under ROC Curve (AUC) of 99\%, compared to EER$\geq$20\% and AUC$\leq$88\% obtained for state of the art adult iris recognition systems. This suggests that it may be feasible to design methods that succesfully extract biometric features from infant irises.	 | 非侵入、高效、无物理标记、准确且稳定的新生儿识别方法可能预防出生时的婴儿交换、限制婴儿被拐和提高跨地域的产后健康监测。在正式（例如，医院）和非正式（例如，人道主义和脆弱环境）医疗部门的背景下，本论文探讨了将虹膜识别应用于4-6周龄婴儿以构建生物特征标识符的可行性。我们（a）使用特别设计的近红外（NIR）虹膜传感器从17名婴儿中采集近红外虹膜图像；（b）评估了六种虹膜识别方法，以评估最先进的虹膜识别技术应用于新生儿和婴儿的成熟度；（c）提出了一种新的分割模型，能够正确检测婴儿虹膜图像中的虹膜纹理，并将其与几种虹膜纹理编码方法相结合，到目前为止提供了首个完全运行的婴儿虹膜识别系统；以及（d）训练了一个基于StyleGAN的模型，以合成模拟从婴儿采集的样本的虹膜图像，向研究人员提供隐私保护的婴儿虹膜图像。该系统结合特别设计的虹膜传感器和分割器，并应用于采集的婴儿虹膜样本，实现了等错误率（EER）为3%和受试者操作特征曲线下面积（AUC）为99%，而最先进的成人虹膜识别系统的EER≥20%且AUC≤88%。这表明设计能够成功从婴儿虹膜中提取生物特征的方法可能是可行的。
2501.01371	 | CLIP-UP: CLIP-Based Unanswerable Problem Detection for Visual Question Answering	 | Ben Vardi,Oron Nir,Ariel Shamir	 | CLIP-UP是一种基于CLIP的轻量级方法，用于增强视觉-语言模型在视觉问答中的不可回答问题检测能力，仅需少量额外训练即可显著提高VQA模型对不可回答问题的处理效果，同时保持其他任务的性能不变。	 | Recent Vision-Language Models (VLMs) have demonstrated remarkable capabilities in visual understanding and reasoning, and in particular on multiple-choice Visual Question Answering (VQA). Still, these models can make distinctly unnatural errors, for example, providing (wrong) answers to unanswerable VQA questions, such as questions asking about objects that do not appear in the image. To address this issue, we propose CLIP-UP: CLIP-based Unanswerable Problem detection, a novel lightweight method for equipping VLMs with the ability to withhold answers to unanswerable questions. By leveraging CLIP to extract question-image alignment information, CLIP-UP requires only efficient training of a few additional layers, while keeping the original VLMs' weights unchanged. Tested across LLaVA models, CLIP-UP achieves state-of-the-art results on the MM-UPD benchmark for assessing unanswerability in multiple-choice VQA, while preserving the original performance on other tasks.	 | 近年来，视觉-语言模型（VLMs）在视觉理解与推理方面展现了卓越的能力，特别是在多项选择的视觉问答（VQA）方面。然而，这些模型仍然可能会做出明显不自然的错误，例如对无法回答的VQA问题给出（错误的）答案，比如问题涉及图像中未出现的物体。为了解决这一问题，我们提出了CLIP-UP：基于CLIP的不可回答问题检测，这是一种新颖的轻量级方法，旨在使VLMs具备对不可回答的问题不予作答的能力。通过利用CLIP提取问题-图像对齐信息，CLIP-UP仅需对少量额外的层进行高效的训练，而保持原始VLMs的权重不变。在LLaVA模型上测试后，CLIP-UP在MM-UPD基准测试中取得了评估多项选择VQA不可回答性方面的最佳结果，同时在其他任务上保持了原始性能。
2501.01368	 | Test-time Controllable Image Generation by Explicit Spatial Constraint Enforcement	 | Z. Zhang,B. Liu,J. Bao,L. Chen,S. Zhu,J. Yu	 | 本文提出了一种新的测试时可控生成方法，该方法能够处理自然文本提示和复杂条件，通过拆分空域条件为语义条件和几何条件分别确保其一致性，并引入了基于扩散过程的潜在变量补充方法以减少伪影，从而在布局一致性评估指标上显著优于现有方法。	 | Recent text-to-image generation favors various forms of spatial conditions, e.g., masks, bounding boxes, and key points. However, the majority of the prior art requires form-specific annotations to fine-tune the original model, leading to poor test-time generalizability. Meanwhile, existing training-free methods work well only with simplified prompts and spatial conditions. In this work, we propose a novel yet generic test-time controllable generation method that aims at natural text prompts and complex conditions. Specifically, we decouple spatial conditions into semantic and geometric conditions and then enforce their consistency during the image-generation process individually. As for the former, we target bridging the gap between the semantic condition and text prompts, as well as the gap between such condition and the attention map from diffusion models. To achieve this, we propose to first complete the prompt w.r.t. semantic condition, and then remove the negative impact of distracting prompt words by measuring their statistics in attention maps as well as distances in word space w.r.t. this condition. To further cope with the complex geometric conditions, we introduce a geometric transform module, in which Region-of-Interests will be identified in attention maps and further used to translate category-wise latents w.r.t. geometric condition. More importantly, we propose a diffusion-based latents-refill method to explicitly remove the impact of latents at the RoI, reducing the artifacts on generated images. Experiments on Coco-stuff dataset showcase 30$\%$ relative boost compared to SOTA training-free methods on layout consistency evaluation metrics.	 | Recent文本到图像生成倾向于各种形式的空域条件，例如掩码、边界框和关键点。然而，大多数先前的技术需要特定形式的注释来微调原始模型，这导致了较差的测试时泛化能力。同时，现有的无需训练的方法仅在简化提示和空域条件方面表现出色。在本文中，我们提出了一种新颖且通用的测试时可控生成方法，旨在处理自然文本提示和复杂条件。具体来说，我们将空域条件拆分为语义条件和几何条件，并在图像生成过程中分别确保它们的一致性。对于前者，我们目标是弥合语义条件与文本提示之间的差距，以及这种条件与扩散模型中的注意力图之间的差距。为此，我们提出首先根据语义条件完成提示，然后通过测量它们在注意力图中的统计特征以及在词汇空间中的距离来消除分散性提示词的负面影响。为了进一步应对复杂的几何条件，我们引入了一个几何变换模块，在该模块中将在注意力图中识别感兴趣区域，并进一步用于将几何条件下的类别特定的潜在变量翻译。更重要的是，我们提出了一种基于扩散过程的潜在变量补充方法，以显式地去除感兴趣区域潜在变量的影响，从而减少生成图像中的伪影。在Coco-stuff数据集上的实验表明，与当前最先进的无需训练的方法相比，在布局一致性评估指标上获得了30%的相对提升。
2501.01366	 | ViGiL3D: A Linguistically Diverse Dataset for 3D Visual Grounding	 | Austin T. Wang,ZeMing Gong,Angel X. Chang	 | 该研究提出了一种用于分析3D视觉定位（3DVG）提示的语言框架，并引入了3D视觉定位多样化语言（ViGiL3D）数据集，以评估视觉定位方法在处理多样化语言模式时的表现，发现现有方法难以识别不在分布范围内的挑战性提示。	 | 3D visual grounding (3DVG) involves localizing entities in a 3D scene referred to by natural language text. Such models are useful for embodied AI and scene retrieval applications, which involve searching for objects or patterns using natural language descriptions. While recent works have focused on LLM-based scaling of 3DVG datasets, these datasets do not capture the full range of potential prompts which could be specified in the English language. To ensure that we are scaling up and testing against a useful and representative set of prompts, we propose a framework for linguistically analyzing 3DVG prompts and introduce Visual Grounding with Diverse Language in 3D (ViGiL3D), a diagnostic dataset for evaluating visual grounding methods against a diverse set of language patterns. We evaluate existing open-vocabulary 3DVG methods to demonstrate that these methods are not yet proficient in understanding and identifying the targets of more challenging, out-of-distribution prompts, toward real-world applications.	 | 3D视觉定位（3DVG）涉及根据自然语言文本在3D场景中定位实体。这类模型对于涉及使用自然语言描述进行物体或模式搜索的嵌入式AI和场景检索应用非常有用。尽管最近的工作主要集中在使用LLM（大型语言模型）扩展3DVG数据集，但这些数据集并未捕捉到英文语言中可能指定的全部范围的提示。为了确保我们在扩展和测试时使用一个有用且具有代表性的提示集，我们提出了一种语言分析3DVG提示的框架，并引入了3D视觉定位多样化语言（ViGiL3D），这是一个用于评估视觉定位方法在面对多样化语言模式时表现的诊断数据集。我们评估现有的开放词汇3DVG方法，以证明这些方法尚未能够理解和识别更具挑战性、不在分布范围内的提示的目标，从而适用于实际应用。
2501.01346	 | Large Vision-Language Model Alignment and Misalignment: A Survey Through the Lens of Explainability	 | Dong Shu,Haiyan Zhao,Jingyu Hu,Weiru Liu,Lu Cheng,Mengnan Du	 | 本文从可解释性的角度全面考察了大型视觉-语言模型（LVLMs）中的对齐和不对齐现象，揭示了其在数据、模型和推理层面的挑战，并概述了现有策略和未来研究方向。	 | Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in processing both visual and textual information. However, the critical challenge of alignment between visual and linguistic representations is not fully understood. This survey presents a comprehensive examination of alignment and misalignment in LVLMs through an explainability lens. We first examine the fundamentals of alignment, exploring its representational and behavioral aspects, training methodologies, and theoretical foundations. We then analyze misalignment phenomena across three semantic levels: object, attribute, and relational misalignment. Our investigation reveals that misalignment emerges from challenges at multiple levels: the data level, the model level, and the inference level. We provide a comprehensive review of existing mitigation strategies, categorizing them into parameter-frozen and parameter-tuning approaches. Finally, we outline promising future research directions, emphasizing the need for standardized evaluation protocols and in-depth explainability studies.	 | 大型视觉-语言模型（LVLMs）在处理视觉和文本信息方面展现了非凡的能力。然而，视觉表示与语言表示之间的对齐问题仍然存在关键性的挑战，尚未完全理解。本文综述从可解释性的角度对LVLMs中的对齐和不对齐现象进行了全面的考察。首先，我们探讨了对齐的各个方面，包括其表示和行为特性、训练方法、以及理论基础。然后，我们从三个语义层次分析了不对齐现象：对象不对齐、属性不对齐和关系不对齐。我们的研究揭示了不对齐现象在多个层面都存在挑战：数据层面、模型层面和推理层面。我们还概述了现有减轻这些挑战的策略，把这些策略分为参数冻结和参数调优两类。最后，我们指出了未来研究有希望的方向，强调了需要标准化评估协议和深入的可解释性研究的重要性。
2501.01320	 | SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration	 | Jianyi Wang,Zhijie Lin,Meng Wei,Yang Zhao,Ceyuan Yang,Chen Change Loy,Lu Jiang	 | SeedVR是一种专门设计用于处理任意长度和分辨率的视频恢复问题的扩散变换器，通过偏移窗口注意力机制在长视频序列上实现有效恢复，并在多种基准测试中表现出优越性能。	 | Video restoration poses non-trivial challenges in maintaining fidelity while recovering temporally consistent details from unknown degradations in the wild. Despite recent advances in diffusion-based restoration, these methods often face limitations in generation capability and sampling efficiency. In this work, we present SeedVR, a diffusion transformer designed to handle real-world video restoration with arbitrary length and resolution. The core design of SeedVR lies in the shifted window attention that facilitates effective restoration on long video sequences. SeedVR further supports variable-sized windows near the boundary of both spatial and temporal dimensions, overcoming the resolution constraints of traditional window attention. Equipped with contemporary practices, including causal video autoencoder, mixed image and video training, and progressive training, SeedVR achieves highly-competitive performance on both synthetic and real-world benchmarks, as well as AI-generated videos. Extensive experiments demonstrate SeedVR's superiority over existing methods for generic video restoration.	 | 视频恢复在保持保真度的同时，从野生环境中的未知退化中恢复时间上一致的细节，面临着非平凡的挑战。尽管基于扩散的方法在近期取得了进展，但这些方法在生成能力和采样效率方面仍然存在限制。在这项工作中，我们提出了SeedVR，一种专门设计用于处理任意长度和分辨率的实际视频恢复问题的扩散变换器。SeedVR的核心设计在于偏移窗口注意力机制，该机制可以在长视频序列上实现有效的恢复。SeedVR还支持在时空维度边界的附近使用可变大小的窗口，从而克服了传统窗口注意力的分辨率限制。借助现代实践，包括因果视频自编码器、混合图像和视频训练以及渐进训练，SeedVR在合成和现实世界的基准测试以及AI生成的视频上都实现了高度竞争力的性能。广泛的实验表明，SeedVR在通用视频恢复方面优于现有的方法。
2501.01311	 | Multi-Head Explainer: A General Framework to Improve Explainability in CNNs and Transformers	 | Bohang Sun,Pietro Liò	 | 该研究提出了Multi-Head Explainer (MHEX)，一种增强CNN和基于Transformer模型解释性和准确性的框架，包含注意力门控、深度监督和等效矩阵三大组件；实验表明MHEX在医学成像和文本分类中不仅能提高分类准确性，还能生成高可解释性的显著性分数。	 | In this study, we introduce the Multi-Head Explainer (MHEX), a versatile and modular framework that enhances both the explainability and accuracy of Convolutional Neural Networks (CNNs) and Transformer-based models. MHEX consists of three core components: an Attention Gate that dynamically highlights task-relevant features, Deep Supervision that guides early layers to capture fine-grained details pertinent to the target class, and an Equivalent Matrix that unifies refined local and global representations to generate comprehensive saliency maps. Our approach demonstrates superior compatibility, enabling effortless integration into existing residual networks like ResNet and Transformer architectures such as BERT with minimal modifications. Extensive experiments on benchmark datasets in medical imaging and text classification show that MHEX not only improves classification accuracy but also produces highly interpretable and detailed saliency scores.	 | 在本研究中，我们提出了Multi-Head Explainer (MHEX)，这是一种既增强卷积神经网络（CNN）和基于Transformer的模型的解释性和准确性的多功能模块化框架。MHEX 包含三个核心组件：注意力门控（Attention Gate），动态突出显示与任务相关的特征；深度监督（Deep Supervision），引导早期层捕捉与目标类别相关的精细细节；等效矩阵（Equivalent Matrix），统一细化的局部和全局表示，生成综合的显著性图。我们的方法具有出色的兼容性，能够轻松集成到现有的残差网络（如ResNet）和Transformer架构（如BERT）中，只需进行少量修改即可。在医学成像和文本分类基准数据集上的广泛实验表明，MHEX 不仅提高了分类准确性，还生成了高度可解释和详细的显著性分数。
2501.01275	 | HybridTrack: A Hybrid Approach for Robust Multi-Object Tracking	 | Leandro Di Bella,Yangxintong Lyu,Bruno Cornelis,Adrian Munteanu	 | HybridTrack 是一种新型的车辆3D多目标跟踪方法，通过结合数据驱动的卡尔曼滤波器消除了手动运动和随机参数建模的需要，在KITTI数据集上取得了优于现有先进方法的HOTA准确率，并实现了高速处理速度达112 FPS。	 | The evolution of Advanced Driver Assistance Systems (ADAS) has increased the need for robust and generalizable algorithms for multi-object tracking. Traditional statistical model-based tracking methods rely on predefined motion models and assumptions about system noise distributions. Although computationally efficient, they often lack adaptability to varying traffic scenarios and require extensive manual design and parameter tuning. To address these issues, we propose a novel 3D multi-object tracking approach for vehicles, HybridTrack, which integrates a data-driven Kalman Filter (KF) within a tracking-by-detection paradigm. In particular, it learns the transition residual and Kalman gain directly from data, which eliminates the need for manual motion and stochastic parameter modeling. Validated on the real-world KITTI dataset, HybridTrack achieves 82.08% HOTA accuracy, significantly outperforming state-of-the-art methods. We also evaluate our method under different configurations, achieving the fastest processing speed of 112 FPS. Consequently, HybridTrack eliminates the dependency on scene-specific designs while improving performance and maintaining real-time efficiency. The code will be publicly available at the time of publishing: https://github.com/leandro-svg/HybridTrack.git.	 | 高级驾驶辅助系统（ADAS）的演变增加了对稳健且通用的多目标跟踪算法的需求。传统的基于统计模型的目标跟踪方法依赖于预先定义的运动模型和系统噪声分布的假设。虽然这些方法计算效率高，但它们往往缺乏对不同交通场景的适应性，并且需要大量的手动设计和参数调整。为了解决这些问题，我们提出了一种名为HybridTrack的新型车辆3D多目标跟踪方法，该方法在跟踪-by-检测范式中结合了数据驱动的卡尔曼滤波器（KF）。具体而言，HybridTrack直接从数据中学习转换残差和卡尔曼增益，从而消除了手动运动和随机参数建模的需要。该方法在真实世界的KITTI数据集上进行了验证，HybridTrack的HOTA准确率达到82.08%，显著优于现有最先进的方法。我们还评估了该方法在不同配置下的表现，实现了最快112 FPS的处理速度。因此，HybridTrack消除了对场景特定设计的依赖，同时提高了性能并保持了实时效率。代码将在发布时公开：https://github.com/leandro-svg/HybridTrack.git。
2501.01262	 | Detail Matters: Mamba-Inspired Joint Unfolding Network for Snapshot Spectral Compressive Imaging	 | Mengjie Qin,Yuchao Feng,Zongliang Wu,Yulun Zhang,Xin Yuan	 | 该研究提出了一种名为MiJUN的联合展开网络，结合了物理嵌入的深度展开网络和基于学习的高光谱成像技术，通过引入加速方案和张量模式-$k$展开，提高了3D高光谱图像重建的准确性和稳定性。	 | In the coded aperture snapshot spectral imaging system, Deep Unfolding Networks (DUNs) have made impressive progress in recovering 3D hyperspectral images (HSIs) from a single 2D measurement. However, the inherent nonlinear and ill-posed characteristics of HSI reconstruction still pose challenges to existing methods in terms of accuracy and stability. To address this issue, we propose a Mamba-inspired Joint Unfolding Network (MiJUN), which integrates physics-embedded DUNs with learning-based HSI imaging. Firstly, leveraging the concept of trapezoid discretization to expand the representation space of unfolding networks, we introduce an accelerated unfolding network scheme. This approach can be interpreted as a generalized accelerated half-quadratic splitting with a second-order differential equation, which reduces the reliance on initial optimization stages and addresses challenges related to long-range interactions. Crucially, within the Mamba framework, we restructure the Mamba-inspired global-to-local attention mechanism by incorporating a selective state space model and an attention mechanism. This effectively reinterprets Mamba as a variant of the Transformer} architecture, improving its adaptability and efficiency. Furthermore, we refine the scanning strategy with Mamba by integrating the tensor mode-$k$ unfolding into the Mamba network. This approach emphasizes the low-rank properties of tensors along various modes, while conveniently facilitating 12 scanning directions. Numerical and visual comparisons on both simulation and real datasets demonstrate the superiority of our proposed MiJUN, and achieving overwhelming detail representation.	 | 在编码孔径快照光谱成像系统中，深度展开网络（DUNs）已经取得了显著进展，能够在单个2D测量中恢复3D高光谱图像（HSIs）。然而，高光谱图像重建固有的非线性和病态特性仍然给现有方法在准确性和稳定性方面带来了挑战。为了解决这个问题，我们提出了一种借鉴Mamba设计理念的联合展开网络（MiJUN），该网络将物理嵌入的DUNs与基于学习的HSI成像相结合。首先，利用梯形离散化概念扩展展开网络的表示空间，我们引入了一种加速展开网络方案。这一方法可以解释为广义加速半二次分裂的二次微分方程，从而减少了对初始优化阶段的依赖，并解决了长程相互作用带来的挑战。至关重要的是，在Mamba框架中，我们通过引入选择状态空间模型和注意力机制，重构了Mamba启发式的全局到局部注意力机制。这实际上重新解释了Mamba作为一种Transformer架构的变体，提高了其适应性和效率。此外，我们通过将张量模式-$k$展开整合到Mamba网络中，进一步改进了扫描策略。这种方法强调了沿各种模式的张量低秩性质，同时方便地实现了12个扫描方向。在模拟和实际数据集上的数值和视觉比较表明，我们提出的MiJUN具有明显的优势，并实现了卓越的细节表示。
2501.01245	 | SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization	 | Yongle Huang,Haodong Chen,Zhenbang Xu,Zihan Jia,Haozhou Sun,Dian Shao	 | 该研究提出了一种名为SeFAR的半监督学习框架，以解决细粒度动作识别任务，通过结合双层次时间元素表示、增强策略和自适应调节，即使在标注数据稀缺的情况下也取得了最先进的性能。	 | Human action understanding is crucial for the advancement of multimodal systems. While recent developments, driven by powerful large language models (LLMs), aim to be general enough to cover a wide range of categories, they often overlook the need for more specific capabilities. In this work, we address the more challenging task of Fine-grained Action Recognition (FAR), which focuses on detailed semantic labels within shorter temporal duration (e.g., "salto backward tucked with 1 turn"). Given the high costs of annotating fine-grained labels and the substantial data needed for fine-tuning LLMs, we propose to adopt semi-supervised learning (SSL). Our framework, SeFAR, incorporates several innovative designs to tackle these challenges. Specifically, to capture sufficient visual details, we construct Dual-level temporal elements as more effective representations, based on which we design a new strong augmentation strategy for the Teacher-Student learning paradigm through involving moderate temporal perturbation. Furthermore, to handle the high uncertainty within the teacher model's predictions for FAR, we propose the Adaptive Regulation to stabilize the learning process. Experiments show that SeFAR achieves state-of-the-art performance on two FAR datasets, FineGym and FineDiving, across various data scopes. It also outperforms other semi-supervised methods on two classical coarse-grained datasets, UCF101 and HMDB51. Further analysis and ablation studies validate the effectiveness of our designs. Additionally, we show that the features extracted by our SeFAR could largely promote the ability of multimodal foundation models to understand fine-grained and domain-specific semantics.	 | 人类行为理解对于多模态系统的进步至关重要。虽然由强大的大型语言模型（LLMs）驱动的最近发展旨在覆盖广泛的主题类别，但它们往往忽视了更具体能力的必要性。在这项工作中，我们解决了更具有挑战性的细粒度动作识别（FAR）任务，该任务侧重于较短时间内的详细语义标签（例如，“向后卷动一次转体”）。鉴于细粒度标签注释的成本高昂以及微调LLMs所需的大量数据，我们提出采用半监督学习（SSL）。我们的框架SeFAR结合了几种创新设计来应对这些挑战。具体而言，为了捕捉足够的视觉细节，我们构建了双层次时间元素作为更有效的表示，基于此，我们设计了一种新的强增强策略，通过引入适度的时间扰动来应用于教师-学生学习范式。此外，为了处理教师模型在FAR中的高不确定性预测，我们提出了自适应调节以稳定学习过程。实验结果显示，SeFAR在两个FAR数据集FineGym和FineDiving上，无论在哪种数据范围内，都实现了最先进的性能。同时，SeFAR在两个经典粗粒度数据集UCF101和HMDB51上也优于其他半监督方法。进一步的分析和消融研究验证了我们设计的有效性。此外，我们展示了由SeFAR提取的特征大大提高了多模态基础模型理解细粒度和领域特定语义的能力。
2501.01243	 | Face-Human-Bench: A Comprehensive Benchmark of Face and Human Understanding for Multi-modal Assistants	 | Lixiong Qin,Shilong Ou,Miaoxuan Zhang,Jiangning Wei,Yuhang Zhang,Xiaoshuai Song,Yuchen Liu,Mei Wang,Weiran Xu	 | 本文提出了一种新的基准测试Face-Human-Bench，包含900个开发集和1800个测试集，用于评估多模态大型语言模型在理解面部和人类方面的能力，并探讨了不同因素对模型性能的影响。	 | Faces and humans are crucial elements in social interaction and are widely included in everyday photos and videos. Therefore, a deep understanding of faces and humans will enable multi-modal assistants to achieve improved response quality and broadened application scope. Currently, the multi-modal assistant community lacks a comprehensive and scientific evaluation of face and human understanding abilities. In this paper, we first propose a hierarchical ability taxonomy that includes three levels of abilities. Then, based on this taxonomy, we collect images and annotations from publicly available datasets in the face and human community and build a semi-automatic data pipeline to produce problems for the new benchmark. Finally, the obtained Face-Human-Bench comprises a development set with 900 problems and a test set with 1800 problems, supporting both English and Chinese. We conduct evaluations over 25 mainstream multi-modal large language models (MLLMs) with our Face-Human-Bench, focusing on the correlation between abilities, the impact of the relative position of targets on performance, and the impact of Chain of Thought (CoT) prompting on performance. Moreover, inspired by multi-modal agents, we also explore which abilities of MLLMs need to be supplemented by specialist models.	 | 面部和人类是社会互动中的关键元素，并且广泛出现在日常照片和视频中。因此，对面部和人类的深入理解将使多模态助手能够在响应质量和应用范围方面取得改进。目前，多模态助手社区缺乏对面部和人类理解能力的全面和科学评估。在本文中，我们首先提出了一种分层能力分类体系，包括三个能力层次。然后，基于此分类体系，我们从面部和人类社区的公开数据集中收集图像和注释，并建立一个半自动数据管道来生成新的基准测试问题。最后，Face-Human-Bench 包含一个包含 900 个问题的开发集和一个包含 1800 个问题的测试集，支持英文和中文。我们使用 Face-Human-Bench 对 25 个主流多模态大型语言模型（MLLMs）进行了评估，重点考察了能力之间的相关性、目标相对位置对性能的影响以及Chain of Thought（CoT）提示对性能的影响。此外，借鉴多模态代理的经验，我们还研究了哪些 MLLMs 的能力需要通过专门模型进行补充。
2501.01240	 | Asymmetric Reinforcing against Multi-modal Representation Bias	 | Xiyuan Gao,Bing Cao,Pengfei Zhu,Nannan Wang,Qinghua Hu	 | 该研究提出了一种对抗多模态表示偏差的不对称强化方法（ARM），该方法动态增强较弱模态同时维持主导模态的表示能力，从而提高多模态学习的性能，并解决了多模态系统在实际应用中的动态模态贡献问题。	 | The strength of multimodal learning lies in its ability to integrate information from various sources, providing rich and comprehensive insights. However, in real-world scenarios, multi-modal systems often face the challenge of dynamic modality contributions, the dominance of different modalities may change with the environments, leading to suboptimal performance in multimodal learning. Current methods mainly enhance weak modalities to balance multimodal representation bias, which inevitably optimizes from a partialmodality perspective, easily leading to performance descending for dominant modalities. To address this problem, we propose an Asymmetric Reinforcing method against Multimodal representation bias (ARM). Our ARM dynamically reinforces the weak modalities while maintaining the ability to represent dominant modalities through conditional mutual information. Moreover, we provide an in-depth analysis that optimizing certain modalities could cause information loss and prevent leveraging the full advantages of multimodal data. By exploring the dominance and narrowing the contribution gaps between modalities, we have significantly improved the performance of multimodal learning, making notable progress in mitigating imbalanced multimodal learning.	 | 多模态学习的强大力量在于其能够整合多种来源的信息，提供丰富而全面的见解。然而，在实际应用场景中，多模态系统常常面临动态模态贡献的问题，不同模态的重要性可能会随着环境的变化而变化，导致多模态学习的性能不佳。当前的方法主要通过增强较弱的模态来平衡多模态表示偏差，这不可避免地是从部分模态的角度进行优化，容易导致主导模态的性能下降。为了解决这个问题，我们提出了一种对抗多模态表示偏差的不对称强化方法（Asymmetric Reinforcing Method against Multimodal representation bias，ARM）。  我们的ARM动态增强较弱的模态，同时通过条件互信息维持主导模态的表示能力。此外，我们进行了深入分析，发现优化某些模态可能会导致信息损失，并且妨碍充分利用多模态数据的优势。通过探索模态的重要性并缩小模态贡献的差异，我们显著提高了多模态学习的性能，并在缓解不平衡多模态学习方面取得了重要进展。
2501.01238	 | EHCTNet: Enhanced Hybrid of CNN and Transformer Network for Remote Sensing Image Change Detection	 | Junjie Yang,Haibo Wan,Zhihai Shang	 | 本文提出了一种增强的卷积神经网络和变换器网络混合模型（EHCTNet），通过提升召回率和整合特征信息的频率成分，有效解决了遥感图像变化检测中的假阴性和不连续性问题。实验结果表明，EHCTNet能够更准确地检测出完整和连续的变化区域，并优于现有最先进的模型。	 | Remote sensing (RS) change detection incurs a high cost because of false negatives, which are more costly than false positives. Existing frameworks, struggling to improve the Precision metric to reduce the cost of false positive, still have limitations in focusing on the change of interest, which leads to missed detections and discontinuity issues. This work tackles these issues by enhancing feature learning capabilities and integrating the frequency components of feature information, with a strategy to incrementally boost the Recall value. We propose an enhanced hybrid of CNN and Transformer network (EHCTNet) for effectively mining the change information of interest. Firstly, a dual branch feature extraction module is used to extract the multi scale features of RS images. Secondly, the frequency component of these features is exploited by a refined module I. Thirdly, an enhanced token mining module based on the Kolmogorov Arnold Network is utilized to derive semantic information. Finally, the semantic change information's frequency component, beneficial for final detection, is mined from the refined module II. Extensive experiments validate the effectiveness of EHCTNet in comprehending complex changes of interest. The visualization outcomes show that EHCTNet detects more intact and continuous changed areas and perceives more accurate neighboring distinction than state of the art models.	 | 基于遥感（RS）的变化检测由于假阴性成本高昂而面临高成本问题，假阴性比假阳性成本更高。现有框架虽然努力通过提高精确率指标来减少假阳性成本，但在关注感兴趣的变化方面仍存在局限性，导致误检和不连续性问题。本文通过增强特征学习能力和整合特征信息的频率成分，提出了一种策略以逐步提升召回率值来解决这些问题。我们提出了一种增强的卷积神经网络（CNN）和变换器网络（Transformer）混合模型（EHCTNet），以有效地挖掘感兴趣的变化信息。首先，使用双分支特征提取模块提取RS图像的多尺度特征。其次，通过精细模块I利用这些特征的频率成分。第三，利用Kolmogorov Arnold网络为基础的增强令牌挖掘模块提取语义信息。最后，从精细模块II中挖掘出对最终检测有益的语义变化信息的频率成分。广泛的实验验证了EHCTNet在理解复杂变化方面的有效性。可视化结果表明，EHCTNet检测到更完整和连续的变化区域，并且比最先进的模型更准确地区分了邻近区域。
2501.01235	 | SVFR: A Unified Framework for Generalized Video Face Restoration	 | Zhiyao Wang,Xu Chen,Chengming Xu,Junwei Zhu,Xiaobin Hu,Jiangning Zhang,Chengjie Wang,Yuqi Liu,Yiyi Zhou,Rongrong Ji	 | 本文提出了一种新的方法来处理广义视频面部恢复任务，通过整合视频超分辨率、修补和着色任务，并利用稳定视频扩散和运动先验，有效增强了时间连贯性和恢复质量。该方法引入了统一框架和新颖的正则化技术，从而提升了视频面部恢复的最新水平。	 | Face Restoration (FR) is a crucial area within image and video processing, focusing on reconstructing high-quality portraits from degraded inputs. Despite advancements in image FR, video FR remains relatively under-explored, primarily due to challenges related to temporal consistency, motion artifacts, and the limited availability of high-quality video data. Moreover, traditional face restoration typically prioritizes enhancing resolution and may not give as much consideration to related tasks such as facial colorization and inpainting. In this paper, we propose a novel approach for the Generalized Video Face Restoration (GVFR) task, which integrates video BFR, inpainting, and colorization tasks that we empirically show to benefit each other. We present a unified framework, termed as stable video face restoration (SVFR), which leverages the generative and motion priors of Stable Video Diffusion (SVD) and incorporates task-specific information through a unified face restoration framework. A learnable task embedding is introduced to enhance task identification. Meanwhile, a novel Unified Latent Regularization (ULR) is employed to encourage the shared feature representation learning among different subtasks. To further enhance the restoration quality and temporal stability, we introduce the facial prior learning and the self-referred refinement as auxiliary strategies used for both training and inference. The proposed framework effectively combines the complementary strengths of these tasks, enhancing temporal coherence and achieving superior restoration quality. This work advances the state-of-the-art in video FR and establishes a new paradigm for generalized video face restoration.	 | 面部恢复（Face Restoration, FR）是图像和视频处理中的一个重要领域，专注于从降质输入中重建高质量的肖像。尽管在图像FR方面取得了进展，但视频FR仍然相对较少被探索，主要是由于时间一致性、运动伪影以及高质量视频数据的稀缺性等因素的挑战。此外，传统的面部恢复通常侧重于增强分辨率，而可能不太关注面部着色和修补等相关的任务。在本文中，我们提出了一种新的方法来处理广义视频面部恢复（Generalized Video Face Restoration, GVFR）任务，该方法将视频BFR、修补和着色任务整合在一起，并通过实验证明这些任务相互受益。我们提供了一个统一框架，称为稳定的视频面部恢复（Stable Video Face Restoration, SVFR），该框架利用了稳定视频扩散（Stable Video Diffusion, SVD）的生成先验和运动先验，并通过统一的面部恢复框架整合了特定任务的信息。可学习的任务嵌入被引入以增强任务识别。同时，我们引入了一种新颖的统一隐变量正则化（Unified Latent Regularization, ULR）方法，以促进不同子任务之间的共享特征表示学习。为了进一步提高恢复质量和时间稳定性，我们引入了面部先验学习和自我参照精炼作为辅助策略，用于训练和推断。所提出的框架有效结合了这些任务的互补优势，增强了时间连贯性，并实现了卓越的恢复质量。这项工作推进了视频面部恢复的最新水平，并建立了一个新的广义视频面部恢复范式。
2501.01231	 | Exploiting Latent Properties to Optimize Neural Codecs	 | Muhammet Balcilar,Bharath Bhushan Damodaran,Karam Naser,Franck Galpin,Pierre Hellier	 | 本文提出了一种改进神经编解码器性能的方法，通过利用向量量化和解码器中的熵梯度，分别采用最优均匀向量量化和利用熵梯度作为重建误差梯度的代理，从而在多种预训练方法中节省了1%到3%的比特率，同时保持相同的质量。此外，基于熵梯度的方法还能显著改善传统编解码器的性能。	 | End-to-end image and video codecs are becoming increasingly competitive, compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques, such as their straightforward adaptation to perceptual distortion metrics and high performance in specific fields thanks to their learning ability. However, current state-of-the-art neural codecs do not fully exploit the benefits of vector quantization and the existence of the entropy gradient in decoding devices. In this paper, we propose to leverage these two properties (vector quantization and entropy gradient) to improve the performance of off-the-shelf codecs. Firstly, we demonstrate that using non-uniform scalar quantization cannot improve performance over uniform quantization. We thus suggest using predefined optimal uniform vector quantization to improve performance. Secondly, we show that the entropy gradient, available at the decoder, is correlated with the reconstruction error gradient, which is not available at the decoder. We therefore use the former as a proxy to enhance compression performance. Our experimental results show that these approaches save between 1 to 3% of the rate for the same quality across various pretrained methods. In addition, the entropy gradient based solution improves traditional codec performance significantly as well.	 | 端到端的图像和视频编解码器在性能上越来越接近传统压缩技术。传统压缩技术经过几十年的手动工程优化。这些可训练的编解码器相比于传统技术具有许多优势，例如能够直接适应感知失真度量，并且由于其学习能力，在特定领域具有高性能。然而，当前最先进的神经编解码器并没有充分利用向量量化和解码设备中存在的熵梯度的优势。在本文中，我们提出利用这两种特性（向量量化和熵梯度）来改进现成编解码器的性能。首先，我们证明使用非均匀标量化不能比均匀标量化提升性能。因此，我们建议使用预定义的最优均匀向量量化来提高性能。其次，我们展示了解码器中存在的熵梯度与重建误差梯度之间存在相关性，而后者在解码器中不可用。因此，我们利用前者作为代理来提升压缩性能。我们的实验结果显示，这些方法在各种预训练方法中可以节省1%到3%的比特率，同时保有相同的质量。此外，基于熵梯度的方法也可以显著改善传统编解码器的性能。
2501.01223	 | Conditional Consistency Guided Image Translation and Enhancement	 | A. V. Subramanyam,Amil Bhagat,Milind Jain	 | 本文提出了条件一致性模型（CCMs），通过引入特定任务的条件输入来改善多域图像翻译任务，展示了其在多个数据集上生成高质量翻译图像的有效性。	 | Consistency models have emerged as a promising alternative to diffusion models, offering high-quality generative capabilities through single-step sample generation. However, their application to multi-domain image translation tasks, such as cross-modal translation and low-light image enhancement remains largely unexplored. In this paper, we introduce Conditional Consistency Models (CCMs) for multi-domain image translation by incorporating additional conditional inputs. We implement these modifications by introducing task-specific conditional inputs that guide the denoising process, ensuring that the generated outputs retain structural and contextual information from the corresponding input domain. We evaluate CCMs on 10 different datasets demonstrating their effectiveness in producing high-quality translated images across multiple domains. Code is available at https://github.com/amilbhagat/Conditional-Consistency-Models.	 | 一致性模型已发展成为扩散模型的一种有前途的替代方案，通过单步采样生成提供高质量的生成能力。然而，它们在跨模态翻译和低光照图像增强等多域图像翻译任务中的应用仍然鲜有探索。本文中，我们通过引入附加条件输入，提出了条件一致性模型（CCMs），以应用于多域图像翻译任务。我们通过引入特定任务的条件输入来实施这些修改，这些输入指导去噪过程，确保生成的输出保留来自相应输入域的结构和上下文信息。我们在这项研究中在10个不同的数据集上评估了CCMs，展示了其在多个领域生成高质量翻译图像的有效性。代码可在<https://github.com/amilbhagat/Conditional-Consistency-Models> 获取。
2501.01212	 | Real-time Cross-modal Cybersickness Prediction in Virtual Reality	 | Yitong Zhu,Tangyao Li,Yuyang Wang	 | 该研究提出了一种轻量级模型，结合基于变压器的编码器、稀疏自注意力机制和PP-TSN网络，利用视觉和生物信号融合来实现沉浸式VR体验中的网络病预测，显著提高了预测精度并支持实时应用。	 | Cybersickness remains a significant barrier to the widespread adoption of immersive virtual reality (VR) experiences, as it can greatly disrupt user engagement and comfort. Research has shown that cybersickness can significantly be reflected in head and eye tracking data, along with other physiological data (e.g., TMP, EDA, and BMP). Despite the application of deep learning techniques such as CNNs and LSTMs, these models often struggle to capture the complex interactions between multiple data modalities and lack the capacity for real-time inference, limiting their practical application. Addressing this gap, we propose a lightweight model that leverages a transformer-based encoder with sparse self-attention to process bio-signal features and a PP-TSN network for video feature extraction. These features are then integrated via a cross-modal fusion module, creating a video-aware bio-signal representation that supports cybersickness prediction based on both visual and bio-signal inputs. Our model, trained with a lightweight framework, was validated on a public dataset containing eye and head tracking data, physiological data, and VR video, and demonstrated state-of-the-art performance in cybersickness prediction, achieving a high accuracy of 93.13\% using only VR video inputs. These findings suggest that our approach not only enables effective, real-time cybersickness prediction but also addresses the longstanding issue of modality interaction in VR environments. This advancement provides a foundation for future research on multimodal data integration in VR, potentially leading to more personalized, comfortable and widely accessible VR experiences.	 | 网络病仍然是沉浸式虚拟现实（VR）体验广泛采用的重要障碍，因为它会严重干扰用户的参与度和舒适度。研究表明，网络病可以通过头部和眼动追踪数据以及其他生理数据（如TMP、EDA和BMP）显著反映出来。尽管应用了深度学习技术，如CNN和LSTM，这些模型往往难以捕捉多种数据模态之间的复杂交互，并且缺乏实时推理的能力，限制了它们的实际应用。为了解决这一问题，我们提出了一种轻量级模型，该模型利用基于变压器的编码器和稀疏自注意力机制处理生物信号特征，并利用PP-TSN网络进行视频特征提取。这些特征通过跨模态融合模块进行整合，创建了一种基于视觉和生物信号输入的视频感知生物信号表示，支持网络病预测。我们的模型使用轻量级框架训练，并在包含眼动和头部追踪数据、生理数据以及VR视频的公开数据集上进行了验证，仅使用VR视频输入实现了高达93.13%的高精度。这些发现表明，我们的方法不仅能够实现有效的实时网络病预测，还解决了VR环境中模态交互的长期问题。这一进步为未来VR多模态数据集成研究奠定了基础，可能有助于提供更加个性化、舒适且易于访问的VR体验。
2501.01197	 | LayeringDiff: Layered Image Synthesis via Generation, then Disassembly with Generative Knowledge	 | Kyoungkook Kang,Gyujin Sim,Geonung Kim,Donguk Kim,Seungho Nam,Sunghyun Cho	 | 本文提出了一种名为LayeringDiff的新分层图像合成管道，它利用现成的图像生成模型生成合成图像并分解为前景和背景分层，从而避免大规模训练，并能够生成多样化的内容和对象尺度。	 | Layers have become indispensable tools for professional artists, allowing them to build a hierarchical structure that enables independent control over individual visual elements. In this paper, we propose LayeringDiff, a novel pipeline for the synthesis of layered images, which begins by generating a composite image using an off-the-shelf image generative model, followed by disassembling the image into its constituent foreground and background layers. By extracting layers from a composite image, rather than generating them from scratch, LayeringDiff bypasses the need for large-scale training to develop generative capabilities for individual layers. Furthermore, by utilizing a pretrained off-the-shelf generative model, our method can produce diverse contents and object scales in synthesized layers. For effective layer decomposition, we adapt a large-scale pretrained generative prior to estimate foreground and background layers. We also propose high-frequency alignment modules to refine the fine-details of the estimated layers. Our comprehensive experiments demonstrate that our approach effectively synthesizes layered images and supports various practical applications.	 | 分层已成为专业艺术家不可或缺的工具，它们允许艺术家构建层次结构，从而独立控制个体视觉元素。在本文中，我们提出了一种名为 LayeringDiff 的新型分层图像合成管道，该管道首先使用现成的图像生成模型生成合成图像，然后将其分解为前景和背景分层。通过从合成图像中提取分层而不是从头生成它们，LayeringDiff 跳过了大规模训练以开发单个分层的生成能力的需要。此外，通过利用现成的预训练生成模型，我们的方法能够在合成分层中产生多样化的内容和对象尺度。为了有效进行分层分解，我们调整了一个大规模预训练生成先验以估算前景和背景分层。我们还提出了高频对齐模块来细化估算分层的细部。我们的全面实验表明，我们的方法有效地合成分层图像并支持多种实际应用。
2501.01196	 | Sparis: Neural Implicit Surface Reconstruction of Indoor Scenes from Sparse Views	 | Yulun Wu,Han Huang,Wenyuan Zhang,Chao Deng,Ge Gao,Ming Gu,Yu-Shen Liu	 | Sparis是一种用于从稀疏视角重建室内表面的新方法，通过引入基于图像间匹配信息的先验和改进的图像间先验策略，提供更准确的深度信息并提高重建准确性。实验结果表明，Sparis在稀疏视角场景重建中表现优越。	 | In recent years, reconstructing indoor scene geometry from multi-view images has achieved encouraging accomplishments. Current methods incorporate monocular priors into neural implicit surface models to achieve high-quality reconstructions. However, these methods require hundreds of images for scene reconstruction. When only a limited number of views are available as input, the performance of monocular priors deteriorates due to scale ambiguity, leading to the collapse of the reconstructed scene geometry. In this paper, we propose a new method, named Sparis, for indoor surface reconstruction from sparse views. Specifically, we investigate the impact of monocular priors on sparse scene reconstruction, introducing a novel prior based on inter-image matching information. Our prior offers more accurate depth information while ensuring cross-view matching consistency. Additionally, we employ an angular filter strategy and an epipolar matching weight function, aiming to reduce errors due to view matching inaccuracies, thereby refining the inter-image prior for improved reconstruction accuracy. The experiments conducted on widely used benchmarks demonstrate superior performance in sparse-view scene reconstruction.	 | 近年来，从多视角图像中重建室内场景几何结构已经取得了令人鼓舞的成果。当前的方法将单目先验信息整合到神经隐式曲面模型中，以实现高质量的重建。然而，这些方法需要数百张图像来进行场景重建。当输入的视角有限时，单目先验信息由于尺度不确定性而性能下降，导致重建的场景几何结构崩溃。本文中，我们提出了一种新的方法，名为Sparis，用于从稀疏视角重建室内表面。具体来说，我们研究了单目先验在稀疏场景重建中的影响，并引入了一种基于图像间匹配信息的新先验。该先验提供更准确的深度信息，同时保证跨视角匹配一致性。此外，我们采用了角度滤波策略和极线匹配权重函数，旨在减少由于视角匹配不准确性引起的误差，从而改进图像间先验以提高重建准确性。在广泛使用的基准测试上进行的实验表明，Sparis在稀疏视角场景重建中表现出优越性能。
2501.01184	 | Vulnerability-Aware Spatio-Temporal Learning for Generalizable and Interpretable Deepfake Video Detection	 | Dat Nguyen,Marcella Astrid,Anis Kacem,Enjie Ghorbel,Djamila Aouada	 | 该研究提出了FakeSTormer，一种多任务学习框架，通过引入空间和时间分支专注于细微的伪影，并使用视频级数据合成算法生成带有细微伪影的假视频，从而提高检测深度假视频的性能和可解释性。实验结果表明，该方法在多个基准测试中表现优于现有先进方法。	 | Detecting deepfake videos is highly challenging due to the complex intertwined spatial and temporal artifacts in forged sequences. Most recent approaches rely on binary classifiers trained on both real and fake data. However, such methods may struggle to focus on important artifacts, which can hinder their generalization capability. Additionally, these models often lack interpretability, making it difficult to understand how predictions are made. To address these issues, we propose FakeSTormer, offering two key contributions. First, we introduce a multi-task learning framework with additional spatial and temporal branches that enable the model to focus on subtle spatio-temporal artifacts. These branches also provide interpretability by highlighting video regions that may contain artifacts. Second, we propose a video-level data synthesis algorithm that generates pseudo-fake videos with subtle artifacts, providing the model with high-quality samples and ground truth data for our spatial and temporal branches. Extensive experiments on several challenging benchmarks demonstrate the competitiveness of our approach compared to recent state-of-the-art methods. The code is available at https://github.com/10Ring/FakeSTormer.	 | 检测深度假视频极具挑战性，因为伪造序列中的空间和时间伪影复杂交织。目前大多数方法依赖于在真实和假视频数据上训练的二元分类器。然而，这些方法可能难以专注于重要的伪影，这会妨碍它们的泛化能力。此外，这些模型通常缺乏可解释性，使得理解预测过程变得困难。为了解决这些问题，我们提出了FakeSTormer，并提供了两个关键贡献。首先，我们引入了一个多任务学习框架，带有额外的空间和时间分支，使模型能够专注于细微的空间-时间伪影。这些分支还通过突出显示可能包含伪影的视频区域提供了可解释性。其次，我们提出了一种视频级数据合成算法，生成带有细微伪影的伪假视频，为我们的空间和时间分支提供高质量样本和真实数据。在几个具有挑战性的基准上的广泛实验表明，我们的方法在与最近的先进方法相比时具有竞争力。代码可在https://github.com/10Ring/FakeSTormer 获取。
2501.01174	 | L3D-Pose: Lifting Pose for 3D Avatars from a Single Camera in the Wild	 | Soumyaratna Debnath,Harish Katti,Shashikant Verma,Shanmuganathan Raman	 | 该研究提出了一种混合方法，利用装备化的虚拟角色生成合成数据集，以解决大规模3D姿态数据集创建的挑战，并设计了一种基于注意力机制的MLP网络将2D姿态转换为3D，同时提出了一种查找表方法来准确重新定位姿态到任意虚拟角色，从而实现野生环境动作的3D姿态估计。	 | While 2D pose estimation has advanced our ability to interpret body movements in animals and primates, it is limited by the lack of depth information, constraining its application range. 3D pose estimation provides a more comprehensive solution by incorporating spatial depth, yet creating extensive 3D pose datasets for animals is challenging due to their dynamic and unpredictable behaviours in natural settings. To address this, we propose a hybrid approach that utilizes rigged avatars and the pipeline to generate synthetic datasets to acquire the necessary 3D annotations for training. Our method introduces a simple attention-based MLP network for converting 2D poses to 3D, designed to be independent of the input image to ensure scalability for poses in natural environments. Additionally, we identify that existing anatomical keypoint detectors are insufficient for accurate pose retargeting onto arbitrary avatars. To overcome this, we present a lookup table based on a deep pose estimation method using a synthetic collection of diverse actions rigged avatars perform. Our experiments demonstrate the effectiveness and efficiency of this lookup table-based retargeting approach. Overall, we propose a comprehensive framework with systematically synthesized datasets for lifting poses from 2D to 3D and then utilize this to re-target motion from wild settings onto arbitrary avatars.	 | 尽管2D姿态估计提高了我们对动物和灵长类动物身体动作解释的能力，但它受限于缺乏深度信息，限制了其应用范围。3D姿态估计通过结合空间深度提供了更全面的解决方案，但由于动物在自然环境中的动态且不可预测的行为，创建大规模的3D姿态数据集极具挑战性。为了解决这一问题，我们提出了一种混合方法，利用装备化的虚拟角色和工作流程生成合成数据集，以获得必要的3D注释用于训练。该方法引入了一种基于注意力机制的MLP网络，用于将2D姿态转换为3D，设计为与输入图像无关，以确保在自然环境中的可扩展性。此外，我们发现现有的解剖学关键点检测器不足以准确地将姿态重新定位到任意虚拟角色上。为克服这一问题，我们基于一种深姿态估计方法，使用一系列不同动作的装备化虚拟角色的合成集合，提出了一种查找表。我们的实验表明，基于查找表的重新定位方法的有效性和效率。总体而言，我们提出了一种综合框架，其中系统性地合成数据集用于从2D提升到3D的姿态，并利用此框架将野生环境中的动作重新定位到任意虚拟角色上。
2501.01166	 | Deep Learning in Palmprint Recognition-A Comprehensive Survey	 | Chengrui Gao,Ziyuan Yang,Wei Jia,Lu Leng,Bob Zhang,Andrew Beng Jin Teoh	 | 本文全面回顾了基于深度学习的掌纹识别技术的最新进展，涵盖了区域分割、特征提取及安全/隐私问题的关键任务，并指出了当前挑战和未来研究机会，为研究人员提供了宝贵的资源。	 | Palmprint recognition has emerged as a prominent biometric technology, widely applied in diverse scenarios. Traditional handcrafted methods for palmprint recognition often fall short in representation capability, as they heavily depend on researchers' prior knowledge. Deep learning (DL) has been introduced to address this limitation, leveraging its remarkable successes across various domains. While existing surveys focus narrowly on specific tasks within palmprint recognition-often grounded in traditional methodologies-there remains a significant gap in comprehensive research exploring DL-based approaches across all facets of palmprint recognition. This paper bridges that gap by thoroughly reviewing recent advancements in DL-powered palmprint recognition. The paper systematically examines progress across key tasks, including region-of-interest segmentation, feature extraction, and security/privacy-oriented challenges. Beyond highlighting these advancements, the paper identifies current challenges and uncovers promising opportunities for future research. By consolidating state-of-the-art progress, this review serves as a valuable resource for researchers, enabling them to stay abreast of cutting-edge technologies and drive innovation in palmprint recognition.	 | 掌纹识别已经成为一种突出的生物识别技术，在多种场景中得到了广泛应用。传统的掌纹识别方法往往在表示能力上存在不足，因为它们高度依赖研究人员的先验知识。深度学习(DL)被引入以解决这一局限性，因为它在各个领域都取得了显著的成功。尽管现有的综述主要集中于掌纹识别中的特定任务，并往往基于传统方法，但在DL在掌纹识别所有方面的综合研究方面仍存在明显的空白。本文通过全面回顾基于DL的掌纹识别的最新进展来填补这一空白。本文系统地探讨了关键任务的进步，包括区域分割、特征提取以及安全/隐私方面的问题。除了强调这些进展之外，本文还指出了当前的挑战，并揭示了未来研究中充满希望的机会。通过集中展示最新的进展，本文为研究人员提供了一个宝贵的资源，使他们能够了解最新的技术并推动掌纹识别领域的创新。
2501.01164	 | Towards Interactive Deepfake Analysis	 | Lixiong Qin,Ning Jiang,Yang Zhang,Yuhan Qiu,Dingheng Zeng,Jiani Hu,Weihong Deng	 | 本文通过指令调优多模态大型语言模型，提出了DFA-Instruct数据集、DFA-Bench基准测试及DFA-GPT系统，旨在解决深度伪造分析中的数据和训练效率问题，并推动相关研究的进展。	 | Existing deepfake analysis methods are primarily based on discriminative models, which significantly limit their application scenarios. This paper aims to explore interactive deepfake analysis by performing instruction tuning on multi-modal large language models (MLLMs). This will face challenges such as the lack of datasets and benchmarks, and low training efficiency. To address these issues, we introduce (1) a GPT-assisted data construction process resulting in an instruction-following dataset called DFA-Instruct, (2) a benchmark named DFA-Bench, designed to comprehensively evaluate the capabilities of MLLMs in deepfake detection, deepfake classification, and artifact description, and (3) construct an interactive deepfake analysis system called DFA-GPT, as a strong baseline for the community, with the Low-Rank Adaptation (LoRA) module. The dataset and code will be made available at https://github.com/lxq1000/DFA-Instruct to facilitate further research.	 | 现有深度伪造分析方法主要基于判别模型，这极大地限制了其应用场景。本文旨在通过在多模态大型语言模型（MLLMs）上进行指令调优来探索互动式的深度伪造分析。这将面临数据集和基准数据缺乏、训练效率低等挑战。为了解决这些问题，我们提出了以下几点： 1. 一种由GPT辅助的数据构建过程，生成了一个名为DFA-Instruct的指令遵循数据集。 2. 一个名为DFA-Bench的基准测试，旨在全面评估MLLMs在深度伪造检测、分类及特征描述方面的能力。 3. 构建了一个名为DFA-GPT的互动深度伪造分析系统，作为社区中的强基线，并使用了Low-Rank Adaptation（LoRA）模块。该系统和相关数据集及代码将发布在https://github.com/lxq1000/DFA-Instruct，以促进进一步的研究。
2501.01163	 | 3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer	 | Jiajun Deng,Tianyu He,Li Jiang,Tianyu Wang,Feras Dayoub,Ian Reid	 | 3D-LLaVA是一种新型的简单而强大的3D大型多模态模型，旨在通过一个简洁的架构和集成的点云输入设计，提升基于3D视觉的对话和推理能力，其核心是Omni Superpoint Transformer，能够处理视觉特征选择、视觉提示编码和引用掩码解码等任务。	 | Current 3D Large Multimodal Models (3D LMMs) have shown tremendous potential in 3D-vision-based dialogue and reasoning. However, how to further enhance 3D LMMs to achieve fine-grained scene understanding and facilitate flexible human-agent interaction remains a challenging problem. In this work, we introduce 3D-LLaVA, a simple yet highly powerful 3D LMM designed to act as an intelligent assistant in comprehending, reasoning, and interacting with the 3D world. Unlike existing top-performing methods that rely on complicated pipelines-such as offline multi-view feature extraction or additional task-specific heads-3D-LLaVA adopts a minimalist design with integrated architecture and only takes point clouds as input. At the core of 3D-LLaVA is a new Omni Superpoint Transformer (OST), which integrates three functionalities: (1) a visual feature selector that converts and selects visual tokens, (2) a visual prompt encoder that embeds interactive visual prompts into the visual token space, and (3) a referring mask decoder that produces 3D masks based on text description. This versatile OST is empowered by the hybrid pretraining to obtain perception priors and leveraged as the visual connector that bridges the 3D data to the LLM. After performing unified instruction tuning, our 3D-LLaVA reports impressive results on various benchmarks. The code and model will be released to promote future exploration.	 | 当前的3D大型多模态模型（3D LMM）在基于3D视觉的对话和推理方面展示了巨大的潜力。然而，如何进一步提升3D LMM以实现精细的场景理解，并促进灵活的人机交互，仍然是一个具有挑战性的问题。在这项工作中，我们引入了3D-LLaVA，这是一种简单但极具强大功能的3D LMM，旨在作为智能助手来理解和与3D世界互动。与其他现有的顶级方法依赖复杂的工作流程（如多视图特征的离线提取或附加的任务特定头部）不同，3D-LLaVA采用了简洁的设计并具有集成的架构，仅接受点云作为输入。  3D-LLaVA的核心是一个新的全视点超点变换器（Omni Superpoint Transformer，OST），它整合了三种功能：（1）一个视觉特征选择器，用于转换和选择视觉标记；（2）一个视觉提示编码器，将互动视觉提示嵌入到视觉标记空间中；（3）一个引用掩码解码器，基于文本描述生成3D掩码。这种多功能的OST通过混合预训练获取感知先验，并作为视觉连接器，将3D数据与LLM连接起来。经过统一指令调优后，我们的3D-LLaVA在各种基准测试中取得了令人印象深刻的结果。代码和模型将被发布，以促进未来的探索。
2501.01156	 | TexAVi: Generating Stereoscopic VR Video Clips from Text Descriptions	 | Vriksha Srihari,R. Bhavya,Shruti Jayaraman,V. Mary Anita Rajam	 | 本文提出了一种结合文本到图像模型、Stable Diffusion和深度估计算法的方法，以生成高质量的立体虚拟现实视频，从而降低虚拟现实内容制作的复杂性和成本。通过使用Fréchet Inception Distance和CLIP Score等定量指标评估生成视频的质量，该方法展示了自然语言驱动图形在虚拟现实模拟中的潜力。	 | While generative models such as text-to-image, large language models and text-to-video have seen significant progress, the extension to text-to-virtual-reality remains largely unexplored, due to a deficit in training data and the complexity of achieving realistic depth and motion in virtual environments. This paper proposes an approach to coalesce existing generative systems to form a stereoscopic virtual reality video from text.   Carried out in three main stages, we start with a base text-to-image model that captures context from an input text. We then employ Stable Diffusion on the rudimentary image produced, to generate frames with enhanced realism and overall quality. These frames are processed with depth estimation algorithms to create left-eye and right-eye views, which are stitched side-by-side to create an immersive viewing experience. Such systems would be highly beneficial in virtual reality production, since filming and scene building often require extensive hours of work and post-production effort.   We utilize image evaluation techniques, specifically Fréchet Inception Distance and CLIP Score, to assess the visual quality of frames produced for the video. These quantitative measures establish the proficiency of the proposed method.   Our work highlights the exciting possibilities of using natural language-driven graphics in fields like virtual reality simulations.	 | 尽管生成模型如文本到图像、大型语言模型和文本到视频已经取得了显著的进展，将这些模型扩展到文本到虚拟现实领域仍然很大程度上未被探索，这主要是由于缺乏训练数据以及在虚拟环境中实现逼真深度和运动的复杂性。本文提出了一种方法，将现有生成系统相结合，从而从文本生成立体虚拟现实视频。该方法分为三个主要阶段：首先，我们使用一个基础的文本到图像模型来捕捉输入文本中的上下文信息。然后，我们使用Stable Diffusion对初步生成的图像进行处理，生成具有增强的真实感和整体质量的帧。接着，这些帧通过深度估计算法生成左眼和右眼视图，并将它们并排放置以创建沉浸式观看体验。这种系统在虚拟现实制作中将非常有益，因为拍摄和场景构建通常需要大量的时间和后期制作努力。   我们利用图像评价技术，特别是Fréchet Inception Distance和CLIP Score，来评估视频中生成的帧的视觉质量。这些定量指标确立了所提方法的熟练程度。   我们的工作突显了使用自然语言驱动图形在虚拟现实模拟等领域中的令人兴奋的可能性。
2501.01142	 | Adaptive Hardness-driven Augmentation and Alignment Strategies for Multi-Source Domain Adaptations	 | Yang Yuxiang,Zeng Xinyi,Zeng Pinxian,Zu Chen,Yan Binyu,Zhou Jiliu,Wang Yan	 | A3MDA是一种新的多源领域适应策略，通过引入自适应难度量化和利用三种不同的度量（基础、平滑和比较度量）来实现数据增强和领域间对齐，相比现有方法表现出更优的性能。	 | Multi-source Domain Adaptation (MDA) aims to transfer knowledge from multiple labeled source domains to an unlabeled target domain. Nevertheless, traditional methods primarily focus on achieving inter-domain alignment through sample-level constraints, such as Maximum Mean Discrepancy (MMD), neglecting three pivotal aspects: 1) the potential of data augmentation, 2) the significance of intra-domain alignment, and 3) the design of cluster-level constraints. In this paper, we introduce a novel hardness-driven strategy for MDA tasks, named "A3MDA" , which collectively considers these three aspects through Adaptive hardness quantification and utilization in both data Augmentation and domain Alignment.To achieve this, "A3MDA" progressively proposes three Adaptive Hardness Measurements (AHM), i.e., Basic, Smooth, and Comparative AHMs, each incorporating distinct mechanisms for diverse scenarios. Specifically, Basic AHM aims to gauge the instantaneous hardness for each source/target sample. Then, hardness values measured by Smooth AHM will adaptively adjust the intensity level of strong data augmentation to maintain compatibility with the model's generalization capacity.In contrast, Comparative AHM is designed to facilitate cluster-level constraints. By leveraging hardness values as sample-specific weights, the traditional MMD is enhanced into a weighted-clustered variant, strengthening the robustness and precision of inter-domain alignment. As for the often-neglected intra-domain alignment, we adaptively construct a pseudo-contrastive matrix by selecting harder samples based on the hardness rankings, enhancing the quality of pseudo-labels, and shaping a well-clustered target feature space. Experiments on multiple MDA benchmarks show that " A3MDA " outperforms other methods.	 | 多源领域适应（MDA）旨在将来自多个标记源域的知识转移至未标记的目标域。然而，传统的方法主要集中在通过样本级约束实现领域间对齐，例如最大均值差异（MMD），而忽略了三个关键方面：1）数据增强的潜力，2）领域内对齐的重要性，以及3）集群级约束的设计。本文介绍了一种新的基于难度驱动的MDA策略，名为“A3MDA”，它通过自适应难度量化和利用，同时考虑这三个方面，实现数据增强和领域对齐。为了实现这一点，“A3MDA”逐步提出了三种自适应难度度量（AHM），即基础、平滑和比较度量，每种度量都针对不同的场景引入了不同的机制。具体来说，基础度量旨在衡量每个源/目标样本的即时难度，然后，通过平滑度量测量得到的难度值会自适应地调整强数据增强的强度，以保持与模型泛化能力的一致性。相比之下，比较度量旨在促进集群级约束。通过利用难度值作为样本特定的权重，传统的MMD被增强为加权集群版本，从而加强了领域间对齐的鲁棒性和精度。至于常被忽视的领域内对齐，我们通过选择难度排名中更难的样本来自适应构建伪对比矩阵，增强伪标签的质量，并塑造一个良好的目标特征空间。在多个MDA基准上的实验表明，“A3MDA”比其他方法表现更好。
2501.01127	 | InDeed: Interpretable image deep decomposition with guaranteed generalizability	 | Sihan Wang,Shangqi Gao,Fuping Wu,Xiahai Zhuang	 | 本文提出了一种结合分层贝叶斯建模和深度学习的可解释深度图像分解框架，通过三个步骤实现图像的基本成分分解，并通过理论分析建立了损失函数与泛化误差界的联系，提升了方法在分布外场景下的适应性。	 | Image decomposition aims to analyze an image into elementary components, which is essential for numerous downstream tasks and also by nature provides certain interpretability to the analysis. Deep learning can be powerful for such tasks, but surprisingly their combination with a focus on interpretability and generalizability is rarely explored. In this work, we introduce a novel framework for interpretable deep image decomposition, combining hierarchical Bayesian modeling and deep learning to create an architecture-modularized and model-generalizable deep neural network (DNN). The proposed framework includes three steps: (1) hierarchical Bayesian modeling of image decomposition, (2) transforming the inference problem into optimization tasks, and (3) deep inference via a modularized Bayesian DNN. We further establish a theoretical connection between the loss function and the generalization error bound, which inspires a new test-time adaptation approach for out-of-distribution scenarios. We instantiated the application using two downstream tasks, \textit{i.e.}, image denoising, and unsupervised anomaly detection, and the results demonstrated improved generalizability as well as interpretability of our methods. The source code will be released upon the acceptance of this paper.	 | 图像分解旨在将图像分解为基本成分，这对于许多下游任务至关重要，同时也天然提供了分析的可解释性。深度学习在这种任务中可以非常强大，但令人惊讶的是，将深度学习与重点关注可解释性和泛化性相结合的探索却很少。在本文中，我们引入了一种新颖的可解释深度图像分解框架，结合了分层贝叶斯建模和深度学习，创建了一个架构模块化和模型泛化性的深度神经网络（DNN）。所提出的框架包括三个步骤：（1）分层贝叶斯建模下的图像分解，（2）将推断问题转化为优化任务，以及（3）通过模块化的贝叶斯DNN进行深度推断。我们进一步建立了损失函数与泛化误差界的理论联系，这启发了一种新的测试时适应方法，用于处理分布外场景。我们使用两个下游任务实例化了该应用，即图像去噪和无监督异常检测，结果表明了我们方法的泛化性和可解释性的提升。该论文被接受后，我们将发布源代码。
2501.01126	 | Source-free Semantic Regularization Learning for Semi-supervised Domain Adaptation	 | Xinyang Huang,Chuang Zhu,Ruiying Ren,Shengjie Liu,Tiejun Huang	 | 本文提出了一种新颖的无监督领域适应框架——语义正则化学习（SERL），通过三种鲁棒的语义正则化技术（SPCR、HMR和TPR）来适应目标域，从多角度捕捉目标语义信息，从而提高模型的分类性能和泛化能力。实验结果表明，SERL在三个基准数据集上显著优于现有方法。	 | Semi-supervised domain adaptation (SSDA) has been extensively researched due to its ability to improve classification performance and generalization ability of models by using a small amount of labeled data on the target domain. However, existing methods cannot effectively adapt to the target domain due to difficulty in fully learning rich and complex target semantic information and relationships. In this paper, we propose a novel SSDA learning framework called semantic regularization learning (SERL), which captures the target semantic information from multiple perspectives of regularization learning to achieve adaptive fine-tuning of the source pre-trained model on the target domain. SERL includes three robust semantic regularization techniques. Firstly, semantic probability contrastive regularization (SPCR) helps the model learn more discriminative feature representations from a probabilistic perspective, using semantic information on the target domain to understand the similarities and differences between samples. Additionally, adaptive weights in SPCR can help the model learn the semantic distribution correctly through the probabilities of different samples. To further comprehensively understand the target semantic distribution, we introduce hard-sample mixup regularization (HMR), which uses easy samples as guidance to mine the latent target knowledge contained in hard samples, thereby learning more complete and complex target semantic knowledge. Finally, target prediction regularization (TPR) regularizes the target predictions of the model by maximizing the correlation between the current prediction and the past learned objective, thereby mitigating the misleading of semantic information caused by erroneous pseudo-labels. Extensive experiments on three benchmark datasets demonstrate that our SERL method achieves state-of-the-art performance.	 | 无监督领域适应（SSDA）因其通过目标域少量标记数据提高模型分类性能和泛化能力而得到了广泛研究。然而，现有的方法由于难以全面学习丰富的目标语义信息及其关系，而在适应目标域方面效果不佳。本文提出了一种新颖的SSDA学习框架，称为语义正则化学习（SERL），该框架从正则化学习的多角度捕捉目标语义信息，实现对源预训练模型在目标域上的适应性微调。SERL 包括三种鲁棒的语义正则化技术。首先，语义概率对比正则化（SPCR）从概率视角帮助模型学习更具判别力的特征表示，利用目标域的语义信息来理解样本间的相似性和差异性。此外，SPCR中的自适应权重可以帮助模型通过不同样本的概率来正确学习语义分布。为了更全面地理解目标语义分布，我们引入了困难样本混叠正则化（HMR），利用容易样本作为指导，挖掘包含在困难样本中的潜在目标知识，从而学习更完整和复杂的语义知识。最后，目标预测正则化（TPR）通过最大化当前预测与以往学习目标之间的相关性，来正则化模型的目标预测，从而减轻由于错误伪标签导致的语义信息误导。在三个基准数据集上的广泛实验表明，我们的SERL方法达到了最先进的性能。
2501.01125	 | DuMo: Dual Encoder Modulation Network for Precise Concept Erasure	 | Feng Han,Kai Chen,Chao Gong,Zhipeng Wei,Jingjing Chen,Yu-Gang Jiang	 | 该研究提出了DuMo网络，通过Eraser with Prior Knowledge (EPR)模块精确去除不宜工作（NSFW）内容并保留图像结构，同时采用Time-Layer MOdulation (TLMO)过程自动平衡擦除效果和生成能力，从而显著提高了内容消除性能。	 | The exceptional generative capability of text-to-image models has raised substantial safety concerns regarding the generation of Not-Safe-For-Work (NSFW) content and potential copyright infringement. To address these concerns, previous methods safeguard the models by eliminating inappropriate concepts. Nonetheless, these models alter the parameters of the backbone network and exert considerable influences on the structural (low-frequency) components of the image, which undermines the model's ability to retain non-target concepts. In this work, we propose our Dual encoder Modulation network (DuMo), which achieves precise erasure of inappropriate target concepts with minimum impairment to non-target concepts. In contrast to previous methods, DuMo employs the Eraser with PRior Knowledge (EPR) module which modifies the skip connection features of the U-NET and primarily achieves concept erasure on details (high-frequency) components of the image. To minimize the damage to non-target concepts during erasure, the parameters of the backbone U-NET are frozen and the prior knowledge from the original skip connection features is introduced to the erasure process. Meanwhile, the phenomenon is observed that distinct erasing preferences for the image structure and details are demonstrated by the EPR at different timesteps and layers. Therefore, we adopt a novel Time-Layer MOdulation process (TLMO) that adjusts the erasure scale of EPR module's outputs across different layers and timesteps, automatically balancing the erasure effects and model's generative ability. Our method achieves state-of-the-art performance on Explicit Content Erasure, Cartoon Concept Removal and Artistic Style Erasure, clearly outperforming alternative methods. Code is available at https://github.com/Maplebb/DuMo	 | 文本到图像模型的卓越生成能力引发了一系列安全问题，特别是关于不宜工作（NSFW）内容的生成和潜在的版权侵权。为了应对这些担忧，先前的方法通过消除不适当的概念来保护模型。然而，这些方法会对骨干网络的参数产生影响，并且对图像的结构（低频）成分造成重大影响，从而削弱了模型保留非目标概念的能力。在此项工作中，我们提出了 Dual encoder Modulation 网络（DuMo），该网络能够以最小的非目标概念损害精确消除不适当的目标概念。与先前的方法不同，DuMo 使用 Eraser with PRior Knowledge (EPR) 模块，该模块修改 U-NET 的跳跃连接特征，并主要在图像的细节（高频）成分上实现概念的消除。为了在消除过程中最小化对非目标概念的损害，骨干 U-NET 的参数被冻结，并且原始跳跃连接特征的先验知识被引入到消除过程中。同时，观察到 EPR 在不同时间步长和层面上对图像结构和细节表现出不同的擦除偏好。因此，我们采用了新颖的 Time-Layer MOdulation 过程（TLMO），该过程在不同层和时间步长上调整 EPR 模块输出的擦除规模，自动平衡擦除效果和模型的生成能力。我们的方法在明确内容消除、卡通概念去除和艺术风格消除方面达到了最先进的性能，显著优于其他方法。代码可在 https://github.com/Maplebb/DuMo 获取。
2501.01121	 | PatchRefiner V2: Fast and Lightweight Real-Domain High-Resolution Metric Depth Estimation	 | Zhenyu Li,Wenqing Cui,Shariq Farooq Bhat,Peter Wonka	 | PatchRefiner V2 (PRV2) 提出了一种轻量级编码器替代重型精修模型的方法，并结合粗到细模块和噪声预训练策略来减少噪声，同时引入了尺度和偏移不变梯度匹配损失以提高合成到真实域的转换效果，在多个数据集上实现了更高的准确性和更快速度。	 | While current high-resolution depth estimation methods achieve strong results, they often suffer from computational inefficiencies due to reliance on heavyweight models and multiple inference steps, increasing inference time. To address this, we introduce PatchRefiner V2 (PRV2), which replaces heavy refiner models with lightweight encoders. This reduces model size and inference time but introduces noisy features. To overcome this, we propose a Coarse-to-Fine (C2F) module with a Guided Denoising Unit for refining and denoising the refiner features and a Noisy Pretraining strategy to pretrain the refiner branch to fully exploit the potential of the lightweight refiner branch. Additionally, we introduce a Scale-and-Shift Invariant Gradient Matching (SSIGM) loss to enhance synthetic-to-real domain transfer. PRV2 outperforms state-of-the-art depth estimation methods on UnrealStereo4K in both accuracy and speed, using fewer parameters and faster inference. It also shows improved depth boundary delineation on real-world datasets like CityScape, ScanNet++, and KITTI, demonstrating its versatility across domains.	 | 尽管当前的高分辨率深度估计方法取得了很好的结果，但它们常常因为依赖于重型模型和多个推理步骤而导致计算效率低下，增加推理时间。为了解决这个问题，我们引入了PatchRefiner V2 (PRV2)，它用轻量级编码器取代了重型精修模型。这减少了模型大小和推理时间，但引入了噪声特征。为克服这一问题，我们提出了一种粗到细（C2F）模块，其中包含一个引导去噪单元，用于精细和去噪精修特征，并提出了一种噪声预训练策略，以充分利用轻量级精修分支的潜力。此外，我们引入了一种尺度和偏移不变梯度匹配（SSIGM）损失，以增强合成到真实域的转移。在UnrealStereo4K数据集上，PRV2在准确性和速度上都优于最先进的深度估计方法，使用更少的参数和更快的推理时间。此外，它还在CityScape、ScanNet++和KITTI等真实世界数据集上表现出改进的深度边界描绘，显示了其在不同领域的适用性。
2501.01120	 | Retrieval-Augmented Dynamic Prompt Tuning for Incomplete Multimodal Learning	 | Jian Lang,Zhangtao Cheng,Ting Zhong,Fan Zhou	 | RAGPT 是一种新颖的检索增强动态提示调优框架，通过多通道检索器、缺失模态生成器和上下文感知提示器三个模块，有效解决预训练多模态变压器在处理不完整模态时的鲁棒性问题，显著优于现有基准方法。	 | Multimodal learning with incomplete modality is practical and challenging. Recently, researchers have focused on enhancing the robustness of pre-trained MultiModal Transformers (MMTs) under missing modality conditions by applying learnable prompts. However, these prompt-based methods face several limitations: (1) incomplete modalities provide restricted modal cues for task-specific inference, (2) dummy imputation for missing content causes information loss and introduces noise, and (3) static prompts are instance-agnostic, offering limited knowledge for instances with various missing conditions. To address these issues, we propose RAGPT, a novel Retrieval-AuGmented dynamic Prompt Tuning framework. RAGPT comprises three modules: (I) the multi-channel retriever, which identifies similar instances through a within-modality retrieval strategy, (II) the missing modality generator, which recovers missing information using retrieved contexts, and (III) the context-aware prompter, which captures contextual knowledge from relevant instances and generates dynamic prompts to largely enhance the MMT's robustness. Extensive experiments conducted on three real-world datasets show that RAGPT consistently outperforms all competitive baselines in handling incomplete modality problems. The code of our work and prompt-based baselines is available at https://github.com/Jian-Lang/RAGPT.	 | 多模态学习在模态不完整的情况下既实用又具有挑战性。最近，研究人员集中精力通过应用可学习提示来增强在缺失模态条件下的预训练多模态变压器（MMTs）的鲁棒性。然而，基于提示的方法面临几个局限性：（1）不完整的模态为特定任务的推理提供的模态线索受到限制，（2）对缺失内容进行的虚假填充会导致信息丢失并引入噪声，（3）静态提示是实例无关的，对于具有不同缺失条件的实例提供的知识有限。为了解决这些问题，我们提出了RAGPT，一种新颖的检索增强动态提示调优框架。RAGPT 包含三个模块：（I）多通道检索器，通过跨模态检索策略识别相似实例，（II）缺失模态生成器，使用检索到的上下文恢复缺失信息，以及（III）上下文感知提示器，从相关实例中捕获上下文知识并生成动态提示，从而极大地增强 MMT 的鲁棒性。在三个真实世界数据集上的大量实验表明，RAGPT 一致地在处理不完整模态问题方面优于所有竞争基线。我们的工作代码和基于提示的基线代码可以在 https://github.com/Jian-Lang/RAGPT 获取。
2501.01119	 | Leverage Cross-Attention for End-to-End Open-Vocabulary Panoptic Reconstruction	 | Xuan Yu,Yuxuan Xie,Yili Liu,Haojian Lu,Rong Xiong,Yiyi Liao,Yue Wang	 | 本文提出了一种名为PanopticRecon++的新端到端方法，通过引入新颖的跨注意力视角利用可学习的3D高斯分布来建模查询和键的关系，从而在整体重建中实现了3D和2D分割与重建性能的提升，并展示了其在机器人模拟器中的应用。	 | Open-vocabulary panoptic reconstruction offers comprehensive scene understanding, enabling advances in embodied robotics and photorealistic simulation. In this paper, we propose PanopticRecon++, an end-to-end method that formulates panoptic reconstruction through a novel cross-attention perspective. This perspective models the relationship between 3D instances (as queries) and the scene's 3D embedding field (as keys) through their attention map. Unlike existing methods that separate the optimization of queries and keys or overlook spatial proximity, PanopticRecon++ introduces learnable 3D Gaussians as instance queries. This formulation injects 3D spatial priors to preserve proximity while maintaining end-to-end optimizability. Moreover, this query formulation facilitates the alignment of 2D open-vocabulary instance IDs across frames by leveraging optimal linear assignment with instance masks rendered from the queries. Additionally, we ensure semantic-instance segmentation consistency by fusing query-based instance segmentation probabilities with semantic probabilities in a novel panoptic head supervised by a panoptic loss. During training, the number of instance query tokens dynamically adapts to match the number of objects. PanopticRecon++ shows competitive performance in terms of 3D and 2D segmentation and reconstruction performance on both simulation and real-world datasets, and demonstrates a user case as a robot simulator. Our project website is at: https://yuxuan1206.github.io/panopticrecon_pp/	 | 开放词汇的整体重建提供了全面的场景理解，能够促进实体机器人和逼真模拟的进步。在本文中，我们提出了一种名为PanopticRecon++的新颖端到端方法，通过新颖的跨注意力视角来表述整体重建。这一视角通过注意力图建模3D实例（作为查询）与场景的3D嵌入场（作为键）之间的关系。与现有方法将查询和键的优化分离，或者忽略空间邻近性不同，PanopticRecon++引入了可学习的3D高斯分布作为实例查询。这种表述通过注入3D空间先验来保持邻近性，同时保持端到端可优化性。此外，这种查询表述通过利用从查询渲染的实例掩码来实现帧间开放词汇实例ID的对齐，利用最优线性分配来实现对齐。此外，我们通过在新颖的整体头部中将基于查询的实例分割概率与语义概率融合，并由整体损失监督，来确保语义-实例分割的一致性。在训练过程中，实例查询令牌的数量动态适应匹配对象的数量。PanopticRecon++在模拟和真实世界数据集上的3D和2D分割与重建性能上表现出了竞争力，并展示了其作为机器人模拟器的应用案例。我们的项目网站为：https://yuxuan1206.github.io/panopticrecon_pp/
2501.01116	 | HarmonyIQA: Pioneering Benchmark and Model for Image Harmonization Quality Assessment	 | Zitong Xu,Huiyu Duan,Guangji Ma,Liu Yang,Jiarui Wang,Qingbo Wu,Xiongkuo Min,Guangtao Zhai,Patrick Le Callet	 | 该研究提出了一个名为HarmonyIQAD的新数据库和HarmonyIQA评估工具，旨在解决现有图像质量评估方法在图像协调方面的不足，并通过大量实验验证了其在评估协调图像人类视觉偏好方面的优越性能。	 | Image composition involves extracting a foreground object from one image and pasting it into another image through Image harmonization algorithms (IHAs), which aim to adjust the appearance of the foreground object to better match the background. Existing image quality assessment (IQA) methods may fail to align with human visual preference on image harmonization due to the insensitivity to minor color or light inconsistency. To address the issue and facilitate the advancement of IHAs, we introduce the first Image Quality Assessment Database for image Harmony evaluation (HarmonyIQAD), which consists of 1,350 harmonized images generated by 9 different IHAs, and the corresponding human visual preference scores. Based on this database, we propose a Harmony Image Quality Assessment (HarmonyIQA), to predict human visual preference for harmonized images. Extensive experiments show that HarmonyIQA achieves state-of-the-art performance on human visual preference evaluation for harmonized images, and also achieves competing results on traditional IQA tasks. Furthermore, cross-dataset evaluation also shows that HarmonyIQA exhibits better generalization ability than self-supervised learning-based IQA methods. Both HarmonyIQAD and HarmonyIQA will be made publicly available upon paper publication.	 | 图像构图涉及从一张图像中提取前景对象并通过图像协调算法（IHAs）将其粘贴到另一张图像中，旨在调整前景对象的外观以更好地匹配背景。现有的图像质量评估（IQA）方法可能无法在图像协调方面与人类视觉偏好保持一致，因为它们对细微的颜色或光线不一致不够敏感。为了解决这一问题并促进IHAs的发展，我们引入了首个用于图像协调评估的图像质量评估数据库（HarmonyIQAD），该数据库包含由9种不同IHAs生成的1,350张协调图像，以及相应的视觉偏好评分。基于此数据库，我们提出了一种协调图像质量评估（HarmonyIQA），用于预测协调图像的人类视觉偏好。广泛实验表明，HarmonyIQA在评估协调图像的人类视觉偏好方面达到了最先进的性能，并且在传统IQA任务上也取得了竞争力的结果。此外，跨数据集评估还显示，HarmonyIQA的泛化能力优于基于自我监督学习的IQA方法。在论文发表后，HarmonyIQAD和HarmonyIQA都将公开提供。
2501.01109	 | BatStyler: Advancing Multi-category Style Generation for Source-free Domain Generalization	 | Xiusheng Xu,Lei Qi,Jingyang Zhou,Xin Geng	 | 源域通用学习（SFDG）致力于开发无需依赖任何源域数据的模型，以在未见过的域上表现良好，但现有方法在多类别配置下性能不佳且风格合成效率较低。为此，本文提出了一种名为BatStyler的方法，通过引入粗粒度语义生成模块和均匀风格生成模块，有效地提高了多类别场景下的风格合成能力，并在多类别数据集上优于现有最先进的方法。	 | Source-Free Domain Generalization (SFDG) aims to develop a model that performs on unseen domains without relying on any source domains. However, the implementation remains constrained due to the unavailability of training data. Research on SFDG focus on knowledge transfer of multi-modal models and style synthesis based on joint space of multiple modalities, thus eliminating the dependency on source domain images. However, existing works primarily work for multi-domain and less-category configuration, but performance on multi-domain and multi-category configuration is relatively poor. In addition, the efficiency of style synthesis also deteriorates in multi-category scenarios. How to efficiently synthesize sufficiently diverse data and apply it to multi-category configuration is a direction with greater practical value. In this paper, we propose a method called BatStyler, which is utilized to improve the capability of style synthesis in multi-category scenarios. BatStyler consists of two modules: Coarse Semantic Generation and Uniform Style Generation modules. The Coarse Semantic Generation module extracts coarse-grained semantics to prevent the compression of space for style diversity learning in multi-category configuration, while the Uniform Style Generation module provides a template of styles that are uniformly distributed in space and implements parallel training. Extensive experiments demonstrate that our method exhibits comparable performance on less-category datasets, while surpassing state-of-the-art methods on multi-category datasets.	 | 源数据无域通用学习（Source-Free Domain Generalization, SFDG）旨在开发一个模型，能够在未见过的域上表现良好，而不需要依赖任何源域的数据。然而，由于缺乏训练数据，其实施仍然受到限制。SFDG 的研究主要集中在多模态模型的知识迁移和基于多种模态联合空间的风格合成，从而消除对源域图像的依赖。然而，现有工作主要适用于多域和类别较少的配置，但在多域和多类别配置上的性能相对较差。此外，在多类别场景中，风格合成的效率也有所降低。如何高效地合成足够多样化的数据，并将其应用于多类别配置，是一个具有更大实际价值的方向。在本文中，我们提出了一种名为 BatStyler 的方法，用于提高多类别场景下风格合成的能力。BatStyler 包含两个模块：粗粒度语义生成模块和均匀风格生成模块。粗粒度语义生成模块提取粗粒度语义，以防止在多类别配置中风格多样性学习的空间压缩，而均匀风格生成模块提供了一种在空间中均匀分布的风格模板，并实现了并行训练。广泛的经验表明，我们的方法在类别较少的数据集上表现出可比的性能，而在多类别数据集上则优于现有最先进的方法。
2501.01106	 | AIM: Additional Image Guided Generation of Transferable Adversarial Attacks	 | Teng Li,Xingjun Ma,Yu-Gang Jiang	 | 本文提出了一种新的方法，通过引入语境注入模块（SIM）来增强目标导向的迁移攻击的效果，该模块利用附加引导图像中的语义信息提高对抗样本的迁移性，并提出了新的损失函数以更有效地整合语境注入模块。实验结果证明了该方法在目标导向和无目标导向的攻击设置下的有效性。	 | Transferable adversarial examples highlight the vulnerability of deep neural networks (DNNs) to imperceptible perturbations across various real-world applications. While there have been notable advancements in untargeted transferable attacks, targeted transferable attacks remain a significant challenge. In this work, we focus on generative approaches for targeted transferable attacks. Current generative attacks focus on reducing overfitting to surrogate models and the source data domain, but they often overlook the importance of enhancing transferability through additional semantics. To address this issue, we introduce a novel plug-and-play module into the general generator architecture to enhance adversarial transferability. Specifically, we propose a \emph{Semantic Injection Module} (SIM) that utilizes the semantics contained in an additional guiding image to improve transferability. The guiding image provides a simple yet effective method to incorporate target semantics from the target class to create targeted and highly transferable attacks. Additionally, we propose new loss formulations that can integrate the semantic injection module more effectively for both targeted and untargeted attacks. We conduct comprehensive experiments under both targeted and untargeted attack settings to demonstrate the efficacy of our proposed approach.	 | 迁移的对抗样本突显了深度神经网络（DNNs）在各种现实应用中对无法察觉的扰动的脆弱性。尽管在无目标迁移攻击方面取得了显著进展，但目标导向的迁移攻击仍然是一项重大挑战。在本文中，我们专注于生成性方法来实现目标导向的迁移攻击。当前的生成性攻击主要集中在减少对替代模型和源数据域的过拟合上，但往往忽略了通过额外的语义增强迁移性的必要性。为了解决这一问题，我们引入了一个新的即插即用模块到通用生成器架构中，以增强对抗样本的迁移性。具体来说，我们提出了一种名为**语境注入模块**（SIM）的新模块，该模块利用附加引导图像中的语义信息来提高迁移性。引导图像提供了一种简单有效的手段，将目标类别的目标语义整合到攻击中，从而创建出既目标导向又高度迁移的攻击。此外，我们还提出了新的损失函数形式，这些形式能够更有效地整合语境注入模块，用于目标导向和无目标导向的攻击。我们在目标导向和无目标导向的攻击设置下进行了一系列全面的实验，以证明我们提出的方法的有效性。
2501.01101	 | Deformable Gaussian Splatting for Efficient and High-Fidelity Reconstruction of Surgical Scenes	 | Jiwei Shan,Zeyu Cai,Cheng-Tai Hsieh,Shing Shin Cheng,Hesheng Wang	 | 该研究提出了一种名为EH-SurGS的高效且高保真重建算法，旨在解决可变形外科手术场景重建中的动态变化处理和分层建模问题，通过整合3D高斯体的生命周期和提出自适应运动分层策略，实现了高质量重建和快速渲染。	 | Efficient and high-fidelity reconstruction of deformable surgical scenes is a critical yet challenging task. Building on recent advancements in 3D Gaussian splatting, current methods have seen significant improvements in both reconstruction quality and rendering speed. However, two major limitations remain: (1) difficulty in handling irreversible dynamic changes, such as tissue shearing, which are common in surgical scenes; and (2) the lack of hierarchical modeling for surgical scene deformation, which reduces rendering speed. To address these challenges, we introduce EH-SurGS, an efficient and high-fidelity reconstruction algorithm for deformable surgical scenes. We propose a deformation modeling approach that incorporates the life cycle of 3D Gaussians, effectively capturing both regular and irreversible deformations, thus enhancing reconstruction quality. Additionally, we present an adaptive motion hierarchy strategy that distinguishes between static and deformable regions within the surgical scene. This strategy reduces the number of 3D Gaussians passing through the deformation field, thereby improving rendering speed. Extensive experiments demonstrate that our method surpasses existing state-of-the-art approaches in both reconstruction quality and rendering speed. Ablation studies further validate the effectiveness and necessity of our proposed components. We will open-source our code upon acceptance of the paper.	 | 高效且高保真重建可变形的外科手术场景是一个关键但具有挑战性的任务。基于近期在3D高斯点绘制方面的进展，当前的方法在重建质量和渲染速度方面均取得了显著改进。然而，仍存在两个主要限制：（1）难以处理不可逆的动态变化，如组织剪切，这些变化在手术场景中很常见；（2）缺乏对手术场景变形的分层建模，这降低了渲染速度。为解决这些挑战，我们提出了一种名为EH-SurGS的高效且高保真重建算法，用于可变形手术场景。我们提出了一种变形建模方法，该方法整合了3D高斯体的生命周期，有效地捕捉到常规和不可逆变形，从而提高重建质量。此外，我们还提出了一种自适应运动分层策略，区分手术场景中的静态和可变形区域。此策略减少了通过变形场的3D高斯体数量，从而提高渲染速度。大量实验表明，我们的方法在重建质量和渲染速度方面都超越了现有最先进的方法。消融研究进一步验证了我们所提组件的有效性和必要性。论文被接受后，我们将开源我们的代码。
2501.01097	 | EliGen: Entity-Level Controlled Image Generation with Regional Attention	 | Hong Zhang,Zhongjie Duan,Xingjun Wang,Yingda Chen,Yu Zhang	 | EliGen是一种新型实体级控制图像生成框架，通过引入区域注意力机制将实体提示和空间遮罩无缝集成，从而实现对图像中个体实体的精细控制，并在位置控制精度和图像质量上超越现有方法。	 | Recent advancements in diffusion models have significantly advanced text-to-image generation, yet global text prompts alone remain insufficient for achieving fine-grained control over individual entities within an image. To address this limitation, we present EliGen, a novel framework for Entity-Level controlled Image Generation. We introduce regional attention, a mechanism for diffusion transformers that requires no additional parameters, seamlessly integrating entity prompts and arbitrary-shaped spatial masks. By contributing a high-quality dataset with fine-grained spatial and semantic entity-level annotations, we train EliGen to achieve robust and accurate entity-level manipulation, surpassing existing methods in both positional control precision and image quality. Additionally, we propose an inpainting fusion pipeline, extending EliGen to multi-entity image inpainting tasks. We further demonstrate its flexibility by integrating it with community models such as IP-Adapter and MLLM, unlocking new creative possibilities. The source code, dataset, and model will be released publicly.	 | 最近在扩散模型方面的进展显著提升了文本到图像的生成能力，但仅依靠全局文本提示仍然无法实现对图像中个体实体的精细控制。为了解决这一局限性，我们提出了EliGen，一种新型的实体级控制图像生成框架。我们引入了区域注意力机制，这是一种无需额外参数的机制，能够无缝地将实体提示和任意形状的空间遮罩集成在一起。通过提供一个包含精细空间和语义实体标注的高质量数据集，我们训练EliGen实现鲁棒且准确的实体级编辑，其在位置控制精度和图像质量方面均超越了现有方法。此外，我们提出了一种填充融合管道，将其扩展应用于多实体图像填充任务。我们进一步展示了其灵活性，将其与社区模型如IP-Adapter和MLLM相结合，解锁了新的创作可能性。源代码、数据集和模型将公开发布。
2501.01072	 | Evidential Calibrated Uncertainty-Guided Interactive Segmentation paradigm for Ultrasound Images	 | Jiang Shang,Yuanmeng Wu,Xiaoxiang Han,Xi Chen,Qi Zhang	 | EUGIS 是一种基于证据不确定性估算的高效分层交互式分割框架，通过优先处理高不确定性区域来改进超声图像分割，减少所需的提示和迭代次数；该方法融合了 Dempster-Shafer 理论和主观逻辑，提出了一种可训练的校准机制以优化不确定性估算的置信度。	 | Accurate and robust ultrasound image segmentation is critical for computer-aided diagnostic systems. Nevertheless, the inherent challenges of ultrasound imaging, such as blurry boundaries and speckle noise, often cause traditional segmentation methods to struggle with performance. Despite recent advancements in universal image segmentation, such as the Segment Anything Model, existing interactive segmentation methods still suffer from inefficiency and lack of specialization. These methods rely heavily on extensive accurate manual or random sampling prompts for interaction, necessitating numerous prompts and iterations to reach satisfactory performance. In response to this challenge, we propose the Evidential Uncertainty-Guided Interactive Segmentation (EUGIS), an end-to-end, efficient tiered interactive segmentation paradigm based on evidential uncertainty estimation for ultrasound image segmentation. Specifically, EUGIS harnesses evidence-based uncertainty estimation, grounded in Dempster-Shafer theory and Subjective Logic, to gauge the level of uncertainty in the predictions of model for different regions. By prioritizing sampling the high-uncertainty region, our method can effectively simulate the interactive behavior of well-trained radiologists, enhancing the targeted of sampling while reducing the number of prompts and iterations required.Additionally, we propose a trainable calibration mechanism for uncertainty estimation, which can further optimize the boundary between certainty and uncertainty, thereby enhancing the confidence of uncertainty estimation.	 | 准确且稳健的超声图像分割对于计算机辅助诊断系统至关重要。然而，超声成像固有的挑战，如模糊边界和斑点噪声，往往导致传统分割方法在性能上难以应对。尽管在通用图像分割方面取得了最近的进步，例如Segment Anything Model，现有的交互式分割方法仍然存在效率低下和缺乏专业性的问题。这些方法高度依赖于广泛的精确手动或随机采样提示进行交互，需要大量的提示和迭代才能达到满意的性能。为应对这一挑战，我们提出了一种基于证据不确定性估算的高效分层交互式分割框架Evidential Uncertainty-Guided Interactive Segmentation (EUGIS)，这是一种端到端的交互式分割范式，基于证据不确定性估算来进行超声图像分割。具体来说，EUGIS 利用基于Dempster-Shafer理论和主观逻辑的证据为基础的不确定性估算方法，评估模型对于不同区域预测的不确定性水平。通过优先对高不确定性区域进行采样，我们的方法可以有效地模拟训练有素放射科医生的交互行为，提高采样的针对性，同时减少所需的提示和迭代次数。此外，我们还提出了一种可训练的校准机制来进行不确定性估算，这可以进一步优化确定性和不确定性的边界，从而增强不确定性估算的置信度。
2501.01049	 | TS-SatMVSNet: Slope Aware Height Estimation for Large-Scale Earth Terrain Multi-view Stereo	 | Song Zhang,Zhiwei Wei,Wenjia Xu,Lili Zhang,Yang Wang,Jinming Zhang,Junyi Liu	 | 本文提出了一种端到端的坡度感知高度估计网络——TS-SatMVSNet，通过引入基于高度的坡度计算策略和两个坡度引导模块，在大规模遥感地形重建中提高了精度，并在多个数据集上取得了最先进的性能。	 | 3D terrain reconstruction with remote sensing imagery achieves cost-effective and large-scale earth observation and is crucial for safeguarding natural disasters, monitoring ecological changes, and preserving the environment.Recently, learning-based multi-view stereo~(MVS) methods have shown promise in this task. However, these methods simply modify the general learning-based MVS framework for height estimation, which overlooks the terrain characteristics and results in insufficient accuracy. Considering that the Earth's surface generally undulates with no drastic changes and can be measured by slope, integrating slope considerations into MVS frameworks could enhance the accuracy of terrain reconstructions. To this end, we propose an end-to-end slope-aware height estimation network named TS-SatMVSNet for large-scale remote sensing terrain reconstruction.To effectively obtain the slope representation, drawing from mathematical gradient concepts, we innovatively proposed a height-based slope calculation strategy to first calculate a slope map from a height map to measure the terrain undulation. To fully integrate slope information into the MVS pipeline, we separately design two slope-guided modules to enhance reconstruction outcomes at both micro and macro levels. Specifically, at the micro level, we designed a slope-guided interval partition module for refined height estimation using slope values. At the macro level, a height correction module is proposed, using a learnable Gaussian smoothing operator to amend the inaccurate height values. Additionally, to enhance the efficacy of height estimation, we proposed a slope direction loss for implicitly optimizing height estimation results. Extensive experiments on the WHU-TLC dataset and MVS3D dataset show that our proposed method achieves state-of-the-art performance and demonstrates competitive generalization ability.	 | 基于遥感影像的3D地形重建实现低成本、大规模的地球观测，并在自然灾害防范、生态变化监测和环境保护等方面至关重要。近年来，基于学习的多视图 stereo（MVS）方法在这项任务中显示出潜力。然而，这些方法只是简单地修改了一般的学习型MVS框架以进行高度估计，而忽略了地形特征，导致精度不足。考虑到地球表面通常有起伏但变化不大，可以通过坡度进行测量，将坡度考虑整合进MVS框架中可以提高地形重建的精度。为此，我们提出了一种端到端的坡度感知高度估计网络——TS-SatMVSNet，用于大规模遥感地形重建。  为了有效获取坡度表示，我们借鉴了数学梯度的概念，创新地提出了一种基于高度的坡度计算策略，首先从高度图计算一个坡度图来测量地形起伏。为了全面将坡度信息融入MVS管道，我们分别设计了两个坡度引导模块，在微观和宏观层面增强重建结果。具体来说，在微观层面，我们设计了一个坡度引导区间分割模块，利用坡度值进行精细化高度估计；在宏观层面，我们提出了一个高度校正模块，利用可学习的高斯平滑操作符修正不准确的高度值。此外，为了提高高度估计的效果，我们提出了一种坡度方向损失，隐式优化高度估计结果。在WHU-TLC数据集和MVS3D数据集上的广泛实验显示，我们的方法达到了最先进的性能，并展示了较强的泛化能力。
2501.01045	 | ZeroFlow: Overcoming Catastrophic Forgetting is Easier than You Think	 | Tao Feng,Wei Li,DiDi Zhu,Hangjie Yuan,Wendi Zheng,Dan Zhang,Jie Tang	 | 该研究引入了ZeroFlow基准测试工具，评估无梯度优化方法在克服灾难性遗忘方面的有效性，并发现仅依靠前向传播即可显著减轻遗忘现象，为前向传播方法在持续学习中的应用提供了新的优化原则和增强方法。	 | Backpropagation provides a generalized configuration for overcoming catastrophic forgetting. Like, SGD and Adam are commonly used for weight updates in continual learning and continual pre-training. In practice, permission to access gradient information is not always granted (the gradient ban), such as black-box APIs, hardware limitations, and non-differentiable systems. To bridge this gap, we introduce the first benchmark ZeroFlow to evaluate gradient-free optimization algorithms for overcoming forgetting. This benchmark examines a suite of forward pass methods across multiple methods, forgetting scenarios, and datasets. We find that forward passes alone are enough to overcome forgetting. Our findings reveal new optimization principles that highlight the potential of forward-pass in mitigating forgetting, managing task conflicts, and reducing memory demands, alongside novel enhancements that further mitigate forgetting with just one forward pass. This work provides essential insights and tools for advancing forward pass methods to overcome forgetting.	 | 反向传播提供了一种通用的方法来克服灾难性遗忘。例如，SGD和Adam通常用于持续学习和持续预训练中的权重更新。在实践中，并不是总能获得访问梯度信息的权限（梯度禁令），比如黑盒API、硬件限制和非可微系统。为了解决这一问题，我们引入了第一个基准测试工具ZeroFlow，用于评估无梯度优化算法以克服遗忘。该基准测试考察了多种前向传播方法在多个遗忘场景和数据集上的表现。我们发现，仅依靠前向传播就能克服遗忘。我们的研究揭示了新的优化原则，这些原则突显了前向传播在减轻遗忘、管理任务冲突和减少内存需求方面的潜力，同时还提出了新的增强方法，只需一次前向传播就能进一步减轻遗忘。这项工作提供了重要的见解和工具，以推动前向传播方法的发展，以克服遗忘。
2501.01042	 | Image-based Multimodal Models as Intruders: Transferable Multimodal Attacks on Video-based MLLMs	 | Linhao Huang,Xue Jiang,Zhiqiang Wang,Wentao Mo,Xi Xiao,Bo Han,Yongjie Yin,Feng Zheng	 | 本文首次探索了对抗视频样本在不同视频-文本多模态语言模型（V-MLLMs）之间的可移植性，并提出了一种名为“图像到视频”多模态语言模型攻击（I2V-MLLM）的方法，通过整合多模态信息和时间信息，以及利用基于图像的多模态模型作为代理模型，提高了对抗样本的可移植性。实验结果表明，该方法在多个视频-文本多模态任务中展示了更高的黑盒攻击成功率。	 | Video-based multimodal large language models (V-MLLMs) have shown vulnerability to adversarial examples in video-text multimodal tasks. However, the transferability of adversarial videos to unseen models--a common and practical real world scenario--remains unexplored. In this paper, we pioneer an investigation into the transferability of adversarial video samples across V-MLLMs. We find that existing adversarial attack methods face significant limitations when applied in black-box settings for V-MLLMs, which we attribute to the following shortcomings: (1) lacking generalization in perturbing video features, (2) focusing only on sparse key-frames, and (3) failing to integrate multimodal information. To address these limitations and deepen the understanding of V-MLLM vulnerabilities in black-box scenarios, we introduce the Image-to-Video MLLM (I2V-MLLM) attack. In I2V-MLLM, we utilize an image-based multimodal model (IMM) as a surrogate model to craft adversarial video samples. Multimodal interactions and temporal information are integrated to disrupt video representations within the latent space, improving adversarial transferability. In addition, a perturbation propagation technique is introduced to handle different unknown frame sampling strategies. Experimental results demonstrate that our method can generate adversarial examples that exhibit strong transferability across different V-MLLMs on multiple video-text multimodal tasks. Compared to white-box attacks on these models, our black-box attacks (using BLIP-2 as surrogate model) achieve competitive performance, with average attack success rates of 55.48% on MSVD-QA and 58.26% on MSRVTT-QA for VideoQA tasks, respectively. Our code will be released upon acceptance.	 | 基于视频的大规模多模态语言模型（V-MLLMs）在视频-文本多模态任务中对对抗样本表现出脆弱性。然而，对抗视频在未见模型中的可移植性——一个常见且实际的现实场景——尚未被研究。在本文中，我们首次探索了对抗视频样本在不同V-MLLMs之间的可移植性。我们发现，现有的对抗攻击方法在应用于V-MLLM的黑盒设置时面临显著的局限性，我们认为这些局限性归因于以下几点：（1）在扰动视频特征方面缺乏泛化能力，（2）仅关注稀疏的关键帧，以及（3）未能整合多模态信息。为了解决这些局限性并加深对V-MLLM在黑盒场景中的脆弱性的理解，我们引入了“图像到视频”多模态语言模型攻击（I2V-MLLM）。在I2V-MLLM中，我们利用基于图像的多模态模型（IMM）作为代理模型来生成对抗视频样本。多模态交互和时间信息被整合以在潜在空间内破坏视频表示，从而提高对抗样本的可移植性。此外，我们引入了一种扰动传播技术来处理不同的未知帧采样策略。实验结果表明，我们的方法可以生成在多个视频-文本多模态任务中不同V-MLLM之间具有良好可移植性的对抗样本。与这些模型的白盒攻击相比，我们的黑盒攻击（使用BLIP-2作为代理模型）在VideoQA任务中分别在MSVD-QA和MSRVTT-QA上达到了55.48%和58.26%的平均攻击成功率。我们的代码将在接受后发布。
2501.01040	 | Event Masked Autoencoder: Point-wise Action Recognition with Event-Based Cameras	 | Jingkai Sun,Qiang Zhang,Jiaxu Wang,Jiahang Cao,Renjing Xu	 | 该研究提出了一种新颖的框架，利用动态视觉传感器（DVS）的事件数据进行动作识别，通过点事件掩码自编码器和改进的事件点片段生成算法保留并利用事件数据的空间-时间结构。该框架首次引入了预训练方法和事件点片段嵌入技术，以提高动作识别的准确性和鲁棒性。	 | Dynamic vision sensors (DVS) are bio-inspired devices that capture visual information in the form of asynchronous events, which encode changes in pixel intensity with high temporal resolution and low latency. These events provide rich motion cues that can be exploited for various computer vision tasks, such as action recognition. However, most existing DVS-based action recognition methods lose temporal information during data transformation or suffer from noise and outliers caused by sensor imperfections or environmental factors. To address these challenges, we propose a novel framework that preserves and exploits the spatiotemporal structure of event data for action recognition. Our framework consists of two main components: 1) a point-wise event masked autoencoder (MAE) that learns a compact and discriminative representation of event patches by reconstructing them from masked raw event camera points data; 2) an improved event points patch generation algorithm that leverages an event data inlier model and point-wise data augmentation techniques to enhance the quality and diversity of event points patches. To the best of our knowledge, our approach introduces the pre-train method into event camera raw points data for the first time, and we propose a novel event points patch embedding to utilize transformer-based models on event cameras.	 | 动态视觉传感器（DVS）是一种受生物启发的器件，能够以异步事件的形式捕获视觉信息，这些事件以高时序分辨率和低延迟编码像素强度的变化。这些事件提供了丰富的运动线索，可以被利用来完成各种计算机视觉任务，如动作识别。然而，大多数现有的基于DVS的动作识别方法在数据转换过程中失去了时间信息，或者由于传感器缺陷或环境因素导致噪声和异常值。为了解决这些挑战，我们提出了一种新型框架，该框架保留并利用事件数据的空间-时间结构来进行动作识别。我们的框架包括两个主要组成部分：1）点事件掩码自编码器（MAE），该模型通过从掩码的原始事件摄像点数据重构事件片段来学习事件片段的紧凑且区分性的表示；2）改进的事件点片段生成算法，该算法利用事件数据中的残差模型和点事件数据增强技术来提高事件点片段的质量和多样性。据我们所知，我们的方法首次将预训练方法引入事件摄像机原始点数据，并提出了一种新的事件点片段嵌入方法，以便在事件摄像机上利用基于转换器的模型。
2501.01032	 | DynamicLip: Shape-Independent Continuous Authentication via Lip Articulator Dynamics	 | Huashan Chen,Yifan Xu,Yue Feng,Ming Jian,Feng Liu,Pengfei Hu,Kebin Peng,Sen He,Zi Wang	 | 本文提出了一种基于唇部发音动态的、与形状无关的连续认证系统，该系统能够实现稳健、与形状无关且连续的认证，特别适用于高安全和隐私要求的场景。实验结果显示，该系统在不同环境和攻击场景下的整体准确率为99.06%，具有高度的稳健性。	 | Biometrics authentication has become increasingly popular due to its security and convenience; however, traditional biometrics are becoming less desirable in scenarios such as new mobile devices, Virtual Reality, and Smart Vehicles. For example, while face authentication is widely used, it suffers from significant privacy concerns. The collection of complete facial data makes it less desirable for privacy-sensitive applications. Lip authentication, on the other hand, has emerged as a promising biometrics method. However, existing lip-based authentication methods heavily depend on static lip shape when the mouth is closed, which can be less robust due to lip shape dynamic motion and can barely work when the user is speaking. In this paper, we revisit the nature of lip biometrics and extract shape-independent features from the lips. We study the dynamic characteristics of lip biometrics based on articulator motion. Building on the knowledge, we propose a system for shape-independent continuous authentication via lip articulator dynamics. This system enables robust, shape-independent and continuous authentication, making it particularly suitable for scenarios with high security and privacy requirements. We conducted comprehensive experiments in different environments and attack scenarios and collected a dataset of 50 subjects. The results indicate that our system achieves an overall accuracy of 99.06% and demonstrates robustness under advanced mimic attacks and AI deepfake attacks, making it a viable solution for continuous biometric authentication in various applications.	 | 生物特征认证由于其安全性和便捷性而变得越来越受欢迎，但在新移动设备、虚拟现实和智能车辆等场景中，传统生物特征认证变得不再十分理想。例如，虽然面部认证广泛使用，但它存在严重的隐私问题。收集完整的面部数据使其在对隐私敏感的应用中不太理想。相比之下，唇部认证作为一种有前景的生物特征认证方法已经出现。然而，现有的基于唇部的认证方法高度依赖于闭合嘴巴时的静态唇形，这会因唇形的动态变化而变得不够稳健，在用户说话时几乎无法运作。在本文中，我们重新审视了唇部生物特征的本质，并从唇部中提取与形状无关的特征。我们基于发音运动研究了唇部生物特征的动态特性。基于这些知识，我们提出了一种基于唇部发音动态的、与形状无关的连续认证系统。该系统能够实现稳健、与形状无关且连续的认证，特别适用于对安全和隐私要求高的场景。我们在不同环境和攻击场景下进行了全面的实验，并收集了50名受试者的数据集。实验结果表明，我们的系统整体准确率为99.06%，并在高级模仿攻击和AI深度伪造攻击下表现出高度的稳健性，使其成为各种应用中连续生物特征认证的可行解决方案。
2501.01023	 | Hadamard Attention Recurrent Transformer: A Strong Baseline for Stereo Matching Transformer	 | Ziyang Chen,Yongjun Zhang,Wenting Li,Bingshu Wang,Yabo Wu,Yong Zhao,C.L. Philip Chen	 | 作者提出了一种新型的立体变换模型——Hadamard Attention Recurrent Stereo Transformer (HART)，该模型通过引入Hadamard乘积、密集注意力内核和多尺度卷积交互(MKOI)等组件，解决了现有立体变压器在表达能力和计算效率上的限制，特别是在处理反射和弱纹理等挑战性条件下表现出色，并在KITTI 2012基准测试中取得最优成绩。	 | In light of the advancements in transformer technology, extant research posits the construction of stereo transformers as a potential solution to the binocular stereo matching challenge. However, constrained by the low-rank bottleneck and quadratic complexity of attention mechanisms, stereo transformers still fail to demonstrate sufficient nonlinear expressiveness within a reasonable inference time. The lack of focus on key homonymous points renders the representations of such methods vulnerable to challenging conditions, including reflections and weak textures. Furthermore, a slow computing speed is not conducive to the application. To overcome these difficulties, we present the \textbf{H}adamard \textbf{A}ttention \textbf{R}ecurrent Stereo \textbf{T}ransformer (HART) that incorporates the following components: 1) For faster inference, we present a Hadamard product paradigm for the attention mechanism, achieving linear computational complexity. 2) We designed a Dense Attention Kernel (DAK) to amplify the differences between relevant and irrelevant feature responses. This allows HART to focus on important details. DAK also converts zero elements to non-zero elements to mitigate the reduced expressiveness caused by the low-rank bottleneck. 3) To compensate for the spatial and channel interaction missing in the Hadamard product, we propose MKOI to capture both global and local information through the interleaving of large and small kernel convolutions. Experimental results demonstrate the effectiveness of our HART. In reflective area, HART ranked \textbf{1st} on the KITTI 2012 benchmark among all published methods at the time of submission. Code is available at \url{https://github.com/ZYangChen/HART}.	 | 基于变压器技术的最新进展，现有研究认为构建立体变压器可能是解决双眼立体匹配挑战的一种潜在解决方案。然而，受到低秩瓶颈和注意力机制的二次复杂性限制，立体变压器仍然无法在合理的时间内展现出充足的非线性表达能力。由于缺乏对关键同名点的关注，这类方法的表示在面对反射和弱纹理等挑战性条件下变得脆弱。此外，计算速度缓慢也不利于实际应用。为克服这些困难，我们提出了 **Hadamard Attention Recurrent Stereo Transformer (HART)**，该模型包含以下组件：1) 为了实现更快的推理速度，我们提出了一种 Hadamard 乘积范式，实现了线性计算复杂度。2) 我们设计了密集注意力内核 (DAK) 以增强相关特征和无关特征响应之间的差异。这使得 HART 能够集中注意力在重要细节上。DAK 还将零元素转换为非零元素，以缓解低秩瓶颈导致的表达能力减弱。3) 为弥补 Hadamard 乘积中缺失的空间和通道交互，我们提出了 MKOI，通过交错使用大核和小核卷积来捕获全局和局部信息。实验结果证明了 HART 的有效性。在反射区域内，HART 在提交时所有已发表的方法中，在 KITTI 2012 基准测试中排名第一。代码已发布在 \url{https://github.com/ZYangChen/HART}。
2501.01022	 | Efficient Connectivity-Preserving Instance Segmentation with Supervoxel-Based Loss Function	 | Anna Grim,Jayaram Chandrashekar,Uygar Sumbul	 | 该研究提出了一种基于数字拓扑的最小计算开销的拓扑感知神经网络分割方法，以解决神经科学中复杂实例分割问题，特别是针对神经元及其轴突的精细局部形态重建。通过实验验证，该方法在小鼠大脑和多个基准数据集上均显示了有效性。	 | Reconstructing the intricate local morphology of neurons and their long-range projecting axons can address many connectivity related questions in neuroscience. The main bottleneck in connectomics pipelines is correcting topological errors, as multiple entangled neuronal arbors is a challenging instance segmentation problem. More broadly, segmentation of curvilinear, filamentous structures continues to pose significant challenges. To address this problem, we extend the notion of simple points from digital topology to connected sets of voxels (i.e. supervoxels) and propose a topology-aware neural network segmentation method with minimal computational overhead. We demonstrate its effectiveness on a new public dataset of 3-d light microscopy images of mouse brains, along with the benchmark datasets DRIVE, ISBI12, and CrackTree.	 | 神经元及其远程投射轴突的精细局部形态重建可以解决神经科学中的许多连接相关问题。连接组学管道中的主要瓶颈在于纠正拓扑错误，因为多个交织的神经树突是一个复杂的实例分割问题。更广泛地说，曲线状、纤维状结构的分割仍然面临着巨大挑战。为了解决这个问题，我们将数字拓扑中的简单点概念扩展到连接体的体素集（即超体素），并提出了一种具有最小计算开销的拓扑感知神经网络分割方法。我们通过一个新的公共3D光学显微镜图像数据集（来自小鼠大脑）以及基准数据集DRIVE、ISBI12和CrackTree，证明了该方法的有效性。
2501.01015	 | Boosting Adversarial Transferability with Spatial Adversarial Alignment	 | Zhaoyu Chen,Haijing Guo,Kaixun Jiang,Jiyuan Fu,Xinyu Zhou,Dingkang Yang,Hao Tang,Bo Li,Wenqiang Zhang	 | 提出了一种名为空间对抗对齐（SAA）的技术，通过利用对齐损失和见证模型微调替代模型，实现高转移性对抗样本，特别是在跨架构攻击中表现优异。SAA通过空间感知对齐和对抗感知对齐两个关键部分，促进模型特征在全局和局部的一致性，从而提高对抗样本的转移性能。	 | Deep neural networks are vulnerable to adversarial examples that exhibit transferability across various models. Numerous approaches are proposed to enhance the transferability of adversarial examples, including advanced optimization, data augmentation, and model modifications. However, these methods still show limited transferability, particularly in cross-architecture scenarios, such as from CNN to ViT. To achieve high transferability, we propose a technique termed Spatial Adversarial Alignment (SAA), which employs an alignment loss and leverages a witness model to fine-tune the surrogate model. Specifically, SAA consists of two key parts: spatial-aware alignment and adversarial-aware alignment. First, we minimize the divergences of features between the two models in both global and local regions, facilitating spatial alignment. Second, we introduce a self-adversarial strategy that leverages adversarial examples to impose further constraints, aligning features from an adversarial perspective. Through this alignment, the surrogate model is trained to concentrate on the common features extracted by the witness model. This facilitates adversarial attacks on these shared features, thereby yielding perturbations that exhibit enhanced transferability. Extensive experiments on various architectures on ImageNet show that aligned surrogate models based on SAA can provide higher transferable adversarial examples, especially in cross-architecture attacks.	 | 深度神经网络对跨多种模型的转移对抗样本非常脆弱。许多方法被提出以增强对抗样本的转移性，包括高级优化、数据增强和模型修改。然而，这些方法在跨架构场景下仍然显示出有限的转移性，例如从CNN到ViT。为了实现高转移性，我们提出了一种称为空间对抗对齐（SAA，Spatial Adversarial Alignment）的技术，该技术使用对齐损失并利用见证模型来微调替代模型。具体而言，SAA 包含两个关键部分：空间感知对齐和对抗感知对齐。首先，我们最小化两个模型在全局和局部区域的特征差异，促进空间对齐。其次，我们引入了一种自我对抗策略，利用对抗样本施加进一步的约束，从对抗视角对齐特征。通过这种对齐，替代模型被训练集中在见证模型提取的共同特征上。这有助于对这些共享特征进行对抗攻击，从而产生表现出更好转移性的扰动。在ImageNet上针对各种架构进行的大量实验显示，基于SAA的对齐替代模型能提供更高转移性的对抗样本，特别是在跨架构攻击中表现尤为明显。
2501.01003	 | EasySplat: View-Adaptive Learning makes 3D Gaussian Splatting Easy	 | Ao Gao,Luosong Guo,Tao Chen,Zhao Wang,Ying Tai,Jian Yang,Zhenyu Zhang	 | 本文提出了一种名为EasySplat的新框架，通过采用基于视图相似性的分组策略和鲁棒的点云先验，实现高效且准确的3D高斯渲染建模，从而在处理新视图合成方面优于现有最先进的技术。	 | 3D Gaussian Splatting (3DGS) techniques have achieved satisfactory 3D scene representation. Despite their impressive performance, they confront challenges due to the limitation of structure-from-motion (SfM) methods on acquiring accurate scene initialization, or the inefficiency of densification strategy. In this paper, we introduce a novel framework EasySplat to achieve high-quality 3DGS modeling. Instead of using SfM for scene initialization, we employ a novel method to release the power of large-scale pointmap approaches. Specifically, we propose an efficient grouping strategy based on view similarity, and use robust pointmap priors to obtain high-quality point clouds and camera poses for 3D scene initialization. After obtaining a reliable scene structure, we propose a novel densification approach that adaptively splits Gaussian primitives based on the average shape of neighboring Gaussian ellipsoids, utilizing KNN scheme. In this way, the proposed method tackles the limitation on initialization and optimization, leading to an efficient and accurate 3DGS modeling. Extensive experiments demonstrate that EasySplat outperforms the current state-of-the-art (SOTA) in handling novel view synthesis.	 | 3D高斯渲染（3DGS）技术已经实现了令人满意的3D场景表示。尽管它们表现出色，但由于结构从运动（SfM）方法在获取准确场景初始化方面的局限性，或者密集化策略的低效性，它们仍然面临着挑战。在本文中，我们引入了一种名为EasySplat的新框架，以实现高质量的3DGS建模。我们不使用SfM进行场景初始化，而是采用一种新的方法来释放大规模点云方法的强大功能。具体来说，我们提出了一种基于视图相似性的高效分组策略，并使用鲁棒的点云先验来获取高质量的点云和相机姿态，用于3D场景初始化。在获得可靠的场景结构后，我们提出了一种新的密集化方法，基于邻近高斯椭球体的平均形状进行自适应的高斯原语分裂，利用KNN方案。这样，所提出的方法克服了初始化和优化的限制，从而实现高效且准确的3DGS建模。广泛的实验表明，EasySplat在处理新视图合成方面优于当前最先进的技术（SOTA）。
2501.00975	 | CoordFlow: Coordinate Flow for Pixel-wise Neural Video Representation	 | Daniel Silver,Ron Kimmel	 | CoordFlow是一种新的像素级神经表示方法，用于视频压缩，它取得了最先进的结果，并且在无监督视频序列分割和视音频时冗余补偿中表现出色，还具备视频上采样、稳定化、修补和去噪等能力。	 | In the field of video compression, the pursuit for better quality at lower bit rates remains a long-lasting goal. Recent developments have demonstrated the potential of Implicit Neural Representation (INR) as a promising alternative to traditional transform-based methodologies. Video INRs can be roughly divided into frame-wise and pixel-wise methods according to the structure the network outputs. While the pixel-based methods are better for upsampling and parallelization, frame-wise methods demonstrated better performance. We introduce CoordFlow, a novel pixel-wise INR for video compression. It yields state-of-the-art results compared to other pixel-wise INRs and on-par performance compared to leading frame-wise techniques. The method is based on the separation of the visual information into visually consistent layers, each represented by a dedicated network that compensates for the layer's motion. When integrated, a byproduct is an unsupervised segmentation of video sequence. Objects motion trajectories are implicitly utilized to compensate for visual-temporal redundancies. Additionally, the proposed method provides inherent video upsampling, stabilization, inpainting, and denoising capabilities.	 | 在视频压缩领域，追求在更低比特率下获得更好的质量一直是一个长期的目标。最近的研究表明，显式神经表示（INR）作为一种替代传统变换方法的有前途的选择具有很大的潜力。视频INR可以大致分为帧级和像素级方法，根据网络输出的结构进行区分。虽然像素级方法更适合上采样和并行化，但帧级方法表现更优。我们提出了CoordFlow，这是一种新的像素级INR方法，用于视频压缩。与其它像素级INR相比，它取得了最先进的结果；与领先的帧级技术相比，其性能也相当。该方法基于将视觉信息分离成视觉一致的层次，并由专门的网络表示和补偿每层的运动。当集成在一起时，这会产生一种无监督的视频序列分割效果。物体的运动轨迹被隐式利用来补偿视音频时冗余。此外，所提出的方法还提供了固有的视频上采样、稳定化、修补和去噪能力。
2501.00962	 | OASIS Uncovers: High-Quality T2I Models, Same Old Stereotypes	 | Sepehr Dehdashtian,Gautam Sreekumar,Vishnu Naresh Boddeti	 | 该研究提出了一种名为OASIS的方法，用于量化生成图像中的文化与职业刻板印象，并分析这些刻板印象在文本生成图像模型中的来源，发现较新的T2I模型如FLUX.1和SDv3仍存在强烈刻板印象倾向，且对互联网足迹较少的国家影响更大。	 | Images generated by text-to-image (T2I) models often exhibit visual biases and stereotypes of concepts such as culture and profession. Existing quantitative measures of stereotypes are based on statistical parity that does not align with the sociological definition of stereotypes and, therefore, incorrectly categorizes biases as stereotypes. Instead of oversimplifying stereotypes as biases, we propose a quantitative measure of stereotypes that aligns with its sociological definition. We then propose OASIS to measure the stereotypes in a generated dataset and understand their origins within the T2I model. OASIS includes two scores to measure stereotypes from a generated image dataset: (M1) Stereotype Score to measure the distributional violation of stereotypical attributes, and (M2) WALS to measure spectral variance in the images along a stereotypical attribute. OASIS also includes two methods to understand the origins of stereotypes in T2I models: (U1) StOP to discover attributes that the T2I model internally associates with a given concept, and (U2) SPI to quantify the emergence of stereotypical attributes in the latent space of the T2I model during image generation. Despite the considerable progress in image fidelity, using OASIS, we conclude that newer T2I models such as FLUX.1 and SDv3 contain strong stereotypical predispositions about concepts and still generate images with widespread stereotypical attributes. Additionally, the quantity of stereotypes worsens for nationalities with lower Internet footprints.	 | 由文本生成图像（T2I）模型生成的图像经常表现出文化及职业概念上的视觉偏见和刻板印象。现有衡量刻板印象的量化指标基于统计平等，这与社会学意义上的刻板印象定义不符，因此错误地将偏见分类为刻板印象。我们不将刻板印象过度简化为偏见，而是提出一种与社会学定义相一致的量化刻板印象的指标。我们还提出了一种名为OASIS的方法，用于衡量生成数据集中的刻板印象及其在T2I模型中的起源。OASIS包括两个分数来衡量生成图像数据集中刻板印象：(M1)刻板印象分数，用于衡量刻板印象属性的分布偏差；(M2)WALS，用于衡量图像沿刻板印象属性的光谱变化。OASIS还包含两种方法来理解T2I模型中刻板印象的起源：(U1)StOP，用于发现T2I模型内部与特定概念关联的属性；(U2)SPI，用于量化在T2I模型的潜在空间中，在生成图像期间刻板印象属性的出现。尽管图像保真度取得了显著进步，但使用OASIS，我们得出结论，如FLUX.1和SDv3等较新的T2I模型对概念存在强烈的刻板印象倾向，仍然生成含有广泛刻板印象属性的图像。此外，刻板印象的数量对于互联网足迹较少的国家影响更大。
2501.00958	 | 2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining	 | Wenqi Zhang,Hang Zhang,Xin Li,Jiashuo Sun,Yongliang Shen,Weiming Lu,Deli Zhao,Yueting Zhuang,Lidong Bing	 | 本文介绍了一种新的多模态教科书语料库，用于视觉-语言模型的预训练，该语料库从教学视频中提取内容，提供了更连贯的上下文和更丰富的知识，特别适用于知识和推理密集型任务。相较于现有数据集，该语料库在多个评估任务中表现出色，并且能够有效利用视觉和文本线索进行少量样本学习。	 | Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality \textbf{multimodal textbook} corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving~\footnote{Our code are available at \url{https://github.com/DAMO-NLP-SG/multimodal_textbook}}.	 | 与图像-文本对数据相比，交错语料库使视觉-语言模型（VLMs）能够像人类一样更自然地理解世界。然而，现有的这些数据集是从网页上抓取的，面临着知识密度低、图像-文本关系松散以及图像之间逻辑连贯性差等挑战。另一方面，互联网上充斥着大量的教学视频（例如，网上几何课程），这些视频被广泛用于人类学习基础学科，但这些宝贵的资源在VLM训练中尚未得到充分利用。在本文中，我们介绍了用于VLM预训练的高质量多模态教科书语料库，其中包含更丰富的基础知识。该语料库收集了超过2.5年的教学视频，总共有22,000课时。我们首先使用一个大型语言模型（LLM）提出的分类法，系统地收集了教学视频。然后，我们逐步从视频中提取和提炼视觉（关键帧）、音频（自动语音识别，ASR）和文本知识（光学字符识别，OCR），并基于时间顺序组织成图像-文本交错语料库。与同类数据集相比，我们的以视频为中心的教科书提供了更加连贯的上下文、更丰富的知识和更好的图像-文本对齐。实验表明，它在知识和推理密集型任务（如ScienceQA和MathVista）中的预训练性能尤为出色。此外，使用我们教科书预训练的VLMs在交错上下文理解方面表现出色，能够利用视觉和文本线索在少量样本中解决任务。我们的代码可以在https://github.com/DAMO-NLP-SG/multimodal_textbook 获取。
2501.00946	 | Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant Computation Elimination in Diffusion Model	 | Omid Saghatchian,Atiyeh Gh. Moghadam,Ahmad Nickabadi	 | 扩散模型在生成高质量高维度图像方面具有潜力，但受到高计算成本和慢推理速度的限制。为此，研究提出了一种称为缓存自适应标记合并（CA-ToMe）的方法，通过计算标记相似度并合并相似标记来加速自注意力机制，实验证明该方法在保持生成质量的同时实现了显著的加速效果。	 | Diffusion models have emerged as a promising approach for generating high-quality, high-dimensional images. Nevertheless, these models are hindered by their high computational cost and slow inference, partly due to the quadratic computational complexity of the self-attention mechanisms with respect to input size. Various approaches have been proposed to address this drawback. One such approach focuses on reducing the number of tokens fed into the self-attention, known as token merging (ToMe). In our method, which is called cached adaptive token merging(CA-ToMe), we calculate the similarity between tokens and then merge the r proportion of the most similar tokens. However, due to the repetitive patterns observed in adjacent steps and the variation in the frequency of similarities, we aim to enhance this approach by implementing an adaptive threshold for merging tokens and adding a caching mechanism that stores similar pairs across several adjacent steps. Empirical results demonstrate that our method operates as a training-free acceleration method, achieving a speedup factor of 1.24 in the denoising process while maintaining the same FID scores compared to existing approaches.	 | 扩散模型已经成为了生成高质量、高维度图像的一种有前景的方法。然而，这些模型由于自注意力机制在输入大小方面的二次计算复杂性而受到高计算成本和慢推理速度的阻碍。多种方法已经被提出以解决这一问题。其中一种方法着重于减少输入到自注意力的标记数量，这种方法被称为标记合并（ToMe）。在我们提出的方法中，称为缓存自适应标记合并（CA-ToMe），我们计算标记之间的相似度，然后合并最相似标记的r比例。但由于相邻步骤中存在的重复模式以及相似频率的变化，我们旨在通过引入自适应合并阈值和缓存机制来改进这一方法，该机制可以在多个相邻步骤中存储相似的标记对。实验证明，我们的方法可以作为一个无需训练的加速方法，在去噪过程中实现了1.24倍的加速因子，同时保持与现有方法相同的FID得分。
2501.00944	 | Diffusion Prism: Enhancing Diversity and Morphology Consistency in Mask-to-Image Diffusion	 | Hao Wang,Xiwen Chen,Ashish Bastola,Jiayou Qin,Abolfazl Razi	 | Diffusion Prism 是一个无需训练的框架，能够高效地将二元掩码转化为多样且逼真的图像，同时保留形态学特征，尤其适用于低熵和稀疏性输入图像，展示了在纳米树状结构等生物模式生成中的优势和广泛应用潜力。	 | The emergence of generative AI and controllable diffusion has made image-to-image synthesis increasingly practical and efficient. However, when input images exhibit low entropy and sparse, the inherent characteristics of diffusion models often result in limited diversity. This constraint significantly interferes with data augmentation. To address this, we propose Diffusion Prism, a training-free framework that efficiently transforms binary masks into realistic and diverse samples while preserving morphological features. We explored that a small amount of artificial noise will significantly assist the image-denoising process. To prove this novel mask-to-image concept, we use nano-dendritic patterns as an example to demonstrate the merit of our method compared to existing controllable diffusion models. Furthermore, we extend the proposed framework to other biological patterns, highlighting its potential applications across various fields.	 | 生成式AI和可控扩散模型的出现使得图像到图像的合成越来越实际和高效。然而，当输入图像具有低熵和稀疏性时，扩散模型的本质特征往往会导致多样性受限。这一限制显著干扰了数据增强的过程。为了解决这个问题，我们提出了Diffusion Prism，这是一种无需训练的框架，可以高效地将二元掩码转换为现实且多样的样本，同时保留形态学特征。我们发现少量的人工噪声将显著帮助图像去噪过程。为了证明这一新颖的掩码到图像的概念，我们使用纳米树状结构模式为例，展示了我们方法相较于现有可控扩散模型的优势。此外，我们将提出的框架扩展到了其他生物模式，突显了其在各个领域的潜在应用。
2501.00935	 | Multiscaled Multi-Head Attention-based Video Transformer Network for Hand Gesture Recognition	 | Mallika Garg,Debashis Ghosh,Pyari Mohan Pradhan	 | 该研究提出了一种多尺度多头注意视频变换网络（MsMHA-VTN）用于动态手部手势识别，该网络在多尺度特征提取和跨模态识别方面表现出优越性能，尤其在NVGesture和Briareo数据集上分别达到了88.22%和99.10%的总体准确率。	 | Dynamic gesture recognition is one of the challenging research areas due to variations in pose, size, and shape of the signer's hand. In this letter, Multiscaled Multi-Head Attention Video Transformer Network (MsMHA-VTN) for dynamic hand gesture recognition is proposed. A pyramidal hierarchy of multiscale features is extracted using the transformer multiscaled head attention model. The proposed model employs different attention dimensions for each head of the transformer which enables it to provide attention at the multiscale level. Further, in addition to single modality, recognition performance using multiple modalities is examined. Extensive experiments demonstrate the superior performance of the proposed MsMHA-VTN with an overall accuracy of 88.22\% and 99.10\% on NVGesture and Briareo datasets, respectively.	 | 动态手势识别是由于手语者手部姿态、大小和形状的变化而成为一个具有挑战性的研究领域。在这封简报中，我们提出了用于动态手部手势识别的多尺度多头注意视频变换网络（MsMHA-VTN）。利用变换器多尺度头部注意力模型提取出多尺度特征的分层金字塔。所提出的模型在变换器的每个头中使用不同的注意维度，使得它能够在多尺度级别提供注意。此外，除了单一模态，我们还评估了使用多种模态的识别性能。广泛的实验表明，提出的MsMHA-VTN在NVGesture和Briareo数据集上的总体准确率分别为88.22%和99.10%，显示出优越的性能。
2501.00917	 | Hierarchical Vision-Language Alignment for Text-to-Image Generation via Diffusion Models	 | Emily Johnson,Noah Wilson	 | VLAD模型是一种结合语义对齐和分层扩散的生成框架，利用上下文组合模块将文本提示分解为全局和局部表示，从而生成高质量、与视觉特征精确对齐的图像；实验结果显示VLAD在图像质量和语义对齐方面显著优于现有方法，并通过人类评估进一步验证了其优越性能。	 | Text-to-image generation has witnessed significant advancements with the integration of Large Vision-Language Models (LVLMs), yet challenges remain in aligning complex textual descriptions with high-quality, visually coherent images. This paper introduces the Vision-Language Aligned Diffusion (VLAD) model, a generative framework that addresses these challenges through a dual-stream strategy combining semantic alignment and hierarchical diffusion. VLAD utilizes a Contextual Composition Module (CCM) to decompose textual prompts into global and local representations, ensuring precise alignment with visual features. Furthermore, it incorporates a multi-stage diffusion process with hierarchical guidance to generate high-fidelity images. Experiments conducted on MARIO-Eval and INNOVATOR-Eval benchmarks demonstrate that VLAD significantly outperforms state-of-the-art methods in terms of image quality, semantic alignment, and text rendering accuracy. Human evaluations further validate the superior performance of VLAD, making it a promising approach for text-to-image generation in complex scenarios.	 | 文本到图像生成在大型视觉-语言模型（LVLMs）的集成下取得了显著进展，但仍面临将复杂的文本描述与高质量、视觉一致的图像对齐的挑战。本文介绍了视觉语言对齐扩散（VLAD）模型，这是一种通过结合语义对齐和分层扩散的双流策略来解决这些挑战的生成框架。VLAD 利用上下文组合模块（CCM）将文本提示分解为全局和局部表示，确保与视觉特征的精确对齐。此外，它还通过多阶段分层指导的扩散过程来生成高保真图像。在 MARIO-Eval 和 INNOVATOR-Eval 基准上的实验表明，VLAD 在图像质量、语义对齐和文本渲染准确性方面均显著优于现有最先进的方法。人类评估进一步验证了 VLAD 的优越性能，使其成为复杂场景下文本到图像生成的有前途的方法。  具体翻译如下：  文本到图像生成在大型视觉-语言模型（LVLMs）的集成下取得了显著进展，但仍面临将复杂的文本描述与高质量、视觉一致的图像对齐的挑战。本文介绍了视觉语言对齐扩散（VLAD）模型，这是一种通过结合语义对齐和分层扩散的双流策略来解决这些挑战的生成框架。VLAD 利用上下文组合模块（CCM）将文本提示分解为全局和局部表示，确保与视觉特征的精确对齐。此外，它还通过多阶段分层指导的扩散过程来生成高保真图像。在 MARIO-Eval 和 INNOVATOR-Eval 基准上的实验表明，VLAD 在图像质量、语义对齐和文本渲染准确性方面均显著优于现有最先进的方法。人类评估进一步验证了 VLAD 的优越性能，使其成为复杂场景下文本到图像生成的有前途的方法。
2501.00912	 | AutoPresent: Designing Structured Visuals from Scratch	 | Jiaxin Ge,Zora Zhiruo Wang,Xuhui Zhou,Yi-Hao Peng,Sanjay Subramanian,Qinyue Tan,Maarten Sap,Alane Suhr,Daniel Fried,Graham Neubig,Trevor Darrell	 | 本文探讨了使用自然语言指令自动生成幻灯片的挑战，并提出了SlidBench基准数据集；该数据集包含7000个训练样本和585个测试样本，验证了基于程序生成的方法能够创建高质量的可交互幻灯片，并展示了迭代设计优化的有效性。	 | Designing structured visuals such as presentation slides is essential for communicative needs, necessitating both content creation and visual planning skills. In this work, we tackle the challenge of automated slide generation, where models produce slide presentations from natural language (NL) instructions. We first introduce the SlidesBench benchmark, the first benchmark for slide generation with 7k training and 585 testing examples derived from 310 slide decks across 10 domains. SlidesBench supports evaluations that are (i)reference-based to measure similarity to a target slide, and (ii)reference-free to measure the design quality of generated slides alone. We benchmark end-to-end image generation and program generation methods with a variety of models, and find that programmatic methods produce higher-quality slides in user-interactable formats. Built on the success of program generation, we create AutoPresent, an 8B Llama-based model trained on 7k pairs of instructions paired with code for slide generation, and achieve results comparable to the closed-source model GPT-4o. We further explore iterative design refinement where the model is tasked to self-refine its own output, and we found that this process improves the slide's quality. We hope that our work will provide a basis for future work on generating structured visuals.	 | 设计结构化的视觉元素，如演示文稿幻灯片，对于沟通需求至关重要，既需要内容创作能力，也需要视觉规划的能力。在这项工作中，我们探讨了自动化幻灯片生成的挑战，其中模型能够从自然语言（NL）指令生成幻灯片展示。我们首先介绍了SlidBench基准，这是第一个用于幻灯片生成的基准，包含7000个训练示例和585个测试示例，这些示例来自涵盖10个领域的310个幻灯片文档。SlidBench支持两种评估方式：(i)基于参考的评估，用于衡量生成幻灯片与目标幻灯片的相似度；(ii)非基于参考的评估，用于单独评估生成幻灯片的设计质量。我们使用多种模型对端到端的图像生成方法和程序生成方法进行了基准测试，发现程序生成方法能够生成更高质量的用户可交互格式的幻灯片。基于程序生成的成功，我们创建了AutoPresent模型，这是一个基于Llama的80亿参数模型，训练数据是7000对指令和对应的幻灯片生成代码，其结果与闭源模型GPT-4o相当。我们还进一步探索了迭代设计优化的方法，即让模型自我优化其输出，发现这种方法能提高幻灯片的质量。我们希望我们的工作能够为未来生成结构化视觉元素的研究提供基础。
2501.00895	 | Text2Earth: Unlocking Text-driven Remote Sensing Image Generation with a Global-Scale Dataset and a Foundation Model	 | Chenyang Liu,Keyan Chen,Rui Zhao,Zhengxia Zou,Zhenwei Shi	 | 本文提出了一个包含1000万图像-文本对的全球规模数据集Git-10M和一种基于扩散框架的Text2Earth基础模型，该模型能够生成全球范围内的多分辨率遥感图像，并在多个任务中表现出色，显著优于现有方法。	 | Generative foundation models have advanced large-scale text-driven natural image generation, becoming a prominent research trend across various vertical domains. However, in the remote sensing field, there is still a lack of research on large-scale text-to-image (text2image) generation technology. Existing remote sensing image-text datasets are small in scale and confined to specific geographic areas and scene types. Besides, existing text2image methods have struggled to achieve global-scale, multi-resolution controllable, and unbounded image generation. To address these challenges, this paper presents two key contributions: the Git-10M dataset and the Text2Earth foundation model. Git-10M is a global-scale image-text dataset comprising 10 million image-text pairs, 5 times larger than the previous largest one. The dataset covers a wide range of geographic scenes and contains resolution information, significantly surpassing existing datasets in both size and diversity. Building on Git-10M, we propose Text2Earth, a 1.3 billion parameter generative foundation model based on the diffusion framework to model global-scale remote sensing scenes. Text2Earth integrates a resolution guidance mechanism, enabling users to specify image resolutions. A dynamic condition adaptation strategy is proposed for training and inference to improve image quality. Text2Earth excels in zero-shot text2image generation and demonstrates robust generalization and flexibility across multiple tasks, including unbounded scene construction, image editing, and cross-modal image generation. This robust capability surpasses previous models restricted to the basic fixed size and limited scene types. On the previous benchmark dataset, Text2Earth outperforms previous models with an improvement of +26.23 FID and +20.95% Zero-shot Cls-OA metric.Our project page is \url{https://chen-yang-liu.github.io/Text2Earth}	 | 生成基础模型在大规模文本驱动自然图像生成方面取得了显著进展，成为各个垂直领域的研究热点。然而，在遥感领域，大型文本到图像（text2image）生成技术的研究仍然相对匮乏。现有的遥感图像-文本数据集规模较小，且局限于特定地理区域和场景类型。此外，现有的text2image方法难以实现全球规模、多分辨率可控和无限制的图像生成。为应对这些挑战，本文提出了两项关键贡献：Git-10M数据集和Text2Earth基础模型。Git-10M是一个包含1000万图像-文本对的全球规模图像-文本数据集，规模是之前最大的数据集的5倍。该数据集覆盖了广泛的地理场景，并包含了分辨率信息，显著超越了现有数据集在规模和多样性上的表现。基于Git-10M，我们提出了Text2Earth，这是一种基于扩散框架的13亿参数生成基础模型，用于建模全球规模的遥感场景。Text2Earth集成了分辨率引导机制，允许用户指定图像分辨率。我们提出了一种动态条件适应策略，以改进图像质量。Text2Earth在零样本文本到图像生成方面表现出色，并且在多个任务中展示了稳健的泛化能力和灵活性，包括无限制场景构建、图像编辑和跨模态图像生成。这种稳健能力超越了之前只能应用于基本固定尺寸和有限场景类型的模型。在之前的基准数据集上，Text2Earth的性能比之前的方法提高了26.23分的FID和20.95%的零样本Clsoa指标。我们的项目页面为 \url{https://chen-yang-liu.github.io/Text2Earth}。
2501.00882	 | FullTransNet: Full Transformer with Local-Global Attention for Video Summarization	 | Libin Lan,Lu Jiang,Tianshu Yu,Xiaojuan Liu,Zhongshi He	 | 该研究提出了一种基于全Transformer的视频摘要方法，使用局部和全局稀疏注意力机制来构建FullTransNet架构，从而在保持高效性和准确性的同时生成高质量的视频摘要。实验结果表明，该方法在SumMe和TVSum数据集上取得了优异性能，F值分别为54.4%和63.9%。	 | Video summarization mainly aims to produce a compact, short, informative, and representative synopsis of raw videos, which is of great importance for browsing, analyzing, and understanding video content. Dominant video summarization approaches are generally based on recurrent or convolutional neural networks, even recent encoder-only transformers. We propose using full transformer as an alternative architecture to perform video summarization. The full transformer with an encoder-decoder structure, specifically designed for handling sequence transduction problems, is naturally suitable for video summarization tasks. This work considers supervised video summarization and casts it as a sequence-to-sequence learning problem. Our key idea is to directly apply the full transformer to the video summarization task, which is intuitively sound and effective. Also, considering the efficiency problem, we replace full attention with the combination of local and global sparse attention, which enables modeling long-range dependencies while reducing computational costs. Based on this, we propose a transformer-like architecture, named FullTransNet, which has a full encoder-decoder structure with local-global sparse attention for video summarization. Specifically, both the encoder and decoder in FullTransNet are stacked the same way as ones in the vanilla transformer, and the local-global sparse attention is used only at the encoder side. Extensive experiments on two public multimedia benchmark datasets SumMe and TVSum demonstrate that our proposed model can outperform other video summarization approaches, achieving F-Measures of 54.4% on SumMe and 63.9% on TVSum with relatively lower compute and memory requirements, verifying its effectiveness and efficiency. The code and models are publicly available on GitHub.	 | 视频摘要的主要目标是生成一段紧凑、简短、信息丰富且具有代表性的视频概要，这对于浏览、分析和理解视频内容非常重要。目前主导的视频摘要方法通常基于循环神经网络或卷积神经网络，甚至包括最近的编码器-only transformers。我们提出使用全Transformer作为替代架构来进行视频摘要。全Transformer具有编码器-解码器结构，专门设计用于处理序列转换问题，因此自然适合视频摘要任务。本工作考虑了监督视频摘要问题，并将其视为序列到序列学习问题。我们的主要想法是直接将全Transformer应用于视频摘要任务，这种方法直观且有效。同时，考虑到效率问题，我们用局部和全局稀疏注意力的组合替代了全注意力机制，从而在抑制计算成本的同时能够建模长距离依赖关系。基于此，我们提出了一种类似于Transformer的架构，称为FullTransNet，它具有全编码器-解码器结构，并且采用局部-全局稀疏注意力机制来进行视频摘要。具体而言，FullTransNet 的编码器和解码器堆叠方式与标准Transformer相同，仅在编码器侧使用局部-全局稀疏注意力机制。在两个公开的多媒体基准数据集SumMe和TVSum上的广泛实验表明，我们提出的方法在SumMe上实现了54.4%的F值，在TVSum上实现了63.9%的F值，且具有相对较低的计算和内存需求，验证了其有效性和效率。代码和模型已在GitHub上公开。
2501.00880	 | Improving Autoregressive Visual Generation with Cluster-Oriented Token Prediction	 | Teng Hu,Jiangning Zhang,Ran Yi,Jieyu Weng,Yabiao Wang,Xianfang Zeng,Zhucun Xue,Lizhuang Ma	 | 本文探讨了在大型语言模型（LLM）框架下视觉嵌入空间的特性，并提出了一种改进的自回归视觉生成方法IAR，通过码本重组策略和面向聚类的交叉熵损失提高生成质量和训练效率，实验表明该方法能显著提升各种LLM基视觉生成模型的性能。	 | Employing LLMs for visual generation has recently become a research focus. However, the existing methods primarily transfer the LLM architecture to visual generation but rarely investigate the fundamental differences between language and vision. This oversight may lead to suboptimal utilization of visual generation capabilities within the LLM framework. In this paper, we explore the characteristics of visual embedding space under the LLM framework and discover that the correlation between visual embeddings can help achieve more stable and robust generation results. We present IAR, an Improved AutoRegressive Visual Generation Method that enhances the training efficiency and generation quality of LLM-based visual generation models. Firstly, we propose a Codebook Rearrangement strategy that uses balanced k-means clustering algorithm to rearrange the visual codebook into clusters, ensuring high similarity among visual features within each cluster. Leveraging the rearranged codebook, we propose a Cluster-oriented Cross-entropy Loss that guides the model to correctly predict the cluster where the token is located. This approach ensures that even if the model predicts the wrong token index, there is a high probability the predicted token is located in the correct cluster, which significantly enhances the generation quality and robustness. Extensive experiments demonstrate that our method consistently enhances the model training efficiency and performance from 100M to 1.4B, reducing the training time by half while achieving the same FID. Additionally, our approach can be applied to various LLM-based visual generation models and adheres to the scaling law, providing a promising direction for future research in LLM-based visual generation.	 | 利用大型语言模型（LLM）进行视觉生成最近已成为研究的热点。然而，现有方法主要将LLM架构转移至视觉生成，但很少探讨语言与视觉之间的根本差异。这种忽视可能导致在LLM框架内未能充分发挥视觉生成能力。本文探讨了在LLM框架下的视觉嵌入空间特性，并发现视觉嵌入之间的相关性有助于实现更稳定和更稳健的生成结果。我们提出了一种改进的自回归视觉生成方法IAR（Improved AutoRegressive），以提高LLM基视觉生成模型的训练效率和生成质量。首先，我们提出了一种码本重组策略，使用平衡的K-means聚类算法重新排序视觉码本，确保每个聚类内视觉特征的高度相似性。利用重组后的码本，我们提出了面向聚类的交叉熵损失，引导模型正确预测令牌所在的聚类。这种方法确保即使模型预测了错误的令牌索引，预测的令牌有很大概率位于正确的聚类中，从而显著提高了生成质量和鲁棒性。广泛的实验表明，我们的方法可以一致地提高模型从100M到1.4B的训练效率和性能，将训练时间减半的同时实现相同的FID。此外，我们的方法可以应用于各种LLM基视觉生成模型，并遵循扩展定律，为未来基于LLM的视觉生成研究提供了有前景的方向。
2501.00877	 | FGAseg: Fine-Grained Pixel-Text Alignment for Open-Vocabulary Semantic Segmentation	 | Bingyu Li,Da Zhang,Zhiyuan Zhao,Junyu Gao,Xuelong Li	 | FGAseg模型通过实现细粒度的像素-文本对齐和类别边界补充，解决了开放词汇分割任务中的关键挑战，尤其是在提供像素级对齐和详细类别边界信息方面优于现有方法。该模型采用跨模态注意机制和文本-像素对齐损失来细化来自CLIP的粗粒度对齐，并通过优化伪掩码来补充类别边界信息，从而在开放词汇语义分割基准测试中表现出色。	 | Open-vocabulary segmentation aims to identify and segment specific regions and objects based on text-based descriptions. A common solution is to leverage powerful vision-language models (VLMs), such as CLIP, to bridge the gap between vision and text information. However, VLMs are typically pretrained for image-level vision-text alignment, focusing on global semantic features. In contrast, segmentation tasks require fine-grained pixel-level alignment and detailed category boundary information, which VLMs alone cannot provide. As a result, information extracted directly from VLMs can't meet the requirements of segmentation tasks. To address this limitation, we propose FGAseg, a model designed for fine-grained pixel-text alignment and category boundary supplementation. The core of FGAseg is a Pixel-Level Alignment module that employs a cross-modal attention mechanism and a text-pixel alignment loss to refine the coarse-grained alignment from CLIP, achieving finer-grained pixel-text semantic alignment. Additionally, to enrich category boundary information, we introduce the alignment matrices as optimizable pseudo-masks during forward propagation and propose Category Information Supplementation module. These pseudo-masks, derived from cosine and convolutional similarity, provide essential global and local boundary information between different categories. By combining these two strategies, FGAseg effectively enhances pixel-level alignment and category boundary information, addressing key challenges in open-vocabulary segmentation. Extensive experiments demonstrate that FGAseg outperforms existing methods on open-vocabulary semantic segmentation benchmarks.	 | 开放词汇分割的目标是基于文本描述识别并分割特定区域和对象。一种常见的解决方案是利用强大的视觉-语言模型（VLMs），如CLIP，来弥合视觉与文本信息之间的差距。然而，这些VLMs通常是在图像级别的视觉-文本对齐中进行预训练，主要关注全局语义特征。相比之下，分割任务需要细粒度的像素级对齐和详细的类别边界信息，而仅凭VLMs无法提供这些信息。因此，直接从VLMs中提取的信息不能满足分割任务的要求。为了解决这一局限性，我们提出了FGAseg模型，该模型旨在实现细粒度的像素-文本对齐和类别边界补充。FGAseg的核心是一个像素级对齐模块，该模块采用了跨模态注意机制和文本-像素对齐损失，以细化来自CLIP的粗粒度对齐，从而实现更细粒度的像素-文本语义对齐。此外，为丰富类别边界信息，我们引入了在前向传播过程中可优化的伪掩码，并提出了类别信息补充模块。这些伪掩码来源于余弦相似度和卷积相似度，提供了不同类别之间重要的全局和局部边界信息。通过结合这两种策略，FGAseg有效提升了像素级对齐和类别边界信息，解决了开放词汇分割中的关键挑战。大量实验表明，FGAseg在开放词汇语义分割基准测试中优于现有方法。
2501.00873	 | Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation	 | Mingjia Li,Shuang Li,Tongrui Su,Longhui Yuan,Jian Liang,Wei Li	 | 该研究揭示了分数生成模型中的语义结构，并提出了一种名为DUSA的方法，利用这些结构化的语义先验来促进图像分类器或密集预测器的测试时适应，同时仅从去噪扩散的一个时间步提取知识，提高了适应效率和性能。	 | Capitalizing on the complementary advantages of generative and discriminative models has always been a compelling vision in machine learning, backed by a growing body of research. This work discloses the hidden semantic structure within score-based generative models, unveiling their potential as effective discriminative priors. Inspired by our theoretical findings, we propose DUSA to exploit the structured semantic priors underlying diffusion score to facilitate the test-time adaptation of image classifiers or dense predictors. Notably, DUSA extracts knowledge from a single timestep of denoising diffusion, lifting the curse of Monte Carlo-based likelihood estimation over timesteps. We demonstrate the efficacy of our DUSA in adapting a wide variety of competitive pre-trained discriminative models on diverse test-time scenarios. Additionally, a thorough ablation study is conducted to dissect the pivotal elements in DUSA. Code is publicly available at https://github.com/BIT-DA/DUSA.	 | 利用生成模型和判别模型互补优势一直是机器学习中的一个极具吸引力的愿景，得到了越来越多研究的支持。本研究揭示了分数生成模型内部隐藏的语义结构，展示了它们作为有效判别先验的潜力。受我们理论发现的启发，我们提出了一种DUSA方法，旨在利用分数扩散下的结构化语义先验来促进图像分类器或密集预测器的测试时适应。值得注意的是，DUSA 仅从去噪扩散的一个时间步提取知识，克服了基于蒙特卡洛方法的时间步似然估计的局限。我们展示了DUSA在多种测试场景下对各种高性能预训练判别模型进行适应的有效性。此外，我们还进行了详细的消融研究，以剖析DUSA中的关键组成部分。代码已公开，可在 https://github.com/BIT-DA/DUSA 获取。
2501.00851	 | Scale-wise Bidirectional Alignment Network for Referring Remote Sensing Image Segmentation	 | Kun Li,George Vosselman,Michael Ying Yang	 | 本文提出了一种名为分尺度双向对齐网络（SBANet）的新框架，该框架通过双向对齐模块（BAM）和特殊的特征选择机制来改进遥感图像分割中的跨模态融合，从而提高在复杂航空场景中的视觉感知能力和跨尺度信息交换，特别是在RRSIS-D和RefSegRS数据集上取得了优于现有方法的性能。	 | The goal of referring remote sensing image segmentation (RRSIS) is to extract specific pixel-level regions within an aerial image via a natural language expression. Recent advancements, particularly Transformer-based fusion designs, have demonstrated remarkable progress in this domain. However, existing methods primarily focus on refining visual features using language-aware guidance during the cross-modal fusion stage, neglecting the complementary vision-to-language flow. This limitation often leads to irrelevant or suboptimal representations. In addition, the diverse spatial scales of ground objects in aerial images pose significant challenges to the visual perception capabilities of existing models when conditioned on textual inputs. In this paper, we propose an innovative framework called Scale-wise Bidirectional Alignment Network (SBANet) to address these challenges for RRSIS. Specifically, we design a Bidirectional Alignment Module (BAM) with learnable query tokens to selectively and effectively represent visual and linguistic features, emphasizing regions associated with key tokens. BAM is further enhanced with a dynamic feature selection block, designed to provide both macro- and micro-level visual features, preserving global context and local details to facilitate more effective cross-modal interaction. Furthermore, SBANet incorporates a text-conditioned channel and spatial aggregator to bridge the gap between the encoder and decoder, enhancing cross-scale information exchange in complex aerial scenarios. Extensive experiments demonstrate that our proposed method achieves superior performance in comparison to previous state-of-the-art methods on the RRSIS-D and RefSegRS datasets, both quantitatively and qualitatively. The code will be released after publication.	 | 遥感图像分割中的引用（Referring Remote Sensing Image Segmentation, RRSIS）的目标是通过自然语言表达在航空图像中提取特定的像素级别区域。近期的进步，特别是基于Transformer的跨模态融合设计，已经在该领域取得了显著的进展。然而，现有的方法主要集中在视觉特征的精炼中，利用语言意识的指导进行跨模态融合，忽略了视觉到语言的互补流程。这种限制通常会导致无关或次优的表示。此外，航空图像中地面对象的多样空间尺度给现有模型在文本输入条件下的视觉感知能力带来了巨大的挑战。在本文中，我们提出了一种创新框架，称为分尺度双向对齐网络（Scale-wise Bidirectional Alignment Network, SBANet），以解决RRSIS中的这些挑战。具体而言，我们设计了一个具有可学习查询令牌的双向对齐模块（Bidirectional Alignment Module, BAM），以选择性地和有效地表示视觉和语言特征，并强调与关键令牌相关联的区域。BAM 进一步增强了动态特征选择块，旨在提供宏观和微观级别的视觉特征，保持全局上下文和局部细节，以促进更有效的跨模态交互。此外，SBANet 还结合了文本条件的通道和空间聚合器，以在编码器和解码器之间缩小差距，增强复杂航空场景中的跨尺度信息交换。广泛的实验表明，我们提出的方法在RRSIS-D和RefSegRS数据集上都优于之前的最先进的方法，在定量和定性方面都取得了更好的性能。代码将在发表后发布。
2501.00848	 | IllusionBench: A Large-scale and Comprehensive Benchmark for Visual Illusion Understanding in Vision-Language Models	 | Yiming Zhang,Zicheng Zhang,Xinyi Wei,Xiaohong Liu,Guangtao Zhai,Xiongkuo Min	 | 本文介绍了IllusionBench，这是一个涵盖经典和现实生活场景中视觉错觉的全面数据集，用于评估视觉语言模型在处理视觉错觉方面的性能，结果显示尽管一些先进模型在某些任务上表现良好，但在语义描述任务中仍存在显著的幻觉问题。	 | Current Visual Language Models (VLMs) show impressive image understanding but struggle with visual illusions, especially in real-world scenarios. Existing benchmarks focus on classical cognitive illusions, which have been learned by state-of-the-art (SOTA) VLMs, revealing issues such as hallucinations and limited perceptual abilities. To address this gap, we introduce IllusionBench, a comprehensive visual illusion dataset that encompasses not only classic cognitive illusions but also real-world scene illusions. This dataset features 1,051 images, 5,548 question-answer pairs, and 1,051 golden text descriptions that address the presence, causes, and content of the illusions. We evaluate ten SOTA VLMs on this dataset using true-or-false, multiple-choice, and open-ended tasks. In addition to real-world illusions, we design trap illusions that resemble classical patterns but differ in reality, highlighting hallucination issues in SOTA models. The top-performing model, GPT-4o, achieves 80.59% accuracy on true-or-false tasks and 76.75% on multiple-choice questions, but still lags behind human performance. In the semantic description task, GPT-4o's hallucinations on classical illusions result in low scores for trap illusions, even falling behind some open-source models. IllusionBench is, to the best of our knowledge, the largest and most comprehensive benchmark for visual illusions in VLMs to date.	 | 当前的视觉语言模型（VLMs）在图像理解方面表现出色，但在处理视觉错觉方面遇到困难，尤其是在真实世界场景中。现有的基准测试主要关注经典的认知错觉，而这些错觉已经被最先进的（SOTA）视觉语言模型所学习，揭示了诸如幻觉和感知能力有限等问题。为了解决这一差距，我们引入了IllusionBench，这是一个全面的视觉错觉数据集，不仅涵盖了经典的认知错觉，还涵盖了现实生活中的场景错觉。该数据集包含1,051张图片、5,548个问答对以及1,051个金色文本描述，这些描述解释了错觉的存在、原因和内容。我们使用真假判断、多项选择和开放性任务来评估十个SOTA视觉语言模型。除了现实生活中的错觉，我们还设计了陷阱错觉，这些错觉在表面上与经典模式相似，但在现实中却不同，这突显了SOTA模型中的幻觉问题。表现最好的模型GPT-4o在真假判断任务中获得了80.59%的准确率，在多项选择问题中获得了76.75%的准确率，但仍落后于人类的表现。在语义描述任务中，GPT-4o在经典错觉上的幻觉导致了陷阱错觉的低得分，甚至落后于一些开源模型。据我们所知，IllusionBench是迄今为止视觉语言模型中最大的和最全面的视觉错觉基准测试。
2501.00843	 | FusionSORT: Fusion Methods for Online Multi-object Visual Tracking	 | Nathanael L. Baisa	 | 本文研究了四种不同的融合方法（基于最小值、基于交并比的加权求和、卡尔曼滤波门限以及线索成本的哈达玛乘积）在多目标视觉跟踪中的轨迹段关联，并发现这些方法在MOT17、MOT20和DanceTrack数据集上的表现表明，数据关联的好坏对整个跟踪性能至关重要。	 | In this work, we investigate four different fusion methods for associating detections to tracklets in multi-object visual tracking. In addition to considering strong cues such as motion and appearance information, we also consider weak cues such as height intersection-over-union (height-IoU) and tracklet confidence information in the data association using different fusion methods. These fusion methods include minimum, weighted sum based on IoU, Kalman filter (KF) gating, and hadamard product of costs due to the different cues. We conduct extensive evaluations on validation sets of MOT17, MOT20 and DanceTrack datasets, and find out that the choice of a fusion method is key for data association in multi-object visual tracking. We hope that this investigative work helps the computer vision research community to use the right fusion method for data association in multi-object visual tracking.	 | 在本工作中，我们研究了在多目标视觉跟踪中将检测结果与轨迹段关联的四种不同融合方法。除了考虑运动和外观等强线索外，我们还在数据关联中考虑了高度交并比（高度-IoU）和轨迹段置信度等弱线索。这些融合方法包括基于最小值、基于交并比的加权求和、卡尔曼滤波（KF）门限以及不同线索引起的成本的哈达玛乘积。我们在MOT17、MOT20和DanceTrack数据集的验证集上进行了广泛评估，并发现融合方法的选择对多目标视觉跟踪中的数据关联至关重要。我们希望这项研究工作能够帮助计算机视觉研究社区正确选择适用于多目标视觉跟踪中数据关联的融合方法。
2501.00838	 | Spatially-guided Temporal Aggregation for Robust Event-RGB Optical Flow Estimation	 | Qianang Zhou,Junhui Hou,Meiyi Yang,Yongjian Deng,Youfu Li,Junlin Xiong	 | 该研究提出了一种创新的跨模态光流估计方法，利用帧数据的丰富纹理和事件相机的时间分辨率，通过事件增强的帧表示和基于变压器的模块实现有效的信息融合，从而在多个数据集上取得了优越的性能，并在推理速度上有所提升。	 | Current optical flow methods exploit the stable appearance of frame (or RGB) data to establish robust correspondences across time. Event cameras, on the other hand, provide high-temporal-resolution motion cues and excel in challenging scenarios. These complementary characteristics underscore the potential of integrating frame and event data for optical flow estimation. However, most cross-modal approaches fail to fully utilize the complementary advantages, relying instead on simply stacking information. This study introduces a novel approach that uses a spatially dense modality to guide the aggregation of the temporally dense event modality, achieving effective cross-modal fusion. Specifically, we propose an event-enhanced frame representation that preserves the rich texture of frames and the basic structure of events. We use the enhanced representation as the guiding modality and employ events to capture temporally dense motion information. The robust motion features derived from the guiding modality direct the aggregation of motion information from events. To further enhance fusion, we propose a transformer-based module that complements sparse event motion features with spatially rich frame information and enhances global information propagation. Additionally, a mix-fusion encoder is designed to extract comprehensive spatiotemporal contextual features from both modalities. Extensive experiments on the MVSEC and DSEC-Flow datasets demonstrate the effectiveness of our framework. Leveraging the complementary strengths of frames and events, our method achieves leading performance on the DSEC-Flow dataset. Compared to the event-only model, frame guidance improves accuracy by 10\%. Furthermore, it outperforms the state-of-the-art fusion-based method with a 4\% accuracy gain and a 45\% reduction in inference time.	 | 当前的光流方法利用帧（或RGB）数据的稳定外观来建立跨时间的稳健对应关系。相比之下，事件相机提供高时间分辨率的运动线索，并在具有挑战性的场景中表现出色。这两种互补特性表明，将帧数据和事件数据集成用于光流估计具有巨大的潜力。然而，大多数跨模态方法未能充分利用这种互补优势，而是简单地堆叠信息。本研究提出了一种新型方法，利用空间密集的模态来引导密集时间模态的聚合，从而实现有效的跨模态融合。具体来说，我们提出了一种事件增强的帧表示，该表示保留了帧的丰富纹理和事件的基本结构。我们使用增强表示作为引导模态，并利用事件捕捉密集的时间运动信息。从引导模态中提取的稳健运动特征指导事件信息的聚合。为了进一步增强融合，我们提出了一种基于变压器的模块，该模块用丰富的空间帧信息补充稀疏的事件运动特征，并增强全局信息传播。此外，我们设计了一种混合融合编码器，从两种模态中提取全面的空间时间上下文特征。在MVSEC和DSEC-Flow数据集上的广泛实验表明了我们框架的有效性。利用帧和事件的互补优势，我们的方法在DSEC-Flow数据集上达到了领先的性能。与仅事件模型相比，帧引导提高了10%的准确性。此外，与最先进的融合方法相比，它提高了4%的准确性，将推理时间减少了45%。
2501.00836	 | Recognizing Artistic Style of Archaeological Image Fragments Using Deep Style Extrapolation	 | Gur Elkin,Ofir Itzhak Shahar,Yaniv Ohayon,Nadav Alali,Ohad Ben-Shahar	 | 该研究提出了一个通用的深度学习框架，用于预测考古发掘中古代艺术品碎片的艺术风格，该框架在不同风格和几何形状的碎片上达到了最先进的性能。	 | Ancient artworks obtained in archaeological excavations usually suffer from a certain degree of fragmentation and physical degradation. Often, fragments of multiple artifacts from different periods or artistic styles could be found on the same site. With each fragment containing only partial information about its source, and pieces from different objects being mixed, categorizing broken artifacts based on their visual cues could be a challenging task, even for professionals. As classification is a common function of many machine learning models, the power of modern architectures can be harnessed for efficient and accurate fragment classification. In this work, we present a generalized deep-learning framework for predicting the artistic style of image fragments, achieving state-of-the-art results for pieces with varying styles and geometries.	 | 考古发掘中获得的古代艺术品通常会遭受一定程度的碎片化和物理退化。通常，同一遗址上可能会发现来自不同时期或不同艺术风格的多个碎片。由于每个碎片仅包含其来源的部分信息，而不同物件的碎片被混合在一起，根据视觉线索对这些碎片进行分类可能是一项具有挑战性的任务，即使是专业人士也不例外。由于分类是许多机器学习模型的常见功能之一，现代架构的力量可以被利用来实现高效的、精准的碎片分类。在本研究中，我们提出了一种通用的深度学习框架，用于预测图像碎片的艺术风格，并且该框架在不同风格和几何形状的碎片上达到了最先进的性能。
2501.00818	 | SPARNet: Continual Test-Time Adaptation via Sample Partitioning Strategy and Anti-Forgetting Regularization	 | Xinru Meng,Han Sun,Jiamei Liu,Ningzhong Liu,Huiyu Zhou	 | 该研究提出了一种名为SPARNet的新框架，旨在解决模型在持续领域变化下的适应问题，通过样本分区策略和抗遗忘正则化来确保可靠样本的贡献并减轻灾难性遗忘。实验结果显示，SPARNet在CIFAR10-C、CIFAR100-C和ImageNet-C上的表现优于现有方法。	 | Test-time Adaptation (TTA) aims to improve model performance when the model encounters domain changes after deployment. The standard TTA mainly considers the case where the target domain is static, while the continual TTA needs to undergo a sequence of domain changes. This encounters a significant challenge as the model needs to adapt for the long-term and is unaware of when the domain changes occur. The quality of pseudo-labels is hard to guarantee. Noisy pseudo-labels produced by simple self-training methods can cause error accumulation and catastrophic forgetting. In this work, we propose a new framework named SPARNet which consists of two parts, sample partitioning strategy and anti-forgetting regularization. The sample partition strategy divides samples into two groups, namely reliable samples and unreliable samples. According to the characteristics of each group of samples, we choose different strategies to deal with different groups of samples. This ensures that reliable samples contribute more to the model. At the same time, the negative impacts of unreliable samples are eliminated by the mean teacher's consistency learning. Finally, we introduce a regularization term to alleviate the catastrophic forgetting problem, which can limit important parameters from excessive changes. This term enables long-term adaptation of parameters in the network. The effectiveness of our method is demonstrated in continual TTA scenario by conducting a large number of experiments on CIFAR10-C, CIFAR100-C and ImageNet-C.	 | 测试时适应（Test-time Adaptation, TTA）旨在提高模型在部署后遇到领域变化时的表现。标准的TTA主要考虑目标领域是静态的情况，而持续的TTA则需要应对一系列的领域变化。这带来了显著的挑战，因为模型需要长期适应，并且无法预知领域变化何时发生。伪标签的质量难以保证。简单的自我训练方法生成的嘈杂伪标签会导致错误累积和灾难性遗忘。在这项研究中，我们提出了一种名为SPARNet的新框架，它包含两个部分：样本分区策略和抗遗忘正则化。样本分区策略将样本分为两类，即可靠样本和不可靠样本。根据每组样本的特点，我们选择不同的策略来处理不同的样本组。这确保了可靠样本对模型的贡献更多。同时，通过均值教师的一致性学习，可以消除不可靠样本的负面影响。最后，我们引入一个正则化项来缓解灾难性遗忘问题，该项可以限制重要参数的过度变化。这使得网络参数能够在长期内进行适应。我们通过在CIFAR10-C、CIFAR100-C和ImageNet-C上进行大量实验，展示了我们方法在持续TTA场景中的有效性。
2501.00816	 | MixSA: Training-free Reference-based Sketch Extraction via Mixture-of-Self-Attention	 | Rui Yang,Xiaojun Wu,Shengfeng He	 | Mixture-of-Self-Attention（MixSA）是一种无需训练的素描提取方法，利用混合自注意力技术通过参考素描的键和值来增强笔触元素，从而实现对纹理密度的精确控制和不同风格之间的插值，提升了素描提取的质量、灵活性和适用性。	 | Current sketch extraction methods either require extensive training or fail to capture a wide range of artistic styles, limiting their practical applicability and versatility. We introduce Mixture-of-Self-Attention (MixSA), a training-free sketch extraction method that leverages strong diffusion priors for enhanced sketch perception. At its core, MixSA employs a mixture-of-self-attention technique, which manipulates self-attention layers by substituting the keys and values with those from reference sketches. This allows for the seamless integration of brushstroke elements into initial outline images, offering precise control over texture density and enabling interpolation between styles to create novel, unseen styles. By aligning brushstroke styles with the texture and contours of colored images, particularly in late decoder layers handling local textures, MixSA addresses the common issue of color averaging by adjusting initial outlines. Evaluated with various perceptual metrics, MixSA demonstrates superior performance in sketch quality, flexibility, and applicability. This approach not only overcomes the limitations of existing methods but also empowers users to generate diverse, high-fidelity sketches that more accurately reflect a wide range of artistic expressions.	 | 当前的素描提取方法要么需要大量的训练，要么无法捕捉广泛的艺术风格，限制了它们的实际应用性和灵活性。我们提出了Mixture-of-Self-Attention（MixSA），这是一种无需训练的素描提取方法，利用强大的扩散先验提高素描感知能力。MixSA的核心在于采用混合自注意力技术，通过将自注意力层的键和值替换为参考素描的键和值，使得笔触元素能够无缝集成到初始轮廓图像中，从而实现对纹理密度的精确控制，并能够在不同风格之间进行插值，生成新的未见风格。  具体而言，MixSA 在后期解码层处理局部纹理时，通过调整笔触样式与彩色图像的纹理和轮廓对齐，解决了常见的颜色均值化问题，从而优化了初始轮廓。通过各种感知度量进行评估，MixSA 在素描质量、灵活性和适用性方面表现出色。这种方法不仅克服了现有方法的局限性，还赋予用户生成多样且高保真度素描的能力，更准确地反映了广泛的艺术表达。
2501.00811	 | Regression Guided Strategy to Automated Facial Beauty Optimization through Image Synthesis	 | Erik Nguyen,Spencer Htin	 | 本文提出了一种新的美颜方法，通过将面部图像投影到预训练的生成对抗网络的潜在空间并进行优化，利用一个新开发的面部美丽评估回归网络来引导优化过程，从而实现动态和广泛的应用。这种方法提供了一种不同于传统基于规则方法的新方向，作为现有技术的一种补充。	 | The use of beauty filters on social media, which enhance the appearance of individuals in images, is a well-researched area, with existing methods proving to be highly effective. Traditionally, such enhancements are performed using rule-based approaches that leverage domain knowledge of facial features associated with attractiveness, applying very specific transformations to maximize these attributes. In this work, we present an alternative approach that projects facial images as points on the latent space of a pre-trained GAN, which are then optimized to produce beautiful faces. The movement of the latent points is guided by a newly developed facial beauty evaluation regression network, which learns to distinguish attractive facial features, outperforming many existing facial beauty evaluation models in this domain. By using this data-driven approach, our method can automatically capture holistic patterns in beauty directly from data rather than relying on predefined rules, enabling more dynamic and potentially broader applications of facial beauty editing. This work demonstrates a potential new direction for automated aesthetic enhancement, offering a complementary alternative to existing methods.	 | 在社交媒体上使用的美颜滤镜能够增强图像中个人的外观，这是一个研究广泛的话题，现有方法已经证明非常有效。传统上，这些增强是通过基于规则的方法实现的，这些方法利用与吸引力相关的面部特征领域的知识，应用非常具体的变化以最大化这些特征。在本项工作中，我们提出了一种替代方法，该方法将面部图像投影到预训练的生成对抗网络（GAN）的潜在空间中，并对其进行优化以生成美丽的面部。潜在点的移动受到一种新开发的面部美丽评估回归网络的引导，该网络学会了区分具有吸引力的面部特征，在此领域中的表现优于许多现有的面部美丽评估模型。通过使用数据驱动的方法，我们的方法可以从数据中自动捕捉整体的美丽模式，而不是依赖于预定义的规则，从而使得面部美丽编辑具有更动态和更广泛的应用潜力。这项工作展示了自动美学增强的一个潜在新方向，提供了对现有方法的一种补充替代方案。
2501.00795	 | Multimodal Large Models Are Effective Action Anticipators	 | Binglu Wang,Yao Tian,Shunzhou Wang,Le Yang	 | 本文提出了一种名为ActionLLM的新框架，将视频序列视为令牌并利用大型语言模型进行未来动作预测，通过简化LLM架构和引入跨模态交互块来提高预测精度。	 | The task of long-term action anticipation demands solutions that can effectively model temporal dynamics over extended periods while deeply understanding the inherent semantics of actions. Traditional approaches, which primarily rely on recurrent units or Transformer layers to capture long-term dependencies, often fall short in addressing these challenges. Large Language Models (LLMs), with their robust sequential modeling capabilities and extensive commonsense knowledge, present new opportunities for long-term action anticipation. In this work, we introduce the ActionLLM framework, a novel approach that treats video sequences as successive tokens, leveraging LLMs to anticipate future actions. Our baseline model simplifies the LLM architecture by setting future tokens, incorporating an action tuning module, and reducing the textual decoder layer to a linear layer, enabling straightforward action prediction without the need for complex instructions or redundant descriptions. To further harness the commonsense reasoning of LLMs, we predict action categories for observed frames and use sequential textual clues to guide semantic understanding. In addition, we introduce a Cross-Modality Interaction Block, designed to explore the specificity within each modality and capture interactions between vision and textual modalities, thereby enhancing multimodal tuning. Extensive experiments on benchmark datasets demonstrate the superiority of the proposed ActionLLM framework, encouraging a promising direction to explore LLMs in the context of action anticipation. Code is available at https://github.com/2tianyao1/ActionLLM.git.	 | 长时动作预测的任务需要能够有效地在长时间段内建模时间动态并深刻理解动作内在语义的解决方案。传统的做法主要依赖于循环单元或Transformer层来捕捉长期依赖关系，但在处理这些挑战时往往不尽如人意。大型语言模型（LLMs）凭借其强大的序列建模能力和丰富的常识知识，为长时动作预测提供了新的机遇。在本文中，我们提出了ActionLLM框架，这是一种新型方法，将视频序列视为连续的令牌，并利用LLMs来预测未来的动作。我们的基础模型简化了LLM架构，通过设置未来的令牌、引入动作调谐模块以及将文本解码层减少为线性层，从而不需复杂指令或冗余描述即可实现简单的动作预测。为进一步利用LLMs的常识推理能力，我们为观测帧预测动作类别，并使用序列文本线索来引导语义理解。此外，我们引入了跨模态交互块，旨在探索每种模态的特定性并捕捉视觉模态与文本模态之间的交互，从而增强多模态调谐。在基准数据集上的大量实验表明，提出的ActionLLM框架具有优越性，鼓励探索LLMs在动作预测中的潜力。代码可在https://github.com/2tianyao1/ActionLLM.git获取。
2501.00765	 | Beyond Words: AuralLLM and SignMST-C for Precise Sign Language Production and Bidirectional Accessibility	 | Yulong Li,Yuxuan Zhang,Feilong Tang,Mian Zhou,Zhixiang Lu,Haochen Xue,Yifang Wang,Kang Dang,Jionglong Su	 | 该研究强调了开发高级手语生成和翻译系统的必要性，并介绍了两个综合数据集CNText2Sign和CNSign，以及两种先进的模型AuraLLM和SignMST-C，以提高手语系统的准确性和适用性。	 | Although sign language recognition aids non-hearing-impaired understanding, many hearing-impaired individuals still rely on sign language alone due to limited literacy, underscoring the need for advanced sign language production and translation (SLP and SLT) systems. In the field of sign language production, the lack of adequate models and datasets restricts practical applications. Existing models face challenges in production accuracy and pose control, making it difficult to provide fluent sign language expressions across diverse scenarios. Additionally, data resources are scarce, particularly high-quality datasets with complete sign vocabulary and pose annotations. To address these issues, we introduce CNText2Sign and CNSign, comprehensive datasets to benchmark SLP and SLT, respectively, with CNText2Sign covering gloss and landmark mappings for SLP, and CNSign providing extensive video-to-text data for SLT. To improve the accuracy and applicability of sign language systems, we propose the AuraLLM and SignMST-C models. AuraLLM, incorporating LoRA and RAG techniques, achieves a BLEU-4 score of 50.41 on the CNText2Sign dataset, enabling precise control over gesture semantics and motion. SignMST-C employs self-supervised rapid motion video pretraining, achieving a BLEU-4 score of 31.03/32.08 on the PHOENIX2014-T benchmark, setting a new state-of-the-art. These models establish robust baselines for the datasets released for their respective tasks.	 | 尽管手语识别有助于非听力障碍人士的理解，但由于听障人士的阅读能力有限，许多听障人士仍主要依赖手语表达，凸显了开发高级手语生成和翻译（SLP和SLT）系统的必要性。在手语生成领域，缺乏足够的模型和数据集限制了其实用应用。现有的模型在生成准确性和动作控制方面面临挑战，使得在多种场景下提供流畅的手语表达变得困难。此外，数据资源稀缺，特别是那些包含完整手语词汇和姿势注释的高质量数据集。为了解决这些问题，我们介绍了CNText2Sign和CNSign两个综合数据集，分别用于评估SLP和SLT。CNText2Sign涵盖了SLP所需的词义和关键点映射，而CNSign提供了丰富的视频到文本数据用于SLT。为了提高手语系统的准确性和适用性，我们提出了AuraLLM和SignMST-C两种模型。AuraLLM结合了LoRA和RAG技术，在CNText2Sign数据集上的BLEU-4得分为50.41，实现了对手势语义和动作的高度控制。SignMST-C采用了自我监督的快速运动视频预训练方法，在PHOENIX2014-T基准测试上的BLEU-4得分为31.03/32.08，创下了新的最先进水平。这些模型为各自任务发布的数据集提供了稳健的基础。
2501.00758	 | Less is More: Token Context-aware Learning for Object Tracking	 | Chenlong Xu,Bineng Zhong,Qihua Liang,Yaozong Zheng,Guorong Li,Shuxiang Song	 | 本文提出了一种新的标记上下文感知跟踪框架LMTrack，通过分析参考标记的重要性分布并收集重要标记来提高跟踪性能。LMTrack设计了标记上下文记忆模块和单向标记注意力机制，以动态收集目标的时空信息并建立有效的跨帧关联，从而在多个跟踪基准测试中取得了最先进的结果。	 | Recently, several studies have shown that utilizing contextual information to perceive target states is crucial for object tracking. They typically capture context by incorporating multiple video frames. However, these naive frame-context methods fail to consider the importance of each patch within a reference frame, making them susceptible to noise and redundant tokens, which deteriorates tracking performance. To address this challenge, we propose a new token context-aware tracking pipeline named LMTrack, designed to automatically learn high-quality reference tokens for efficient visual tracking. Embracing the principle of Less is More, the core idea of LMTrack is to analyze the importance distribution of all reference tokens, where important tokens are collected, continually attended to, and updated. Specifically, a novel Token Context Memory module is designed to dynamically collect high-quality spatio-temporal information of a target in an autoregressive manner, eliminating redundant background tokens from the reference frames. Furthermore, an effective Unidirectional Token Attention mechanism is designed to establish dependencies between reference tokens and search frame, enabling robust cross-frame association and target localization. Extensive experiments demonstrate the superiority of our tracker, achieving state-of-the-art results on tracking benchmarks such as GOT-10K, TrackingNet, and LaSOT.	 | 近年来，多项研究表明，在利用上下文信息感知目标状态方面，这对于物体跟踪至关重要。这些研究通常通过结合多个视频帧来捕获上下文信息。然而，这些简单的帧上下文方法未能考虑到参考帧中每个片段的重要性，因此容易受到噪声和冗余标记的影响，从而降低跟踪性能。为了解决这一挑战，我们提出了一种新的标记上下文感知跟踪框架LMTrack，旨在自动学习高质量的参考标记以实现高效的视觉跟踪。秉持“少即是多”的原则，LMTrack的核心思想是对所有参考标记的重要性分布进行分析，收集重要标记并持续关注和更新。具体而言，设计了一个新颖的标记上下文记忆模块，以自回归方式动态收集目标的时空信息，从而消除参考帧中的冗余背景标记。此外，还设计了一种有效的单向标记注意力机制，用于建立参考标记与搜索帧之间的依赖关系，从而实现稳健的跨帧关联和目标定位。实验结果表明，我们的跟踪器具有优越性，在如GOT-10K、TrackingNet和LaSOT等跟踪基准测试中取得了最先进的结果。
2501.00752	 | Foreground-Covering Prototype Generation and Matching for SAM-Aided Few-Shot Segmentation	 | Suho Park,SuBeen Lee,Hyun Seok Seong,Jaejoon Yoo,Jae-Pil Heo	 | 该研究提出了一种前景覆盖原型生成与匹配方法，通过结合SAM图像编码器特征和ResNet特征，解决少样本分割问题，生成类别一致的查询原型，并通过SAM Mask解码器生成对象掩码，展示了在多种数据集上的优越性能。	 | We propose Foreground-Covering Prototype Generation and Matching to resolve Few-Shot Segmentation (FSS), which aims to segment target regions in unlabeled query images based on labeled support images. Unlike previous research, which typically estimates target regions in the query using support prototypes and query pixels, we utilize the relationship between support and query prototypes. To achieve this, we utilize two complementary features: SAM Image Encoder features for pixel aggregation and ResNet features for class consistency. Specifically, we construct support and query prototypes with SAM features and distinguish query prototypes of target regions based on ResNet features. For the query prototype construction, we begin by roughly guiding foreground regions within SAM features using the conventional pseudo-mask, then employ iterative cross-attention to aggregate foreground features into learnable tokens. Here, we discover that the cross-attention weights can effectively alternate the conventional pseudo-mask. Therefore, we use the attention-based pseudo-mask to guide ResNet features to focus on the foreground, then infuse the guided ResNet feature into the learnable tokens to generate class-consistent query prototypes. The generation of the support prototype is conducted symmetrically to that of the query one, with the pseudo-mask replaced by the ground-truth mask. Finally, we compare these query prototypes with support ones to generate prompts, which subsequently produce object masks through the SAM Mask Decoder. Our state-of-the-art performances on various datasets validate the effectiveness of the proposed method for FSS. Our official code is available at https://github.com/SuhoPark0706/FCP	 | 我们提出了一种前景覆盖原型生成与匹配方法来解决少样本分割（Few-Shot Segmentation, FSS）问题，该问题旨在基于标记的支持图像对未标记的查询图像中的目标区域进行分割。与以往研究通常使用支持原型和查询像素来估计查询的目标区域不同，我们利用了支持原型与查询原型之间的关系。为了实现这一目标，我们利用了两个互补的特性：使用SAM图像编码器特征进行像素聚合，以及使用ResNet特征保持类别一致性。具体而言，我们利用SAM特征构建支持原型和查询原型，并根据ResNet特征区分目标区域的查询原型。在构建查询原型时，我们首先使用传统的伪掩码粗略地引导SAM特征中的前景区域，然后使用迭代交叉注意力将前景特征聚合到可学习的 tokens 中。我们发现，交叉注意力权重能够有效地替代传统的伪掩码。因此，我们利用基于注意力的伪掩码来引导ResNet特征聚焦于前景区域，然后将引导后的ResNet特征注入到可学习的tokens 中，生成类别一致的查询原型。支持原型的生成是对称进行的，伪掩码被真实掩码所替代。最后，我们通过SAM Mask解码器将这些查询原型与支持原型进行比较，生成提示信息，从而生成对象掩码。我们在多种数据集上的最新表现验证了所提出方法的有效性。我们的官方代码可以在 https://github.com/SuhoPark0706/FCP 中获取。
2501.00741	 | Towards End-to-End Neuromorphic Voxel-based 3D Object Reconstruction Without Physical Priors	 | Chuanzhi Xu,Langyi Chen,Vincent Qu,Haodong Chen,Vera Chung	 | 本文提出了一种端到端的方法，利用神经形态相机进行稠密体素3D重建，通过新颖的事件表示增强边缘特征，提升了重建精度，比基准方法提高了54.6%。	 | Neuromorphic cameras, also known as event cameras, are asynchronous brightness-change sensors that can capture extremely fast motion without suffering from motion blur, making them particularly promising for 3D reconstruction in extreme environments. However, existing research on 3D reconstruction using monocular neuromorphic cameras is limited, and most of the methods rely on estimating physical priors and employ complex multi-step pipelines. In this work, we propose an end-to-end method for dense voxel 3D reconstruction using neuromorphic cameras that eliminates the need to estimate physical priors. Our method incorporates a novel event representation to enhance edge features, enabling the proposed feature-enhancement model to learn more effectively. Additionally, we introduced Optimal Binarization Threshold Selection Principle as a guideline for future related work, using the optimal reconstruction results achieved with threshold optimization as the benchmark. Our method achieves a 54.6% improvement in reconstruction accuracy compared to the baseline method.	 | 神经形态相机，也称为事件相机，是一种异步亮度变化传感器，可以捕捉极快的运动而不受运动模糊的影响，使其特别适用于极端环境下的3D重建。然而，现有的关于单目神经形态相机进行3D重建的研究有限，大多数方法依赖于估计物理先验并通过复杂多步骤的流水线来实现。在这项工作中，我们提出了一种端到端的方法，用于使用神经形态相机进行稠密体素3D重建，从而消除了估计物理先验的必要性。我们的方法结合了一种新颖的事件表示，以增强边缘特征，使所提出的特征增强模型能够更有效地学习。此外，我们引入了最优二值化阈值选择原则作为未来相关工作的指导原则，使用通过阈值优化实现的最佳重建结果作为基准。与基准方法相比，我们的方法在重建精度上实现了54.6%的提升。
2501.00740	 | RORem: Training a Robust Object Remover with Human-in-the-Loop	 | Ruibin Li,Tao Yang,Song Guo,Lei Zhang	 | 尽管现有对象移除方法在不完全移除、不正确的内容合成和模糊的合成区域方面存在不足，导致成功率低，本文提出了一种“人在回路”的半监督学习策略，通过收集和筛选高质量的数据对来训练稳健的对象移除器（RORem），从而显著提高了对象移除的成功率和图像质量。	 | Despite the significant advancements, existing object removal methods struggle with incomplete removal, incorrect content synthesis and blurry synthesized regions, resulting in low success rates. Such issues are mainly caused by the lack of high-quality paired training data, as well as the self-supervised training paradigm adopted in these methods, which forces the model to in-paint the masked regions, leading to ambiguity between synthesizing the masked objects and restoring the background. To address these issues, we propose a semi-supervised learning strategy with human-in-the-loop to create high-quality paired training data, aiming to train a Robust Object Remover (RORem). We first collect 60K training pairs from open-source datasets to train an initial object removal model for generating removal samples, and then utilize human feedback to select a set of high-quality object removal pairs, with which we train a discriminator to automate the following training data generation process. By iterating this process for several rounds, we finally obtain a substantial object removal dataset with over 200K pairs. Fine-tuning the pre-trained stable diffusion model with this dataset, we obtain our RORem, which demonstrates state-of-the-art object removal performance in terms of both reliability and image quality. Particularly, RORem improves the object removal success rate over previous methods by more than 18\%. The dataset, source code and trained model are available at https://github.com/leeruibin/RORem.	 | 尽管取得了显著的进步，现有的对象移除方法在不完全移除、不正确的内容合成和模糊的合成区域方面仍然存在问题，导致成功率较低。这些问题主要是由于缺乏高质量的配对训练数据，以及这些方法采用的自我监督训练范式，迫使模型填充遮罩区域，导致合成掩膜对象和恢复背景之间的模糊性。为了解决这些问题，我们提出了一种带有“人在回路”的半监督学习策略，以创建高质量的配对训练数据，旨在训练一个稳健的对象移除器（RORem）。我们首先从开源数据集中收集了60,000个训练对来训练初始的对象移除模型以生成移除样本，然后利用人类反馈来选择一组高质量的对象移除对，使用这些对训练判别器以自动化接下来的训练数据生成过程。通过多次迭代此过程，最终我们获得了包含超过20万对的大型对象移除数据集。使用此数据集微调预先训练的稳定扩散模型，我们获得了RORem，它在可靠性和图像质量方面都表现出最先进的对象移除性能。特别是，RORem相较于之前的方 法，提高了超过18%的对象移除成功率。数据集、源代码和训练模型可在https://github.com/leeruibin/RORem获取。
2501.00734	 | DDD: Discriminative Difficulty Distance for plant disease diagnosis	 | Yuji Arima,Satoshi Kagiwada,Hitoshi Iyatomi	 | 该研究提出了一个名为差异性困难距离（DDD）的新概念，用于衡量训练集和测试集之间的领域差距，并评估测试数据的分类难度，从而帮助开发更具多样性和鲁棒性的数据集，提高植物病害诊断的准确性。研究结果显示，使用作物和病害多样化的图像构建的DDD度量标准能够显著提高与独立开发的分类器相关性，最高可达0.909。	 | Recent studies on plant disease diagnosis using machine learning (ML) have highlighted concerns about the overestimated diagnostic performance due to inappropriate data partitioning, where training and test datasets are derived from the same source (domain). Plant disease diagnosis presents a challenging classification task, characterized by its fine-grained nature, vague symptoms, and the extensive variability of image features within each domain. In this study, we propose the concept of Discriminative Difficulty Distance (DDD), a novel metric designed to quantify the domain gap between training and test datasets while assessing the classification difficulty of test data. DDD provides a valuable tool for identifying insufficient diversity in training data, thus supporting the development of more diverse and robust datasets. We investigated multiple image encoders trained on different datasets and examined whether the distances between datasets, measured using low-dimensional representations generated by the encoders, are suitable as a DDD metric. The study utilized 244,063 plant disease images spanning four crops and 34 disease classes collected from 27 domains. As a result, we demonstrated that even if the test images are from different crops or diseases than those used to train the encoder, incorporating them allows the construction of a distance measure for a dataset that strongly correlates with the difficulty of diagnosis indicated by the disease classifier developed independently. Compared to the base encoder, pre-trained only on ImageNet21K, the correlation higher by 0.106 to 0.485, reaching a maximum of 0.909.	 | 近年来，植物病害诊断中使用机器学习（ML）的研究突显了由于不适当的分数据集导致的诊断性能高估问题，其中训练集和测试集来自相同的领域（Domain）。植物病害诊断是一个具有挑战性的分类任务，其特点是细粒度、症状模糊以及每个领域内图像特征的广泛变化。在本研究中，我们提出了一个名为差异性困难距离（Discriminative Difficulty Distance, DDD）的概念，这是一种新的度量标准，用于量化训练集和测试集之间领域的差距，并评估测试数据的分类难度。DDD 提供了一个有价值的工具，可以识别训练数据的不足多样性，从而支持开发更具多样性和鲁棒性的数据集。我们研究了在不同数据集上训练的多种图像编码器，并检查了使用编码器生成的低维表示测度的领域间距离是否适合作为 DDD 度量标准。研究使用了来自 27 个领域的 244,063 张植物病害图像，涵盖了四种作物和 34 个病害类。结果表明，即使测试图像与用于训练编码器的图像来自不同的作物或病害，将这些图像纳入其中仍可以构建一个与独立开发的病害分类器所指示的诊断难度高度相关的距离度量。与仅在 ImageNet21K 上预训练的基础编码器相比，相关性提高了 0.106 到 0.485，最高达到 0.909。
2501.00707	 | Everywhere Attack: Attacking Locally and Globally to Boost Targeted Transferability	 | Hui Zeng,Sanshuai Cui,Biwei Chen,Anjie Peng	 | 本文提出了一种提升目标可转移性攻击的方法，通过在图像的局部区域同时优化多个目标攻击，减少代理模型和受害模型之间的注意力不一致性，从而提高可转移性。实验结果表明，该方法可以显著提升包括Logit攻击在内的多种攻击的可转移性，特别是在ImageNet上效果显著，最高可提高300%。	 | Adversarial examples' (AE) transferability refers to the phenomenon that AEs crafted with one surrogate model can also fool other models. Notwithstanding remarkable progress in untargeted transferability, its targeted counterpart remains challenging. This paper proposes an everywhere scheme to boost targeted transferability. Our idea is to attack a victim image both globally and locally. We aim to optimize 'an army of targets' in every local image region instead of the previous works that optimize a high-confidence target in the image. Specifically, we split a victim image into non-overlap blocks and jointly mount a targeted attack on each block. Such a strategy mitigates transfer failures caused by attention inconsistency between surrogate and victim models and thus results in stronger transferability. Our approach is method-agnostic, which means it can be easily combined with existing transferable attacks for even higher transferability. Extensive experiments on ImageNet demonstrate that the proposed approach universally improves the state-of-the-art targeted attacks by a clear margin, e.g., the transferability of the widely adopted Logit attack can be improved by 28.8%-300%.We also evaluate the crafted AEs on a real-world platform: Google Cloud Vision. Results further support the superiority of the proposed method.	 |  adversarial 例子（AE）的可转移性是指使用一个代理模型生成的AE可以欺骗其他模型的现象。尽管无目标可转移性取得了显著进展，但其对应的目标可转移性仍然具有挑战性。本文提出了一种普遍方案来提升目标可转移性。我们的思路是同时在全局和局部攻击受害图像。我们的目标是在每个局部图像区域中优化“一支目标大军”而不是先前工作在图像中优化高置信度的目标。具体而言，我们将受害图像拆分为不重叠的块，并在同一块上联合实施目标攻击。这种策略减轻了由于代理模型和受害模型之间的注意力不一致性而导致的转移失败问题，从而增强了可转移性。我们的方法是无特定方法的，这意味着它可以轻松地与现有的可转移攻击结合使用，以实现更高的可转移性。在ImageNet上的广泛实验表明，所提出的方法在目标攻击方面普遍提高了现有的最佳性能，例如，广泛采用的Logit攻击的可转移性可以提高28.8%-300%。我们还评估了生成的AE在实际平台——Google Cloud Vision上。结果进一步支持了所提出方法的优越性。
2501.00700	 | Knowledge-Guided Prompt Learning for Deepfake Facial Image Detection	 | Hao Wang,Cheng Deng,Zhidong Zhao	 | 该研究提出了一种知识引导的提示学习方法，用于检测深fake人脸图像，通过从大型语言模型中获取与伪造相关的提示作为专家知识，并在测试时进行提示微调，从而提高检测性能。实验结果表明该方法在DeepFakeFaceForensics数据集上优于现有方法。	 | Recent generative models demonstrate impressive performance on synthesizing photographic images, which makes humans hardly to distinguish them from pristine ones, especially on realistic-looking synthetic facial images. Previous works mostly focus on mining discriminative artifacts from vast amount of visual data. However, they usually lack the exploration of prior knowledge and rarely pay attention to the domain shift between training categories (e.g., natural and indoor objects) and testing ones (e.g., fine-grained human facial images), resulting in unsatisfactory detection performance. To address these issues, we propose a novel knowledge-guided prompt learning method for deepfake facial image detection. Specifically, we retrieve forgery-related prompts from large language models as expert knowledge to guide the optimization of learnable prompts. Besides, we elaborate test-time prompt tuning to alleviate the domain shift, achieving significant performance improvement and boosting the application in real-world scenarios. Extensive experiments on DeepFakeFaceForensics dataset show that our proposed approach notably outperforms state-of-the-art methods.	 | 近年来，生成模型在合成摄影图像方面表现出色，使其难以与原始图像区分开来，特别是在合成面部图像方面显得尤为逼真。以往的研究主要集中在从大量图像数据中挖掘判别特征。然而，这些研究通常缺乏对先验知识的探索，并很少关注训练类别（例如，自然物体和室内物体）与测试类别（例如，细粒度的人脸图像）之间的领域转移，导致检测性能不佳。为了解决这些问题，我们提出了一种新的知识引导的提示学习方法，用于深假人脸图像检测。具体来说，我们从大型语言模型中检索与伪造相关的提示作为专家知识，以指导可学习提示的优化。此外，我们进一步探讨了测试时提示微调，减轻领域转移问题，从而在检测性能和实际应用中取得了显著提升。在DeepFakeFaceForensics数据集上的大量实验表明，我们提出的方法显著优于现有方法。
2501.00654	 | ICONS: Influence Consensus for Vision-Language Data Selection	 | Xindi Wu,Mengzhou Xia,Rulin Shao,Zhiwei Deng,Pang Wei Koh,Olga Russakovsky	 | 本文提出了一种基于梯度驱动的影响力共识方法ICONS，用于选择高效的多任务训练数据集，减少了冗余信息并降低了计算成本，实验表明使用选择的数据集训练的模型性能接近使用完整数据集训练的模型。同时，还发布了一个名为LLaVA-ICONS-133K的高度信息丰富的紧凑子集数据集。	 | Visual Instruction Tuning typically requires a large amount of vision-language training data. This data often containing redundant information that increases computational costs without proportional performance gains. In this work, we introduce ICONS, a gradient-driven Influence CONsensus approach for vision-language data Selection that selects a compact training dataset for efficient multi-task training. The key element of our approach is cross-task influence consensus, which uses majority voting across task-specific influence matrices to identify samples that are consistently valuable across multiple tasks, allowing us to effectively prioritize data that optimizes for overall performance. Experiments show that models trained on our selected data (20% of LLaVA-665K) achieve 98.6% of the relative performance obtained using the full dataset. Additionally, we release this subset, LLaVA-ICONS-133K, a compact yet highly informative subset of LLaVA-665K visual instruction tuning data, preserving high impact training data for efficient vision-language model development.	 | 视觉指令调优通常需要大量的视觉-语言训练数据。这些数据往往包含冗余信息，增加了计算成本但未能带来相应的性能提升。本文我们引入了ICONS（Influence CONsensus），一种基于梯度驱动的影响力共识方法，用于选择高效的多任务训练数据集。我们方法的关键要素是跨任务影响力的共识，它利用特定任务的影响力矩阵进行多数投票，以识别在多个任务中始终有价值的样本，从而有效优先选择优化整体性能的数据。实验表明，使用我们选择的数据集（LLaVA-665K中的20%）训练的模型，其相对性能达到使用完整数据集训练的模型的98.6%。此外，我们还发布了这一子集，即LLaVA-ICONS-133K，这是LLaVA-665K视觉指令调优数据集的一个紧凑且高度信息丰富的子集，保留了高效的视觉-语言模型开发所需的重要训练数据。
2501.00651	 | Taming Feed-forward Reconstruction Models as Latent Encoders for 3D Generative Models	 | Suttisak Wizadwongsa,Jinfan Zhou,Edward Li,Jeong Joon Park	 | 本研究将现有的图像到3D重建方法作为潜在编码器，用于训练3D生成模型，并通过后处理管道和新的损失函数优化潜在特征，从而提高生成质量；同时提出了一种多流基于变换器的校正流动架构，实现了高效且高质量的文本条件3D生成。	 | Recent AI-based 3D content creation has largely evolved along two paths: feed-forward image-to-3D reconstruction approaches and 3D generative models trained with 2D or 3D supervision. In this work, we show that existing feed-forward reconstruction methods can serve as effective latent encoders for training 3D generative models, thereby bridging these two paradigms. By reusing powerful pre-trained reconstruction models, we avoid computationally expensive encoder network training and obtain rich 3D latent features for generative modeling for free. However, the latent spaces of reconstruction models are not well-suited for generative modeling due to their unstructured nature. To enable flow-based model training on these latent features, we develop post-processing pipelines, including protocols to standardize the features and spatial weighting to concentrate on important regions. We further incorporate a 2D image space perceptual rendering loss to handle the high-dimensional latent spaces. Finally, we propose a multi-stream transformer-based rectified flow architecture to achieve linear scaling and high-quality text-conditioned 3D generation. Our framework leverages the advancements of feed-forward reconstruction models to enhance the scalability of 3D generative modeling, achieving both high computational efficiency and state-of-the-art performance in text-to-3D generation.	 | 近年来，基于AI的3D内容创建主要沿两条路径发展：前向的图像到3D重建方法和通过2D或3D监督训练的3D生成模型。在本项工作中，我们展示了现有的前向重建方法可以作为有效的潜在编码器，用于训练3D生成模型，从而将这两类方法连接起来。通过重用预先训练好的重建模型，我们避免了昂贵的编码器网络训练，并且能够免费获得丰富的3D潜在特征，用于生成建模。然而，重建模型的潜在空间由于其无结构的性质，并不适合用于生成建模。为了在这些潜在特征上进行流动模型训练，我们开发了后处理管道，包括标准化特征的协议和空间加权，以集中于重要区域。我们还引入了2D图像空间感知渲染损失来处理高维潜在空间。最后，我们提出了一种多流基于变换器的校正流动架构，以实现线性扩展和高质量的文本条件3D生成。我们的框架利用前向重建模型的进展，增强了3D生成建模的可扩展性，同时实现了高计算效率和在文本到3D生成方面的领先性能。
2501.00645	 | SoundBrush: Sound as a Brush for Visual Scene Editing	 | Kim Sung-Bin,Kim Jun-Seong,Junseok Ko,Yewon Kim,Tae-Hyun Oh	 | SoundBrush模型利用声音作为画笔编辑和操控视觉场景，通过扩展潜在扩散模型的生成能力并结合音频信息，能够在保持原始内容的同时准确操控整体场景和插入发声物体；该模型结合新颖的视图合成技术，还能用于编辑3D场景。	 | We propose SoundBrush, a model that uses sound as a brush to edit and manipulate visual scenes. We extend the generative capabilities of the Latent Diffusion Model (LDM) to incorporate audio information for editing visual scenes. Inspired by existing image-editing works, we frame this task as a supervised learning problem and leverage various off-the-shelf models to construct a sound-paired visual scene dataset for training. This richly generated dataset enables SoundBrush to learn to map audio features into the textual space of the LDM, allowing for visual scene editing guided by diverse in-the-wild sound. Unlike existing methods, SoundBrush can accurately manipulate the overall scenery or even insert sounding objects to best match the audio inputs while preserving the original content. Furthermore, by integrating with novel view synthesis techniques, our framework can be extended to edit 3D scenes, facilitating sound-driven 3D scene manipulation. Demos are available at https://soundbrush.github.io/.	 | 我们提出了SoundBrush模型，该模型利用声音作为画笔来编辑和操控视觉场景。我们扩展了潜在扩散模型（LDM）的生成能力，使其能够结合音频信息来编辑视觉场景。受现有图像编辑工作的启发，我们将此任务视为监督学习问题，并利用各种现成的模型构建了一个配对的声景视觉场景数据集进行训练。这个丰富生成的数据集使SoundBrush能够学习将音频特征映射到LDM的文本空间，从而实现由多种野生声音引导的视觉场景编辑。与现有方法不同，SoundBrush能够准确地操控整体场景，甚至插入发出声音的物体，以最好地匹配音频输入，同时保持原始内容。此外，通过与新颖的视图合成技术相结合，我们的框架可以扩展到编辑3D场景，实现基于声音的3D场景操控。演示可以在https://soundbrush.github.io/获得。
2501.00637	 | Flash-Split: 2D Reflection Removal with Flash Cues and Latent Diffusion Separation	 | Tianfu Wang,Mingyang Xie,Haoming Cai,Sachin Shah,Christopher A. Metzler	 | Flash-Split 是一种利用单张对齐不准确的闪光/非闪光图像对来分离透过光和反射光的框架，通过双重分支扩散模型和交叉隐空间解码过程实现高分辨率的反射分离，表现出优于基准方法的性能。	 | Transparent surfaces, such as glass, create complex reflections that obscure images and challenge downstream computer vision applications. We introduce Flash-Split, a robust framework for separating transmitted and reflected light using a single (potentially misaligned) pair of flash/no-flash images. Our core idea is to perform latent-space reflection separation while leveraging the flash cues. Specifically, Flash-Split consists of two stages. Stage 1 separates apart the reflection latent and transmission latent via a dual-branch diffusion model conditioned on an encoded flash/no-flash latent pair, effectively mitigating the flash/no-flash misalignment issue. Stage 2 restores high-resolution, faithful details to the separated latents, via a cross-latent decoding process conditioned on the original images before separation. By validating Flash-Split on challenging real-world scenes, we demonstrate state-of-the-art reflection separation performance and significantly outperform the baseline methods.	 | 透明表面，如玻璃，会产生复杂的反射，这些反射会模糊图像并挑战下游计算机视觉应用。我们引入了 Flash-Split，这是一种利用单张（可能对齐不准确的）闪光/非闪光图像对来分离透过光和反射光的稳健框架。我们的核心思想是在利用闪光线索的同时进行隐空间反射分离。具体来说，Flash-Split 包含两个阶段。第一阶段通过一个双重分支扩散模型分离反射隐空间和透过隐空间，该模型基于编码的闪光/非闪光隐空间对，有效地缓解了闪光/非闪光对齐不准确的问题。第二阶段通过一个交叉隐空间解码过程恢复高分辨率、忠实的细节，该过程基于分离前的原始图像。通过在具有挑战性的实际场景上验证 Flash-Split，我们展示了最先进的反射分离性能，并显著优于基准方法。
2501.00625	 | Gaussian Building Mesh (GBM): Extract a Building's 3D Mesh with Google Earth and Gaussian Splatting	 | Kyle Gao,Liangzhi Li,Hongjie He,Dening Lu,Linlin Xu,Jonathan Li	 | 该研究利用SAM2+GroundingDINO和2D Gaussian Splatting等技术，结合掩膜细化方法，实现了通过文本提示从多视角2D图像中一致分割感兴趣物体，并进一步生成建筑物的3D网格模型。	 | Recently released open-source pre-trained foundational image segmentation and object detection models (SAM2+GroundingDINO) allow for geometrically consistent segmentation of objects of interest in multi-view 2D images. Users can use text-based or click-based prompts to segment objects of interest without requiring labeled training datasets. Gaussian Splatting allows for the learning of the 3D representation of a scene's geometry and radiance based on 2D images. Combining Google Earth Studio, SAM2+GroundingDINO, 2D Gaussian Splatting, and our improvements in mask refinement based on morphological operations and contour simplification, we created a pipeline to extract the 3D mesh of any building based on its name, address, or geographic coordinates.	 | 近期发布的开源预训练基础图像分割和物体检测模型（SAM2+GroundingDINO），使得在多视角2D图像中一致地分割感兴趣物体成为可能。用户可以通过文本提示或点击提示来分割感兴趣物体，而无需使用标注的训练数据集。Gaussian Splatting 允许基于2D图像学习场景几何和辐射的3D表示。结合 Google Earth Studio、SAM2+GroundingDINO、2D Gaussian Splatting 以及我们在基于形态学操作和轮廓简化基础上改进的掩膜细化技术，我们创建了一个管道，可以根据建筑物的名称、地址或地理坐标提取其3D网格。
2501.00603	 | DiC: Rethinking Conv3x3 Designs in Diffusion Models	 | Yuchuan Tian,Jing Han,Chengcheng Wang,Yuchen Liang,Chao Xu,Hanting Chen	 | 研究提出了一种基于纯卷积的扩散模型架构——扩散CNN（DiC），通过引入稀疏跳跃连接、阶段特定嵌入等改进措施，显著提升了模型的性能和速度，优于现有基于Transformer的扩散模型。	 | Diffusion models have shown exceptional performance in visual generation tasks. Recently, these models have shifted from traditional U-Shaped CNN-Attention hybrid structures to fully transformer-based isotropic architectures. While these transformers exhibit strong scalability and performance, their reliance on complicated self-attention operation results in slow inference speeds. Contrary to these works, we rethink one of the simplest yet fastest module in deep learning, 3x3 Convolution, to construct a scaled-up purely convolutional diffusion model. We first discover that an Encoder-Decoder Hourglass design outperforms scalable isotropic architectures for Conv3x3, but still under-performing our expectation. Further improving the architecture, we introduce sparse skip connections to reduce redundancy and improve scalability. Based on the architecture, we introduce conditioning improvements including stage-specific embeddings, mid-block condition injection, and conditional gating. These improvements lead to our proposed Diffusion CNN (DiC), which serves as a swift yet competitive diffusion architecture baseline. Experiments on various scales and settings show that DiC surpasses existing diffusion transformers by considerable margins in terms of performance while keeping a good speed advantage. Project page: https://github.com/YuchuanTian/DiC	 | 扩散模型在视觉生成任务中展现了出色的表现。最近，这些模型从传统的U型CNN-注意力混合结构转向了完全基于Transformer的等向性架构。虽然这些Transformer表现出强大的可扩展性和性能，但由于依赖复杂自注意力操作，其推理速度较慢。与此相反，我们重新思考了深度学习中一个既简单又快速的模块——3x3卷积，构建了一个大幅扩展的纯卷积扩散模型。我们首先发现，编码器-解码器Hourglass设计在Conv3x3上优于等向性架构，但仍未能达到我们的预期。进一步改进架构，我们引入了稀疏跳跃连接以减少冗余并提高可扩展性。基于该架构，我们引入了包括阶段特定嵌入、中间块条件注入和条件门控在内的条件改进措施。这些改进导致我们提出了扩散CNN（DiC），这是一种既快速又具有竞争力的扩散模型基准架构。在不同规模和设置下的实验表明，DiC在性能上明显优于现有扩散Transformer模型，同时保持良好的速度优势。项目页面：https://github.com/YuchuanTian/DiC
2501.00602	 | STORM: Spatio-Temporal Reconstruction Model for Large-Scale Outdoor Scenes	 | Jiawei Yang,Jiahui Huang,Yuxiao Chen,Yan Wang,Boyi Li,Yurong You,Apoorva Sharma,Maximilian Igl,Peter Karkus,Danfei Xu,Boris Ivanovic,Yue Wang,Marco Pavone	 | STORM是一种数据驱动的Transformer架构，能够在单次前向传递中直接推断动态3D场景表示，从而实现快速、高质量的动态户外场景重建，相较于现有方法显著提高了PSNR并缩短了重建时间至200毫秒。	 | We present STORM, a spatio-temporal reconstruction model designed for reconstructing dynamic outdoor scenes from sparse observations. Existing dynamic reconstruction methods often rely on per-scene optimization, dense observations across space and time, and strong motion supervision, resulting in lengthy optimization times, limited generalization to novel views or scenes, and degenerated quality caused by noisy pseudo-labels for dynamics. To address these challenges, STORM leverages a data-driven Transformer architecture that directly infers dynamic 3D scene representations--parameterized by 3D Gaussians and their velocities--in a single forward pass. Our key design is to aggregate 3D Gaussians from all frames using self-supervised scene flows, transforming them to the target timestep to enable complete (i.e., "amodal") reconstructions from arbitrary viewpoints at any moment in time. As an emergent property, STORM automatically captures dynamic instances and generates high-quality masks using only reconstruction losses. Extensive experiments on public datasets show that STORM achieves precise dynamic scene reconstruction, surpassing state-of-the-art per-scene optimization methods (+4.3 to 6.6 PSNR) and existing feed-forward approaches (+2.1 to 4.7 PSNR) in dynamic regions. STORM reconstructs large-scale outdoor scenes in 200ms, supports real-time rendering, and outperforms competitors in scene flow estimation, improving 3D EPE by 0.422m and Acc5 by 28.02%. Beyond reconstruction, we showcase four additional applications of our model, illustrating the potential of self-supervised learning for broader dynamic scene understanding.	 | 我们提出了STORM，一种用于从稀疏观测重建动态户外场景的空间-时间重建模型。现有的动态重建方法通常依赖于针对每个场景的优化、空间和时间上的密集观测以及强烈的运动监督，这导致了较长的优化时间、对新视点或场景的一般化能力有限，以及由于动态的噪声伪标签而导致的退化质量。为了解决这些挑战，STORM利用了一个数据驱动的Transformer架构，该架构可以在单次前向传递中直接推断出动态的3D场景表示——这些表示由3D高斯分布及其速度参数化。我们的关键设计是使用自监督场景流聚合所有帧的3D高斯分布，并将它们转换为目标时间戳，从而实现任意时刻、任意视角的完整（即“无遮挡的”）重建。作为一种涌现性质，STORM仅通过重建损失就可以自动捕捉动态实例并生成高质量的掩码。在公开数据集上的大量实验表明，STORM在动态区域的动态场景重建方面表现优异，优于最先进的逐场景优化方法（+4.3到6.6 PSNR）和现有的前馈方法（+2.1到4.7 PSNR）。STORM可以在200毫秒内重建大规模的户外场景，支持实时渲染，并在场景流估计方面超越竞争对手，提高3D EPE（端到端平移误差）0.422米和Acc5（准确率）28.02%。此外，我们展示了模型的四个额外应用，展示了自监督学习在更广泛动态场景理解中的潜力。
2501.00601	 | DreamDrive: Generative 4D Scene Modeling from Street View Images	 | Jiageng Mao,Boyi Li,Boris Ivanovic,Yuxiao Chen,Yan Wang,Yurong You,Chaowei Xiao,Danfei Xu,Marco Pavone,Yue Wang	 | DreamDrive提出了一种结合生成模型和重建方法的4D时空场景生成技术，能够合成具有高保真度和3D一致性的驱动视频，从而增强自动驾驶模型的训练可扩展性和感知规划能力。	 | Synthesizing photo-realistic visual observations from an ego vehicle's driving trajectory is a critical step towards scalable training of self-driving models. Reconstruction-based methods create 3D scenes from driving logs and synthesize geometry-consistent driving videos through neural rendering, but their dependence on costly object annotations limits their ability to generalize to in-the-wild driving scenarios. On the other hand, generative models can synthesize action-conditioned driving videos in a more generalizable way but often struggle with maintaining 3D visual consistency. In this paper, we present DreamDrive, a 4D spatial-temporal scene generation approach that combines the merits of generation and reconstruction, to synthesize generalizable 4D driving scenes and dynamic driving videos with 3D consistency. Specifically, we leverage the generative power of video diffusion models to synthesize a sequence of visual references and further elevate them to 4D with a novel hybrid Gaussian representation. Given a driving trajectory, we then render 3D-consistent driving videos via Gaussian splatting. The use of generative priors allows our method to produce high-quality 4D scenes from in-the-wild driving data, while neural rendering ensures 3D-consistent video generation from the 4D scenes. Extensive experiments on nuScenes and street view images demonstrate that DreamDrive can generate controllable and generalizable 4D driving scenes, synthesize novel views of driving videos with high fidelity and 3D consistency, decompose static and dynamic elements in a self-supervised manner, and enhance perception and planning tasks for autonomous driving.	 | 从自主车辆的驾驶轨迹合成逼真的视觉观察是训练自动驾驶模型可扩展性的一个关键步骤。基于重建的方法可以从驾驶日志中生成3D场景，并通过神经渲染合成几何上一致的驾驶视频，但它们对昂贵的对象注释的依赖限制了其在野外驾驶场景中的泛化能力。另一方面，生成模型可以以更泛化的方式合成动作条件下的驾驶视频，但往往难以保持3D视觉一致性。在本文中，我们提出了一种结合生成和重建优点的4D时空场景生成方法DreamDrive，以合成可泛化的4D驾驶场景和具有3D一致性的动态驾驶视频。具体而言，我们利用视频扩散模型的生成能力来合成一系列视觉参考，并通过一种新颖的混合高斯表示进一步将其提升到4D。给定一个驾驶轨迹，我们通过高斯着色进行3D一致的驾驶视频渲染。使用生成先验使我们的方法能够从野外驾驶数据中生成高质量的4D场景，而神经渲染则确保从4D场景生成3D一致的视频。在nuScenes和街景图像上的大量实验表明，DreamDrive可以生成可控且可泛化的4D驾驶场景，合成高保真且具有3D一致性的新视角驾驶视频，在无监督的方式下分解静态和动态元素，并增强自主驾驶中的感知和规划任务。
2501.00599	 | VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM	 | Yuqian Yuan,Hang Zhang,Wentong Li,Zesen Cheng,Boqiang Zhang,Long Li,Xin Li,Deli Zhao,Wenqiao Zhang,Yueting Zhuang,Jianke Zhu,Lidong Bing	 | VideoRefer Suite通过引入大规模高质量的对象级视频指令数据集VideoRefer-700K、新的VideoRefer模型以及全面的VideoRefer-Bench基准，显著提升了视频大型语言模型在细粒度空间-时间视频理解方面的性能。	 | Video Large Language Models (Video LLMs) have recently exhibited remarkable capabilities in general video understanding. However, they mainly focus on holistic comprehension and struggle with capturing fine-grained spatial and temporal details. Besides, the lack of high-quality object-level video instruction data and a comprehensive benchmark further hinders their advancements. To tackle these challenges, we introduce the VideoRefer Suite to empower Video LLM for finer-level spatial-temporal video understanding, i.e., enabling perception and reasoning on any objects throughout the video. Specially, we thoroughly develop VideoRefer Suite across three essential aspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent data engine to meticulously curate a large-scale, high-quality object-level video instruction dataset, termed VideoRefer-700K. Next, we present the VideoRefer model, which equips a versatile spatial-temporal object encoder to capture precise regional and sequential representations. Finally, we meticulously create a VideoRefer-Bench to comprehensively assess the spatial-temporal understanding capability of a Video LLM, evaluating it across various aspects. Extensive experiments and analyses demonstrate that our VideoRefer model not only achieves promising performance on video referring benchmarks but also facilitates general video understanding capabilities.	 | 视频大型语言模型（Video LLMs）最近在通用视频理解方面展现了显著的能力。然而，它们主要关注整体理解，而在捕捉精细的空间和时间细节方面存在困难。此外，高质量的对象级视频指令数据的缺失和综合性基准的缺乏进一步阻碍了其发展。为了解决这些挑战，我们引入了VideoRefer Suite，以增强Video LLM在细粒度空间-时间视频理解方面的能力，即使其能够在视频中对任何对象进行感知和推理。特别地，我们在三个关键方面全面开发了VideoRefer Suite：数据集、模型和基准。  首先，我们介绍了一个多智能体数据引擎，用于精心整理一个大规模、高质量的对象级视频指令数据集，称为VideoRefer-700K。其次，我们提出了VideoRefer模型，该模型配备了多功能的空间-时间对象编码器，以捕捉精确的区域和序列表示。最后，我们精心创建了VideoRefer-Bench，以全面评估Video LLM的空间-时间理解能力，从多个方面对其进行评估。广泛的实验和分析表明，我们的VideoRefer模型不仅在视频引用基准测试中取得了令人鼓舞的性能，还增强了视频理解的能力。
2501.00585	 | Sidewalk Hazard Detection Using Variational Autoencoder and One-Class SVM	 | Edgar Guzman,Robert D. Howe	 | 本文提出了一种结合变分自编码器（VAE）和一类支持向量机（OCSVM）的方法，用于检测人行道上的异常情况，以确保安全导航。该系统在大量视频数据集上表现良好，能够有效区分危险和非危险场景，准确率达到91.4%，AUC值为0.94。	 | The unpredictable nature of outdoor settings introduces numerous safety concerns, making hazard detection crucial for safe navigation. This paper introduces a novel system for sidewalk safety navigation utilizing a hybrid approach that combines a Variational Autoencoder (VAE) with a One-Class Support Vector Machine (OCSVM). The system is designed to detect anomalies on sidewalks that could potentially pose walking hazards. A dataset comprising over 15,000 training frames and 5,000 testing frames was collected using video recordings, capturing various sidewalk scenarios, including normal and hazardous conditions. During deployment, the VAE utilizes its reconstruction mechanism to detect anomalies within a frame. Poor reconstruction by the VAE implies the presence of an anomaly, after which the OCSVM is used to confirm whether the anomaly is hazardous or non-hazardous. The proposed VAE model demonstrated strong performance, with a high Area Under the Curve (AUC) of 0.94, effectively distinguishing anomalies that could be potential hazards. The OCSVM is employed to reduce the detection of false hazard anomalies, such as manhole or water valve covers. This approach achieves an accuracy of 91.4%, providing a highly reliable system for distinguishing between hazardous and non-hazardous scenarios. These results suggest that the proposed system offers a robust solution for hazard detection in uncertain environments.	 | 户外环境的不确定性带来了众多安全问题，因此危险检测对于安全导航至关重要。本文介绍了一种结合变分自编码器（VAE）和一类支持向量机（OCSVM）的混合方法，用于人行道安全导航的新系统。该系统旨在检测可能导致行走危险的人行道上的异常情况。该研究收集了一组包含超过15,000个训练帧和5,000个测试帧的视频数据，捕捉了各种人行道场景，包括正常和危险条件。在部署过程中，VAE 利用其重构机制检测帧内的异常。VAE 的重构表现不佳表明存在异常，随后使用OCSVM验证该异常是否为危险异常。所提出的VAE模型表现出色，AUC值高达0.94，有效地区分了可能的危险异常。OCSVM被用于减少误报，如井盖或水阀盖等假定的危险异常的检测。这种方法的准确率达到91.4%，提供了一个高度可靠的系统来区分危险和非危险场景。这些结果表明，提出的系统为不确定环境中危险检测提供了稳健的解决方案。
2501.00584	 | Online Video Understanding: A Comprehensive Benchmark and Memory-Augmented Method	 | Zhenpeng Huang,Xinhao Li,Jiaqi Li,Jing Wang,Xiangyu Zeng,Cheng Liang,Tao Wu,Xi Chen,Liang Li,Limin Wang	 | 本文从评估基准、模型结构和训练策略三个角度探索了多模态大型语言模型在在线视频理解中的应用挑战，并提出了OVBench、金字塔记忆库（PMB）和离线到在线的学习范式，开发了VideoChat-Online模型，该模型在多种基准测试中表现出色，验证了所提出方法的有效性。	 | Multimodal Large Language Models (MLLMs) have shown significant progress in offline video understanding. However, applying these models to real-world scenarios, such as autonomous driving and human-computer interaction, presents unique challenges due to the need for real-time processing of continuous online video streams. To this end, this paper presents systematic efforts from three perspectives: evaluation benchmark, model architecture, and training strategy. First, we introduce OVBench, a comprehensive question-answering benchmark specifically designed to evaluate models' ability to perceive, memorize, and reason within online video contexts. It features six core task types across three temporal contexts-past, present, and future-forming 16 subtasks from diverse datasets. Second, we propose a new Pyramid Memory Bank (PMB) that effectively retains key spatiotemporal information in video streams. Third, we proposed an offline-to-online learning paradigm, designing an interleaved dialogue format for online video data and constructing an instruction-tuning dataset tailored for online video training. This framework led to the development of VideoChat-Online, a robust and efficient model for online video understanding. Despite the lower computational cost and higher efficiency, VideoChat-Online outperforms existing state-of-the-art offline and online models across popular offline video benchmarks and OVBench, demonstrating the effectiveness of our model architecture and training strategy.	 | 多模态大型语言模型（MLLMs）在离线视频理解方面取得了显著进展。然而，将这些模型应用于自动驾驶和人机交互等实际场景中，由于需要对连续的在线视频流进行实时处理，因此面临着独特的挑战。为应对这些挑战，本文从三个角度进行了系统性的探索：评估基准、模型结构和训练策略。首先，我们介绍了OVBench，这是一种综合型的问题回答基准，专门用于评估模型在在线视频上下文中感知、记忆和推理的能力。它包括六种核心任务类型，涵盖过去、现在和未来三种时间上下文，形成了来自不同数据集的16个子任务。其次，我们提出了一种新的金字塔记忆库（PMB），有效保留了视频流中的关键时空信息。第三，我们提出了离线到在线的学习范式，设计了一种交错的对话格式来处理在线视频数据，并构建了一个专门用于在线视频训练的指令调优数据集。这一框架推动了VideoChat-Online模型的开发，该模型是一种适用于在线视频理解的稳健且高效的模型。尽管计算成本较低且更高效，VideoChat-Online在流行的离线视频基准和OVBench上均优于现有的离线和在线模型，这证明了我们模型结构和训练策略的有效性。
2501.00574	 | VideoChat-Flash: Hierarchical Compression for Long-Context Video Modeling	 | Xinhao Li,Yi Wang,Jiashuo Yu,Xiangyu Zeng,Yuhan Zhu,Haian Huang,Jianfei Gao,Kunchang Li,Yinan He,Chenting Wang,Yu Qiao,Yali Wang,Limin Wang	 | 本文介绍了HiCo（层次视觉token压缩）方法及其在VideoChat-Flash中的应用，该方法通过压缩冗余视觉信息来实现高效处理长视频，同时保持关键特征，并在多个长视频基准测试中取得了优越性能，特别是在大规模模型中展现了卓越的能力。	 | Long-context modeling is a critical capability for multimodal large language models (MLLMs), enabling them to process long-form contents with implicit memorization. Despite its advances, handling extremely long videos remains challenging due to the difficulty in maintaining crucial features over extended sequences. This paper introduces a Hierarchical visual token Compression (HiCo) method designed for high-fidelity representation and a practical context modeling system VideoChat-Flash tailored for multimodal long-sequence processing. HiCo capitalizes on the redundancy of visual information in long videos to compress long video context from the clip-level to the video-level, reducing the compute significantly while preserving essential details. VideoChat-Flash features a multi-stage short-to-long learning scheme, a rich dataset of real-world long videos named LongVid, and an upgraded "Needle-In-A-video-Haystack" (NIAH) for evaluating context capacities. In extensive experiments, VideoChat-Flash shows the leading performance on both mainstream long and short video benchmarks at the 7B model scale. It firstly gets 99.1% accuracy over 10,000 frames in NIAH among open-source models.	 | 长上下文建模是多模态大型语言模型（MLLMs）的一项关键能力，使其能够处理具有潜在记忆的长格式内容。尽管取得了进展，但处理非常长的视频仍然具有挑战性，因为难以在长时间序列中保持关键特征。本文介绍了HiCo（层次视觉token压缩）方法，该方法旨在实现高保真的表示，并针对多模态长序列处理设计了一个实用的上下文建模系统VideoChat-Flash。HiCo利用长视频中的冗余视觉信息，从片段级别压缩到视频级别上下文，显著减少了计算量同时保留了重要细节。VideoChat-Flash 具有多阶段短到长的学习方案、名为LongVid的真实世界长视频数据集以及升级的“视频 haystack 中的针”（NIAH）用于评估上下文容量。在广泛的实验中，VideoChat-Flash 在7B模型规模下在主流的长和短视频基准测试中表现出领先性能。它首次在开放源代码模型中实现了在NIAH上超过10,000帧的99.1%精度。
2501.00569	 | Probing Visual Language Priors in VLMs	 | Tiange Luo,Ang Cao,Gunhee Lee,Justin Johnson,Honglak Lee	 | 本文介绍了ViLP，一个旨在评估视觉语言模型进行真正视觉推理能力的新基准，通过该基准发现现代VLMs表现不佳，并提出了一种自我改进框架来提升模型的视觉推理能力。	 | Despite recent advances in Vision-Language Models (VLMs), many still over-rely on visual language priors present in their training data rather than true visual reasoning. To examine the situation, we introduce ViLP, a visual question answering (VQA) benchmark that pairs each question with three potential answers and three corresponding images: one image whose answer can be inferred from text alone, and two images that demand visual reasoning. By leveraging image generative models, we ensure significant variation in texture, shape, conceptual combinations, hallucinated elements, and proverb-based contexts, making our benchmark images distinctly out-of-distribution. While humans achieve near-perfect accuracy, modern VLMs falter; for instance, GPT-4 achieves only 66.17% on ViLP. To alleviate this, we propose a self-improving framework in which models generate new VQA pairs and images, then apply pixel-level and semantic corruptions to form "good-bad" image pairs for self-training. Our training objectives compel VLMs to focus more on actual visual inputs and have demonstrated their effectiveness in enhancing the performance of open-source VLMs, including LLaVA-v1.5 and Cambrian.	 | 尽管最近在视觉语言模型（VLMs）方面取得了进展，许多模型仍然过度依赖于训练数据中存在的视觉语言先验，而不是真正的视觉推理。为了探究这一现象，我们引入了ViLP，这是一个视觉问答（VQA）基准，每个问题配对了三个潜在答案和三个相应的图像：一个图像的答案可以通过文本本身推断出来，而另外两个图像则需要视觉推理。通过利用图像生成模型，我们确保了在纹理、形状、概念组合、幻觉元素和谚语背景方面具有显著的多样性，使我们的基准图像具有明显的分布外特征。人类在这项基准测试中几乎达到了完美的准确性，而现代VLMs则表现不佳；例如，GPT-4在ViLP上的得分仅为66.17%。为了缓解这一问题，我们提出了一种自我改进框架，该框架中模型生成新的VQA对和图像，然后对这些图像进行像素级和语义破坏，形成“好-坏”图像对进行自我训练。我们的训练目标促使VLMs更加关注实际的视觉输入，并证明了其在增强开源VLMs（如LLaVA-v1.5和Cambrian）性能方面的有效性。
2501.00527	 | Exploiting Boundary Loss for the Hierarchical Panoptic Segmentation of Plants and Leaves	 | Madeleine Darbyshire,Elizabeth Sklar,Simon Parsons	 | 精准农业利用数据和机器学习技术优化作物管理，通过仅对杂草施用除草剂和仅对需要的作物施用肥料来提高产量并减少资源消耗。为此，研究提出了一种结合焦点损失和边界损失的分层全景分割方法，提高了对叶片和杂草等小目标的分割性能，并提升了叶片计数的准确性。	 | Precision agriculture leverages data and machine learning so that farmers can monitor their crops and target interventions precisely. This enables the precision application of herbicide only to weeds, or the precision application of fertilizer only to undernourished crops, rather than to the entire field. The approach promises to maximize yields while minimizing resource use and harm to the surrounding environment. To this end, we propose a hierarchical panoptic segmentation method that simultaneously determines leaf count (as an identifier of plant growth)and locates weeds within an image. In particular, our approach aims to improve the segmentation of smaller instances like the leaves and weeds by incorporating focal loss and boundary loss. Not only does this result in competitive performance, achieving a PQ+ of 81.89 on the standard training set, but we also demonstrate we can improve leaf-counting accuracy with our method. The code is available at https://github.com/madeleinedarbyshire/HierarchicalMask2Former.	 | 精准农业通过利用数据和机器学习技术，使农民能够监测作物并精确实施干预措施。这种方法使得除草剂仅施用于杂草，肥料仅施用于营养不良的作物，而不是整个田地。该方法承诺在最大限度提高产量的同时，最大限度地减少资源使用和对周边环境的损害。为此，我们提出了一种分层全景分割方法，该方法同时确定叶片数量（作为植物生长的标识符）并定位图像中的杂草。特别是，我们的方法通过结合焦点损失和边界损失，旨在提高对较小实例如叶片和杂草的分割性能。这不仅在标准训练集上实现了竞争性的性能，PQ+得分为81.89，而且还证明我们的方法可以提高叶片计数的准确性。相关代码可在https://github.com/madeleinedarbyshire/HierarchicalMask2Former获取。
2501.00525	 | Is Segment Anything Model 2 All You Need for Surgery Video Segmentation? A Systematic Evaluation	 | Cheng Yuan,Jian Jiang,Kunyi Yang,Lv Wu,Rui Wang,Zi Meng,Haonan Ping,Ziyu Xu,Yifan Zhou,Wanli Song,Hesheng Wang,Qi Dou,Yutong Ban	 | 本文系统评估了SAM2模型在零样本手术视频分割任务中的性能，通过不同配置实验和多数据集测试，探索了其在多种手术类型中的应用潜力。	 | Surgery video segmentation is an important topic in the surgical AI field. It allows the AI model to understand the spatial information of a surgical scene. Meanwhile, due to the lack of annotated surgical data, surgery segmentation models suffer from limited performance. With the emergence of SAM2 model, a large foundation model for video segmentation trained on natural videos, zero-shot surgical video segmentation became more realistic but meanwhile remains to be explored. In this paper, we systematically evaluate the performance of SAM2 model in zero-shot surgery video segmentation task. We conducted experiments under different configurations, including different prompting strategies, robustness, etc. Moreover, we conducted an empirical evaluation over the performance, including 9 datasets with 17 different types of surgeries.	 | 手术视频分割是手术人工智能领域的重要话题。它允许AI模型理解手术场景的空间信息。同时，由于缺乏标注的手术数据，手术分割模型的性能受到限制。随着SAM2模型的出现，这是一种在自然视频上训练的大型基础模型，零样本手术视频分割变得更加现实，但仍需进一步探索。在本文中，我们系统性地评估了SAM2模型在零样本手术视频分割任务中的性能。我们进行了不同配置下的实验，包括不同的提示策略、鲁棒性等。此外，我们对性能进行了实证评估，涉及9个数据集和17种不同类型的手术。
2501.00520	 | Innovative Silicosis and Pneumonia Classification: Leveraging Graph Transformer Post-hoc Modeling and Ensemble Techniques	 | Bao Q. Bui,Tien T.T. Nguyen,Duy M. Le,Cong Tran,Cuong Pham	 | 该研究创建了名为SVBCX的新数据集，并提出了一种结合图变压器网络和传统深度神经网络的新型深度学习架构，用于矽肺相关肺炎症的分类，实验结果显示出显著的改进，宏F1分数达到0.9749，AUC ROC分数超过0.99。	 | This paper presents a comprehensive study on the classification and detection of Silicosis-related lung inflammation. Our main contributions include 1) the creation of a newly curated chest X-ray (CXR) image dataset named SVBCX that is tailored to the nuances of lung inflammation caused by distinct agents, providing a valuable resource for silicosis and pneumonia research community; and 2) we propose a novel deep-learning architecture that integrates graph transformer networks alongside a traditional deep neural network module for the effective classification of silicosis and pneumonia. Additionally, we employ the Balanced Cross-Entropy (BalCE) as a loss function to ensure more uniform learning across different classes, enhancing the model's ability to discern subtle differences in lung conditions. The proposed model architecture and loss function selection aim to improve the accuracy and reliability of inflammation detection, particularly in the context of Silicosis. Furthermore, our research explores the efficacy of an ensemble approach that combines the strengths of diverse model architectures. Experimental results on the constructed dataset demonstrate promising outcomes, showcasing substantial enhancements compared to baseline models. The ensemble of models achieves a macro-F1 score of 0.9749 and AUC ROC scores exceeding 0.99 for each class, underscoring the effectiveness of our approach in accurate and robust lung inflammation classification.	 | 本文对该领域进行了全面研究，探讨了矽肺相关肺炎症的分类和检测。我们的主要贡献包括：1）创建了一个名为SVBCX的新数据集，专为不同致病因素引起的肺炎症设计，为矽肺和肺炎研究社区提供了一个宝贵的数据资源；以及2）我们提出了一个新颖的深度学习架构，该架构结合了图变压器网络和传统的深度神经网络模块，以有效地对矽肺和肺炎进行分类。此外，我们采用平衡交叉熵（BalCE）作为损失函数，以确保不同类别的均匀学习，从而增强模型识别肺部细微差异的能力。所提出的模型架构和损失函数选择旨在提高炎症检测的准确性和可靠性，特别是在矽肺的背景下。此外，我们的研究探讨了结合多种模型架构优点的集成方法的效果。在构建的数据集上进行的实验结果显示出令人鼓舞的结果，与基准模型相比，显示出显著的改进。模型集合实现了宏F1分数为0.9749，AUC ROC分数超过0.99，突显了我们方法在准确和稳健的肺部炎症分类方面的有效性。
2501.00513	 | Fine-grained Video-Text Retrieval: A New Benchmark and Method	 | Yifan Xu,Xinhao Li,Yichun Yang,Rui Huang,Limin Wang	 | FIBER是一个细粒度视频检索基准，包含1000个来自FineAction数据集的视频，并提供了详细的时空注释，旨在评估视频语言模型的时空检索能力；实验表明，所提出的视频大语言编码器在传统基准上的表现与基于CLIP的模型相当，但在细粒度表示和时空偏见方面表现更优。	 | The ability of perceiving fine-grained spatial and temporal information is crucial for video-language retrieval. However, the existing video retrieval benchmarks, such as MSRVTT and MSVD, fail to efficiently evaluate the fine-grained retrieval ability of video-language models (VLMs) due to a lack of detailed annotations. To address this problem, we present FIBER, a FIne-grained BEnchmark for text to video Retrieval, containing 1,000 videos sourced from the FineAction dataset. Uniquely, our FIBER benchmark provides detailed human-annotated spatial annotations and temporal annotations for each video, making it possible to independently evaluate the spatial and temporal bias of VLMs on video retrieval task. Besides, we employ a text embedding method to unlock the capability of fine-grained video-language understanding of Multimodal Large Language Models (MLLMs). Surprisingly, the experiment results show that our Video Large Language Encoder (VLLE) performs comparably to CLIP-based models on traditional benchmarks and has a stronger capability of fine-grained representation with lower spatial-temporal bias. Project page: https://fiber-bench.github.io.	 | 细粒度空间和时间信息的感知能力对于视频语言检索至关重要。然而，现有的视频检索基准，如MSRVTT和MSVD，由于缺乏详细的注释，无法有效地评估视频语言模型（VLMs）的细粒度检索能力。为了解决这个问题，我们提出了FIBER，一个细粒度视频检索基准，包含来自FineAction数据集的1,000个视频。独特的是，我们的FIBER基准为每个视频提供了详细的由人类注释的空间和时间注释，使得独立评估VLMs在视频检索任务中的空间和时间偏见成为可能。此外，我们采用了一种文本嵌入方法来解锁多模态大语言模型（MLLMs）对细粒度视频语言理解的能力。令人惊讶的是，实验结果表明，我们的视频大语言编码器（VLLE）在传统基准上的表现与基于CLIP的模型相当，并且在细粒度表示方面具有更低的空间-时间偏见和更强的能力。项目页面：https://fiber-bench.github.io。
2501.00437	 | Unleashing Text-to-Image Diffusion Prior for Zero-Shot Image Captioning	 | Jianjie Luo,Jingwen Chen,Yehao Li,Yingwei Pan,Jianlin Feng,Hongyang Chao,Ting Yao	 | 本文提出了一种新颖的分割区域跨模态特征Mix-up（PCM）机制，通过在训练过程中自适应地缓解图像和文本之间的语义不一致，显著提升了零样本图像描述的性能。实验结果表明，PCM-Net在多个数据集上优于现有方法，并在领域内和跨领域的零样本图像描述任务中排名第一。	 | Recently, zero-shot image captioning has gained increasing attention, where only text data is available for training. The remarkable progress in text-to-image diffusion model presents the potential to resolve this task by employing synthetic image-caption pairs generated by this pre-trained prior. Nonetheless, the defective details in the salient regions of the synthetic images introduce semantic misalignment between the synthetic image and text, leading to compromised results. To address this challenge, we propose a novel Patch-wise Cross-modal feature Mix-up (PCM) mechanism to adaptively mitigate the unfaithful contents in a fine-grained manner during training, which can be integrated into most of encoder-decoder frameworks, introducing our PCM-Net. Specifically, for each input image, salient visual concepts in the image are first detected considering the image-text similarity in CLIP space. Next, the patch-wise visual features of the input image are selectively fused with the textual features of the salient visual concepts, leading to a mixed-up feature map with less defective content. Finally, a visual-semantic encoder is exploited to refine the derived feature map, which is further incorporated into the sentence decoder for caption generation. Additionally, to facilitate the model training with synthetic data, a novel CLIP-weighted cross-entropy loss is devised to prioritize the high-quality image-text pairs over the low-quality counterparts. Extensive experiments on MSCOCO and Flickr30k datasets demonstrate the superiority of our PCM-Net compared with state-of-the-art VLMs-based approaches. It is noteworthy that our PCM-Net ranks first in both in-domain and cross-domain zero-shot image captioning. The synthetic dataset SynthImgCap and code are available at https://jianjieluo.github.io/SynthImgCap.	 | 近年来，零样本图像描述得到了越来越多的关注，其中仅可用文本数据进行训练。文本到图像的扩散模型的显著进步为通过利用由该预训练先验生成的合成图像-描述对来解决该任务提供了潜力。然而，合成图像中的缺陷细节在显著区域引入了合成图像与文本之间的语义不一致，导致了性能受损的结果。为了解决这一挑战，我们提出了一种新颖的分割区域跨模态特征Mix-up（PCM）机制，以在训练过程中以精细的方式自适应地缓解不忠实的内容，该机制可以集成到大多数编码器-解码器框架中，引入了我们的PCM-Net。具体而言，对于每个输入图像，首先在CLIP空间中考虑图像-文本相似性来检测图像中的显著视觉概念。接下来，输入图像的分割区域视觉特征与显著视觉概念的文本特征选择性地融合，从而生成一个包含较少缺陷内容的混合特征图。最后，使用视觉-语义编码器来细化所提取的特征图，并将其进一步合并到句子解码器中用于生成描述。此外，为了便于使用合成数据进行模型训练，我们设计了一种新颖的CLIP加权交叉熵损失，以优先处理高质量的图像-文本对而不是低质量的对。在MSCOCO和Flickr30k数据集上的大量实验表明，与基于VLM的方法相比，我们的PCM-Net具有优越性。值得注意的是，我们的PCM-Net在领域内和跨领域的零样本图像描述中均排名第一。合成数据集SynthImgCap和代码可以在https://jianjieluo.github.io/SynthImgCap获取。
2501.00432	 | OV-HHIR: Open Vocabulary Human Interaction Recognition Using Cross-modal Integration of Large Language Models	 | Lala Shakti Swarup Ray,Bo Zhou,Sungho Suh,Paul Lukowicz	 | 本文提出了一种基于开放词汇的框架，用于识别人类互动，通过大规模语言模型生成开放性文本描述，并创建了一个全面的人类互动数据集作为基准，从而提高了视频理解的灵活性和准确性。	 | Understanding human-to-human interactions, especially in contexts like public security surveillance, is critical for monitoring and maintaining safety. Traditional activity recognition systems are limited by fixed vocabularies, predefined labels, and rigid interaction categories that often rely on choreographed videos and overlook concurrent interactive groups. These limitations make such systems less adaptable to real-world scenarios, where interactions are diverse and unpredictable. In this paper, we propose an open vocabulary human-to-human interaction recognition (OV-HHIR) framework that leverages large language models to generate open-ended textual descriptions of both seen and unseen human interactions in open-world settings without being confined to a fixed vocabulary. Additionally, we create a comprehensive, large-scale human-to-human interaction dataset by standardizing and combining existing public human interaction datasets into a unified benchmark. Extensive experiments demonstrate that our method outperforms traditional fixed-vocabulary classification systems and existing cross-modal language models for video understanding, setting the stage for more intelligent and adaptable visual understanding systems in surveillance and beyond.	 | 理解人类之间的互动，尤其是在公共安全监控等情境中，对于监控和维护安全至关重要。传统的活动识别系统受到固定词汇、预定义标签和刚性互动类别等方面的限制，这些类别往往依赖于编排好的视频并忽视了同时进行的互动群体。这些限制使得这类系统在面对真实世界多变且不可预测的互动时不够灵活。在本文中，我们提出了一种基于开放词汇的人类之间互动识别（OV-HHIR）框架，通过利用大规模语言模型来生成在开放世界设置中对已见和未见的人类互动的开放性文本描述，而不受固定词汇的限制。此外，我们通过标准化并整合现有的公开人类互动数据集，创建了一个全面且大规模的人类之间互动数据集，作为统一的基准。广泛实验表明，我们的方法在视频理解领域超越了传统的固定词汇分类系统和现有的跨模态语言模型，为监控以及更广泛的领域内的更智能、更灵活的视觉理解系统奠定了基础。
2501.00426	 | B2Net: Camouflaged Object Detection via Boundary Aware and Boundary Fusion	 | Junmin Cai,Han Sun,Ningzhong Liu	 | B2Net是一种新的网络架构，通过在网络不同阶段重复使用边界感知模块来提高伪装目标检测的准确性，该方法包括残差特征增强模块、边界感知模块和跨尺度边界融合模块，并在三个基准数据集上优于其他最先进的方法。	 | Camouflaged object detection (COD) aims to identify objects in images that are well hidden in the environment due to their high similarity to the background in terms of texture and color. However, existing most boundary-guided camouflage object detection algorithms tend to generate object boundaries early in the network, and inaccurate edge priors often introduce noises in object detection. Address on this issue, we propose a novel network named B2Net aiming to enhance the accuracy of obtained boundaries by reusing boundary-aware modules at different stages of the network. Specifically, we present a Residual Feature Enhanced Module (RFEM) with the goal of integrating more discriminative feature representations to enhance detection accuracy and reliability. After that, the Boundary Aware Module (BAM) is introduced to explore edge cues twice by integrating spatial information from low-level features and semantic information from high-level features. Finally, we design the Cross-scale Boundary Fusion Module(CBFM) that integrate information across different scales in a top-down manner, merging boundary features with object features to obtain a comprehensive feature representation incorporating boundary information. Extensive experimental results on three challenging benchmark datasets demonstrate that our proposed method B2Net outperforms 15 state-of-art methods under widely used evaluation metrics. Code will be made publicly available.	 | 伪装目标检测（COD）旨在识别那些因与背景在纹理和颜色方面的高度相似而在图像中被很好地隐藏的目标。然而，现有的大多数边界引导的伪装目标检测算法倾向于在网络早期生成目标边界，不准确的边缘先验往往会引入检测中的噪声。为了解决这个问题，我们提出了一种名为B2Net的新网络，旨在通过在网络的不同阶段重新使用边界感知模块来增强获得边界的准确性。具体来说，我们提出了一种残差特征增强模块（RFEM），旨在整合更多具有区分性的特征表示，以提高检测的准确性和可靠性。随后，我们引入了边界感知模块（BAM），通过结合低级特征的空间信息和高级特征的语义信息，两次探索边缘线索。最后，我们设计了跨尺度边界融合模块（CBFM），以自上而下的方式整合不同尺度的信息，将边界特征与对象特征合并，从而获得包含边界信息的综合特征表示。在三个具有挑战性的基准数据集上的广泛实验结果表明，我们的方法B2Net在广泛使用的评估指标下优于15种最先进的方法。代码将公开发布。
2501.00375	 | Token Pruning for Caching Better: 9 Times Acceleration on Stable Diffusion for Free	 | Evelyn Zhang,Bang Xiao,Jiayi Tang,Qianli Ma,Chang Zou,Xuefei Ning,Xuming Hu,Linfeng Zhang	 | 文章介绍了一种名为动态感知的令牌剪枝（DaTo）的方法，用于解决特征缓存的局限性，通过选择性修剪动态较低的令牌，使得特征在时间步之间保持动态变化，从而在加速稳定扩散的同时提高图像质量。在ImageNet和COCO-30k数据集上的实验表明，DaTo可以分别实现9倍和7倍的速度提升，并且FID分别降低了0.33和2.17。	 | Stable Diffusion has achieved remarkable success in the field of text-to-image generation, with its powerful generative capabilities and diverse generation results making a lasting impact. However, its iterative denoising introduces high computational costs and slows generation speed, limiting broader adoption. The community has made numerous efforts to reduce this computational burden, with methods like feature caching attracting attention due to their effectiveness and simplicity. Nonetheless, simply reusing features computed at previous timesteps causes the features across adjacent timesteps to become similar, reducing the dynamics of features over time and ultimately compromising the quality of generated images. In this paper, we introduce a dynamics-aware token pruning (DaTo) approach that addresses the limitations of feature caching. DaTo selectively prunes tokens with lower dynamics, allowing only high-dynamic tokens to participate in self-attention layers, thereby extending feature dynamics across timesteps. DaTo combines feature caching with token pruning in a training-free manner, achieving both temporal and token-wise information reuse. Applied to Stable Diffusion on the ImageNet, our approach delivered a 9$\times$ speedup while reducing FID by 0.33, indicating enhanced image quality. On the COCO-30k, we observed a 7$\times$ acceleration coupled with a notable FID reduction of 2.17.	 | 稳定扩散（Stable Diffusion）在文本到图像生成领域取得了显著的成功，其强大的生成能力和多样化的生成结果产生了持久的影响。然而，其迭代去噪引入了高昂的计算成本并减慢了生成速度，限制了其更广泛的采用。社区已经做出了许多努力来减少这种计算负担，方法如特征缓存因为其有效性与简洁性而受到关注。不过，简单地在先前时间步中重复使用计算出的特征会导致相邻时间步的特征变得相似，从而降低了特征随时间的变化性，并最终损害了生成图像的质量。在本文中，我们介绍了一种动态感知的令牌剪枝（DaTo）方法，以解决特征缓存的局限性。DaTo 选择性地修剪动态较低的令牌，仅允许高动态的令牌参与自注意力层，从而在时间步之间延续特征动态。DaTo 以无需训练的方式结合了特征缓存与令牌修剪，实现了时间和令牌级的信息重用。将 DaTo 应用于 ImageNet 上的稳定扩散，我们的方法实现了 9 倍的速度提升，同时将 FID 减少了 0.33，表明图像质量得到了增强。在 COCO-30k 上，我们观察到了 7 倍的加速，并且 FID 减少了 2.17。
2501.00360	 | A Novel Shape Guided Transformer Network for Instance Segmentation in Remote Sensing Images	 | Dawen Yu,Shunping Ji	 | 该研究提出了一种新颖的形状引导变压器网络（SGTN），结合了垂直和水平的一维全局自注意力机制的LSwin编码器和形状引导模块（SGM），以提高遥感图像（RSI）中的实例分割性能，并在多个数据集上取得了最高的平均精度分数。	 | Instance segmentation performance in remote sensing images (RSIs) is significantly affected by two issues: how to extract accurate boundaries of objects from remote imaging through the dynamic atmosphere, and how to integrate the mutual information of related object instances scattered over a vast spatial region. In this study, we propose a novel Shape Guided Transformer Network (SGTN) to accurately extract objects at the instance level. Inspired by the global contextual modeling capacity of the self-attention mechanism, we propose an effective transformer encoder termed LSwin, which incorporates vertical and horizontal 1D global self-attention mechanisms to obtain better global-perception capacity for RSIs than the popular local-shifted-window based Swin Transformer. To achieve accurate instance mask segmentation, we introduce a shape guidance module (SGM) to emphasize the object boundary and shape information. The combination of SGM, which emphasizes the local detail information, and LSwin, which focuses on the global context relationships, achieve excellent RSI instance segmentation. Their effectiveness was validated through comprehensive ablation experiments. Especially, LSwin is proved better than the popular ResNet and Swin transformer encoder at the same level of efficiency. Compared to other instance segmentation methods, our SGTN achieves the highest average precision (AP) scores on two single-class public datasets (WHU dataset and BITCC dataset) and a multi-class public dataset (NWPU VHR-10 dataset). Code will be available at http://gpcv.whu.edu.cn/data/.	 | 遥感图像（RSI）中的实例分割性能受到两个问题的显著影响：如何通过动态大气从遥感成像中准确提取物体边界，以及如何整合分布在广阔空间区域中的相关物体实例的相互信息。在本研究中，我们提出了一种新颖的形状引导变压器网络（SGTN），以在实例级别准确提取物体。受到自注意力机制的全局上下文建模能力的启发，我们提出了一种有效的变压器编码器LSwin，它结合了垂直和水平的一维全局自注意力机制，从而在RSIs方面获得了优于流行的基于局部移窗的Swin Transformer更好的全局感知能力。为了实现准确的实例掩码分割，我们引入了一个形状引导模块（SGM），以强调物体边界和形状信息。SGM强调局部细节信息与LSwin关注全局上下文关系的结合，实现了出色的RSI实例分割效果。其有效性通过全面的消融实验得到了验证。特别地，LSwin在效率相同的情况下证明优于流行的ResNet和Swin Transformer编码器。与其它实例分割方法相比，我们的SGTN在两个单类公共数据集（WHU数据集和BITCC数据集）和一个多类公共数据集（NWPU VHR-10数据集）上分别获得了最高的平均精度（AP）分数。代码将可以在http://gpcv.whu.edu.cn/data/获取。
2501.00358	 | Embodied VideoAgent: Persistent Memory from Egocentric Videos and Embodied Sensors Enables Dynamic Scene Understanding	 | Yue Fan,Xiaojian Ma,Rongpeng Su,Jun Guo,Rujie Wu,Xi Chen,Qing Li	 | 本文提出了一个基于大规模语言模型的具身代理VideoAgent，它能够从第一人称视角理解动态3D场景并构建场景记忆，该代理在多项复杂推理和规划任务中表现出色，相比现有方法分别提升了4.9%至11.7%。	 | This paper investigates the problem of understanding dynamic 3D scenes from egocentric observations, a key challenge in robotics and embodied AI. Unlike prior studies that explored this as long-form video understanding and utilized egocentric video only, we instead propose an LLM-based agent, Embodied VideoAgent, which constructs scene memory from both egocentric video and embodied sensory inputs (e.g. depth and pose sensing). We further introduce a VLM-based approach to automatically update the memory when actions or activities over objects are perceived. Embodied VideoAgent attains significant advantages over counterparts in challenging reasoning and planning tasks in 3D scenes, achieving gains of 4.9% on Ego4D-VQ3D, 5.8% on OpenEQA, and 11.7% on EnvQA. We have also demonstrated its potential in various embodied AI tasks including generating embodied interactions and perception for robot manipulation. The code and demo will be made public.	 | 本文研究了从第一人称视角理解动态3D场景的问题，这是机器人技术和具身AI中的一个关键挑战。与之前的研究不同，这些研究将这一问题视为长时间视频理解，并仅使用第一人称视角视频，我们提出了一个基于LLM的代理——具身VideoAgent，它从第一人称视频和具身感官输入（例如深度和姿态感应）中构建场景记忆。我们还引入了一种基于VLM的方法，可以在感知到动作或物体活动时自动更新记忆。在3D场景中的复杂推理和规划任务中，具身VideoAgent表现出显著的优势，分别在Ego4D-VQ3D、OpenEQA和EnvQA上取得了4.9%、5.8%和11.7%的提升。我们还展示了它在各种具身AI任务中的潜力，包括为机器人操作生成具身交互和感知。代码和演示将在未来公开。
2501.00352	 | PanoSLAM: Panoptic 3D Scene Reconstruction via Gaussian SLAM	 | Runnan Chen,Zhaoqing Wang,Jiepeng Wang,Yuexin Ma,Mingming Gong,Wenping Wang,Tongliang Liu	 | PanoSLAM是一种新颖的SLAM系统，能够在统一框架中实现几何重建、3D语义分割和3D实例分割，显著提升了机器人技术和增强现实应用中的场景理解能力。通过在线时空提升模块，PanoSLAM能够有效从RGB-D视频中重建开放世界的全景三维场景，其建图和跟踪准确性优于现有语义SLAM方法。	 | Understanding geometric, semantic, and instance information in 3D scenes from sequential video data is essential for applications in robotics and augmented reality. However, existing Simultaneous Localization and Mapping (SLAM) methods generally focus on either geometric or semantic reconstruction. In this paper, we introduce PanoSLAM, the first SLAM system to integrate geometric reconstruction, 3D semantic segmentation, and 3D instance segmentation within a unified framework. Our approach builds upon 3D Gaussian Splatting, modified with several critical components to enable efficient rendering of depth, color, semantic, and instance information from arbitrary viewpoints. To achieve panoptic 3D scene reconstruction from sequential RGB-D videos, we propose an online Spatial-Temporal Lifting (STL) module that transfers 2D panoptic predictions from vision models into 3D Gaussian representations. This STL module addresses the challenges of label noise and inconsistencies in 2D predictions by refining the pseudo labels across multi-view inputs, creating a coherent 3D representation that enhances segmentation accuracy. Our experiments show that PanoSLAM outperforms recent semantic SLAM methods in both mapping and tracking accuracy. For the first time, it achieves panoptic 3D reconstruction of open-world environments directly from the RGB-D video. (https://github.com/runnanchen/PanoSLAM)	 | 理解三维场景中的几何、语义和实例信息对于机器人技术和增强现实应用至关重要。然而，现有的Simultaneous Localization and Mapping (SLAM)方法大多专注于几何重建或语义重建。本文介绍了一种名为PanoSLAM的新方法，它是第一个在统一框架中整合几何重建、3D语义分割和3D实例分割的SLAM系统。我们的方法基于三维Gaussian Splatting，并通过几个关键组件的修改，能够高效地从任意视角渲染深度、颜色、语义和实例信息。为了从序贯RGB-D视频中实现全景三维场景重建，我们提出了一种在线时空提升(Online Spatial-Temporal Lifting, STL)模块，将视觉模型的2D全景预测转换为3D Gaussian表示。该STL模块通过在多视角输入中细化伪标签来解决2D预测中的标签噪声和不一致问题，从而创建一个连贯的3D表示，提高分割准确性。我们的实验表明，PanoSLAM在建图和跟踪准确性方面优于最近的语义SLAM方法。首次实现了直接从RGB-D视频中重建开放世界环境的全景3D重建。（https://github.com/runnanchen/PanoSLAM）
2501.00346	 | CNC: Cross-modal Normality Constraint for Unsupervised Multi-class Anomaly Detection	 | Xiaolei Wang,Xiaoyang Wang,Huihui Bai,Eng Gee Lim,Jimin Xiao	 | 现有的无监督蒸馏方法依赖于解码器重构正常样本的特征，但该解码器在重构异常样本特征时仍表现良好，导致性能下降。为此，提出了一种新方法，通过类无感知的学习提示和门控混合专家模块，来抑制解码器对异常模式的泛化，从而提高多类异常检测的性能。	 | Existing unsupervised distillation-based methods rely on the differences between encoded and decoded features to locate abnormal regions in test images. However, the decoder trained only on normal samples still reconstructs abnormal patch features well, degrading performance. This issue is particularly pronounced in unsupervised multi-class anomaly detection tasks. We attribute this behavior to over-generalization(OG) of decoder: the significantly increasing diversity of patch patterns in multi-class training enhances the model generalization on normal patches, but also inadvertently broadens its generalization to abnormal patches. To mitigate OG, we propose a novel approach that leverages class-agnostic learnable prompts to capture common textual normality across various visual patterns, and then apply them to guide the decoded features towards a normal textual representation, suppressing over-generalization of the decoder on abnormal patterns. To further improve performance, we also introduce a gated mixture-of-experts module to specialize in handling diverse patch patterns and reduce mutual interference between them in multi-class training. Our method achieves competitive performance on the MVTec AD and VisA datasets, demonstrating its effectiveness.	 | 现有的无监督蒸馏方法依赖于编码和解码特征之间的差异来检测测试图像中的异常区域。然而，仅在正常样本上训练的解码器仍然能够很好地重构异常区域的特征，这会降低性能。特别是在无监督多类异常检测任务中，这一问题尤为明显。我们将其归因于解码器的过度泛化（OG）现象：多类训练中斑块模式的显著多样性增强了模型在正常斑块上的泛化能力，但也无意中扩展了其对异常斑块的泛化能力。为了减轻OG现象，我们提出了一种新颖的方法，利用类无感知的学习提示来捕获各种视觉模式下的共同文本正常性，并将这些提示应用于引导解码后的特征向正常的文本表示转变，从而抑制解码器对异常模式的过度泛化。为了进一步提高性能，我们还引入了一个门控混合专家模块，以专门处理多类训练中的各种斑块模式，并减少它们之间的相互干扰。我们的方法在MVTec AD和VisA数据集上取得了竞争力的表现，证明了其有效性。
2501.00342	 | SG-Splatting: Accelerating 3D Gaussian Splatting with Spherical Gaussians	 | Yiwen Wang,Siyuan Chen,Ran Yi	 | 本文提出了一种基于球面高斯的颜色表示方法 SG-Splatting，以提高新颖视角合成的渲染速度和质量，通过减少参数数量和优化组织策略，显著降低了存储需求和计算开销。此外，还提出了一种混合表示方法，结合低阶球面谐波，进一步提升渲染质量，使其适用于实时应用。	 | 3D Gaussian Splatting is emerging as a state-of-the-art technique in novel view synthesis, recognized for its impressive balance between visual quality, speed, and rendering efficiency. However, reliance on third-degree spherical harmonics for color representation introduces significant storage demands and computational overhead, resulting in a large memory footprint and slower rendering speed. We introduce SG-Splatting with Spherical Gaussians based color representation, a novel approach to enhance rendering speed and quality in novel view synthesis. Our method first represents view-dependent color using Spherical Gaussians, instead of three degree spherical harmonics, which largely reduces the number of parameters used for color representation, and significantly accelerates the rendering process. We then develop an efficient strategy for organizing multiple Spherical Gaussians, optimizing their arrangement to achieve a balanced and accurate scene representation. To further improve rendering quality, we propose a mixed representation that combines Spherical Gaussians with low-degree spherical harmonics, capturing both high- and low-frequency color information effectively. SG-Splatting also has plug-and-play capability, allowing it to be easily integrated into existing systems. This approach improves computational efficiency and overall visual fidelity, making it a practical solution for real-time applications.	 | 3D 高斯点积技术正在成为新颖视角合成领域的前沿方法，因其在视觉质量、速度和渲染效率方面表现出色而受到认可。然而，依赖于三次球面谐波进行颜色表示引入了显著的存储需求和计算开销，导致较大的内存占用和较慢的渲染速度。我们提出了一种基于球面高斯的颜色表示方法 SG-Splatting，以增强新颖视角合成中的渲染速度和质量。该方法首先使用球面高斯来表示视角依赖的颜色，而不是三阶球面谐波，这大大减少了颜色表示所使用的参数数量，并显著加快了渲染过程。我们还开发了一种高效的策略来组织多个球面高斯，优化它们的排列以实现场景表示的平衡和准确性。为了进一步提高渲染质量，我们提出了一种混合表示方法，结合了球面高斯和低阶球面谐波，有效地捕捉了高频和低频颜色信息。SG-Splatting 具有即插即用的功能，可以轻松集成到现有系统中。这种方法提高了计算效率并增强了整体视觉保真度，使其成为实时应用的实用解决方案。
2501.00340	 | Dynamic Prompt Adjustment for Multi-Label Class-Incremental Learning	 | Haifeng Zhao,Yuguang Jin,Leilei Ma	 | 该研究旨在解决多标签类增量学习（MLCIL）问题，通过结合改进的数据重播机制和提示损失，有效防止模型的知识遗忘，从而在多个基准数据集上显著提高了MLCIL任务的性能。	 | Significant advancements have been made in single label incremental learning (SLCIL),yet the more practical and challenging multi label class incremental learning (MLCIL) remains understudied. Recently,visual language models such as CLIP have achieved good results in classification tasks. However,directly using CLIP to solve MLCIL issue can lead to catastrophic forgetting. To tackle this issue, we integrate an improved data replay mechanism and prompt loss to curb knowledge forgetting. Specifically,our model enhances the prompt information to better adapt to multi-label classification tasks and employs confidence-based replay strategy to select representative samples. Moreover, the prompt loss significantly reduces the model's forgetting of previous knowledge. Experimental results demonstrate that our method has substantially improved the performance of MLCIL tasks across multiple benchmark datasets,validating its effectiveness.	 | 在单标签增量学习（SLCIL）方面已经取得了显著的进步，但更为实际和具有挑战性的多标签类增量学习（MLCIL）仍未得到充分研究。最近，视觉语言模型如CLIP在分类任务中取得了良好的结果。然而，直接使用CLIP解决MLCIL问题可能会导致灾难性遗忘。为了解决这一问题，我们整合了一个改进的数据重播机制和提示损失，以遏制知识遗忘。具体来说，我们的模型增强了提示信息，使其更好地适应多标签分类任务，并采用基于置信度的重播策略选择代表性样本。此外，提示损失显著降低了模型对先前知识的遗忘。实验结果表明，我们的方法在多个基准数据集上大大提高了MLCIL任务的性能，验证了其有效性。
2501.00326	 | OVGaussian: Generalizable 3D Gaussian Segmentation with Open Vocabularies	 | Runnan Chen,Xiangyu Sun,Zhaoqing Wang,Youquan Liu,Jiepeng Wang,Lingdong Kong,Jiankang Deng,Mingming Gong,Liang Pan,Wenping Wang,Tongliang Liu	 | 本文提出了一种基于3D高斯表示的通用开放词汇（Open-Vocabulary, OV）3D语义分割框架OVGaussian，通过构建大规模3D场景数据集SegGaussian并引入通用语义栅化（GSR）与跨模态一致性学习（CCL）框架，实现了跨场景、跨域和新颖视角的一般化能力。	 | Open-vocabulary scene understanding using 3D Gaussian (3DGS) representations has garnered considerable attention. However, existing methods mostly lift knowledge from large 2D vision models into 3DGS on a scene-by-scene basis, restricting the capabilities of open-vocabulary querying within their training scenes so that lacking the generalizability to novel scenes. In this work, we propose \textbf{OVGaussian}, a generalizable \textbf{O}pen-\textbf{V}ocabulary 3D semantic segmentation framework based on the 3D \textbf{Gaussian} representation. We first construct a large-scale 3D scene dataset based on 3DGS, dubbed \textbf{SegGaussian}, which provides detailed semantic and instance annotations for both Gaussian points and multi-view images. To promote semantic generalization across scenes, we introduce Generalizable Semantic Rasterization (GSR), which leverages a 3D neural network to learn and predict the semantic property for each 3D Gaussian point, where the semantic property can be rendered as multi-view consistent 2D semantic maps. In the next, we propose a Cross-modal Consistency Learning (CCL) framework that utilizes open-vocabulary annotations of 2D images and 3D Gaussians within SegGaussian to train the 3D neural network capable of open-vocabulary semantic segmentation across Gaussian-based 3D scenes. Experimental results demonstrate that OVGaussian significantly outperforms baseline methods, exhibiting robust cross-scene, cross-domain, and novel-view generalization capabilities. Code and the SegGaussian dataset will be released. (https://github.com/runnanchen/OVGaussian).	 | 使用3D高斯（3DGS）表示进行开放词汇场景理解已经引起了广泛关注。然而，现有的方法大多是从大型2D视觉模型中提取知识并逐场景地应用到3DGS上，这限制了开放词汇查询的能力，使其在训练场景内的查询能力受限，缺乏对新场景的一般化能力。在本文中，我们提出了一个基于3D高斯表示的通用开放词汇（Open-Vocabulary, OV）3D语义分割框架，命名为\textbf{OVGaussian}。我们首先基于3DGS构建了一个大规模的3D场景数据集，命名为\textbf{SegGaussian}，提供了高斯点和多视角图像的详细语义和实例注释。为了促进场景间语义的一般化，我们引入了通用语义栅格化（GSR），利用三维神经网络来学习和预测每个三维高斯点的语义属性，该语义属性可以渲染为多视角一致的2D语义图。之后，我们提出了一种跨模态一致性学习（CCL）框架，利用SegGaussian中的2D图像和3D高斯点的开放词汇注释来训练三维神经网络，使其能够在基于高斯的三维场景中进行开放词汇语义分割。实验结果表明，OVGaussian显著优于基线方法，展示了跨场景、跨域和新颖视角的一般化能力。代码和SegGaussian数据集将被发布。（https://github.com/runnanchen/OVGaussian）。
2501.00321	 | OCRBench v2: An Improved Benchmark for Evaluating Large Multimodal Models on Visual Text Localization and Reasoning	 | Ling Fu,Biao Yang,Zhebin Kuang,Jiajun Song,Yuzhe Li,Linghao Zhu,Qidi Luo,Xinyu Wang,Hao Lu,Mingxin Huang,Zhang Li,Guozhi Tang,Bin Shan,Chunhui Lin,Qi Liu,Binghong Wu,Hao Feng,Hao Liu,Can Huang,Jingqun Tang,Wei Chen,Lianwen Jin,Yuliang Liu,Xiang Bai	 | OCRBench v2 是一个大规模双语文本中心基准测试，包含4倍多的任务、31种场景和详尽的评估指标，旨在评估大型多模态模型在文本识别任务中的表现，发现多数模型在不常见文本识别、精细感知等方面存在局限性。	 | Scoring the Optical Character Recognition (OCR) capabilities of Large Multimodal Models (LMMs) has witnessed growing interest recently. Existing benchmarks have highlighted the impressive performance of LMMs in text recognition; however, their abilities on certain challenging tasks, such as text localization, handwritten content extraction, and logical reasoning, remain underexplored. To bridge this gap, we introduce OCRBench v2, a large-scale bilingual text-centric benchmark with currently the most comprehensive set of tasks (4x more tasks than the previous multi-scene benchmark OCRBench), the widest coverage of scenarios (31 diverse scenarios including street scene, receipt, formula, diagram, and so on), and thorough evaluation metrics, with a total of 10,000 human-verified question-answering pairs and a high proportion of difficult samples. After carefully benchmarking state-of-the-art LMMs on OCRBench v2, we find that 20 out of 22 LMMs score below 50 (100 in total) and suffer from five-type limitations, including less frequently encountered text recognition, fine-grained perception, layout perception, complex element parsing, and logical reasoning. The benchmark and evaluation scripts are available at https://github.com/Yuliang-liu/MultimodalOCR.	 | 近年来，评估大型多模态模型（LMMs）的光学字符识别（OCR）能力逐渐引起了人们的关注。现有的基准测试显示了LMMs在文本识别任务中的出色表现；然而，在某些具有挑战性的任务，如文本定位、手写内容提取和逻辑推理方面的能力，仍然有待探索。为了解决这一问题，我们引入了OCRBench v2，这是一个大规模双语文本中心基准测试，具有目前最为全面的任务集（比之前的多场景基准测试OCRBench多出4倍的任务）、最广泛的场景覆盖（包括街头场景、收据、公式、图表等31种多样场景）以及详尽的评估指标，其中包括10,000对经过人工验证的问题-回答对和大量困难样本。通过仔细对OCRBench v2上的最先进的LMMs进行基准测试，我们发现22个LMM中有20个的分数低于50（满分为100分），并存在五种局限性，包括不常见的文本识别、精细感知、布局感知、复杂元素解析以及逻辑推理。该基准测试和评估脚本可在https://github.com/Yuliang-liu/MultimodalOCR获取。
2501.00318	 | Improving Text-based Person Search via Part-level Cross-modal Correspondence	 | Jicheol Park,Boseung Jeong,Dongwon Kim,Suha Kwak	 | 该研究提出了一种高效的编码器-解码器模型，用于根据自然语言描述查找最相关的人员图像，该模型能够在无需大量监督的情况下提取从粗到细的嵌入向量，并引入了一种新的基于共性的边际排名损失来学习细粒度的部位细节。	 | Text-based person search is the task of finding person images that are the most relevant to the natural language text description given as query. The main challenge of this task is a large gap between the target images and text queries, which makes it difficult to establish correspondence and distinguish subtle differences across people. To address this challenge, we introduce an efficient encoder-decoder model that extracts coarse-to-fine embedding vectors which are semantically aligned across the two modalities without supervision for the alignment. There is another challenge of learning to capture fine-grained information with only person IDs as supervision, where similar body parts of different individuals are considered different due to the lack of part-level supervision. To tackle this, we propose a novel ranking loss, dubbed commonality-based margin ranking loss, which quantifies the degree of commonality of each body part and reflects it during the learning of fine-grained body part details. As a consequence, it enables our method to achieve the best records on three public benchmarks.	 | 基于文本的人员搜索是根据给定的自然语言文本描述找到最相关的人员图像的任务。这项任务的主要挑战是在目标图像和文本查询之间存在巨大的差距，这使得难以建立对应关系并区分不同人的细微差异。为了解决这一挑战，我们引入了一种高效的编码器-解码器模型，该模型提取从粗到细的嵌入向量，这些向量在两个模态之间语义对齐，而不依赖于监督来实现对齐。另一个挑战是如何仅通过人员ID作为监督来学习捕捉细粒度信息，由于缺乏部分级别的监督，不同个体的相似部位被认为是不同的。为了解决这个问题，我们提出了一种新的排名损失，称为基于共性的边际排名损失，该损失量化了每个部位的共性程度，并在学习细粒度部位细节时反映了这一点。因此，它使我们的方法在三个公开的基准测试中取得了最佳记录。
2501.00317	 | Spatio-Temporal Multi-Subgraph GCN for 3D Human Motion Prediction	 | Jiexin Wang,Yiju Guo,Bing Su	 | 本文提出了时空多子图图形卷积网络（STMS-GCN），以捕捉人体运动中的复杂时空依赖性，通过分离时域和空域依赖性的建模，并利用时空信息一致性约束机制实现多尺度的跨领域知识转移，从而在标准HMP基准数据集上取得了优越性能。	 | Human motion prediction (HMP) involves forecasting future human motion based on historical data. Graph Convolutional Networks (GCNs) have garnered widespread attention in this field for their proficiency in capturing relationships among joints in human motion. However, existing GCN-based methods tend to focus on either temporal-domain or spatial-domain features, or they combine spatio-temporal features without fully leveraging the complementarity and cross-dependency of these two features. In this paper, we propose the Spatial-Temporal Multi-Subgraph Graph Convolutional Network (STMS-GCN) to capture complex spatio-temporal dependencies in human motion. Specifically, we decouple the modeling of temporal and spatial dependencies, enabling cross-domain knowledge transfer at multiple scales through a spatio-temporal information consistency constraint mechanism. Besides, we utilize multiple subgraphs to extract richer motion information and enhance the learning associations of diverse subgraphs through a homogeneous information constraint mechanism. Extensive experiments on the standard HMP benchmarks demonstrate the superiority of our method.	 | 人体运动预测（HMP）涉及基于历史数据预测未来的人体运动。图形卷积网络（GCNs）在这一领域引起了广泛关注，因为它们能够在捕捉人体运动中关节之间的关系方面表现出色。然而，现有的基于GCN的方法往往侧重于时域或空域特征，或者结合时空特征但未能充分利用这两种特征之间的互补性和交叉依赖性。在本文中，我们提出了时空多子图图形卷积网络（STMS-GCN），旨在捕捉人体运动中的复杂时空依赖性。具体来说，我们分离了时域和空域依赖性的建模，通过时空信息一致性约束机制实现多尺度的跨领域知识转移。此外，我们利用多个子图提取更丰富的运动信息，并通过同质信息约束机制增强不同子图之间的学习关联。在标准HMP基准数据集上的广泛实验结果表明了我们方法的优越性。
2501.00315	 | Temporal Dynamics Decoupling with Inverse Processing for Enhancing Human Motion Prediction	 | Jiexin Wang,Yiju Guo,Bing Su	 | 本文提出了一种名为$TD^2IP$的新方法，通过时间解耦解码与逆向处理，分别处理历史和未来的运动序列，并利用双向时间相关性，从而改善了人体运动的预测性能。实验结果表明，该方法在多种现有方法中表现出高度的适应性和优越性。	 | Exploring the bridge between historical and future motion behaviors remains a central challenge in human motion prediction. While most existing methods incorporate a reconstruction task as an auxiliary task into the decoder, thereby improving the modeling of spatio-temporal dependencies, they overlook the potential conflicts between reconstruction and prediction tasks. In this paper, we propose a novel approach: Temporal Decoupling Decoding with Inverse Processing (\textbf{$TD^2IP$}). Our method strategically separates reconstruction and prediction decoding processes, employing distinct decoders to decode the shared motion features into historical or future sequences. Additionally, inverse processing reverses motion information in the temporal dimension and reintroduces it into the model, leveraging the bidirectional temporal correlation of human motion behaviors. By alleviating the conflicts between reconstruction and prediction tasks and enhancing the association of historical and future information, \textbf{$TD^2IP$} fosters a deeper understanding of motion patterns. Extensive experiments demonstrate the adaptability of our method within existing methods.	 | 探索历史和未来运动行为之间的桥梁仍然是人体运动预测中的一个核心挑战。虽然大多数现有方法将重建任务作为辅助任务嵌入到解码器中，从而改善了时空依赖性的建模，但它们忽略了重建任务和预测任务之间的潜在冲突。在本文中，我们提出了一种新的方法：时间解耦解码与逆向处理（\textbf{$TD^2IP$}）。我们的方法战略性地分离了重建和预测解码过程，分别使用不同的解码器将共享的运动特征解码为历史或未来序列。此外，逆向处理在时间维度上反转运动信息并将其重新引入模型中，利用了人体运动行为的双向时间相关性。通过缓解重建和预测任务之间的冲突，并增强历史和未来信息的关联性，\textbf{$TD^2IP$} 深化了对运动模式的理解。广泛的实验表明，我们的方法在现有方法中具有高度的适应性。
2501.00303	 | SAM-Aware Graph Prompt Reasoning Network for Cross-Domain Few-Shot Segmentation	 | Shi-Feng Peng,Guolei Sun,Yong Li,Hongsong Wang,Guo-Sen Xie	 | 本文提出了一个SAM感知的图提示推理网络（GPRN），该网络通过SAM生成掩码并构建图结构来增强跨域少量样本分割的特征表示，从而提高预测准确性并在四个标准数据集上取得了最佳性能。	 | The primary challenge of cross-domain few-shot segmentation (CD-FSS) is the domain disparity between the training and inference phases, which can exist in either the input data or the target classes. Previous models struggle to learn feature representations that generalize to various unknown domains from limited training domain samples. In contrast, the large-scale visual model SAM, pre-trained on tens of millions of images from various domains and classes, possesses excellent generalizability. In this work, we propose a SAM-aware graph prompt reasoning network (GPRN) that fully leverages SAM to guide CD-FSS feature representation learning and improve prediction accuracy. Specifically, we propose a SAM-aware prompt initialization module (SPI) to transform the masks generated by SAM into visual prompts enriched with high-level semantic information. Since SAM tends to divide an object into many sub-regions, this may lead to visual prompts representing the same semantic object having inconsistent or fragmented features. We further propose a graph prompt reasoning (GPR) module that constructs a graph among visual prompts to reason about their interrelationships and enable each visual prompt to aggregate information from similar prompts, thus achieving global semantic consistency. Subsequently, each visual prompt embeds its semantic information into the corresponding mask region to assist in feature representation learning. To refine the segmentation mask during testing, we also design a non-parameter adaptive point selection module (APS) to select representative point prompts from query predictions and feed them back to SAM to refine inaccurate segmentation results. Experiments on four standard CD-FSS datasets demonstrate that our method establishes new state-of-the-art results. Code: https://github.com/CVL-hub/GPRN.	 | 跨域少量样本分割（CD-FSS）的主要挑战在于训练阶段和推理阶段之间的领域差异，这种差异可能存在于输入数据或目标类别中。先前的模型难以从有限的训练域样本中学习出能够推广到各种未知领域的特征表示。相比之下，大规模视觉模型SAM在数十万张来自不同领域和类别的图像上进行预训练，具有出色的泛化能力。在本文中，我们提出了一个SAM感知的图提示推理网络（GPRN），该网络充分利用SAM来指导CD-FSS特征表示学习，并提高预测准确性。具体而言，我们提出了一种SAM感知的提示初始化模块（SPI），将SAM生成的掩码转换为富含高层语义信息的视觉提示。由于SAM倾向于将一个对象分割成许多子区域，这可能导致表示相同语义对象的视觉提示具有不一致或碎片化的特征。为进一步解决这一问题，我们提出了一个图提示推理（GPR）模块，该模块在视觉提示之间构建图以推理它们之间的关系，并使每个视觉提示能够从相似的提示中聚合信息，从而实现全局语义一致性。随后，每个视觉提示将其实现的语义信息嵌入到相应的掩码区域，以辅助特征表示学习。为了在测试期间细化分割掩码，我们还设计了一个非参数自适应点选择模块（APS），从查询预测中选择代表性点提示，并将它们反馈给SAM以细化不准确的分割结果。在四个标准CD-FSS数据集上的实验表明，我们的方法建立了新的最佳性能。代码：https://github.com/CVL-hub/GPRN。
2501.00300	 | Research on vehicle detection based on improved YOLOv8 network	 | Haocheng Guo,Yaqiong Zhang,Lieyang Chen,Arfat Ahmad Khan	 | 本文提出了一种改进的YOLOv8车辆检测方法，通过使用FasterNet网络替换骨干网络、在Neck部分添加CBAM注意力机制以及修改损失函数为WIoU，提升了检测准确性和速度，实现了在车辆、行人和摩托车检测方面的显著性能提升。	 | The key to ensuring the safe obstacle avoidance function of autonomous driving systems lies in the use of extremely accurate vehicle recognition techniques. However, the variability of the actual road environment and the diverse characteristics of vehicles and pedestrians together constitute a huge obstacle to improving detection accuracy, posing a serious challenge to the realization of this goal. To address the above issues, this paper proposes an improved YOLOv8 vehicle detection method. Specifically, taking the YOLOv8n-seg model as the base model, firstly, the FasterNet network is used to replace the backbone network to achieve the purpose of reducing the computational complexity and memory while improving the detection accuracy and speed; secondly, the feature enhancement is achieved by adding the attention mechanism CBAM to the Neck; and lastly, the loss function CIoU is modified to WIoU, which optimizes the detection box localization while improving the segmentation accuracy. The results show that the improved model achieves 98.3%, 89.1% and 88.4% detection accuracy for car, Person and Motorcycle. Compared with the pre-improvement and YOLOv9 models in six metrics such as Precision.	 | 确保自动驾驶系统安全避障功能的关键在于采用极其精确的车辆识别技术。然而，实际道路环境的多变性和车辆及行人的多样化特征共同构成了提高检测准确性的巨大障碍，对实现这一目标构成了严重挑战。为应对上述问题，本文提出了改进的 YOLOv8 车辆检测方法。具体而言，以 YOLOv8n-seg 模型为基础模型，首先使用 FasterNet 网络替换骨干网络，从而在降低计算复杂度和内存使用的同时提升检测准确性和速度；其次，通过在 Neck 部分添加注意力机制 CBAM 来实现特征增强；最后，将损失函数 CIoU 修改为 WIoU，优化检测框定位的同时提高分割精度。结果显示，改进后的模型在车辆、行人和摩托车的检测准确率分别达到了 98.3%、89.1% 和 88.4%。与改进前的 YOLOv9 模型和六个指标（如 Precision）相比，改进后的模型表现更佳。
2501.00289	 | Dual Diffusion for Unified Image Generation and Understanding	 | Zijie Li,Henry Li,Yichun Shi,Amir Barati Farimani,Yuval Kluger,Linjie Yang,Peng Wang	 | 本文提出了一种大规模且端到端的多模态扩散模型，该模型在视觉语言建模方面取得了显著进步，并能同时处理图像生成、描述和视觉问答等多种任务，展示了多模态扩散建模作为一种替代自回归下个词预测模型的潜力。	 | Diffusion models have gained tremendous success in text-to-image generation, yet still lag behind with visual understanding tasks, an area dominated by autoregressive vision-language models. We propose a large-scale and fully end-to-end diffusion model for multi-modal understanding and generation that significantly improves on existing diffusion-based multimodal models, and is the first of its kind to support the full suite of vision-language modeling capabilities. Inspired by the multimodal diffusion transformer (MM-DiT) and recent advances in discrete diffusion language modeling, we leverage a cross-modal maximum likelihood estimation framework that simultaneously trains the conditional likelihoods of both images and text jointly under a single loss function, which is back-propagated through both branches of the diffusion transformer. The resulting model is highly flexible and capable of a wide range of tasks including image generation, captioning, and visual question answering. Our model attained competitive performance compared to recent unified image understanding and generation models, demonstrating the potential of multimodal diffusion modeling as a promising alternative to autoregressive next-token prediction models.	 | 扩散模型在文本到图像生成方面取得了巨大成功，但在视觉理解任务上仍落后于由自回归视觉语言模型主导的领域。我们提出了一种大规模且完全端到端的多模态扩散模型，该模型在现有的多模态扩散模型上取得了显著改进，并且是首个支持全套视觉语言建模能力的模型。我们的灵感来源于多模态扩散变压器（MM-DiT）和离散扩散语言模型的最新进展，我们利用了一个跨模态最大似然估计框架，该框架在单一损失函数下同时训练图像和文本的条件似然性，该损失函数通过扩散变压器的两个分支进行反向传播。该模型具有高度灵活性，并能完成包括图像生成、描述和视觉问答等一系列任务。我们的模型在与最近统一的图像理解和生成模型的性能比较中表现竞争力，展示了多模态扩散建模作为一种有前景的替代自回归下个词预测模型的可能性。
2501.00243	 | Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained Image Recognition	 | Edwin Arkel Rios,Jansen Christopher Yuanda,Vincent Leon Ghanz,Cheng-Wei Yu,Bo-Cheng Lai,Min-Chun Hu	 | 超细粒度图像识别任务通过分类物种内的子类别（如植物品种）而更具挑战性，本文提出了一种新的跨层聚合分类头和跨层缓存机制，以在显著降低计算成本的同时保持高识别准确性，尤其在使用高分辨率图像时效果显著。	 | Ultra-fine-grained image recognition (UFGIR) is a challenging task that involves classifying images within a macro-category. While traditional FGIR deals with classifying different species, UFGIR goes beyond by classifying sub-categories within a species such as cultivars of a plant. In recent times the usage of Vision Transformer-based backbones has allowed methods to obtain outstanding recognition performances in this task but this comes at a significant cost in terms of computation specially since this task significantly benefits from incorporating higher resolution images. Therefore, techniques such as token reduction have emerged to reduce the computational cost. However, dropping tokens leads to loss of essential information for fine-grained categories, specially as the token keep rate is reduced. Therefore, to counteract the loss of information brought by the usage of token reduction we propose a novel Cross-Layer Aggregation Classification Head and a Cross-Layer Cache mechanism to recover and access information from previous layers in later locations. Extensive experiments covering more than 2000 runs across diverse settings including 5 datasets, 9 backbones, 7 token reduction methods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the proposed plug-and-play modules and allow us to push the boundaries of accuracy vs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to 10\% while maintaining a competitive accuracy to state-of-the-art models. Code is available at: \url{https://github.com/arkel23/CLCA}	 | 超细粒度图像识别（Ultra-fine-grained Image Recognition, UFGIR）是一项挑战性任务，涉及在宏观类别内对图像进行分类。传统精细粒度图像识别（Fine-grained Image Recognition, FGIR）主要关注不同物种的分类，而UFGIR则更进一步，通过分类物种内的子类别，如植物的品种。近年来，基于Vision Transformer的骨干网络的应用使得方法在这一任务中获得了出色的识别性能，但这也带来了显著的计算成本，特别是在该任务可以从高分辨率图像中受益的情况下。因此，诸如token缩减等技术应运而生，以降低计算成本。然而，减少token会损失对细粒度类别而言重要的信息，特别是随着保持率降低时更为明显。因此，为了抵消token缩减带来的信息损失，我们提出了一种新的跨层聚合分类头（Cross-Layer Aggregation Classification Head）和跨层缓存机制（Cross-Layer Cache），以从先前层恢复并访问信息。涵盖超过2000次运行、多种不同设置包括5个数据集、9个骨干网络、7种token缩减方法、5种保持率以及2种图像大小的广泛实验显示，提出的即插即用模块的有效性，并使我们能够在极低的保持token比（最高可达10%）下，仍能保持与最新模型具有竞争力的准确性，从而推动UFGIR的准确性和成本边界。相关代码可在以下链接获取：\url{https://github.com/arkel23/CLCA}
2501.00237	 | Make Domain Shift a Catastrophic Forgetting Alleviator in Class-Incremental Learning	 | Wei Chen,Yi Zhou	 | 本文发现通过引入领域迁移可以显著降低逐步分类学习中的遗忘率，并提出了一种名为DisCo的方法，该方法通过轻量级原型池和对比学习减少任务间干扰，从而提升CIL性能。实验结果验证了DisCo方法在多种CIL任务中的有效性。	 | In the realm of class-incremental learning (CIL), alleviating the catastrophic forgetting problem is a pivotal challenge. This paper discovers a counter-intuitive observation: by incorporating domain shift into CIL tasks, the forgetting rate is significantly reduced. Our comprehensive studies demonstrate that incorporating domain shift leads to a clearer separation in the feature distribution across tasks and helps reduce parameter interference during the learning process. Inspired by this observation, we propose a simple yet effective method named DisCo to deal with CIL tasks. DisCo introduces a lightweight prototype pool that utilizes contrastive learning to promote distinct feature distributions for the current task relative to previous ones, effectively mitigating interference across tasks. DisCo can be easily integrated into existing state-of-the-art class-incremental learning methods. Experimental results show that incorporating our method into various CIL methods achieves substantial performance improvements, validating the benefits of our approach in enhancing class-incremental learning by separating feature representation and reducing interference. These findings illustrate that DisCo can serve as a robust fashion for future research in class-incremental learning.	 | 在逐步分类学习（CIL，Class-Incremental Learning）的领域中，缓解灾难性遗忘问题是一个至关重要的挑战。本文发现了一个出乎意料的观察结果：通过将领域迁移纳入CIL任务中，可以显著降低遗忘率。我们的全面研究证明，引入领域迁移能够使任务间的特征分布更加清晰，有助于减少学习过程中的参数干扰。受此观察的启发，我们提出了一种简单而有效的名为DisCo的方法来处理CIL任务。DisCo引入了一个轻量级的原型池，利用对比学习促进当前任务与先前任务的特征分布的独特性，从而有效地减少了任务之间的干扰。DisCo可以轻松集成到现有的先进CIL方法中。实验结果表明，将我们的方法集成到各种CIL方法中可以实现显著的性能提升，验证了通过分离特征表示和减少干扰来增强CIL的方法益处。这些发现表明，DisCo可以作为一种稳健的方法，为未来CIL的研究提供参考。
2501.00220	 | DecoratingFusion: A LiDAR-Camera Fusion Network with the Combination of Point-level and Feature-level Fusion	 | Zixuan Yin,Han Sun,Ningzhong Liu,Huiyu Zhou,Jiaquan Shen	 | 本文提出了一种名为DecoratorFusion的新方法，它结合了特征层面和点层面的融合，利用校准矩阵建立硬关联来生成对象查询，从而在自动驾驶中提供更有效的三维检测。实验结果表明，该方法在KITTI和Waymo数据集上优于现有方法，显示出显著的优越性。	 | Lidars and cameras play essential roles in autonomous driving, offering complementary information for 3D detection. The state-of-the-art fusion methods integrate them at the feature level, but they mostly rely on the learned soft association between point clouds and images, which lacks interpretability and neglects the hard association between them. In this paper, we combine feature-level fusion with point-level fusion, using hard association established by the calibration matrices to guide the generation of object queries. Specifically, in the early fusion stage, we use the 2D CNN features of images to decorate the point cloud data, and employ two independent sparse convolutions to extract the decorated point cloud features. In the mid-level fusion stage, we initialize the queries with a center heatmap and embed the predicted class labels as auxiliary information into the queries, making the initial positions closer to the actual centers of the targets. Extensive experiments conducted on two popular datasets, i.e. KITTI, Waymo, demonstrate the superiority of DecoratingFusion.	 | 激光雷达和摄像头在自动驾驶中扮演着至关重要的角色，它们提供了互补的三维检测信息。最先进的融合方法在特征层面进行集成，但它们主要依赖于点云和图像之间的学习得到的软关联，缺乏可解释性，并且忽略了它们之间的硬关联。本文中，我们将特征层面的融合与点层面的融合结合起来，利用校准矩阵建立的硬关联来引导生成对象查询。具体来说，在早期融合阶段，我们使用图像的2D CNN特征来装饰点云数据，并使用两个独立的稀疏卷积提取装饰后的点云特征。在中间融合阶段，我们用中心热图初始化查询，并将预测的类别标签嵌入到查询中作为辅助信息，从而使初始位置更接近目标的实际中心。在KITTI和Waymo两个流行数据集上进行的 extensive 实验表明，DecoratorFusion 具有明显的优越性。
2501.00192	 | MLLM-as-a-Judge for Image Safety without Human Labeling	 | Zhenting Wang,Shuming Hu,Shiyu Zhao,Xiaowen Lin,Felix Juefei-Xu,Zhuowei Li,Ligong Han,Harihar Subramanyam,Li Chen,Jianfa Chen,Nan Jiang,Lingjuan Lyu,Shiqing Ma,Dimitris N. Metaxas,Ankit Jain	 | 随着AI生成内容的兴起，识别有害图像成为一个重要挑战，传统的基于人类标注的微调方法存在耗时、昂贵和难以适应复杂安全规则更新的问题。为此，研究提出了一种新的基于预训练多模态大型语言模型的方法，通过客观化安全规则、评估规则与图像的相关性及进行去偏见的标记概率快速判断和逻辑推理，有效提升了零样本图像安全判断的准确性。	 | Image content safety has become a significant challenge with the rise of visual media on online platforms. Meanwhile, in the age of AI-generated content (AIGC), many image generation models are capable of producing harmful content, such as images containing sexual or violent material. Thus, it becomes crucial to identify such unsafe images based on established safety rules. Pre-trained Multimodal Large Language Models (MLLMs) offer potential in this regard, given their strong pattern recognition abilities. Existing approaches typically fine-tune MLLMs with human-labeled datasets, which however brings a series of drawbacks. First, relying on human annotators to label data following intricate and detailed guidelines is both expensive and labor-intensive. Furthermore, users of safety judgment systems may need to frequently update safety rules, making fine-tuning on human-based annotation more challenging. This raises the research question: Can we detect unsafe images by querying MLLMs in a zero-shot setting using a predefined safety constitution (a set of safety rules)? Our research showed that simply querying pre-trained MLLMs does not yield satisfactory results. This lack of effectiveness stems from factors such as the subjectivity of safety rules, the complexity of lengthy constitutions, and the inherent biases in the models. To address these challenges, we propose a MLLM-based method includes objectifying safety rules, assessing the relevance between rules and images, making quick judgments based on debiased token probabilities with logically complete yet simplified precondition chains for safety rules, and conducting more in-depth reasoning with cascaded chain-of-thought processes if necessary. Experiment results demonstrate that our method is highly effective for zero-shot image safety judgment tasks.	 | 随着在线平台上视觉媒体的兴起，图像内容安全成为了一个重要挑战。同时，在AI生成内容（AIGC）的时代，许多图像生成模型能够生成有害内容，如包含色情或暴力的图像。因此，根据既定的安全规则识别这些不安全的图像变得至关重要。预训练的多模态大型语言模型（MLLMs）在这方面具有潜在的优势，因为它们具有强大的模式识别能力。现有的方法通常通过人类标注的数据集对MLLMs进行微调，但这种方式带来了许多缺点。首先，依赖人类注释员按照复杂而详细的指南进行标注既耗时又昂贵。此外，使用安全评估系统的用户可能需要频繁更新安全规则，使得基于人类标注的微调更加困难。这引发了研究问题：我们是否可以通过在预定义的安全宪法（一套安全规则）的情况下查询预训练的MLLMs来进行零样本图像安全判断？我们的研究显示，仅通过查询预训练的MLLMs并不能取得满意的结果。这种无效性源于安全规则的主观性、宪法的复杂性以及模型中的固有偏见等因素。为了解决这些挑战，我们提出了一种基于MLLM的方法，包括客观化安全规则、评估规则与图像的相关性、基于去偏见的标记概率进行快速判断，并在必要时使用逻辑完备但简化了的前提链进行安全规则的深入推理。实验结果表明，我们的方法在零样本图像安全判断任务中非常有效。
2501.00142	 | Minimalist Vision with Freeform Pixels	 | Jeremy Klotz,Shree K. Nayar	 | 最小主义视觉系统通过使用自由形状的非方形像素减少视觉任务所需的像素数量，该系统可以被建模为神经网络的第一层，并且能够在无需外部电源的情况下实现自供电。	 | A minimalist vision system uses the smallest number of pixels needed to solve a vision task. While traditional cameras use a large grid of square pixels, a minimalist camera uses freeform pixels that can take on arbitrary shapes to increase their information content. We show that the hardware of a minimalist camera can be modeled as the first layer of a neural network, where the subsequent layers are used for inference. Training the network for any given task yields the shapes of the camera's freeform pixels, each of which is implemented using a photodetector and an optical mask. We have designed minimalist cameras for monitoring indoor spaces (with 8 pixels), measuring room lighting (with 8 pixels), and estimating traffic flow (with 8 pixels). The performance demonstrated by these systems is on par with a traditional camera with orders of magnitude more pixels. Minimalist vision has two major advantages. First, it naturally tends to preserve the privacy of individuals in the scene since the captured information is inadequate for extracting visual details. Second, since the number of measurements made by a minimalist camera is very small, we show that it can be fully self-powered, i.e., function without an external power supply or a battery.	 | 最小主义视觉系统采用解决视觉任务所需的最少像素数量。传统相机使用大量的方形像素网格，而最小主义相机则采用自由形状的像素，可以任意变形以增加信息量。我们展示了最小主义相机的硬件可以被建模为神经网络的第一层，而后续层则用于推断。对于任何给定任务训练该网络可以得到相机自由形状像素的具体形状，每个像素由光电探测器和光学掩膜实现。我们设计了用于监控室内空间（8个像素）、测量房间照明（8个像素）和估计交通流量（8个像素）的最小主义相机。这些系统的性能与数量级多出数倍的传统相机相当。最小主义视觉有两个主要优势。首先，它自然地倾向于保护场景中个人的隐私，因为捕获的信息不足以提取视觉细节。其次，由于最小主义相机进行的测量数量非常少，我们展示了它可以完全自供电，即无需外部电源或电池即可运行。
2501.00136	 | Detection-Fusion for Knowledge Graph Extraction from Videos	 | Taniya Das,Louis Mahon,Thomas Lukasiewicz	 | 本文提出了一个基于深度学习的视频理解方法，通过构建知识图谱来标注视频内容，以解决现有系统过于依赖语言模型和难以评估的问题；该方法包括一个初始模型用于预测个体及关系，以及一个扩展模型可在构建知识图谱时融入背景知识。	 | One of the challenging tasks in the field of video understanding is extracting semantic content from video inputs. Most existing systems use language models to describe videos in natural language sentences, but this has several major shortcomings. Such systems can rely too heavily on the language model component and base their output on statistical regularities in natural language text rather than on the visual contents of the video. Additionally, natural language annotations cannot be readily processed by a computer, are difficult to evaluate with performance metrics and cannot be easily translated into a different natural language. In this paper, we propose a method to annotate videos with knowledge graphs, and so avoid these problems. Specifically, we propose a deep-learning-based model for this task that first predicts pairs of individuals and then the relations between them. Additionally, we propose an extension of our model for the inclusion of background knowledge in the construction of knowledge graphs.	 | 视频理解领域中的一个挑战性任务是从视频输入中提取语义内容。目前大多数现有系统使用语言模型来用自然语言句子描述视频，但这种方法存在几个重大不足。此类系统可能过于依赖语言模型组件，并且其输出基于自然语言文本中的统计规律，而不是视频的视觉内容。此外，自然语言注释不能被计算机轻易处理，难以用性能指标来评估，并且不能轻易翻译成其他自然语言。在本文中，我们提出了一种使用知识图谱对视频进行标注的方法，以避免上述问题。具体而言，我们提出了一种基于深度学习的模型，该模型首先预测个体之间的配对，然后预测它们之间的关系。此外，我们还提出了一种扩展模型，使其能够在构建知识图谱时包含背景知识。
2501.00124	 | PQD: Post-training Quantization for Efficient Diffusion Models	 | Jiaojiao Ye,Zhen Wang,Linnan Jiang	 | 本文提出了一种新的后训练量化方法（PQD），可以在无需训练的情况下将全精度扩散模型量化为8位或4位模型，同时保持较高的生成图像质量，并首次使512x512文本引导图像生成成为可能。	 | Diffusionmodels(DMs)havedemonstratedremarkableachievements in synthesizing images of high fidelity and diversity. However, the extensive computational requirements and slow generative speed of diffusion models have limited their widespread adoption. In this paper, we propose a novel post-training quantization for diffusion models (PQD), which is a time-aware optimization framework for diffusion models based on post-training quantization. The proposed framework optimizes the inference process by selecting representative samples and conducting time-aware calibration. Experimental results show that our proposed method is able to directly quantize full-precision diffusion models into 8-bit or 4-bit models while maintaining comparable performance in a training-free manner, achieving a few FID change on ImageNet for unconditional image generation. Our approach demonstrates compatibility and can also be applied to 512x512 text-guided image generation for the first time.	 | 扩散模型（DMs）在生成高保真度和多样性的图像方面已经取得了显著成就。然而，扩散模型的计算需求量大和生成速度慢限制了它们的广泛应用。本文提出了一种新的后训练量化方法（PQD），这是一种基于后训练量化的时间感知优化框架。所提出的框架通过选择代表性样本并进行时间感知校准来优化推理过程。实验结果表明，我们的方法可以在无需训练的情况下直接将全精度扩散模型量化为8位或4位模型，同时在无条件图像生成方面保持相当的性能，在ImageNet上的FID变化很小。我们的方法具有兼容性，并且首次可以应用于512x512文本引导图像生成。
2501.00116	 | Text-to-Image GAN with Pretrained Representations	 | Xiaozhou You,Jian Zhang	 | TIGER是一种基于GAN的文本到图像合成模型，它通过引入视觉增强的判别器和高容量生成器来提升性能，能够在复杂场景下更快地生成高质量图像，并在标准和零样本任务中表现出色，特别是在FID分数上取得了最先进的结果。	 | Generating desired images conditioned on given text descriptions has received lots of attention. Recently, diffusion models and autoregressive models have demonstrated their outstanding expressivity and gradually replaced GAN as the favored architectures for text-to-image synthesis. However, they still face some obstacles: slow inference speed and expensive training costs. To achieve more powerful and faster text-to-image synthesis under complex scenes, we propose TIGER, a text-to-image GAN with pretrained representations. To be specific, we propose a vision-empowered discriminator and a high-capacity generator. (i) The vision-empowered discriminator absorbs the complex scene understanding ability and the domain generalization ability from pretrained vision models to enhance model performance. Unlike previous works, we explore stacking multiple pretrained models in our discriminator to collect multiple different representations. (ii) The high-capacity generator aims to achieve effective text-image fusion while increasing the model capacity. The high-capacity generator consists of multiple novel high-capacity fusion blocks (HFBlock). And the HFBlock contains several deep fusion modules and a global fusion module, which play different roles to benefit our model. Extensive experiments demonstrate the outstanding performance of our proposed TIGER both on standard and zero-shot text-to-image synthesis tasks. On the standard text-to-image synthesis task, TIGER achieves state-of-the-art performance on two challenging datasets, which obtain a new FID 5.48 (COCO) and 9.38 (CUB). On the zero-shot text-to-image synthesis task, we achieve comparable performance with fewer model parameters, smaller training data size and faster inference speed. Additionally, more experiments and analyses are conducted in the Supplementary Material.	 | 基于给定的文本描述生成所需图像已经受到了广泛关注。最近，扩散模型和自回归模型展示了其出色的表达能力，并逐渐取代了GAN，成为文本到图像合成的首选架构。然而，它们仍然面临一些障碍：推理速度慢和昂贵的训练成本。为了在复杂场景下实现更强大和更快的文本到图像合成，我们提出了TIGER——一种带有预训练表示的文本到图像的GAN。具体来说，我们提出了一种视觉增强的判别器和高容量生成器。(i) 视觉增强的判别器从预训练视觉模型中吸收复杂场景理解和领域泛化能力，以增强模型性能。与以往工作不同，我们探索在判别器中堆叠多个预训练模型，以收集多种不同的表示。(ii) 高容量生成器旨在在增加模型容量的同时实现有效的文本-图像融合。高容量生成器由多个新型高容量融合块（HFBlock）组成，而HFBlock包含多个深度融合模块和全局融合模块，各自发挥不同的作用以利于我们的模型。大量实验表明，我们提出的TIGER在标准和零样本文本到图像合成任务中均表现出色。在标准文本到图像合成任务中，TIGER在两个具有挑战性的数据集上实现了最先进的性能，分别获得新的FID分数5.48（COCO）和9.38（CUB）。在零样本文本到图像合成任务中，我们使用更少的模型参数和更小的训练数据量实现了可比的性能，并且具有更快的推理速度。此外，我们在补充材料中进行了更多的实验和分析。
2501.00103	 | LTX-Video: Realtime Video Latent Diffusion	 | Yoav HaCohen,Nisan Chiprut,Benny Brazowski,Daniel Shalem,Dudu Moshe,Eitan Richardson,Eran Levin,Guy Shiran,Nir Zabari,Ori Gordon,Poriya Panet,Sapir Weissbuch,Victor Kulikov,Yaki Bitterman,Zeev Melumian,Ofir Bibi	 | LTX-Video 是一种基于变换器的潜扩散模型，通过无缝整合视频VAE和去噪变换器，以高效且高质量的方式生成视频。该模型利用高度压缩的潜空间进行全时空自注意力操作，同时采用VAE解码器直接在像素空间生成结果，提升了细节表示能力，实现了快速的高分辨率视频生成。	 | We introduce LTX-Video, a transformer-based latent diffusion model that adopts a holistic approach to video generation by seamlessly integrating the responsibilities of the Video-VAE and the denoising transformer. Unlike existing methods, which treat these components as independent, LTX-Video aims to optimize their interaction for improved efficiency and quality. At its core is a carefully designed Video-VAE that achieves a high compression ratio of 1:192, with spatiotemporal downscaling of 32 x 32 x 8 pixels per token, enabled by relocating the patchifying operation from the transformer's input to the VAE's input. Operating in this highly compressed latent space enables the transformer to efficiently perform full spatiotemporal self-attention, which is essential for generating high-resolution videos with temporal consistency. However, the high compression inherently limits the representation of fine details. To address this, our VAE decoder is tasked with both latent-to-pixel conversion and the final denoising step, producing the clean result directly in pixel space. This approach preserves the ability to generate fine details without incurring the runtime cost of a separate upsampling module. Our model supports diverse use cases, including text-to-video and image-to-video generation, with both capabilities trained simultaneously. It achieves faster-than-real-time generation, producing 5 seconds of 24 fps video at 768x512 resolution in just 2 seconds on an Nvidia H100 GPU, outperforming all existing models of similar scale. The source code and pre-trained models are publicly available, setting a new benchmark for accessible and scalable video generation.	 | 我们介绍了LTX-Video，一种基于变换器的潜扩散模型，它通过无缝整合视频VAE和去噪变换器的责任，采用了一种整体的方法来进行视频生成。与现有方法不同，这些组件在LTX-Video中被视为相互依赖的，其目标是优化它们之间的交互，以提高效率和质量。其核心是一个精心设计的视频VAE，实现了高达1:192的压缩比，每token进行了32 x 32 x 8像素的时空下采样，通过将切片操作从变换器的输入移至VAE的输入来实现这一点。在这种高度压缩的潜空间中操作，使变换器能够高效地执行全时空自注意力，这对于生成具有时间一致性的高分辨率视频至关重要。然而，这种高度压缩会不可避免地限制对细节的表示。为了解决这个问题，我们的VAE解码器负责潜空间到像素空间的转换和最终的去噪步骤，直接在像素空间中生成干净的结果。这种方法保留了生成细节点的能力，而无需额外的上采样模块的运行时开销。该模型支持多种应用场景，包括文本转视频和图像转视频生成，并且这两种能力同时进行训练。它实现了比真实时间更快的生成速度，在Nvidia H100 GPU上只需2秒即可生成768x512分辨率的24帧每秒的5秒视频，超过了所有类似规模的现有模型。我们的源代码和预训练模型均已公开，为可访问和可扩展的视频生成设定了新的基准。
2501.01392	 | ProjectedEx: Enhancing Generation in Explainable AI for Prostate Cancer	 | Xuyin Qi,Zeyu Zhang,Aaron Berliano Handoko,Huazhan Zheng,Mingxi Chen,Ta Duc Huy,Vu Minh Hieu Phan,Lei Zhang,Linqi Cheng,Shiyu Jiang,Zhiwei Zhang,Zhibin Liao,Yang Zhao,Minh-Son To	 | 本文提出了一种名为ProjectedEx的生成框架，该框架能够提供可解释的多属性解释，将医学图像特征与分类器决策关联起来，并通过增强编码器模块提高生成解释的质量，从而提升前列腺癌诊断的精确性和临床相关性。	 | Prostate cancer, a growing global health concern, necessitates precise diagnostic tools, with Magnetic Resonance Imaging (MRI) offering high-resolution soft tissue imaging that significantly enhances diagnostic accuracy. Recent advancements in explainable AI and representation learning have significantly improved prostate cancer diagnosis by enabling automated and precise lesion classification. However, existing explainable AI methods, particularly those based on frameworks like generative adversarial networks (GANs), are predominantly developed for natural image generation, and their application to medical imaging often leads to suboptimal performance due to the unique characteristics and complexity of medical image. To address these challenges, our paper introduces three key contributions. First, we propose ProjectedEx, a generative framework that provides interpretable, multi-attribute explanations, effectively linking medical image features to classifier decisions. Second, we enhance the encoder module by incorporating feature pyramids, which enables multiscale feedback to refine the latent space and improves the quality of generated explanations. Additionally, we conduct comprehensive experiments on both the generator and classifier, demonstrating the clinical relevance and effectiveness of ProjectedEx in enhancing interpretability and supporting the adoption of AI in medical settings. Code will be released at https://github.com/Richardqiyi/ProjectedEx	 | 前列腺癌是日益增长的全球健康问题，需要精确的诊断工具，而磁共振成像（MRI）提供了高分辨率软组织成像，显著提高了诊断准确性。近年来，可解释的人工智能和表示学习的进步显著提升了前列腺癌的诊断能力，使其能够实现自动化和精确的病灶分类。然而，现有的解释性人工智能方法，特别是基于生成对抗网络（GAN）等框架的方法，主要集中在自然图像生成，应用于医学影像时往往因为医学图像的独特特性和复杂性而导致性能不佳。为了解决这些挑战，我们在这篇论文中提出了三个关键贡献。首先，我们提出了一种生成框架ProjectedEx，该框架提供了可解释的多属性解释，有效地将医学图像特征与分类器决策联系起来。其次，我们通过引入特征金字塔增强了编码器模块，这使得多尺度反馈能够细化潜在空间，从而提高生成解释的质量。此外，我们在生成器和分类器上进行了全面的实验，展示了ProjectedEx在提高可解释性并支持人工智能在医疗环境中应用方面的临床相关性和有效性。代码将在https://github.com/Richardqiyi/ProjectedEx公开。
2501.01372	 | ScarNet: A Novel Foundation Model for Automated Myocardial Scar Quantification from LGE in Cardiac MRI	 | Neda Tavakoli,Amir Ali Rahsepar,Brandon C. Benefield,Daming Shen,Santiago López-Tapia,Florian Schiffers,Jeffrey J. Goldberger,Christine M. Albert,Edwin Wu,Aggelos K. Katsaggelos,Daniel C. Lee,Daniel Kim	 | 研究开发了一种名为ScarNet的混合模型，能够准确分割心脏LGE图像中的心肌和瘢痕边界，其性能显著优于现有的MedSAM和nnU-Net模型。	 | Background: Late Gadolinium Enhancement (LGE) imaging is the gold standard for assessing myocardial fibrosis and scarring, with left ventricular (LV) LGE extent predicting major adverse cardiac events (MACE). Despite its importance, routine LGE-based LV scar quantification is hindered by labor-intensive manual segmentation and inter-observer variability. Methods: We propose ScarNet, a hybrid model combining a transformer-based encoder from the Medical Segment Anything Model (MedSAM) with a convolution-based U-Net decoder, enhanced by tailored attention blocks. ScarNet was trained on 552 ischemic cardiomyopathy patients with expert segmentations of myocardial and scar boundaries and tested on 184 separate patients. Results: ScarNet achieved robust scar segmentation in 184 test patients, yielding a median Dice score of 0.912 (IQR: 0.863--0.944), significantly outperforming MedSAM (median Dice = 0.046, IQR: 0.043--0.047) and nnU-Net (median Dice = 0.638, IQR: 0.604--0.661). ScarNet demonstrated lower bias (-0.63%) and coefficient of variation (4.3%) compared to MedSAM (bias: -13.31%, CoV: 130.3%) and nnU-Net (bias: -2.46%, CoV: 20.3%). In Monte Carlo simulations with noise perturbations, ScarNet achieved significantly higher scar Dice (0.892 \pm 0.053, CoV = 5.9%) than MedSAM (0.048 \pm 0.112, CoV = 233.3%) and nnU-Net (0.615 \pm 0.537, CoV = 28.7%). Conclusion: ScarNet outperformed MedSAM and nnU-Net in accurately segmenting myocardial and scar boundaries in LGE images. The model exhibited robust performance across diverse image qualities and scar patterns.	 | 背景：晚期钆增强（Late Gadolinium Enhancement, LGE）成像是评估心肌纤维化和瘢痕的金标准，左心室（Left Ventricular, LV）LGE范围预测主要心脏不良事件（Major Adverse Cardiac Events, MACE）。尽管其重要性不言而喻，但由于劳动密集型的手动分割和观察者间差异，常规的LGE基线LV瘢痕定量受到阻碍。方法：我们提出了一种名为ScarNet的混合模型，该模型结合了来自Medical Segment Anything Model（MedSAM）的基于变压器的编码器和基于卷积的U-Net解码器，并通过定制的注意力模块增强。ScarNet在包含552名缺血性心肌病患者的专家标记的心肌和瘢痕边界数据上进行了训练，并在另外184名患者上进行了测试。结果：ScarNet在184名测试患者中实现了稳健的瘢痕分割，中位Dice分数为0.912（四分位数范围：0.863-0.944），显著优于MedSAM（中位Dice = 0.046，四分位数范围：0.043-0.047）和nnU-Net（中位Dice = 0.638，四分位数范围：0.604-0.661）。与MedSAM相比，ScarNet的偏差（-0.63%）和变异系数（4.3%）更低，而与nnU-Net相比，偏差（-2.46%）和变异系数（20.3%）也更低。在有噪声扰动的蒙特卡洛模拟中，ScarNet在瘢痕Dice（0.892 ± 0.053，变异系数 = 5.9%）方面的表现显著优于MedSAM（0.048 ± 0.112，变异系数 = 233.3%）和nnU-Net（0.615 ± 0.537，变异系数 = 28.7%）。结论：ScarNet在LGE图像中准确分割心肌和瘢痕边界方面优于MedSAM和nnU-Net。该模型表现出稳健的性能，适用于不同质量的心脏图像和不同类型的瘢痕模式。
2501.01326	 | Domain-invariant feature learning in brain MR imaging for content-based image retrieval	 | Shuya Tobari,Shuhei Tomoshige,Hayato Muraki,Kenichi Oishi,Hitoshi Iyatomi	 | 该研究提出了一种新的风格编码对抗领域适应（SE-ADA）方法，用于减少不同设施之间大脑MR图像成像差异，同时保留病理特征，从而提高疾病搜索准确性。实验结果显示，SE-ADA在多个公开大脑MR数据集中有效减少了领域信息，保留了关键的大脑结构特征。	 | When conducting large-scale studies that collect brain MR images from multiple facilities, the impact of differences in imaging equipment and protocols at each site cannot be ignored, and this domain gap has become a significant issue in recent years. In this study, we propose a new low-dimensional representation (LDR) acquisition method called style encoder adversarial domain adaptation (SE-ADA) to realize content-based image retrieval (CBIR) of brain MR images. SE-ADA reduces domain differences while preserving pathological features by separating domain-specific information from LDR and minimizing domain differences using adversarial learning.   In evaluation experiments comparing SE-ADA with recent domain harmonization methods on eight public brain MR datasets (ADNI1/2/3, OASIS1/2/3/4, PPMI), SE-ADA effectively removed domain information while preserving key aspects of the original brain structure and demonstrated the highest disease search accuracy.	 | 在从多个设施收集大脑MR图像的大规模研究中，每个站点使用的成像设备和协议之间的差异不容忽视，这一领域差距已成为近年来的一个重大问题。在本研究中，我们提出了一种新的低维表示（LDR）获取方法，称为风格编码对抗领域适应（SE-ADA），以实现大脑MR图像的内容基础图像检索（CBIR）。SE-ADA通过从LDR中分离领域特定信息并使用对抗学习最小化领域差异来减少领域差异，从而保持病理特征。在使用八个多公开的大脑MR数据集（ADNI1/2/3、OASIS1/2/3/4、PPMI）进行的评估实验中，SE-ADA有效地移除了领域信息，保留了原始大脑结构的关键方面，并展示了最高的疾病搜索准确性。
2501.01282	 | CultureVLM: Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries	 | Shudong Liu,Yiqiao Jin,Cheng Li,Derek F. Wong,Qingsong Wen,Lichao Sun,Haipeng Chen,Xing Xie,Jindong Wang	 | 本文构建了CultureVerse，一个大规模多模态基准，旨在提高视觉-语言模型的文化理解能力，并提出了CultureVLM系列模型在该数据集上的微调方法，显著提升了模型的文化感知能力，同时保持了在一般VLM基准测试中的性能。	 | Vision-language models (VLMs) have advanced human-AI interaction but struggle with cultural understanding, often misinterpreting symbols, gestures, and artifacts due to biases in predominantly Western-centric training data. In this paper, we construct CultureVerse, a large-scale multimodal benchmark covering 19, 682 cultural concepts, 188 countries/regions, 15 cultural concepts, and 3 question types, with the aim of characterizing and improving VLMs' multicultural understanding capabilities. Then, we propose CultureVLM, a series of VLMs fine-tuned on our dataset to achieve significant performance improvement in cultural understanding. Our evaluation of 16 models reveals significant disparities, with a stronger performance in Western concepts and weaker results in African and Asian contexts. Fine-tuning on our CultureVerse enhances cultural perception, demonstrating cross-cultural, cross-continent, and cross-dataset generalization without sacrificing performance on models' general VLM benchmarks. We further present insights on cultural generalization and forgetting. We hope that this work could lay the foundation for more equitable and culturally aware multimodal AI systems.	 | 视觉-语言模型（VLMs）在推进人类与人工智能的交互方面取得了进展，但它们在文化理解方面存在困难，常常由于训练数据主要偏向西方中心主义而导致误读符号、手势和文化遗物。在本文中，我们构建了CultureVerse，这是一个大规模的多模态基准，涵盖了19,682个文化概念、188个国家和地区、15个文化概念和3种问题类型，旨在刻画和提升VLMs的多元文化理解能力。然后，我们提出了CultureVLM，这是一种在我们数据集上微调的VLM系列，以显著提高其文化理解性能。我们对16种模型的评估显示了显著的差异，西方概念的表现更好，而非洲和亚洲的语境则表现较弱。通过对我们的CultureVerse进行微调，可以增强文化感知能力，这种能力在跨文化、跨洲际和跨数据集方面表现出通用性，不会牺牲模型在一般VLM基准测试中的性能。此外，我们还展示了文化通用性和遗忘方面的见解。我们期望这项工作能够为构建更加公平和文化意识更强的多模态AI系统奠定基础。
2501.01132	 | Missing Data as Augmentation in the Earth Observation Domain: A Multi-View Learning Approach	 | Francisco Mena,Diego Arenas,Andreas Dengel	 | 本文提出了一种针对地球观测应用的多视图学习（MVL）方法，该方法通过模拟所有可能的缺失视图组合并使用动态合并函数进行训练，以提升模型在面对缺失数据时的鲁棒性和预测性能。实验结果表明，该方法在中等缺失数据情况下能显著提高模型的鲁棒性，并在视图齐全的情况下进一步提升预测性能。	 | Multi-view learning (MVL) leverages multiple sources or views of data to enhance machine learning model performance and robustness. This approach has been successfully used in the Earth Observation (EO) domain, where views have a heterogeneous nature and can be affected by missing data. Despite the negative effect that missing data has on model predictions, the ML literature has used it as an augmentation technique to improve model generalization, like masking the input data. Inspired by this, we introduce novel methods for EO applications tailored to MVL with missing views. Our methods integrate the combination of a set to simulate all combinations of missing views as different training samples. Instead of replacing missing data with a numerical value, we use dynamic merge functions, like average, and more complex ones like Transformer. This allows the MVL model to entirely ignore the missing views, enhancing its predictive robustness. We experiment on four EO datasets with temporal and static views, including state-of-the-art methods from the EO domain. The results indicate that our methods improve model robustness under conditions of moderate missingness, and improve the predictive performance when all views are present. The proposed methods offer a single adaptive solution to operate effectively with any combination of available views.	 | 多视图学习（MVL）利用多种数据源或视图来提升机器学习模型的性能和鲁棒性。这种方法在地球观测（EO）领域已经取得了成功，其中视图具有异质性，并且可能会受到数据缺失的影响。尽管缺失数据对模型预测有负面影响，机器学习文献中却将其用作一种增强技术，通过屏蔽输入数据来提高模型的泛化能力。受到这种做法的启发，我们提出了适用于EO应用的新方法，专门针对具有缺失视图的MVL。我们的方法整合了一组来模拟所有可能的缺失视图组合，并将其作为不同的训练样本。我们不使用数值来填补缺失数据，而是使用动态合并函数，如平均值，以及更复杂的函数如Transformer。这种方法使MVL模型可以完全忽略缺失视图，从而提升其预测的鲁棒性。我们使用四个包含时间和静态视图的EO数据集进行实验，并与EO领域的先进方法进行比较。结果表明，在中等缺失情况下，我们的方法可以提高模型的鲁棒性，在所有视图都存在的情况下，还可以提高预测性能。所提出的方法提供了一种适应性解决方案，可以有效处理任何可用视图的组合。
2501.01090	 | HoneypotNet: Backdoor Attacks Against Model Extraction	 | Yixu Wang,Tianle Gu,Yan Teng,Yingchun Wang,Xingjun Ma	 | 该研究提出了一种新的防御模型提取攻击的方法——Attack as Defense，通过在受害模型中引入具有毒性的输出，阻止攻击者训练有效的替代模型。具体而言，研究者开发了一种称为HoneypotNet的新颖轻量级后门攻击方法，能够在保持原始性能的同时注入具有毒性的后门，从而成功抵御模型提取攻击。	 | Model extraction attacks are one type of inference-time attacks that approximate the functionality and performance of a black-box victim model by launching a certain number of queries to the model and then leveraging the model's predictions to train a substitute model. These attacks pose severe security threats to production models and MLaaS platforms and could cause significant monetary losses to the model owners. A body of work has proposed to defend machine learning models against model extraction attacks, including both active defense methods that modify the model's outputs or increase the query overhead to avoid extraction and passive defense methods that detect malicious queries or leverage watermarks to perform post-verification. In this work, we introduce a new defense paradigm called attack as defense which modifies the model's output to be poisonous such that any malicious users that attempt to use the output to train a substitute model will be poisoned. To this end, we propose a novel lightweight backdoor attack method dubbed HoneypotNet that replaces the classification layer of the victim model with a honeypot layer and then fine-tunes the honeypot layer with a shadow model (to simulate model extraction) via bi-level optimization to modify its output to be poisonous while remaining the original performance. We empirically demonstrate on four commonly used benchmark datasets that HoneypotNet can inject backdoors into substitute models with a high success rate. The injected backdoor not only facilitates ownership verification but also disrupts the functionality of substitute models, serving as a significant deterrent to model extraction attacks.	 | 模型提取攻击是一种在推理时进行的攻击，通过向目标模型发送一定数量的查询请求，并利用模型的预测结果来训练一个替代模型，从而近似目标模型的功能和性能。这类攻击对生产模型和MLaaS平台构成严重的安全威胁，并可能导致模型所有者遭受重大经济损失。已有大量研究提出了防御机器学习模型免受模型提取攻击的方法，包括积极防御方法（修改模型输出或增加查询开销以避免提取）和被动防御方法（检测恶意查询或通过水印进行事后验证）。在本文中，我们提出了一种新的防御范式，称为攻击作为防御（Attack as Defense），即修改模型的输出，使其具有毒性，使得任何试图利用输出训练替代模型的恶意用户都会中毒。为此，我们提出了一种新颖的轻量级后门攻击方法，称为HoneypotNet，该方法用蜜罐层替代受害模型的分类层，然后利用双层优化方法通过影子模型进行微调蜜罐层，使其输出具有毒性，同时保持原始性能。我们通过四个常用基准数据集的实验证明，HoneypotNet能够以高成功率注入后门。注入的后门不仅有助于所有权验证，还能破坏替代模型的功能，成为模型提取攻击的重要威慑。
2501.01087	 | Bridging Simplicity and Sophistication using GLinear: A Novel Architecture for Enhanced Time Series Prediction	 | Syed Tahir Hussain Rizvi,Neel Kanwal,Muddasar Naeem,Alfredo Cuzzocrea,Antonio Coronato	 | GLinear是一种新的数据效率架构，用于多变量时间序列预测，利用周期性模式提高精度，即使使用较少的历史数据也能优于多个最先进的线性预测器和基于Transformer的模型。	 | Time Series Forecasting (TSF) is an important application across many fields. There is a debate about whether Transformers, despite being good at understanding long sequences, struggle with preserving temporal relationships in time series data. Recent research suggests that simpler linear models might outperform or at least provide competitive performance compared to complex Transformer-based models for TSF tasks. In this paper, we propose a novel data-efficient architecture, GLinear, for multivariate TSF that exploits periodic patterns to provide better accuracy. It also provides better prediction accuracy by using a smaller amount of historical data compared to other state-of-the-art linear predictors. Four different datasets (ETTh1, Electricity, Traffic, and Weather) are used to evaluate the performance of the proposed predictor. A performance comparison with state-of-the-art linear architectures (such as NLinear, DLinear, and RLinear) and transformer-based time series predictor (Autoformer) shows that the GLinear, despite being parametrically efficient, significantly outperforms the existing architectures in most cases of multivariate TSF. We hope that the proposed GLinear opens new fronts of research and development of simpler and more sophisticated architectures for data and computationally efficient time-series analysis. The source code is publicly available on GitHub.	 | 时间序列预测（TSF）是众多领域中的一个重要应用。关于Transformer是否能够很好地理解长序列但难以保持时间序列中的时间关系，存在一些争议。近期的研究表明，对于TSF任务，简单的线性模型可能表现出更好的性能，甚至能够与基于Transformer的复杂模型竞争。本文提出了一种新的数据效率架构GLinear，用于多变量TSF，该架构利用周期性模式提供更好的精度。与其它最先进的线性预测器相比，GLinear使用较少的历史数据也能提供更好的预测精度。我们使用四个不同的数据集（ETTh1、Electricity、Traffic和Weather）来评估提出的预测器的性能。与最先进的线性架构（如NLinear、DLinear、RLinear）和基于Transformer的时间序列预测器（Autoformer）进行性能比较，结果显示，尽管GLinear在参数效率上具有优势，但在大多数多变量TSF情况下，GLinear显著优于现有架构。我们希望GLinear的提出能够开启更简单且更为复杂架构的新研究领域，以实现数据和计算上的高效时间序列分析。源代码已在GitHub上公开。
2501.01037	 | MSC-Bench: Benchmarking and Analyzing Multi-Sensor Corruption for Driving Perception	 | Xiaoshuai Hao,Guanqun Liu,Yuting Zhao,Yuheng Ji,Mengchuan Wei,Haimei Zhao,Lingdong Kong,Rong Yin,Yu Liu	 | 多传感器融合模型在自动驾驶感知中至关重要，尤其是3D物体检测和高精度地图构建，但现有方法依赖完整传感器输入，鲁棒性差。为此，研究引入了多传感器损坏基准（MSC-Bench），评估模型在多种传感器损坏情况下的表现，结果显示性能显著下降，突显安全问题。	 | Multi-sensor fusion models play a crucial role in autonomous driving perception, particularly in tasks like 3D object detection and HD map construction. These models provide essential and comprehensive static environmental information for autonomous driving systems. While camera-LiDAR fusion methods have shown promising results by integrating data from both modalities, they often depend on complete sensor inputs. This reliance can lead to low robustness and potential failures when sensors are corrupted or missing, raising significant safety concerns. To tackle this challenge, we introduce the Multi-Sensor Corruption Benchmark (MSC-Bench), the first comprehensive benchmark aimed at evaluating the robustness of multi-sensor autonomous driving perception models against various sensor corruptions. Our benchmark includes 16 combinations of corruption types that disrupt both camera and LiDAR inputs, either individually or concurrently. Extensive evaluations of six 3D object detection models and four HD map construction models reveal substantial performance degradation under adverse weather conditions and sensor failures, underscoring critical safety issues. The benchmark toolkit and affiliated code and model checkpoints have been made publicly accessible.	 | 多传感器融合模型在自动驾驶感知中扮演着至关重要的角色，特别是在3D物体检测和高精度地图构建等任务中。这些模型为自动驾驶系统提供了必不可少且全面的静态环境信息。虽然基于摄像机-LiDAR融合的方法通过整合两种模态的数据展示了有希望的结果，但它们通常依赖于完整的传感器输入。这种依赖性会导致传感器损坏或缺失时的低鲁棒性和潜在失效，从而引发重大的安全问题。为应对这一挑战，我们引入了多传感器损坏基准（MSC-Bench），这是一个全面的基准，旨在评估多传感器自动驾驶感知模型在面对各种传感器损坏时的鲁棒性。我们的基准包括16种不同的损坏类型，这些类型会干扰摄像机和LiDAR输入，无论是单独还是同时干扰。对六种3D物体检测模型和四种高精度地图构建模型的广泛评估显示，在恶劣天气条件和传感器故障下性能显著下降，这凸显了关键的安全问题。该基准工具包及相关代码和模型检查点均已公开发布。
2501.00961	 | The Silent Majority: Demystifying Memorization Effect in the Presence of Spurious Correlations	 | Chenyu You,Haocheng Dai,Yifei Min,Jasjeet S. Sekhon,Sarang Joshi,James S. Duncan	 | 这项研究揭示了神经网络中一小部分神经元或通道中存在的虚假特征可能导致少数群体和多数群体之间的测试性能不平衡，并通过实验验证了这些神经元具有记忆少数群体信息的特性。通过消除这些虚假记忆，研究展示了可以显著改善模型在少数群体上的性能，为减轻虚假相关性的影响提供了新的方向。	 | Machine learning models often rely on simple spurious features -- patterns in training data that correlate with targets but are not causally related to them, like image backgrounds in foreground classification. This reliance typically leads to imbalanced test performance across minority and majority groups. In this work, we take a closer look at the fundamental cause of such imbalanced performance through the lens of memorization, which refers to the ability to predict accurately on \textit{atypical} examples (minority groups) in the training set but failing in achieving the same accuracy in the testing set. This paper systematically shows the ubiquitous existence of spurious features in a small set of neurons within the network, providing the first-ever evidence that memorization may contribute to imbalanced group performance. Through three experimental sources of converging empirical evidence, we find the property of a small subset of neurons or channels in memorizing minority group information. Inspired by these findings, we articulate the hypothesis: the imbalanced group performance is a byproduct of ``noisy'' spurious memorization confined to a small set of neurons. To further substantiate this hypothesis, we show that eliminating these unnecessary spurious memorization patterns via a novel framework during training can significantly affect the model performance on minority groups. Our experimental results across various architectures and benchmarks offer new insights on how neural networks encode core and spurious knowledge, laying the groundwork for future research in demystifying robustness to spurious correlation.	 | 机器学习模型通常依赖于简单的虚假特征——训练数据中的模式虽然与目标相关，但却未与其因果相关，例如在前景分类中图像的背景。这种依赖通常会导致测试性能在少数群体和多数群体之间不平衡。在这项工作中，我们通过记忆的观点——即能够准确预测训练集中异常的（少数群体）样本，但在测试集中无法达到相同的精度——更深入地探讨了这种不平衡性能的根本原因。本文系统地展示了在神经网络中一小部分神经元或通道中普遍存在虚假特征，首次证实记忆可能对少数群体的性能造成不平衡的影响。通过三种实验证据的验证，我们发现一小部分神经元或通道具有记忆少数群体信息的特性。受这些发现的启发，我们提出假设：不平衡的群体性能是少数神经元中存在“噪声”的虚假记忆的副产品。为了进一步证明这一假设，我们展示了在训练过程中通过一个新颖的框架消除这些不必要的虚假记忆模式，可以显著影响模型在少数群体上的表现。跨多个架构和基准的实验结果为神经网络如何编码核心和虚假知识提供了新的见解，并为未来研究如何减轻虚假相关性的影响奠定了基础。
2501.00954	 | Enhancing Early Diabetic Retinopathy Detection through Synthetic DR1 Image Generation: A StyleGAN3 Approach	 | Sagarnil Das,Pradeep Walia	 | 本研究利用StyleGAN3生成高质量且具有多样性的糖尿病视网膜病变第1阶段图像，以解决数据稀缺问题并提升分类器性能，结果表明生成的图像在人工图灵测试中表现逼真，具备重要应用潜力。	 | Diabetic Retinopathy (DR) is a leading cause of preventable blindness. Early detection at the DR1 stage is critical but is hindered by a scarcity of high-quality fundus images. This study uses StyleGAN3 to generate synthetic DR1 images characterized by microaneurysms with high fidelity and diversity. The aim is to address data scarcity and enhance the performance of supervised classifiers. A dataset of 2,602 DR1 images was used to train the model, followed by a comprehensive evaluation using quantitative metrics, including Frechet Inception Distance (FID), Kernel Inception Distance (KID), and Equivariance with respect to translation (EQ-T) and rotation (EQ-R). Qualitative assessments included Human Turing tests, where trained ophthalmologists evaluated the realism of synthetic images. Spectral analysis further validated image quality. The model achieved a final FID score of 17.29, outperforming the mean FID of 21.18 (95 percent confidence interval - 20.83 to 21.56) derived from bootstrap resampling. Human Turing tests demonstrated the model's ability to produce highly realistic images, though minor artifacts near the borders were noted. These findings suggest that StyleGAN3-generated synthetic DR1 images hold significant promise for augmenting training datasets, enabling more accurate early detection of Diabetic Retinopathy. This methodology highlights the potential of synthetic data in advancing medical imaging and AI-driven diagnostics.	 | 糖尿病视网膜病变（DR）是可预防性失明的主要原因之一。早期在DR1阶段的检测至关重要，但由于高质量视网膜图像稀缺，这一目标难以实现。本研究利用StyleGAN3生成具有高保真度和多样性的DR1图像，特征为微动脉瘤，旨在解决数据稀缺问题并提升监督分类器的性能。研究使用了2602张DR1图像来训练模型，并通过多种定量指标进行了全面评估，包括弗雷彻入射距离（FID）、内核入射距离（KID）以及平移（EQ-T）和旋转（EQ-R）不变性。定性评估包括由经验丰富的视网膜科医生进行的人工图灵测试，他们评估合成图像的真实性。频谱分析进一步验证了图像质量。该模型的最终FID得分为17.29，优于通过自助重采样得到的平均FID（95％置信区间 - 20.83至21.56）的21.18。人工图灵测试表明该模型能够生成高度逼真的图像，尽管在边缘附近存在一些细微的伪影。这些发现表明，StyleGAN3生成的DR1图像合成数据在扩充训练数据集、促进糖尿病视网膜病变早期准确检测方面具有重要潜力。此方法强调了合成数据在推进医学成像和AI驱动诊断方面的潜在价值。
2501.00942	 | Efficient Unsupervised Shortcut Learning Detection and Mitigation in Transformers	 | Lukas Kuhn,Sari Sadiya,Jorg Schlotterer,Christin Seifert,Gemma Roig	 | 本研究开发了一种无监督框架，用于检测并缓解变换器模型中的短路径学习问题，特别是在敏感决策任务中。该框架在多个数据集上验证有效，提高了最坏群体准确率和平均准确率，同时减少标注工作量，并具有高的计算效率，可在消费级硬件上运行。	 | Shortcut learning, i.e., a model's reliance on undesired features not directly relevant to the task, is a major challenge that severely limits the applications of machine learning algorithms, particularly when deploying them to assist in making sensitive decisions, such as in medical diagnostics. In this work, we leverage recent advancements in machine learning to create an unsupervised framework that is capable of both detecting and mitigating shortcut learning in transformers. We validate our method on multiple datasets. Results demonstrate that our framework significantly improves both worst-group accuracy (samples misclassified due to shortcuts) and average accuracy, while minimizing human annotation effort. Moreover, we demonstrate that the detected shortcuts are meaningful and informative to human experts, and that our framework is computationally efficient, allowing it to be run on consumer hardware.	 | 短路径学习，即模型对与任务直接相关性不强的非期望特征的依赖，是机器学习算法应用中的一个重要挑战，特别是在将其部署到辅助做出敏感决策（如医学诊断）时。在本项研究中，我们利用最近在机器学习领域的进步，开发了一种无监督框架，该框架能够检测并缓解变换器中的短路径学习。我们在多个数据集上验证了该方法。结果表明，我们的框架显著提高了最坏群体准确率（因短路径而误分类的样本）和平均准确率，同时最大限度地减少了人类标注工作量。此外，我们展示了检测到的短路径对人类专家具有实际意义和信息价值，并且我们的框架计算效率高，可以在消费级硬件上运行。
2501.00941	 | A Novel Diffusion Model for Pairwise Geoscience Data Generation with Unbalanced Training Dataset	 | Junhuan Yang,Yuzhou Zhang,Yi Sheng,Youzuo Lin,Lei Yang	 | 近年来，生成式AI技术在日常生活中的应用已取得显著进展，但在科学计算领域仍处于初级阶段。本文介绍了一种名为“UB-Diff”的新颖扩散模型，用于处理多模态配对科学数据生成，特别是在地下地质物理学中的地震成像领域，该模型在OpenFWI数据集上的实验结果显著优于现有技术。	 | Recently, the advent of generative AI technologies has made transformational impacts on our daily lives, yet its application in scientific applications remains in its early stages. Data scarcity is a major, well-known barrier in data-driven scientific computing, so physics-guided generative AI holds significant promise. In scientific computing, most tasks study the conversion of multiple data modalities to describe physical phenomena, for example, spatial and waveform in seismic imaging, time and frequency in signal processing, and temporal and spectral in climate modeling; as such, multi-modal pairwise data generation is highly required instead of single-modal data generation, which is usually used in natural images (e.g., faces, scenery). Moreover, in real-world applications, the unbalance of available data in terms of modalities commonly exists; for example, the spatial data (i.e., velocity maps) in seismic imaging can be easily simulated, but real-world seismic waveform is largely lacking. While the most recent efforts enable the powerful diffusion model to generate multi-modal data, how to leverage the unbalanced available data is still unclear. In this work, we use seismic imaging in subsurface geophysics as a vehicle to present ``UB-Diff'', a novel diffusion model for multi-modal paired scientific data generation. One major innovation is a one-in-two-out encoder-decoder network structure, which can ensure pairwise data is obtained from a co-latent representation. Then, the co-latent representation will be used by the diffusion process for pairwise data generation. Experimental results on the OpenFWI dataset show that UB-Diff significantly outperforms existing techniques in terms of Fréchet Inception Distance (FID) score and pairwise evaluation, indicating the generation of reliable and useful multi-modal pairwise data.	 | 近年来，生成式AI技术的出现极大地改变了我们的日常生活，但在科学应用中的应用仍处于早期阶段。数据驱动的科学计算中一个众所周知的重大障碍是数据稀缺性，因此，基于物理的生成式AI具有巨大的潜力。在科学计算中，大多数任务涉及将多种数据模态转换为描述物理现象，例如在地震成像中的空间和波形，在信号处理中的时间和频率，在气候建模中的时间和谱；因此，多模态配对数据生成高度需要，而不是单模态数据生成，后者在自然图像（例如面部、风景）中经常使用。此外，在实际应用中，各种模态下可用数据的不平衡性是常见的；例如，在地震成像中，空间数据（即速度图）可以很容易地模拟，但实际地震波形却很少见。尽管最近的努力使强大的扩散模型能够生成多模态数据，但如何利用不平衡的可用数据仍然不清楚。在这项工作中，我们以地下地质物理学中的地震成像为例，介绍了一种名为“UB-Diff”的新颖扩散模型，用于多模态配对科学数据生成。一个主要创新是一入二出的编码器-解码器网络结构，可以确保从共享潜在表示中获得配对数据。然后，共享潜在表示将被用于生成配对数据的过程。在OpenFWI数据集上的实验结果显示，UB-Diff在弗雷彻引入距离（FID）得分和配对评估方面显著优于现有技术，表明生成了可靠且有用的数据。
2501.00876	 | A Novel Approach using CapsNet and Deep Belief Network for Detection and Identification of Oral Leukopenia	 | Hirthik Mathesh GV,Kavin Chakravarthy M,Sentil Pandi S	 | 本研究通过收集全球临床专家的标注数据，采用深度信念网络结合CAPSNET的方法，开发了自动检测和分类口腔病变的系统，达到了较高的准确率和F1分数，有助于促进口腔癌的早期发现。	 | Oral cancer constitutes a significant global health concern, resulting in 277,484 fatalities in 2023, with the highest prevalence observed in low- and middle-income nations. Facilitating automation in the detection of possibly malignant and malignant lesions in the oral cavity could result in cost-effective and early disease diagnosis. Establishing an extensive repository of meticulously annotated oral lesions is essential. In this research photos are being collected from global clinical experts, who have been equipped with an annotation tool to generate comprehensive labelling. This research presents a novel approach for integrating bounding box annotations from various doctors. Additionally, Deep Belief Network combined with CAPSNET is employed to develop automated systems that extracted intricate patterns to address this challenging problem. This study evaluated two deep learning-based computer vision methodologies for the automated detection and classification of oral lesions to facilitate the early detection of oral cancer: image classification utilizing CAPSNET. Image classification attained an F1 score of 94.23% for detecting photos with lesions 93.46% for identifying images necessitating referral. Object detection attained an F1 score of 89.34% for identifying lesions for referral. Subsequent performances are documented about classification based on the sort of referral decision. Our preliminary findings indicate that deep learning possesses the capability to address this complex problem.	 | 口腔癌构成了全球健康的重要关切，2023年导致了277,484例死亡，其中低收入和中收入国家的发病率最高。在口腔中检测可能恶性及恶性病变的自动化技术可以实现成本效益高且早期的疾病诊断。建立一个详细注释的大规模口腔病变库至关重要。在这项研究中，来自全球临床专家的照片被收集，并配备了注释工具以生成全面的标注。本研究提出了一种新的方法，用于整合来自不同医生的边界框注释。此外，本研究采用了深度信念网络（Deep Belief Network）结合CAPSNET的方法，开发了自动系统以提取复杂的模式来解决这一具有挑战性的问题。本研究评估了基于深度学习的两种计算机视觉方法，用于口腔病变的自动检测和分类，以促进口腔癌的早期发现：使用CAPSNET进行图像分类。图像分类在检测带有病变的照片方面达到了94.23%的F1分数，93.46%的准确率识别需要转诊的图像。对象检测在识别需要转诊的病变方面达到了89.34%的F1分数。后续性能记录了根据转诊决定的不同分类情况。初步结果显示，深度学习具有解决这一复杂问题的能力。
2501.00751	 | HCMA-UNet: A Hybrid CNN-Mamba UNet with Inter-Slice Self-Attention for Efficient Breast Cancer Segmentation	 | Haoxuan Li,Wei song,Peiwu Qin,Xi Yuan,Zhenglin Chen	 | 本研究提出了一种名为HCMA-UNet的混合分割网络，结合轻量级CNN骨干和Mamba模块，用于乳腺癌DCE-MRI病灶分割，并通过特征引导的区域感知损失函数提升分割准确性，实验结果显示该方法在保持高效的同时达到了最先进的性能。	 | Breast cancer lesion segmentation in DCE-MRI remains challenging due to heterogeneous tumor morphology and indistinct boundaries. To address these challenges, this study proposes a novel hybrid segmentation network, HCMA-UNet, for lesion segmentation of breast cancer. Our network consists of a lightweight CNN backbone and a Multi-view Inter-Slice Self-Attention Mamba (MISM) module. The MISM module integrates Visual State Space Block (VSSB) and Inter-Slice Self-Attention (ISSA) mechanism, effectively reducing parameters through Asymmetric Split Channel (ASC) strategy to achieve efficient tri-directional feature extraction. Our lightweight model achieves superior performance with 2.87M parameters and 126.44 GFLOPs. A Feature-guided Region-aware loss function (FRLoss) is proposed to enhance segmentation accuracy. Extensive experiments on one private and two public DCE-MRI breast cancer datasets demonstrate that our approach achieves state-of-the-art performance while maintaining computational efficiency. FRLoss also exhibits good cross-architecture generalization capabilities. The source code and dataset is available on this link.	 | 乳腺癌DCE-MRI中的病灶分割仍然具有挑战性，因为肿瘤形态各异且边界模糊。为了应对这些挑战，本研究提出了一种新颖的混合分割网络HCMA-UNet，用于乳腺癌病灶分割。我们的网络由轻量级的CNN骨干网络和多视图层间自我注意力Mamba（MISM）模块组成。MISM模块结合了视觉状态空间块（VSSB）和层间自我注意力（ISSA）机制，通过不对称分裂通道（ASC）策略有效减少参数数，实现高效的多方向特征提取。我们的轻量级模型仅使用2.87M参数和126.44 GFLOPs就实现了出色的表现。我们还提出了一种特征引导的区域感知损失函数（FRLoss）来提高分割准确性。在一项私人和两项公开的DCE-MRI乳腺癌数据集上的广泛实验表明，我们的方法在保持计算效率的同时达到了最先进的性能。FRLoss还展示了良好的跨架构泛化能力。源代码和数据集可在以下链接获取。
2501.00725	 | Automatic Construction of Pattern Classifiers Capable of Continuous Incremental Learning and Unlearning Tasks Based on Compact-Sized Probabilistic Neural Network	 | Tetsuya Hoya,Shunpei Morita	 | 本文提出了一种基于紧凑型概率神经网络的新型模式分类方法，该网络能够进行连续的增量学习和去学习任务，无需调整超参数，仿真结果表明该方法在分类性能上与多层感知机神经网络相当，并且在连续学习任务中表现出色。	 | This paper proposes a novel approach to pattern classification using a probabilistic neural network model. The strategy is based on a compact-sized probabilistic neural network capable of continuous incremental learning and unlearning tasks. The network is constructed/reconstructed using a simple, one-pass network-growing algorithm with no hyperparameter tuning. Then, given the training dataset, its structure and parameters are automatically determined and can be dynamically varied in continual incremental and decremental learning situations. The algorithm proposed in this work involves no iterative or arduous matrix-based parameter approximations but a simple data-driven updating scheme. Simulation results using nine publicly available databases demonstrate the effectiveness of this approach, showing that compact-sized probabilistic neural networks constructed have a much smaller number of hidden units compared to the original probabilistic neural network model and yet can achieve a similar classification performance to that of multilayer perceptron neural networks in standard classification tasks, while also exhibiting sufficient capability in continuous class incremental learning and unlearning tasks.	 | 这篇论文提出了一种使用概率神经网络模型进行模式分类的新方法。该策略基于一个紧凑型的概率神经网络，能够进行连续的增量学习和去学习任务。该网络通过一个简单的、一次性的网络生长算法构建/重建，无需调整超参数。然后，给定训练数据集，其结构和参数会自动确定，并且可以在连续的增量和去学习情况下动态变化。本文提出的方法的算法不涉及迭代或基于矩阵的参数近似，而是一个简单的数据驱动更新方案。使用九个公开可用的数据集进行的仿真结果表明了该方法的有效性，显示了紧凑型概率神经网络相比原始概率神经网络模型具有更少的隐藏单元数量，但在标准分类任务中仍然能够达到与多层感知机神经网络类似的分类性能，同时在连续的类别增量学习和去学习任务中也表现出足够的能力。
2501.00677	 | Deeply Learned Robust Matrix Completion for Large-scale Low-rank Data Recovery	 | HanQin Cai,Chandra Kundu,Jialin Liu,Wotao Yin	 | 学习稳健矩阵填充（LRMC）是一种可扩展且可学习的非凸方法，专门用于大规模的稳健矩阵填充问题，通过深度展开技术实现线性收敛和最佳性能；在多种实际应用中，LRMC表现出优越的实证性能，优于现有最先进的方法。	 | Robust matrix completion (RMC) is a widely used machine learning tool that simultaneously tackles two critical issues in low-rank data analysis: missing data entries and extreme outliers. This paper proposes a novel scalable and learnable non-convex approach, coined Learned Robust Matrix Completion (LRMC), for large-scale RMC problems. LRMC enjoys low computational complexity with linear convergence. Motivated by the proposed theorem, the free parameters of LRMC can be effectively learned via deep unfolding to achieve optimum performance. Furthermore, this paper proposes a flexible feedforward-recurrent-mixed neural network framework that extends deep unfolding from fix-number iterations to infinite iterations. The superior empirical performance of LRMC is verified with extensive experiments against state-of-the-art on synthetic datasets and real applications, including video background subtraction, ultrasound imaging, face modeling, and cloud removal from satellite imagery.	 | 稳健矩阵填充（RMC）是一种广泛使用的机器学习工具，它同时解决了低秩数据分析中的两个关键问题：缺失的数据项和极端离群值。本文提出了一种新颖的可扩展且可学习的非凸方法——称为学习稳健矩阵填充（LRMC），专门用于大规模的RMC问题。LRMC具有较低的计算复杂度，并且具有线性收敛特性。受到所提出的定理的启发，LRMC的自由参数可以通过深度展开来有效学习，从而实现最佳性能。此外，本文提出了一种灵活的前馈-循环混合神经网络框架，将深度展开从固定次数的迭代扩展到无限次迭代。通过在合成数据集和实际应用（包括视频背景减除、超声成像、面部建模以及卫星图像中的云层去除）上进行广泛实验，验证了LRMC的优越实证性能，优于最先进的方法。
2501.00669	 | Leaf diseases detection using deep learning methods	 | El Houcine El Fatimi	 | 本研究开发了一种基于深度学习的新方法，通过叶片图像数据集来识别和检测植物叶片病害，并提出了一种高效网络架构来提高病害检测的准确性。此外，研究还提出了一种基于CNN的新模型，并评估了其性能，与一些最先进的预训练架构相比具有竞争力。	 | This study, our main topic is to devlop a new deep-learning approachs for plant leaf disease identification and detection using leaf image datasets. We also discussed the challenges facing current methods of leaf disease detection and how deep learning may be used to overcome these challenges and enhance the accuracy of disease detection. Therefore, we have proposed a novel method for the detection of various leaf diseases in crops, along with the identification and description of an efficient network architecture that encompasses hyperparameters and optimization methods. The effectiveness of different architectures was compared and evaluated to see the best architecture configuration and to create an effective model that can quickly detect leaf disease. In addition to the work done on pre-trained models, we proposed a new model based on CNN, which provides an efficient method for identifying and detecting plant leaf disease. Furthermore, we evaluated the efficacy of our model and compared the results to those of some pre-trained state-of-the-art architectures.	 | 本研究的主要主题是开发一种新的深度学习方法，用于通过叶片图像数据集识别和检测植物叶片病害。我们还讨论了当前叶片病害检测方法所面临的挑战，以及深度学习如何克服这些挑战并提高病害检测的准确性。因此，我们提出了一种新型方法来检测作物的各种叶片病害，并且描述了一个高效网络架构，该架构包括超参数和优化方法。不同的网络架构的有效性被比较和评估，以确定最佳的架构配置，并创建一个能够快速检测叶片病害的有效模型。除了对预训练模型的研究之外，我们还提出了一种基于CNN的新模型，该模型提供了一种高效的方法来识别和检测植物叶片病害。此外，我们评估了我们模型的有效性，并将其结果与一些最先进的预训练架构进行了比较。
2501.00647	 | Lightweight G-YOLOv11: Advancing Efficient Fracture Detection in Pediatric Wrist X-rays	 | Abdesselam Ferdi	 | 本文提出了一种基于Ghost卷积的轻量级YOLOv11（G-YOLOv11）骨折检测系统，该系统在保持较高检测精度的同时显著减少了计算资源需求，在NVIDIA A10 GPU上的推断时间为2.4毫秒，优于现有检测器。	 | Computer-aided diagnosis (CAD) systems have greatly improved the interpretation of medical images by radiologists and surgeons. However, current CAD systems for fracture detection in X-ray images primarily rely on large, resource-intensive detectors, which limits their practicality in clinical settings. To address this limitation, we propose a novel lightweight CAD system based on the YOLO detector for fracture detection. This system, named ghost convolution-based YOLOv11 (G-YOLOv11), builds on the latest version of the YOLO detector family and incorporates the ghost convolution operation for feature extraction. The ghost convolution operation generates the same number of feature maps as traditional convolution but requires fewer linear operations, thereby reducing the detector's computational resource requirements. We evaluated the performance of the proposed G-YOLOv11 detector on the GRAZPEDWRI-DX dataset, achieving an mAP@0.5 of 0.535 with an inference time of 2.4 ms on an NVIDIA A10 GPU. Compared to the standard YOLOv11l, G-YOLOv11l achieved reductions of 13.6% in mAP@0.5 and 68.7% in size. These results establish a new state-of-the-art benchmark in terms of efficiency, outperforming existing detectors. Code and models are available at https://github.com/AbdesselamFerdi/G-YOLOv11.	 | 计算机辅助诊断（CAD）系统极大地提高了放射科医生和外科医生对医学图像的解释能力。然而，目前用于X光影像骨折检测的CAD系统主要依赖于大型且资源密集型的检测器，这限制了它们在临床环境中的实用性。为了解决这一限制，我们提出了一种基于Ghost卷积的YOLOv11（G-YOLOv11）的新型轻量级CAD系统。该系统建立在YOLO检测器家族的最新版本上，并结合了Ghost卷积操作进行特征提取。Ghost卷积操作生成与传统卷积相同数量的特征图，但所需的线性操作更少，从而减少检测器的计算资源需求。我们在GRAZPEDWRI-DX数据集上评估了所提出的G-YOLOv11检测器，其在NVIDIA A10 GPU上的推断时间为2.4毫秒时，mAP@0.5为0.535。与标准YOLOv11相比，G-YOLOv11在mAP@0.5上的减少幅度为13.6%，大小减少了68.7%。这些结果在效率方面确立了一个新的最先进基准，优于现有的检测器。相关代码和模型可在https://github.com/AbdesselamFerdi/G-YOLOv11获得。
2501.00636	 | Applying Graph Explanation to Operator Fusion	 | Keith G. Mills,Muhammad Fetrat Qharabagh,Weichen Qiu,Fred X. Han,Mohammad Salameh,Wei Lu,Shangling Jui,Di Niu	 | 该研究引入了图解释技术（GET）到层融合中，通过递归拆分融合组以最小化DRAM访问，从而提高深度神经网络的推理效率，特别是在经典卷积神经网络中显著减少了DRAM访问次数，例如在EfficientNet-B3中实现了超过20%的减少。	 | Layer fusion techniques are critical to improving the inference efficiency of deep neural networks (DNN) for deployment. Fusion aims to lower inference costs by reducing data transactions between an accelerator's on-chip buffer and DRAM. This is accomplished by grouped execution of multiple operations like convolution and activations together into single execution units - fusion groups. However, on-chip buffer capacity limits fusion group size and optimizing fusion on whole DNNs requires partitioning into multiple fusion groups. Finding the optimal groups is a complex problem where the presence of invalid solutions hampers traditional search algorithms and demands robust approaches. In this paper we incorporate Explainable AI, specifically Graph Explanation Techniques (GET), into layer fusion. Given an invalid fusion group, we identify the operations most responsible for group invalidity, then use this knowledge to recursively split the original fusion group via a greedy tree-based algorithm to minimize DRAM access. We pair our scheme with common algorithms and optimize DNNs on two types of layer fusion: Line-Buffer Depth First (LBDF) and Branch Requirement Reduction (BRR). Experiments demonstrate the efficacy of our scheme on several popular and classical convolutional neural networks like ResNets and MobileNets. Our scheme achieves over 20% DRAM Access reduction on EfficientNet-B3.	 | 层融合技术对于提高部署中深度神经网络（DNN）的推理效率至关重要。融合旨在通过减少加速器片上缓冲区与DRAM之间的数据传输来降低推理成本。这通过将多个操作（如卷积和激活）分组执行并整合到单一执行单元中来实现——融合组。然而，片上缓冲区容量限制了融合组的大小，并且在优化整个DNN时需要将其划分为多个融合组。找到最优组是一个复杂的问题，其中无效解的存在阻碍了传统搜索算法的应用，并需要稳健的方法。在这篇论文中，我们将可解释的人工智能（Explainable AI）特别引入层融合，具体来说是图解释技术（GET）。给定一个无效的融合组，我们确定最负责该组无效的操作，然后利用这些知识通过贪婪树形算法递归地拆分原始融合组以最小化DRAM访问。我们将该方案与常见算法结合，并针对两种类型的层融合优化DNN：线缓冲深度优先（LBDF）和分支需求减少（BRR）。实验表明，我们的方案在ResNets和MobileNets等流行的经典卷积神经网络中有效地减少了DRAM访问。我们的方案在EfficientNet-B3中实现了超过20%的DRAM访问减少。
2501.00586	 | Advanced Lung Nodule Segmentation and Classification for Early Detection of Lung Cancer using SAM and Transfer Learning	 | Asha V,Bhavanishankar K	 | 本研究提出了一种结合分割一切模型（SAM）和迁移学习的创新方法，用于提高肺结节分割的精度，从而支持早期肺癌诊断，结果显示该方法在分割和分类上的DSC为97.08%，IoU为95.6%，分类精度为96.71%，表现出色。	 | Lung cancer is an extremely lethal disease primarily due to its late-stage diagnosis and significant mortality rate, making it the major cause of cancer-related demises globally. Machine Learning (ML) and Convolution Neural network (CNN) based Deep Learning (DL) techniques are primarily used for precise segmentation and classification of cancerous nodules in the CT (Computed Tomography) or MRI images. This study introduces an innovative approach to lung nodule segmentation by utilizing the Segment Anything Model (SAM) combined with transfer learning techniques. Precise segmentation of lung nodules is crucial for the early detection of lung cancer. The proposed method leverages Bounding Box prompts and a vision transformer model to enhance segmentation performance, achieving high accuracy, Dice Similarity Coefficient (DSC) and Intersection over Union (IoU) metrics. The integration of SAM and Transfer Learning significantly improves Computer-Aided Detection (CAD) systems in medical imaging, particularly for lung cancer diagnosis. The findings demonstrate the proposed model effectiveness in precisely segmenting lung nodules from CT scans, underscoring its potential to advance early detection and improve patient care outcomes in lung cancer diagnosis. The results show SAM Model with transfer learning achieving a DSC of 97.08% and an IoU of 95.6%, for segmentation and accuracy of 96.71% for classification indicates that ,its performance is noteworthy compared to existing techniques.	 | 肺癌是一种由于其晚期诊断和高死亡率而极具致命性的疾病，使其成为全球癌症相关死亡的主要原因。基于机器学习（ML）和卷积神经网络（CNN）的深度学习（DL）技术主要用于精确分割和分类CT（计算机断层扫描）或MRI图像中的癌性结节。本研究引入了一种创新的方法，通过结合分割一切模型（SAM）和迁移学习技术来实现肺结节分割。精确分割肺结节对于早期发现肺癌至关重要。所提出的方法利用边界框提示和视觉转换器模型来提高分割性能，实现了高精度、Dice相似系数（DSC）和交并比（IoU）指标。SAM与迁移学习的结合显著提高了医学影像中的计算机辅助检测（CAD）系统，特别是在肺癌诊断中的表现。研究结果表明，所提出的模型在CT扫描中精确分割肺结节方面表现出有效性，突显了其在肺癌诊断中早期检测和提高患者护理结果的潜力。结果显示，结合迁移学习的SAM模型在分割和分类上的DSC为97.08%，IoU为95.6%，分类精度为96.71%，表明其性能优于现有技术。
2501.00514	 | H-Net: A Multitask Architecture for Simultaneous 3D Force Estimation and Stereo Semantic Segmentation in Intracardiac Catheters	 | Pedram Fekri,Mehrdad Zadeh,Javad Dargahi	 | 该研究提出了一种轻量级的多输入多输出编码器-解码器架构，能够同时从两个角度分割导管并估计在三维空间中的力，该架构利用来自双平面射影成像系统的X射线图像，并通过共享参数的并行子网络和立体视觉技术实现。	 | The success rate of catheterization procedures is closely linked to the sensory data provided to the surgeon. Vision-based deep learning models can deliver both tactile and visual information in a sensor-free manner, while also being cost-effective to produce. Given the complexity of these models for devices with limited computational resources, research has focused on force estimation and catheter segmentation separately. However, there is a lack of a comprehensive architecture capable of simultaneously segmenting the catheter from two different angles and estimating the applied forces in 3D. To bridge this gap, this work proposes a novel, lightweight, multi-input, multi-output encoder-decoder-based architecture. It is designed to segment the catheter from two points of view and concurrently measure the applied forces in the x, y, and z directions. This network processes two simultaneous X-Ray images, intended to be fed by a biplane fluoroscopy system, showing a catheter's deflection from different angles. It uses two parallel sub-networks with shared parameters to output two segmentation maps corresponding to the inputs. Additionally, it leverages stereo vision to estimate the applied forces at the catheter's tip in 3D. The architecture features two input channels, two classification heads for segmentation, and a regression head for force estimation through a single end-to-end architecture. The output of all heads was assessed and compared with the literature, demonstrating state-of-the-art performance in both segmentation and force estimation. To the best of the authors' knowledge, this is the first time such a model has been proposed	 | 该研究导管操作的成功率与外科医生所获得的感官数据密切相关。基于视觉的深度学习模型能够在不依赖传感器的情况下，同时提供触觉和视觉信息，并且生产成本较低。鉴于这些模型在资源有限的设备中的复杂性，研究主要集中在分别估计力和导管分割上。然而，目前缺乏一种同时从两个角度分割导管并在三维空间中估计所施加力的全面架构。为解决这一问题，本项工作提出了一种新颖的、轻量级的、多输入多输出的编码器-解码器架构。该架构旨在从两个角度分割导管，并同时测量施加在三个方向（x、y、z）上的力。该网络处理来自双平面射影成像系统的两个同时X射线图像，显示导管从不同角度的弯曲情况。该网络利用两个具有共享参数的并行子网络，输出与输入相对应的两个分割图。此外，它利用立体视觉从三维空间估计导管尖端的施力。该架构包括两个输入通道、两个用于分割的分类头以及一个用于通过单个端到端架构进行力估计的回归头。所有头的输出都被评估并与其他文献进行了比较，展示了在分割和力估计方面的最新技术水平。据作者所知，这是首次提出这样的模型。
2501.00463	 | SAT-LDM: Provably Generalizable Image Watermarking for Latent Diffusion Models with Self-Augmented Training	 | Lu Zhang,Liang Zeng	 | 本文提出了一种适用于潜在扩散模型的自我增强训练（SAT-LDM）方法，通过增强水印模块的泛化能力，实现稳健的图像水印效果并减少伪影，从而有效保护人工智能生成内容的知识产权。	 | The proliferation of AI-generated images necessitates effective watermarking to protect intellectual property and identify fake content. While existing training-based watermarking methods show promise, they often struggle with generalization across diverse prompts and tend to produce noticeable artifacts. To this end, we introduce a provably generalizable image watermarking method for Latent Diffusion Models with Self-Augmented Training (SAT-LDM), which aligns the training and testing phases by a free generation distribution to bolster the watermarking module's generalization capabilities. We theoretically consolidate our method by proving that the free generation distribution contributes to its tight generalization bound without the need to collect new data. Extensive experimental results show that SAT-LDM achieves robust watermarking while significantly improving the quality of watermarked images across diverse prompts. Furthermore, we conduct experimental analyses to demonstrate the strong generalization abilities of SAT-LDM. We hope our method offers a practical and convenient solution for securing high-fidelity AI-generated content.	 | 人工智能生成的图像的泛滥迫切需要有效的水印技术来保护知识产权并识别虚假内容。虽然现有的基于训练的水印方法显示出潜力，但它们往往难以在多种提示下进行泛化，并且常常会产生明显的伪影。为此，我们提出了一种适用于潜在扩散模型的自我增强训练（SAT-LDM）的可验证泛化图像水印方法。该方法通过一种自由生成分布来对齐训练和测试阶段，从而增强水印模块的泛化能力。我们从理论上证明了，由于自由生成分布的存在，我们的方法不需要收集新数据即可实现紧密的泛化边界。大量的实验结果显示，SAT-LDM 在多种提示下实现了稳健的水印效果，并显著改善了水印图像的质量。此外，我们还进行了实验分析以展示 SAT-LDM 强大的泛化能力。我们希望我们的方法能为保护高质量的人工智能生成内容提供一种实用和便捷的解决方案。
2501.00457	 | Differentiable Prompt Learning for Vision Language Models	 | Zhenhan Huang,Tejaswini Pedapati,Pin-Yu Chen,Jianxi Gao	 | 一种称为可微分提示学习（DPL）的方法被提出，用于自动优化预训练模型中的深层连续模式，从而在多个下游任务中显著提升性能，平均测试准确率提高了2.60%。	 | Prompt learning is an effective way to exploit the potential of large-scale pre-trained foundational models. Continuous prompts parameterize context tokens in prompts by turning them into differentiable vectors. Deep continuous prompts insert prompts not only in the input but also in the intermediate hidden representations. Manually designed deep continuous prompts exhibit a remarkable improvement compared to the zero-shot pre-trained model on downstream tasks. How to automate the continuous prompt design is an underexplored area, and a fundamental question arises, is manually designed deep prompt strategy optimal? To answer this question, we propose a method dubbed differentiable prompt learning (DPL). The DPL method is formulated as an optimization problem to automatically determine the optimal context length of the prompt to be added to each layer, where the objective is to maximize the performance. We test the DPL method on the pre-trained CLIP. We empirically find that by using only limited data, our DPL method can find deep continuous prompt configuration with high confidence. The performance on the downstream tasks exhibits the superiority of the automatic design: our method boosts the average test accuracy by 2.60% on 11 datasets compared to baseline methods. Besides, our method focuses only on the prompt configuration (i.e. context length for each layer), which means that our method is compatible with the baseline methods that have sophisticated designs to boost the performance. The DPL method can be deployed to large language models or computer vision models at no cost.	 | 大规模预训练基础模型具有潜力的有效利用方式是模式学习。连续模式通过将提示中的上下文令牌转换为可微分向量来参数化上下文令牌。深层连续模式不仅在输入中插入提示，还在中间隐藏表示中插入提示。手工设计的深层连续模式在下游任务中与零样本预训练模型相比表现出显著的改进。如何自动化连续模式设计是一个未被充分探索的领域，一个基本问题随之而来：手工设计的深层提示策略是否是最优的？为了回答这一问题，我们提出了一种称为可微分提示学习（DPL）的方法。DPL方法被形式化为一个优化问题，以自动确定添加到每层的提示的最佳上下文长度，目标是最大化性能。我们在预训练的CLIP上测试了DPL方法。实验证明，仅使用少量数据，我们的DPL方法可以找到具有高信心的深层连续提示配置。在下游任务上的表现显示了自动化设计的优势：我们的方法在11个数据集上将平均测试准确率提高了2.60%，比基线方法更高。此外，我们的方法仅关注提示配置（即每层的上下文长度），这意味着我们的方法与能够通过复杂设计提高性能的基线方法兼容。DPL方法可以在没有任何成本的情况下部署到大型语言模型或计算机视觉模型中。
2501.00378	 | STARFormer: A Novel Spatio-Temporal Aggregation Reorganization Transformer of FMRI for Brain Disorder Diagnosis	 | Wenhao Dong,Yueyang Li,Weiming Zeng,Lei Chen,Hongjie Yan,Wai Ting Siok,Nizhuan Wang	 | 研究提出了一种名为STARFormer的新方法，通过有效整合BOLD信号的空间和时间特征来提高自闭症谱系障碍和注意力缺陷多动障碍的分类准确性，实验结果表明该方法优于现有技术，并已在公开数据集上进行了验证。	 | Many existing methods that use functional magnetic resonance imaging (fMRI) classify brain disorders, such as autism spectrum disorder (ASD) and attention deficit hyperactivity disorder (ADHD), often overlook the integration of spatial and temporal dependencies of the blood oxygen level-dependent (BOLD) signals, which may lead to inaccurate or imprecise classification results. To solve this problem, we propose a Spatio-Temporal Aggregation eorganization ransformer (STARFormer) that effectively captures both spatial and temporal features of BOLD signals by incorporating three key modules. The region of interest (ROI) spatial structure analysis module uses eigenvector centrality (EC) to reorganize brain regions based on effective connectivity, highlighting critical spatial relationships relevant to the brain disorder. The temporal feature reorganization module systematically segments the time series into equal-dimensional window tokens and captures multiscale features through variable window and cross-window attention. The spatio-temporal feature fusion module employs a parallel transformer architecture with dedicated temporal and spatial branches to extract integrated features. The proposed STARFormer has been rigorously evaluated on two publicly available datasets for the classification of ASD and ADHD. The experimental results confirm that the STARFormer achieves state-of-the-art performance across multiple evaluation metrics, providing a more accurate and reliable tool for the diagnosis of brain disorders and biomedical research. The codes will be available at: https://github.com/NZWANG/STARFormer.	 | 许多现有的功能性磁共振成像(fMRI)方法在分类自闭症谱系障碍(ASD)和注意力缺陷多动障碍(ADHD)等脑部疾病时，往往忽略了血液氧水平依赖(BOLD)信号的空间和时间依赖性整合，这可能导致分类结果不准确或不够精确。为了解决这个问题，我们提出了一种时空聚合变压器(STARFormer)，通过结合三个关键模块有效地捕获BOLD信号的空间和时间特征。兴趣区域(ROI)的空间结构分析模块利用特征向量中心性(EC)重新组织脑区，基于有效连接性突出与脑部疾病相关的关键空间关系。时间特征重新组织模块系统地将时间序列分割成等维度窗口令牌，并通过可变窗口和窗口间注意机制捕获多尺度特征。时空特征融合模块采用带有专门时间分支和空间分支的并行变压器架构来提取集成特征。所提出的STARFormer已经在两个公开可用的数据集上对ASD和ADHD的分类进行了严格评估。实验结果证实，STARFormer在多个评估指标上达到了最先进的性能，提供了一个更准确和可靠的脑部疾病诊断工具和生物医学研究的工具。代码将在以下地址提供：https://github.com/NZWANG/STARFormer。
2501.00296	 | Predicate Invention from Pixels via Pretrained Vision-Language Models	 | Ashay Athalye,Nishanth Kumar,Tom Silver,Yichao Liang,Tomás Lozano-Pérez,Leslie Pack Kaelbling	 | 本文提出了一种利用预训练视觉语言模型直接从图像中发明谓词的方法，以解决机器人领域的长期决策问题，并在两个模拟环境中实现了对新颖、复杂任务的有效泛化。	 | Our aim is to learn to solve long-horizon decision-making problems in highly-variable, combinatorially-complex robotics domains given raw sensor input in the form of images. Previous work has shown that one way to achieve this aim is to learn a structured abstract transition model in the form of symbolic predicates and operators, and then plan within this model to solve novel tasks at test time. However, these learned models do not ground directly into pixels from just a handful of demonstrations. In this work, we propose to invent predicates that operate directly over input images by leveraging the capabilities of pretrained vision-language models (VLMs). Our key idea is that, given a set of demonstrations, a VLM can be used to propose a set of predicates that are potentially relevant for decision-making and then to determine the truth values of these predicates in both the given demonstrations and new image inputs. We build upon an existing framework for predicate invention, which generates feature-based predicates operating on object-centric states, to also generate visual predicates that operate on images. Experimentally, we show that our approach -- pix2pred -- is able to invent semantically meaningful predicates that enable generalization to novel, complex, and long-horizon tasks across two simulated robotic environments.	 | 我们的目标是在具有高度变异性且组合复杂性的机器人领域中，通过原始传感器输入（如图像）来学习解决长期决策问题。以往的工作表明，实现这一目标的一种方法是学习一种结构化的抽象转换模型，形式为符号谓词和操作符，然后在这种模型中进行规划，以在测试时间解决新颖任务。然而，这些学习到的模型并不能直接从少量的演示中将信息映射到像素上。在这项工作中，我们提出利用预训练的视觉语言模型（VLM）的能力，直接在输入图像上发明谓词。我们的核心思想是，给定一组演示，可以用VLM建议一组可能与决策相关的谓词，然后确定这些谓词在给定的演示和新的图像输入中的真值。我们在此基础上扩展了一个现有的谓词发明框架，该框架生成基于特征的谓词操作于以对象为中心的状态，以生成操作于图像的视觉谓词。实验中，我们展示了我们的方法——pix2pred——能够发明语义上有意义的谓词，使模型能够在两个模拟的机器人环境中对新颖、复杂和长期决策任务进行泛化。
2501.00265	 | Outlier-Robust Training of Machine Learning Models	 | Rajat Talak,Charis Georgiou,Jingnan Shi,Luca Carlone	 | 本文针对异常值情况下的机器学习模型训练问题，提出了一个统一的鲁棒损失框架，并设计了一个自适应交替算法（AAA），该算法通过迭代更新非鲁棒损失的加权版本来训练模型，实验结果验证了其在回归、分类和神经场景重建问题上的有效性。	 | Robust training of machine learning models in the presence of outliers has garnered attention across various domains. The use of robust losses is a popular approach and is known to mitigate the impact of outliers. We bring to light two literatures that have diverged in their ways of designing robust losses: one using M-estimation, which is popular in robotics and computer vision, and another using a risk-minimization framework, which is popular in deep learning. We first show that a simple modification of the Black-Rangarajan duality provides a unifying view. The modified duality brings out a definition of a robust loss kernel $σ$ that is satisfied by robust losses in both the literatures. Secondly, using the modified duality, we propose an Adaptive Alternation Algorithm (AAA) for training machine learning models with outliers. The algorithm iteratively trains the model by using a weighted version of the non-robust loss, while updating the weights at each iteration. The algorithm is augmented with a novel parameter update rule by interpreting the weights as inlier probabilities, and obviates the need for complex parameter tuning. Thirdly, we investigate convergence of the adaptive alternation algorithm to outlier-free optima. Considering arbitrary outliers (i.e., with no distributional assumption on the outliers), we show that the use of robust loss kernels σ increases the region of convergence. We experimentally show the efficacy of our algorithm on regression, classification, and neural scene reconstruction problems. We release our implementation code: https://github.com/MIT-SPARK/ORT.	 | 在存在异常值的情况下训练机器学习模型的鲁棒性问题已经引起了各个领域的关注。使用鲁棒损失是流行的方法之一，已知能够减轻异常值的影响。我们在文献中揭示了两种设计鲁棒损失的方法出现了分歧：一种是使用M-估计方法，在机器人学和计算机视觉领域比较流行；另一种是使用风险最小化框架，在深度学习领域较为流行。首先，我们展示了Black-Rangarajan对偶性的简单修改提供了一个统一的视角。修改后的对偶性揭示了一种鲁棒损失核 $σ$ 的定义，这种定义适用于两种文献中的鲁棒损失。其次，利用修改后的对偶性，我们提出了一个自适应交替算法（AAA），用于训练存在异常值的机器学习模型。该算法通过使用非鲁棒损失的加权版本迭代训练模型，并在每次迭代中更新权重。通过将权重解释为内点概率，算法增加了新的参数更新规则，从而省去了复杂的参数调优。第三，我们研究了自适应交替算法收敛到无异常值最优解的情况。考虑任意异常值（即对异常值不做分布假设），我们证明了使用鲁棒损失核 $σ$ 可以扩大收敛区域。我们在回归、分类和神经场景重建问题上通过实验验证了我们算法的有效性。我们发布了我们的实现代码：https://github.com/MIT-SPARK/ORT。
2501.00184	 | TrajLearn: Trajectory Prediction Learning using Deep Generative Models	 | Amirhossein Nadiri,Jing Li,Ali Faraji,Ghadeer Abuoda,Manos Papagelis	 | TrajLearn 是一种基于六边形空间表示的新模型，用于生成高阶移动流以进行轨迹预测，并通过集成自定义束搜索方法显著提高了预测性能，相较于现有方法在多个实际轨迹数据集上性能提升高达40%。此外，该模型还支持混合分辨率地图生成，以优化存储和计算资源的使用。	 | Trajectory prediction aims to estimate an entity's future path using its current position and historical movement data, benefiting fields like autonomous navigation, robotics, and human movement analytics. Deep learning approaches have become key in this area, utilizing large-scale trajectory datasets to model movement patterns, but face challenges in managing complex spatial dependencies and adapting to dynamic environments. To address these challenges, we introduce TrajLearn, a novel model for trajectory prediction that leverages generative modeling of higher-order mobility flows based on hexagonal spatial representation. TrajLearn predicts the next $k$ steps by integrating a customized beam search for exploring multiple potential paths while maintaining spatial continuity. We conducted a rigorous evaluation of TrajLearn, benchmarking it against leading state-of-the-art approaches and meaningful baselines. The results indicate that TrajLearn achieves significant performance gains, with improvements of up to ~40% across multiple real-world trajectory datasets. In addition, we evaluated different prediction horizons (i.e., various values of $k$), conducted resolution sensitivity analysis, and performed ablation studies to assess the impact of key model components. Furthermore, we developed a novel algorithm to generate mixed-resolution maps by hierarchically subdividing hexagonal regions into finer segments within a specified observation area. This approach supports selective detailing, applying finer resolution to areas of interest or high activity (e.g., urban centers) while using coarser resolution for less significant regions (e.g., rural areas), effectively reducing data storage requirements and computational overhead. We promote reproducibility and adaptability by offering complete code, data, and detailed documentation with flexible configuration options for various applications.	 | 轨迹预测旨在利用实体的当前位置及其历史运动数据来估计其未来的路径，这一目标在自主导航、机器人技术以及人类运动分析等领域具有重要作用。深度学习方法已成为这一领域的关键工具，通过利用大规模轨迹数据集来建模运动模式，但它们在管理复杂的空间依赖关系和适应动态环境方面面临着挑战。为解决这些挑战，我们提出了一种名为TrajLearn的新模型，该模型基于六边形空间表示生成高阶移动流的生成模型来进行轨迹预测。TrajLearn通过集成自定义的束搜索方法来预测接下来的 $k$ 步，该方法在探索多个潜在路径的同时保持空间连续性。  我们对TrajLearn进行了严格的评估，并将其与当前领先的方法和有意义的基线进行比较。结果表明，相较于现有的方法，TrajLearn实现了显著的性能提升，多个实际轨迹数据集上的改进幅度可达~40%。此外，我们还评估了不同的预测时间窗（即不同的 $k$ 值），进行了分辨率敏感性分析，并进行了消融研究以评估关键模型组件的影响。同时，我们开发了一种新算法，通过在指定观测区域内将六边形区域逐级细分为更精细的区域来生成混合分辨率地图。这种方法能够根据兴趣区域或高活动区域（例如城市中心）应用更精细的分辨率，而在不那么重要的区域（例如农村地区）使用更粗的分辨率，从而有效减少数据存储需求和计算开销。我们通过提供完整的代码、数据和详细的文档，支持灵活的配置选项来促进研究的可重复性和适应性。
2501.00057	 | VisTabNet: Adapting Vision Transformers for Tabular Data	 | Witold Wydmański,Ulvi Movsum-zada,Jacek Tabor,Marek Śmieja	 | VisTabNet 是一种跨模态的迁移学习方法，利用预训练的 Vision Transformer (ViT) 处理表格数据，通过将表格输入投影为 ViT 可接受的补丁嵌入，直接应用预训练的 Transformer 编码器，实验结果显示该方法在小型表格数据集上的性能优于传统方法和最近的深度学习模型。	 | Although deep learning models have had great success in natural language processing and computer vision, we do not observe comparable improvements in the case of tabular data, which is still the most common data type used in biological, industrial and financial applications. In particular, it is challenging to transfer large-scale pre-trained models to downstream tasks defined on small tabular datasets. To address this, we propose VisTabNet -- a cross-modal transfer learning method, which allows for adapting Vision Transformer (ViT) with pre-trained weights to process tabular data. By projecting tabular inputs to patch embeddings acceptable by ViT, we can directly apply a pre-trained Transformer Encoder to tabular inputs. This approach eliminates the conceptual cost of designing a suitable architecture for processing tabular data, while reducing the computational cost of training the model from scratch. Experimental results on multiple small tabular datasets (less than 1k samples) demonstrate VisTabNet's superiority, outperforming both traditional ensemble methods and recent deep learning models. The proposed method goes beyond conventional transfer learning practice and shows that pre-trained image models can be transferred to solve tabular problems, extending the boundaries of transfer learning.	 | 尽管深度学习模型在自然语言处理和计算机视觉领域取得了巨大成功，但在表格数据方面却没有观察到类似的进步，而表格数据仍然是生物、工业和金融应用中最常用的数据类型。特别是在将大型预训练模型应用于小型表格数据集定义的下游任务时，存在很大的挑战。为了解决这个问题，我们提出了一种跨模态的迁移学习方法——VisTabNet，该方法允许利用预训练权重的Vision Transformer（ViT）来处理表格数据。通过将表格输入投影为ViT可接受的补丁嵌入，可以直接应用预训练的Transformer编码器处理表格输入。这种方法消除了为设计适合处理表格数据的架构而付出的概念性成本，并减少了从头训练模型的计算成本。在多个小型表格数据集（样本数少于1000）上的实验结果表明，VisTabNet在性能上优于传统的集成方法和最近的深度学习模型。所提出的方法超越了传统的迁移学习实践，表明预训练的图像模型可以被转移以解决表格问题，从而扩展了迁移学习的边界。
