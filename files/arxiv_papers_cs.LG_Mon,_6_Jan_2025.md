| ID | Title | Authors | Summary (zh) | Abstract (en) | Abstract (zh) | 
| --- | --- | --- | --- | --- | --- |
2501.01951	 | MixGCN: Scalable GCN Training by Mixture of Parallelism and Mixture of Accelerators	 | Cheng Wan,Runkao Tao,Zheng Du,Yang Katie Zhao,Yingyan Celine Lin	 | MixGCN通过结合混合并行性和专门的加速器设计，有效解决了全图训练图卷积网络时内存消耗大和计算资源利用复杂的问题，从而显著提升了训练效率和可扩展性。	 | Graph convolutional networks (GCNs) have demonstrated superiority in graph-based learning tasks. However, training GCNs on full graphs is particularly challenging, due to the following two challenges: (1) the associated feature tensors can easily explode the memory and block the communication bandwidth of modern accelerators, and (2) the computation workflow in training GCNs alternates between sparse and dense matrix operations, complicating the efficient utilization of computational resources. Existing solutions for scalable distributed full-graph GCN training mostly adopt partition parallelism, which is unsatisfactory as they only partially address the first challenge while incurring scaled-out communication volume. To this end, we propose MixGCN aiming to simultaneously address both the aforementioned challenges towards GCN training. To tackle the first challenge, MixGCN integrates mixture of parallelism. Both theoretical and empirical analysis verify its constant communication volumes and enhanced balanced workload; For handling the second challenge, we consider mixture of accelerators (i.e., sparse and dense accelerators) with a dedicated accelerator for GCN training and a fine-grain pipeline. Extensive experiments show that MixGCN achieves boosted training efficiency and scalability.	 | 图卷积网络（GCNs）在基于图的学习任务中展现出了优越性。然而，在全图上训练GCNs尤其具有挑战性，原因如下：（1）相关特征张量容易消耗大量内存并阻塞现代加速器的通信带宽；（2）在训练GCNs的过程中，计算工作流在稀疏和密集矩阵操作之间交替进行，这使得高效利用计算资源变得复杂。现有可扩展分布式全图GCN训练的解决方案大多采用划分并行性，这种方法虽然在一定程度上解决了第一个挑战，但增加了扩展通信的开销，效果并不理想。为此，我们提出了MixGCN，旨在同时解决上述两个挑战，以提高GCN训练的效率。为了应对第一个挑战，MixGCN 结合了混合并行性。理论和实验分析验证了其通信量的恒定性和工作负载的均衡性；为了应对第二个挑战，我们考虑了混合加速器（即稀疏和密集加速器），并为GCN训练分配了一个专用加速器和细粒度流水线。广泛的实验表明，MixGCN 实现了训练效率和可扩展性的显著提升。
2501.01950	 | MADGEN -- Mass-Spec attends to De Novo Molecular generation	 | Yinkai Wang,Xiaohui Chen,Liping Liu,Soha Hassoun	 | MADGEN是一种基于质谱数据的从头分子结构生成方法，通过两个阶段——支架检索和基于光谱条件的分子生成，有效减少搜索空间并提高生成准确性；在三个数据集上的评估表明，MADGEN能够利用注意力机制整合光谱信息，从而改善分子结构的注释性能。	 | The annotation (assigning structural chemical identities) of MS/MS spectra remains a significant challenge due to the enormous molecular diversity in biological samples and the limited scope of reference databases. Currently, the vast majority of spectral measurements remain in the "dark chemical space" without structural annotations. To improve annotation, we propose MADGEN (Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based method for de novo molecular structure generation guided by mass spectrometry data. MADGEN operates in two stages: scaffold retrieval and spectra-conditioned molecular generation starting with the scaffold. In the first stage, given an MS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employ contrastive learning to align mass spectra with candidate molecular scaffolds. In the second stage, starting from the retrieved scaffold, we employ the MS/MS spectrum to guide an attention-based generative model to generate the final molecule. Our approach constrains the molecular generation search space, reducing its complexity and improving generation accuracy. We evaluate MADGEN on three datasets (NIST23, CANOPUS, and MassSpecGym) and evaluate MADGEN's performance with a predictive scaffold retriever and with an oracle retriever. We demonstrate the effectiveness of using attention to integrate spectral information throughout the generation process to achieve strong results with the oracle retriever.	 | MS/MS光谱的注释（分配结构化学身份）仍然是一个重大挑战，因为在生物样品中存在巨大的分子多样性，而参考数据库的范围有限。目前，绝大多数光谱测量仍然处于“黑暗化学空间”中，缺乏结构注释。为改善注释，我们提出了一种基于支架的方法MADGEN（Mass-spec Attends to De Novo Molecular GENeration），这是一种由质谱数据指导的从头分子结构生成方法。MADGEN分为两个阶段：支架检索和基于光谱条件的分子生成。在第一个阶段，给定一个MS/MS光谱，我们将支架检索形式化为一个排序问题，并使用对比学习来对齐质量谱和候选分子支架。在第二个阶段，从检索到的支架开始，我们利用MS/MS光谱来指导一个基于注意力的生成模型生成最终的分子。我们的方法限制了分子生成的搜索空间，减少了其复杂性并提高了生成准确度。我们在三个数据集（NIST23、CANOPUS和MassSpecGym）上评估了MADGEN，并分别使用预测支架检索器和先验检索器评估了MADGEN的性能。我们展示了使用注意力在整个生成过程中整合光谱信息的有效性，从而在先验检索器的情况下取得了良好的结果。
2501.01936	 | Improving Transducer-Based Spoken Language Understanding with Self-Conditioned CTC and Knowledge Transfer	 | Vishal Sunder,Eric Fosler-Lussier	 | 本文提出了一种通过结合CTC自条件目标改进端到端语音语言理解的方法，该方法类似于E2E可微分级联模型，依次执行ASR和SLU任务，并通过声学嵌入与BERT模型对齐进一步提升性能；该策略显著提高了SLU性能，并在多个基线上表现出色，参数量远少于大型模型如Whisper。	 | In this paper, we propose to improve end-to-end (E2E) spoken language understand (SLU) in an RNN transducer model (RNN-T) by incorporating a joint self-conditioned CTC automatic speech recognition (ASR) objective. Our proposed model is akin to an E2E differentiable cascaded model which performs ASR and SLU sequentially and we ensure that the SLU task is conditioned on the ASR task by having CTC self conditioning. This novel joint modeling of ASR and SLU improves SLU performance significantly over just using SLU optimization. We further improve the performance by aligning the acoustic embeddings of this model with the semantically richer BERT model. Our proposed knowledge transfer strategy makes use of a bag-of-entity prediction layer on the aligned embeddings and the output of this is used to condition the RNN-T based SLU decoding. These techniques show significant improvement over several strong baselines and can perform at par with large models like Whisper with significantly fewer parameters.	 | 在本文中，我们提出了一种通过在RNN转录模型（RNN-T）中结合联合自条件CTC自动语音识别（ASR）目标来改进端到端（E2E）语音语言理解（SLU）的方法。我们提出的方法类似于一个E2E可微分级联模型，该模型依次执行ASR和SLU任务，并通过CTC自条件确保SLU任务依赖于ASR任务。这种新颖的ASR和SLU联合建模方法显著提高了SLU性能，超过了仅使用SLU优化的情况。我们进一步通过将该模型的声学嵌入与语义更丰富的BERT模型对齐来提高性能。我们提出的知识迁移策略利用了对齐嵌入上的实体预测层，并将其输出用于条件RNN-T基于的SLU解码。这些技术在几个强大的基线模型上显示出显著的改进，并且可以在参数数量远少于大型模型（如Whisper）的情况下与之媲美。
2501.01934	 | Fusion DeepONet: A Data-Efficient Neural Operator for Geometry-Dependent Hypersonic Flows on Arbitrary Grids	 | Ahmad Peyvan,Varun Kumar	 | 该研究评估了四种先进的神经算子模型（DeepONet、参数条件下的U-Net、Fourier神经算子和MeshGraphNet）在预测椭圆体周围超音速流场方面的性能，并开发了一种名为Fusion DeepONet的新框架，该框架在处理稀缺数据集时表现出色，尤其在不规则网格上优于其他模型，同时具有较高的计算效率。	 | Designing re-entry vehicles requires accurate predictions of hypersonic flow around their geometry. Rapid prediction of such flows can revolutionize vehicle design, particularly for morphing geometries. We evaluate advanced neural operator models such as Deep Operator Networks (DeepONet), parameter-conditioned U-Net, Fourier Neural Operator (FNO), and MeshGraphNet, with the objective of addressing the challenge of learning geometry-dependent hypersonic flow fields with limited data. Specifically, we compare the performance of these models for two grid types: uniform Cartesian and irregular grids. To train these models, we use 36 unique elliptic geometries for generating high-fidelity simulations with a high-order entropy-stable DGSEM solver, emphasizing the challenge of working with a scarce dataset. We evaluate and compare the four operator-based models for their efficacy in predicting hypersonic flow field around the elliptic body. Moreover, we develop a novel framework, called Fusion DeepONet, which leverages neural field concepts and generalizes effectively across varying geometries. Despite the scarcity of training data, Fusion DeepONet achieves performance comparable to parameter-conditioned U-Net on uniform grids while it outperforms MeshGraphNet and vanilla DeepONet on irregular, arbitrary grids. Fusion DeepONet requires significantly fewer trainable parameters as compared to U-Net, MeshGraphNet, and FNO, making it computationally efficient. We also analyze the basis functions of the Fusion DeepONet model using Singular Value Decomposition. This analysis reveals that Fusion DeepONet generalizes effectively to unseen solutions and adapts to varying geometries and grid points, demonstrating its robustness in scenarios with limited training data.	 | 设计再入航天器需要准确预测其几何形状周围超音速流动。快速预测这些流动可以彻底改变航天器的设计，特别是对于可变几何形状。我们评估了先进的神经算子模型，如深度算子网络（DeepONet）、参数条件下的U-Net、Fourier神经算子（FNO）和MeshGraphNet，旨在解决用有限数据学习几何依赖的超音速流场的挑战。具体而言，我们比较了这些模型在两种网格类型上的性能：均匀笛卡尔网格和不规则网格。为了训练这些模型，我们使用36个独特的椭圆几何形状，通过高阶熵稳定DGSEM求解器生成高保真模拟，突显了处理稀缺数据集的挑战。我们评估并比较了这四种基于算子的模型，以评估其在预测椭圆体周围超音速流场方面的有效性。此外，我们开发了一种名为Fusion DeepONet的新框架，该框架利用了神经场的概念，并能有效推广到不同的几何形状。尽管训练数据稀缺，Fusion DeepONet在均匀网格上的性能与参数条件下的U-Net相当，而在不规则的任意网格上则优于MeshGraphNet和原始DeepONet。Fusion DeepONet所需的可训练参数数量显著少于U-Net、MeshGraphNet和FNO，使其计算效率更高。我们还使用奇异值分解分析了Fusion DeepONet模型的基础函数。这一分析表明，Fusion DeepONet能够有效地推广到未见过的解，并适应不同的几何形状和网格点，展示了其在数据稀缺场景中的稳健性。
2501.01930	 | GoBERT: Gene Ontology Graph Informed BERT for Universal Gene Function Prediction	 | Yuwei Miao,Yuzhi Guo,Hehuan Ma,Jingquan Yan,Feng Jiang,Rui Liao,Junzhou Huang	 | 该研究提出了一种名为GoBERT的新方法，通过探索Gene Ontology图和结合预训练的BERT模型来解析基因功能之间的潜在关系，从而预测基因和基因产物的新功能，这种方法能够有效发现潜在的功能关系并预测新功能。	 | Exploring the functions of genes and gene products is crucial to a wide range of fields, including medical research, evolutionary biology, and environmental science. However, discovering new functions largely relies on expensive and exhaustive wet lab experiments. Existing methods of automatic function annotation or prediction mainly focus on protein function prediction with sequence, 3D-structures or protein family information. In this study, we propose to tackle the gene function prediction problem by exploring Gene Ontology graph and annotation with BERT (GoBERT) to decipher the underlying relationships among gene functions. Our proposed novel function prediction task utilizes existing functions as inputs and generalizes the function prediction to gene and gene products. Specifically, two pre-train tasks are designed to jointly train GoBERT to capture both explicit and implicit relations of functions. Neighborhood prediction is a self-supervised multi-label classification task that captures the explicit function relations. Specified masking and recovering task helps GoBERT in finding implicit patterns among functions. The pre-trained GoBERT possess the ability to predict novel functions for various gene and gene products based on known functional annotations. Extensive experiments, biological case studies, and ablation studies are conducted to demonstrate the superiority of our proposed GoBERT.	 | 探索基因和基因产物的功能对于医学研究、进化生物学和环境科学等多个领域至关重要。然而，发现新的功能很大程度上依赖于昂贵且耗时的湿实验。现有的自动功能注释或预测方法主要集中在基于序列、三维结构或蛋白质家族信息的蛋白质功能预测。在本研究中，我们提出了一种新的方法，通过探索Gene Ontology图和注释结合BERT（GoBERT）来解析基因功能之间的潜在关系。我们提出的新型功能预测任务将现有功能作为输入，并将功能预测推广到基因和基因产物。具体而言，设计了两个预训练任务，以共同训练GoBERT，捕捉功能的显性和隐性关系。邻域预测是一项自我监督的多标签分类任务，用于捕捉显性功能关系。特定的掩蔽和恢复任务帮助GoBERT在功能之间找到隐含的模式。预训练的GoBERT能够基于已知的功能注释预测多种基因和基因产物的新功能。进行了广泛的实验、生物案例研究和消融研究，以证明我们提出的GoBERT的优势。
2501.01915	 | Social Processes: Probabilistic Meta-learning for Adaptive Multiparty Interaction Forecasting	 | Augustinas Jučas,Chirag Raman	 | 本文提出了一种基于元学习的方法，通过社会过程（SP）模型在未见过的团队中进行多模态的人类行为预测，强调考虑每个团队的独特动力学特性以填补团队互动预测研究的空白。该模型能够根据团队内成员的先前行为预测未来行为，并通过现实合成数据集展示了其在输出和潜在空间方面的泛化能力。	 | Adaptively forecasting human behavior in social settings is an important step toward achieving Artificial General Intelligence. Most existing research in social forecasting has focused either on unfocused interactions, such as pedestrian trajectory prediction, or on monadic and dyadic behavior forecasting. In contrast, social psychology emphasizes the importance of group interactions for understanding complex social dynamics. This creates a gap that we address in this paper: forecasting social interactions at the group (conversation) level. Additionally, it is important for a forecasting model to be able to adapt to groups unseen at train time, as even the same individual behaves differently across different groups. This highlights the need for a forecasting model to explicitly account for each group's unique dynamics. To achieve this, we adopt a meta-learning approach to human behavior forecasting, treating every group as a separate meta-learning task. As a result, our method conditions its predictions on the specific behaviors within the group, leading to generalization to unseen groups. Specifically, we introduce Social Process (SP) models, which predict a distribution over future multimodal cues jointly for all group members based on their preceding low-level multimodal cues, while incorporating other past sequences of the same group's interactions. In this work we also analyze the generalization capabilities of SP models in both their outputs and latent spaces through the use of realistic synthetic datasets.	 | 在社会环境中自适应地预测人类行为是实现通用人工智能的重要步骤。目前大多数社会预测研究主要集中在无目标的互动上，如行人轨迹预测，或者关注单人和双人行为预测。相比之下，社会心理学强调团队互动在理解复杂社会动态中的重要性。这在一定程度上造成了一个缺口，而本文则致力于解决这一问题：在团队（对话）层面进行社会互动预测。此外，在训练时未见过的团队中，预测模型也需要具有适应能力，因为即使是同一个个体，在不同团队中的表现也有所不同。这强调了预测模型需要明确考虑每个团队的独特动力学特性的重要性。为了实现这一目标，我们采用了一种元学习方法来进行人类行为预测，将每个团队视为一个单独的元学习任务。因此，我们的方法根据团队内特定的行为条件其预测，从而实现对未见过的团队的泛化。具体来说，我们引入了社会过程（Social Process，SP）模型，该模型根据每个成员先前的低级多模态线索，联合预测未来多模态线索的分布，同时整合同一团队其他过往的互动序列。在本文中，我们还通过使用现实的合成数据集，分析了SP模型在输出和潜在空间方面的泛化能力。
2501.01905	 | Alleviating Overfitting in Transformation-Interaction-Rational Symbolic Regression with Multi-Objective Optimization	 | Fabricio Olivetti de Franca	 | 转换-交互-理性是一种用于符号回归的表示法，它将搜索空间限制为特定的函数形式，以偏向于更简单的表达式，这种方法在多种基准测试中表现优于其前身，并且引入多目标优化后在某些情况下能提升整体性能，特别是在处理小数据集时。	 | The Transformation-Interaction-Rational is a representation for symbolic regression that limits the search space of functions to the ratio of two nonlinear functions each one defined as the linear regression of transformed variables. This representation has the main objective to bias the search towards simpler expressions while keeping the approximation power of standard approaches.   The performance of using Genetic Programming with this representation was substantially better than with its predecessor (Interaction-Transformation) and ranked close to the state-of-the-art on a contemporary Symbolic Regression benchmark. On a closer look at these results, we observed that the performance could be further improved with an additional selective pressure for smaller expressions when the dataset contains just a few data points. The introduction of a penalization term applied to the fitness measure improved the results on these smaller datasets. One problem with this approach is that it introduces two additional hyperparameters: i) a criteria to when the penalization should be activated and, ii) the amount of penalization to the fitness function.   In this paper, we extend Transformation-Interaction-Rational to support multi-objective optimization, specifically the NSGA-II algorithm, and apply that to the same benchmark. A detailed analysis of the results show that the use of multi-objective optimization benefits the overall performance on a subset of the benchmarks while keeping the results similar to the single-objective approach on the remainder of the datasets. Specifically to the small datasets, we observe a small (and statistically insignificant) improvement of the results suggesting that further strategies must be explored.	 | 转换-交互-理性是一种用于符号回归的表示法，它将函数搜索空间限制为两个非线性函数的比率，每个非线性函数都由转换变量的线性回归定义。这种表示的主要目标是在保持标准方法的逼近能力的同时，偏向于搜索更简单的表达式。使用遗传编程的这种方法性能显著优于其前身（交互-转换），并且在当代的符号回归基准测试中接近最先进的水平。更仔细地分析这些结果，我们发现当数据集包含的数据点较少时，通过引入对更简短表达式的选择压力，可以进一步提高性能。将惩罚项应用于适应度度量可以改善小数据集上的结果。然而，这种做法的一个问题是引入了两个额外的超参数：i) 惩罚项何时激活的标准；ii) 对适应度函数的惩罚程度。  在这篇论文中，我们扩展了转换-交互-理性表示法以支持多目标优化，特别是NSGA-II算法，并将其应用于同一基准测试。详细的分析结果表明，使用多目标优化在某些基准测试上整体性能有所提升，而在其余数据集上保持结果与单目标方法相似。特别是对于小数据集，我们观察到结果有轻微（统计上不显著）的改进，这表明还需要进一步探索其他策略。
2501.01889	 | Exploring Equality: An Investigation into Custom Loss Functions for Fairness Definitions	 | Gordon Lee,Simeon Sayer	 | 本文构建了基于Gupta等人提出的Group Accuracy Parity（GAP）框架的神经网络，并应用于COMPAS系统，通过组合分析程序展示了GAP框架在公平性和准确性的平衡上优于传统方法，但仍需法律和制度变革以克服外部偏见和透明度不足的问题。	 | This paper explores the complex tradeoffs between various fairness metrics such as equalized odds, disparate impact, and equal opportunity and predictive accuracy within COMPAS by building neural networks trained with custom loss functions optimized to specific fairness criteria. This paper creates the first fairness-driven implementation of the novel Group Accuracy Parity (GAP) framework, as theoretically proposed by Gupta et al. (2024), and applies it to COMPAS. To operationalize and accurately compare the fairness of COMPAS models optimized to differing fairness ideals, this paper develops and proposes a combinatory analytical procedure that incorporates Pareto front and multivariate analysis, leveraging data visualizations such as violin graphs. This paper concludes that GAP achieves an enhanced equilibrium between fairness and accuracy compared to COMPAS's current nationwide implementation and alternative implementations of COMPAS optimized to more traditional fairness definitions. While this paper's algorithmic improvements of COMPAS significantly augment its fairness, external biases undermine the fairness of its implementation. Practices such as predictive policing and issues such as the lack of transparency regarding COMPAS's internal workings have contributed to the algorithm's historical injustice. In conjunction with developments regarding COMPAS's predictive methodology, legal and institutional changes must happen for COMPAS's just deployment.	 | 本文探讨了诸如平等机会、不同影响和平等机会与预测准确度之间的复杂权衡关系，通过构建使用针对特定公平标准优化的自定义损失函数训练的神经网络来研究Fairness（公平性）与Accuracy（准确度）之间的权衡。本文首次实现了由Gupta等人（2024）理论提出的创新性Group Accuracy Parity（GAP）框架，并将其应用于COMPAS系统。为了实现和准确比较旨在不同公平理想优化的COMPAS模型的公平性，本文开发并提出了一个结合Pareto前沿和多元分析的组合分析程序，利用小提琴图等数据可视化工具。本文得出结论，与当前全国范围内的COMPAS实施以及更多传统公平定义优化的COMPAS替代实现相比，GAP框架实现了公平性和准确性的更好平衡。尽管本文对COMPAS算法的改进显著增强了其公平性，但外部偏见仍然削弱了其实施的公平性。诸如预测警务等实践以及COMPAS内部运作缺乏透明度的问题，都导致了算法的历史性不公正。结合COMPAS预测方法的发展，法律和制度变革对于实现COMPAS的公正部署是必需的。
2501.01874	 | DFF: Decision-Focused Fine-tuning for Smarter Predict-then-Optimize with Limited Data	 | Jiaqi Yang,Enming Liang,Zicheng Su,Zhichao Zou,Peng Zhen,Jiecheng Guo,Wanjing Ma,Kun An	 | 决策导向微调（DFF）是一种通过嵌入一个新的偏差修正模块到预测-优化（PO）流水线中来解决决策导向学习（DFL）挑战的方法，它在保持模型预测准确性的同时，提升决策性能并限制预测偏差。在多种合成和真实世界数据集上的评估表明，DFF能够在有限数据情况下有效减少预测变化，提高决策性能。	 | Decision-focused learning (DFL) offers an end-to-end approach to the predict-then-optimize (PO) framework by training predictive models directly on decision loss (DL), enhancing decision-making performance within PO contexts. However, the implementation of DFL poses distinct challenges. Primarily, DL can result in deviation from the physical significance of the predictions under limited data. Additionally, some predictive models are non-differentiable or black-box, which cannot be adjusted using gradient-based methods. To tackle the above challenges, we propose a novel framework, Decision-Focused Fine-tuning (DFF), which embeds the DFL module into the PO pipeline via a novel bias correction module. DFF is formulated as a constrained optimization problem that maintains the proximity of the DL-enhanced model to the original predictive model within a defined trust region. We theoretically prove that DFF strictly confines prediction bias within a predetermined upper bound, even with limited datasets, thereby substantially reducing prediction shifts caused by DL under limited data. Furthermore, the bias correction module can be integrated into diverse predictive models, enhancing adaptability to a broad range of PO tasks. Extensive evaluations on synthetic and real-world datasets, including network flow, portfolio optimization, and resource allocation problems with different predictive models, demonstrate that DFF not only improves decision performance but also adheres to fine-tuning constraints, showcasing robust adaptability across various scenarios.	 | 决策导向学习（DFL）提供了一种端到端的方法来优化预测-优化（PO）框架，通过直接在决策损失（DL）上训练预测模型，从而在PO上下文中提升决策性能。然而，DFL的实施也带来了独特的挑战。首先，DL可能导致在数据有限的情况下预测结果偏离物理意义。其次，一些预测模型是非可微的或黑盒模型，不能使用基于梯度的方法进行调整。为了解决上述挑战，我们提出了一种新的框架——决策导向微调（DFF），该框架通过一个新颖的偏差修正模块将DFL模块嵌入到PO流水线中。DFF被形式化为一个受限优化问题，该问题在定义的可信区域中保持DL增强模型与原始预测模型的接近性。我们从理论上证明，DFF严格地将预测偏差限制在预设的上限内，即使在数据有限的情况下也能显著减少由DL引起的预测变化。此外，偏差修正模块可以集成到各种不同的预测模型中，增强其对广泛PO任务的适应性。在合成数据集和真实世界数据集上的广泛评估，包括网络流、投资组合优化和资源分配问题，表明DFF不仅可以提高决策性能，还能遵循微调约束，展示出在各种场景下的稳健适应性。
2501.01850	 | LCFed: An Efficient Clustered Federated Learning Framework for Heterogeneous Data	 | Yuxin Zhang,Haoyu Chen,Zheng Lin,Zhe Chen,Jin Zhao	 | LCFed是一种高效的集群联邦学习框架，通过模型分割和不同的聚合策略整合全球知识，实现最优的训练性能，并通过低秩模型定制计算高效的模型相似性度量方法，实现实时的集群更新。	 | Clustered federated learning (CFL) addresses the performance challenges posed by data heterogeneity in federated learning (FL) by organizing edge devices with similar data distributions into clusters, enabling collaborative model training tailored to each group. However, existing CFL approaches strictly limit knowledge sharing to within clusters, lacking the integration of global knowledge with intra-cluster training, which leads to suboptimal performance. Moreover, traditional clustering methods incur significant computational overhead, especially as the number of edge devices increases. In this paper, we propose LCFed, an efficient CFL framework to combat these challenges. By leveraging model partitioning and adopting distinct aggregation strategies for each sub-model, LCFed effectively incorporates global knowledge into intra-cluster co-training, achieving optimal training performance. Additionally, LCFed customizes a computationally efficient model similarity measurement method based on low-rank models, enabling real-time cluster updates with minimal computational overhead. Extensive experiments show that LCFed outperforms state-of-the-art benchmarks in both test accuracy and clustering computational efficiency.	 | 以下是该摘要的中文翻译：  集群联邦学习（Clustered Federated Learning, CFL）通过将具有相似数据分布的边缘设备组织成集群，来解决联邦学习（Federated Learning, FL）中由数据异质性带来的性能挑战，从而实现针对每个集群的定制化模型训练。然而，现有CFL方法严格限制了跨集群的知识共享，忽略了全球知识与集群内部训练的整合，导致性能不佳。此外，传统的聚类方法在计算开销上消耗巨大，尤其是在边缘设备数量增加时更为明显。在本文中，我们提出了一种高效的CFL框架LCFed，以应对这些挑战。通过利用模型分割，并为每个子模型采用不同的聚合策略，LCFed有效地将全球知识整合到集群内部协同训练中，实现了最优的训练性能。此外，LCFed基于低秩模型定制了一种计算高效的模型相似性度量方法，使得可以实现实时的集群更新，同时具有极小的计算开销。广泛的实验表明，LCFed在测试准确率和聚类计算效率方面均优于现有的基准方法。
2501.01844	 | Learning from Ambiguous Data with Hard Labels	 | Zeke Xie,Zheng He,Nan Lu,Lichen Bai,Bao Li,Shuo Yang,Mingming Sun,Ping Li	 | 本文提出了一种名为量化标签学习（QLL）的新框架，旨在通过处理模糊数据和量化标签来提高模型的泛化性能，实验结果表明该方法能够显著优于基线方法。	 | Real-world data often contains intrinsic ambiguity that the common single-hard-label annotation paradigm ignores. Standard training using ambiguous data with these hard labels may produce overly confident models and thus leading to poor generalization. In this paper, we propose a novel framework called Quantized Label Learning (QLL) to alleviate this issue. First, we formulate QLL as learning from (very) ambiguous data with hard labels: ideally, each ambiguous instance should be associated with a ground-truth soft-label distribution describing its corresponding probabilistic weight in each class, however, this is usually not accessible; in practice, we can only observe a quantized label, i.e., a hard label sampled (quantized) from the corresponding ground-truth soft-label distribution, of each instance, which can be seen as a biased approximation of the ground-truth soft-label. Second, we propose a Class-wise Positive-Unlabeled (CPU) risk estimator that allows us to train accurate classifiers from only ambiguous data with quantized labels. Third, to simulate ambiguous datasets with quantized labels in the real world, we design a mixing-based ambiguous data generation procedure for empirical evaluation. Experiments demonstrate that our CPU method can significantly improve model generalization performance and outperform the baselines.	 | 现实世界的数据经常包含内在的模糊性，而这种模糊性通常被标准的单一硬标签注解范式所忽略。使用这些硬标签进行标准训练可能会生成过于自信的模型，从而导致泛化性能较差。在本文中，我们提出了一种名为量化标签学习（QLL）的新框架，以解决这一问题。首先，我们将QLL形式化为从非常模糊的数据中学习带有硬标签的数据：理想情况下，每个模糊实例应该与一个描述其在每个类中概率权重的地面真实软标签分布相关联，然而这通常是不可获取的；在实践中，我们只能观测到每个实例的量化标签，即从其对应的地面真实软标签分布中采样的（量化）硬标签，可以认为这是一种对地面真实软标签的有偏近似。其次，我们提出了一个类别内正样本-未标注样本（CPU）风险估计器，使我们能够仅通过模糊数据和量化标签进行准确分类器的训练。第三，为了在实际世界中模拟带有量化标签的模糊数据集，我们设计了一种基于混合的模糊数据生成程序，用于实证评估。实验结果表明，我们的CPU方法可以显著提高模型的泛化性能，并优于基线方法。
2501.01836	 | Practical machine learning is learning on small samples	 | Marina Sapir	 | 该研究提出了一种实用学习范式，认为实际的机器学习依赖于“平滑”假设，即基础依赖关系在接近数据点时不会发生突然变化，从而在有限的数据集上找到有效的模型。通过这一范式，多种流行的机器学习方法如局部平滑、k-NN、决策树等都可以被解释为实现这一假设的不同策略。	 | Based on limited observations, machine learning discerns a dependence which is expected to hold in the future. What makes it possible? Statistical learning theory imagines indefinitely increasing training sample to justify its approach. In reality, there is no infinite time or even infinite general population for learning. Here I argue that practical machine learning is based on an implicit assumption that underlying dependence is relatively ``smooth" : likely, there are no abrupt differences in feedback between cases with close data points. From this point of view learning shall involve selection of the hypothesis ``smoothly" approximating the training set. I formalize this as Practical learning paradigm. The paradigm includes terminology and rules for description of learners. Popular learners (local smoothing, k-NN, decision trees, Naive Bayes, SVM for classification and for regression) are shown here to be implementations of this paradigm.	 | 基于有限的观察，机器学习能够识别出一种预期在未来仍然有效的依赖关系。这是如何实现的呢？统计学习理论设想无限增加训练样本量来证明其方法的有效性。然而，在现实中，并没有无限的时间，甚至也没有无限大的总体可供学习。在此，我提出观点认为，实际的机器学习实际上是基于一个隐含的假设：即基础依赖关系相对“平滑”：很可能，在具有接近数据点的情况下，反馈之间不存在突然的差异。从这个角度来看，学习应当涉及选择一个“平滑”地逼近训练集的假设。我把这个表述为实用学习范式。该范式包括对学习者的术语和规则描述。这里展示了流行的几种学习者（局部平滑、k-NN、决策树、朴素贝叶斯、SVM，适用于分类和回归）都是这一范式的实现。
2501.01793	 | Creating Artificial Students that Never Existed: Leveraging Large Language Models and CTGANs for Synthetic Data Generation	 | Mohammad Khalil,Farhad Vadiee,Ronas Shakya,Qinyi Liu	 | 本研究利用生成对抗网络（GANs）和大型语言模型（LLMs）生成合成学生数据，以克服真实数据获取和使用中的隐私限制，并验证这些方法在生成高质量、统计上与真实数据相似的数据集方面的潜力。	 | In this study, we explore the growing potential of AI and deep learning technologies, particularly Generative Adversarial Networks (GANs) and Large Language Models (LLMs), for generating synthetic tabular data. Access to quality students data is critical for advancing learning analytics, but privacy concerns and stricter data protection regulations worldwide limit their availability and usage. Synthetic data offers a promising alternative. We investigate whether synthetic data can be leveraged to create artificial students for serving learning analytics models. Using the popular GAN model CTGAN and three LLMs- GPT2, DistilGPT2, and DialoGPT, we generate synthetic tabular student data. Our results demonstrate the strong potential of these methods to produce high-quality synthetic datasets that resemble real students data. To validate our findings, we apply a comprehensive set of utility evaluation metrics to assess the statistical and predictive performance of the synthetic data and compare the different generator models used, specially the performance of LLMs. Our study aims to provide the learning analytics community with valuable insights into the use of synthetic data, laying the groundwork for expanding the field methodological toolbox with new innovative approaches for learning analytics data generation.	 | 在本研究中，我们探讨了人工智能和深度学习技术，特别是生成对抗网络（GANs）和大型语言模型（LLMs），在生成合成表格数据方面的潜力。高质量的学生数据对于推进学习分析至关重要，但隐私担忧和全球范围内的更严格数据保护法规限制了这些数据的获取和使用。合成数据提供了有前途的替代方案。我们研究了合成数据是否可以用来创建虚拟学生，以服务于学习分析模型。利用流行的CTGAN GAN模型和三种LLM模型（GPT2、DistilGPT2 和 DialoGPT），我们生成了合成的学生数据表格。研究结果表明，这些方法具有很强的潜力，能够生成高质量的合成数据集，这些数据集类似于真实的学生数据。为了验证我们的发现，我们应用了一整套实用性评估指标来评估合成数据的统计和预测性能，并比较了使用不同生成器模型的结果，特别是LLM的性能。本研究旨在为学习分析社区提供有关合成数据使用的宝贵见解，为扩展学习分析数据生成的方法工具箱奠定基础，引入新的创新方法。
2501.01785	 | Can Synthetic Data be Fair and Private? A Comparative Study of Synthetic Data Generation and Fairness Algorithms	 | Qinyi Liu,Oscar Deho,Farhad Vadiee,Mohammad Khalil,Srecko Joksimovic,George Siemens	 | 本研究探讨了合成数据生成器在提高学习分析模型的隐私性和公平性方面的效果，并发现DEbiasing Causal Fairness (DECAF) 算法能够在两者之间取得最佳平衡，尽管其在实用性上略逊一筹；此外，研究还表明将预处理公平性算法应用于合成数据可以进一步提升公平性。	 | The increasing use of machine learning in learning analytics (LA) has raised significant concerns around algorithmic fairness and privacy. Synthetic data has emerged as a dual-purpose tool, enhancing privacy and improving fairness in LA models. However, prior research suggests an inverse relationship between fairness and privacy, making it challenging to optimize both. This study investigates which synthetic data generators can best balance privacy and fairness, and whether pre-processing fairness algorithms, typically applied to real datasets, are effective on synthetic data. Our results highlight that the DEbiasing CAusal Fairness (DECAF) algorithm achieves the best balance between privacy and fairness. However, DECAF suffers in utility, as reflected in its predictive accuracy. Notably, we found that applying pre-processing fairness algorithms to synthetic data improves fairness even more than when applied to real data. These findings suggest that combining synthetic data generation with fairness pre-processing offers a promising approach to creating fairer LA models.	 | 随着机器学习在学习分析（LA）中的应用日益增加，算法公平性和隐私问题引起了广泛关注。合成数据作为一种双重用途工具，能够提高LA模型的隐私性并改善其公平性。然而，先前的研究表明公平性和隐私性之间存在反向关系，这使得同时优化两者变得颇具挑战性。本研究旨在探究哪些合成数据生成器能够在隐私和公平性之间取得最佳平衡，并评估预处理公平性算法在合成数据上的效果，这些算法通常应用于真实数据集。研究结果表明，DEbiasing Causal Fairness (DECAF) 算法在隐私和公平性之间达到了最佳平衡。然而，DECAF 在实用性方面表现不佳，体现在预测准确性上。值得注意的是，我们发现将预处理公平性算法应用于合成数据可以进一步提高公平性，甚至超过其在真实数据上的效果。这些发现表明，结合合成数据生成和公平性预处理，可以为创建更公平的LA模型提供一种有前景的方法。
2501.01774	 | A Unifying View of Linear Function Approximation in Off-Policy RL Through Matrix Splitting and Preconditioning	 | Zechen Wu,Amy Greenwald,Ronald Parr	 | 本文通过将TD、FQI和PFQI统一为解决LSTD系统的迭代方法，展示了增加更新次数实际上是从恒定预条件器向数据-特征自适应预条件器的过渡，从而简化了这些算法的收敛性分析，并揭示了它们之间的收敛关系。	 | Traditionally, TD and FQI are viewed as differing in the number of updates toward the target value function: TD makes one update, FQI makes an infinite number, and Partial Fitted Q-Iteration (PFQI) performs a finite number, such as the use of a target network in Deep Q-Networks (DQN) in the OPE setting. This perspective, however, fails to capture the convergence connections between these algorithms and may lead to incorrect conclusions, for example, that the convergence of TD implies the convergence of FQI. In this paper, we focus on linear value function approximation and offer a new perspective, unifying TD, FQI, and PFQI as the same iterative method for solving the Least Squares Temporal Difference (LSTD) system, but using different preconditioners and matrix splitting schemes. TD uses a constant preconditioner, FQI employs a data-feature adaptive preconditioner, and PFQI transitions between the two. Then, we reveal that in the context of linear function approximation, increasing the number of updates under the same target value function essentially represents a transition from using a constant preconditioner to data-feature adaptive preconditioner. This unifying perspective also simplifies the analyses of the convergence conditions for these algorithms and clarifies many issues. Consequently, we fully characterize the convergence of each algorithm without assuming specific properties of the chosen features (e.g., linear independence). We also examine how common assumptions about feature representations affect convergence, and discover new conditions on features that are important for convergence. These convergence conditions allow us to establish the convergence connections between these algorithms and to address important questions.	 | 传统上，TD（Temporal Difference）和FQI（Fitted Q-Iteration）被认为在目标价值函数的更新次数上有所不同：TD进行一次更新，FQI进行无限次更新，而半适应性Fitted Q-迭代（Partial Fitted Q-Iteration, PFQI）则进行有限次更新，例如在强化学习的外部策略评估（Off-Policy Evaluation, OPE）设置中的深度Q网络（Deep Q-Networks, DQN）中使用目标网络。然而，这一观点未能捕捉到这些算法之间的收敛关系，可能导致错误的结论，例如认为TD的收敛性意味着FQI的收敛性。本文重点关注线性价值函数逼近，并提供了一个新的视角，将TD、FQI和PFQI统一为解决最小二乘时序差分（Least Squares Temporal Difference, LSTD）系统的同一迭代方法，但使用不同的预条件器和矩阵分裂方案。TD使用恒定预条件器，FQI使用数据-特征自适应预条件器，而PFQI则在这两者之间转换。然后，我们揭示，在线性函数逼近的背景下，相同目标价值函数下增加更新次数实际上代表了从使用恒定预条件器到使用数据-特征自适应预条件器的一种过渡。这种统一视角简化了这些算法收敛条件的分析，并澄清了许多问题。因此，我们完全表征了每个算法的收敛性，而不假设所选特征的特定属性（例如线性无关性）。我们还研究了关于特征表示的常见假设如何影响收敛性，并发现了对于收敛性很重要的新特征条件。这些收敛条件使我们能够建立这些算法之间的收敛关系，并回答了许多重要问题。
2501.01765	 | SaLoRA: Safety-Alignment Preserved Low-Rank Adaptation	 | Mingjie Li,Wai Man Si,Michael Backes,Yang Zhang,Yisen Wang	 | 随着参数高效微调方法如LoRA的广泛应用，其对大型语言模型安全对齐的潜在威胁引发了关注，为此本文提出了一种新的方法SaLoRA，它能够在不对模型原始对齐造成破坏的情况下进行针对性修改，从而保持模型的安全对齐。	 | As advancements in large language models (LLMs) continue and the demand for personalized models increases, parameter-efficient fine-tuning (PEFT) methods (e.g., LoRA) will become essential due to their efficiency in reducing computation costs. However, recent studies have raised alarming concerns that LoRA fine-tuning could potentially compromise the safety alignment in LLMs, posing significant risks for the model owner. In this paper, we first investigate the underlying mechanism by analyzing the changes in safety alignment related features before and after fine-tuning. Then, we propose a fixed safety module calculated by safety data and a task-specific initialization for trainable parameters in low-rank adaptations, termed Safety-alignment preserved Low-Rank Adaptation (SaLoRA). Unlike previous LoRA methods and their variants, SaLoRA enables targeted modifications to LLMs without disrupting their original alignments. Our experiments show that SaLoRA outperforms various adapters-based approaches across various evaluation metrics in different fine-tuning tasks.	 | 随着大型语言模型（LLMs）的不断进步和对个性化模型需求的增加，参数高效微调（PEFT）方法（如LoRA）由于其在降低计算成本方面的高效性而变得尤为重要。然而，近期的研究表明，LoRA微调可能会对LLMs的安全对齐造成潜在威胁，这给模型的所有者带来了重大风险。在本文中，我们首先通过分析微调前后与安全对齐相关的特征变化来探讨其背后的机制。然后，我们提出了一种由安全数据计算的安全模块和针对低秩适应的可训练参数的任务特定初始化，称之为保持安全对齐的低秩适应（SaLoRA）。与之前的LoRA方法及其变体不同，SaLoRA能够在不破坏其原始对齐的情况下对LLMs进行针对性的修改。我们的实验表明，SaLoRA在不同微调任务的各种评估指标上均优于基于适配器的方法。
2501.01707	 | Catch Causal Signals from Edges for Label Imbalance in Graph Classification	 | Fengrui Zhang,Yujia Yin,Hongzong Li,Yifan Chen,Tianyi Qu	 | 本文通过利用边特征来分离和重塑图表示，提高了图因果检测的性能，特别是在处理标签不平衡问题的图分类任务中表现出改进；实验结果在实际数据集上验证了方法的有效性，并强调了边特征在图因果检测中的重要性。	 | Despite significant advancements in causal research on graphs and its application to cracking label imbalance, the role of edge features in detecting the causal effects within graphs has been largely overlooked, leaving existing methods with untapped potential for further performance gains. In this paper, we enhance the causal attention mechanism through effectively leveraging edge information to disentangle the causal subgraph from the original graph, as well as further utilizing edge features to reshape graph representations. Capturing more comprehensive causal signals, our design leads to improved performance on graph classification tasks with label imbalance issues. We evaluate our approach on real-word datasets PTC, Tox21, and ogbg-molhiv, observing improvements over baselines. Overall, we highlight the importance of edge features in graph causal detection and provide a promising direction for addressing label imbalance challenges in graph-level tasks. The model implementation details and the codes are available on https://github.com/fengrui-z/ECAL	 | 尽管在图上的因果研究以及解决标签不平衡问题的应用方面取得了显著进展，但在检测图中因果效应时边特征的作用却一直被忽视，这使得现有方法尚未充分利用其潜在性能提升空间。在本文中，我们通过有效利用边信息来分离因果子图，并进一步利用边特征重塑图表示，从而捕捉到更全面的因果信号。我们的设计在处理标签不平衡问题的图分类任务中表现出改进的性能。我们在实际数据集PTC、Tox21和ogbg-molhiv上评估了我们的方法，发现比基准模型有所改进。总体而言，我们强调了边特征在图因果检测中的重要性，并为解决图级任务中的标签不平衡问题提供了有前景的方向。模型的实现细节和代码可在https://github.com/fengrui-z/ECAL 上获得。
2501.01694	 | Comparative Study of Deep Learning Architectures for Textual Damage Level Classification	 | Aziida Nanyonga,Hassan Wasswa,Graham Wild	 | 研究利用自然语言处理和深度学习模型分析航空事件叙述，以分类航空器损伤程度，结果显示LSTM、BLSTM、GRU和sRNN等模型均表现出色，尤其是sRNN模型在召回率和准确率方面表现最佳，达到了89%。	 | Given the paramount importance of safety in the aviation industry, even minor operational anomalies can have significant consequences. Comprehensive documentation of incidents and accidents serves to identify root causes and propose safety measures. However, the unstructured nature of incident event narratives poses a challenge for computer systems to interpret. Our study aimed to leverage Natural Language Processing (NLP) and deep learning models to analyze these narratives and classify the aircraft damage level incurred during safety occurrences. Through the implementation of LSTM, BLSTM, GRU, and sRNN deep learning models, our research yielded promising results, with all models showcasing competitive performance, achieving an accuracy of over 88% significantly surpassing the 25% random guess threshold for a four-class classification problem. Notably, the sRNN model emerged as the top performer in terms of recall and accuracy, boasting a remarkable 89%. These findings underscore the potential of NLP and deep learning models in extracting actionable insights from unstructured text narratives, particularly in evaluating the extent of aircraft damage within the realm of aviation safety occurrences.	 | 鉴于航空业安全至关重要的地位，即使是轻微的操作异常也可能产生重大后果。全面记录事件和事故有助于识别根本原因并提出安全措施。然而，事件叙述的无结构性质使计算机系统难以解释。我们的研究旨在利用自然语言处理（NLP）和深度学习模型来分析这些叙述，并对安全事件中航空器的损伤程度进行分类。通过实施LSTM、BLSTM、GRU和sRNN等深度学习模型，我们的研究取得了令人鼓舞的结果，所有模型均表现出竞争性的性能，准确率超过88%，远超四类分类问题中随机猜测的25%阈值。值得注意的是，sRNN模型在召回率和准确率方面表现最佳，达到惊人的89%。这些发现表明，NLP和深度学习模型有可能从非结构化文本叙述中提取出具有行动价值的见解，特别是在评估航空安全事件中航空器损伤的程度方面。
2501.01693	 | Denoising and Adaptive Online Vertical Federated Learning for Sequential Multi-Sensor Data in Industrial Internet of Things	 | Heqiang Wang,Xiaoxiong Zhong,Kang Liu,Fangming Liu,Weizhe Zhang	 | 本文提出了一种去噪和自适应在线垂直联邦学习（DAO-VFL）算法，用于解决工业装配线场景中的分布式学习问题，有效管理数据流并解决通信开销和隐私问题；实验证明该算法在两个真实数据集上的性能优于基准算法。	 | With the continuous improvement in the computational capabilities of edge devices such as intelligent sensors in the Industrial Internet of Things, these sensors are no longer limited to mere data collection but are increasingly capable of performing complex computational tasks. This advancement provides both the motivation and the foundation for adopting distributed learning approaches. This study focuses on an industrial assembly line scenario where multiple sensors, distributed across various locations, sequentially collect real-time data characterized by distinct feature spaces. To leverage the computational potential of these sensors while addressing the challenges of communication overhead and privacy concerns inherent in centralized learning, we propose the Denoising and Adaptive Online Vertical Federated Learning (DAO-VFL) algorithm. Tailored to the industrial assembly line scenario, DAO-VFL effectively manages continuous data streams and adapts to shifting learning objectives. Furthermore, it can address critical challenges prevalent in industrial environment, such as communication noise and heterogeneity of sensor capabilities. To support the proposed algorithm, we provide a comprehensive theoretical analysis, highlighting the effects of noise reduction and adaptive local iteration decisions on the regret bound. Experimental results on two real-world datasets further demonstrate the superior performance of DAO-VFL compared to benchmarks algorithms.	 | 随着工业物联网中的智能传感器等边缘设备计算能力的持续提升，这些传感器不再仅限于简单的数据收集，而是逐渐能够执行复杂的计算任务。这一进步为采用分布式学习方法提供了动机和基础。本文聚焦于工业装配线场景，其中多个传感器分布在不同位置，按顺序收集具有不同特征空间的实时数据。为了利用这些传感器的计算潜力，同时解决集中式学习中固有的通信开销和隐私问题，我们提出了一种去噪和自适应在线垂直联邦学习（DAO-VFL）算法。该算法针对工业装配线场景，有效管理连续数据流并适应变化的学习目标。此外，DAO-VFL还可以解决工业环境中普遍存在的通信噪声和传感器能力异质性等关键挑战。为了支持所提出的算法，我们提供了全面的理论分析，阐述了噪声减少和自适应局部迭代决策对遗憾边界的影响。实验结果在两个真实数据集上的表现进一步证明了DAO-VFL相较于基准算法的优越性能。
2501.01690	 | Analyzing Aviation Safety Narratives with LDA, NMF and PLSA: A Case Study Using Socrata Datasets	 | Aziida Nanyonga,Graham Wild	 | 本研究应用了LDA、NMF和PLSA三种主题建模技术分析了1908年至2009年的Socrata数据集，发现了包括飞行员错误、机械故障、天气条件和训练不足在内的关键主题，并探讨了每种方法的优势和统计性能，为航空安全管理提供了有价值的见解和未来研究方向。	 | This study explores the application of topic modelling techniques Latent Dirichlet Allocation (LDA), Nonnegative Matrix Factorization (NMF), and Probabilistic Latent Semantic Analysis (PLSA) on the Socrata dataset spanning from 1908 to 2009. Categorized by operator type (military, commercial, and private), the analysis identified key themes such as pilot error, mechanical failure, weather conditions, and training deficiencies. The study highlights the unique strengths of each method: LDA ability to uncover overlapping themes, NMF production of distinct and interpretable topics, and PLSA nuanced probabilistic insights despite interpretative complexity. Statistical analysis revealed that PLSA achieved a coherence score of 0.32 and a perplexity value of -4.6, NMF scored 0.34 and 37.1, while LDA achieved the highest coherence of 0.36 but recorded the highest perplexity at 38.2. These findings demonstrate the value of topic modelling in extracting actionable insights from unstructured aviation safety narratives, aiding in the identification of risk factors and areas for improvement across sectors. Future directions include integrating additional contextual variables, leveraging neural topic models, and enhancing aviation safety protocols. This research provides a foundation for advanced text-mining applications in aviation safety management.	 | 本研究探讨了主题建模技术，包括潜在狄利克雷分配（LDA）、非负矩阵分解（NMF）和概率潜在语义分析（PLSA）在1908年至2009年Socrata数据集上的应用。这些数据根据操作类型（军事、商业和私营）进行了分类，分析中识别出的关键主题包括飞行员错误、机械故障、天气条件和训练不足。研究强调了每种方法的独特优势：LDA能够发现重叠的主题，NMF生成明确且可解释的主题，而PLSA尽管具有解释上的复杂性，仍能提供细腻的概率见解。统计分析显示，PLSA获得了0.32的连贯性得分和-4.6的困惑度值，NMF获得了0.34和37.1的得分，而LDA尽管连贯性最高（0.36），但困惑度最高（38.2）。这些发现展示了主题建模在从航空安全的非结构化叙述中提取可操作见解方面的价值，有助于识别风险因素和改进措施，覆盖各个领域。未来研究方向包括整合更多背景变量、利用神经主题模型以及增强航空安全规程。本研究为航空安全管理工作中的高级文本挖掘应用奠定了基础。
2501.01669	 | Inversely Learning Transferable Rewards via Abstracted States	 | Yikang Gui,Prashant Doshi	 | 该研究提出了一种从多个实例的行为轨迹中逆向学习抽象奖励函数的方法，并验证了该抽象奖励函数在未见过的领域实例中能够有效学习任务行为，从而推动了机器人在新任务中的应用集成。	 | Inverse reinforcement learning (IRL) has progressed significantly toward accurately learning the underlying rewards in both discrete and continuous domains from behavior data. The next advance is to learn {\em intrinsic} preferences in ways that produce useful behavior in settings or tasks which are different but aligned with the observed ones. In the context of robotic applications, this helps integrate robots into processing lines involving new tasks (with shared intrinsic preferences) without programming from scratch. We introduce a method to inversely learn an abstract reward function from behavior trajectories in two or more differing instances of a domain. The abstract reward function is then used to learn task behavior in another separate instance of the domain. This step offers evidence of its transferability and validates its correctness. We evaluate the method on trajectories in tasks from multiple domains in OpenAI's Gym testbed and AssistiveGym and show that the learned abstract reward functions can successfully learn task behaviors in instances of the respective domains, which have not been seen previously.	 | 逆强化学习（IRL）在从行为数据中准确学习离散和连续域中的潜在奖励方面取得了显著进展。下一步是学习内在偏好，这些内在偏好能够生成在不同但与观察到的任务或设置对齐的任务中的有用行为。在机器人应用的背景下，这有助于在无需从头开始编程的情况下，将机器人集成到涉及新任务（具有共享内在偏好）的生产线上。我们提出了一种方法，可以从两个或多个不同实例中的行为轨迹中逆向学习抽象的奖励函数。然后，使用该抽象的奖励函数来学习另一个分离实例中的任务行为。这一步提供了其可转移性的证据，并验证了其正确性。我们使用OpenAI的Gym测试平台和AssistiveGym中的多个领域中的任务轨迹评估了该方法，并展示了所学习的抽象奖励函数能够在未见过的相应领域实例中成功学习任务行为。
2501.01665	 | FairSense: Long-Term Fairness Analysis of ML-Enabled Systems	 | Yining She,Sumon Biswas,Christian Kästner,Eunsuk Kang	 | 本文提出了一种基于仿真的框架FairSense，用于检测和分析机器学习驱动系统在长期运行中的不公平性问题，通过蒙特卡洛模拟和灵敏度分析来评估不同配置对系统公平性的影响。FairSense适用于多种实际应用场景，如贷款借贷、阿片类药物风险评分和预测警务。	 | Algorithmic fairness of machine learning (ML) models has raised significant concern in the recent years. Many testing, verification, and bias mitigation techniques have been proposed to identify and reduce fairness issues in ML models. The existing methods are model-centric and designed to detect fairness issues under static settings. However, many ML-enabled systems operate in a dynamic environment where the predictive decisions made by the system impact the environment, which in turn affects future decision-making. Such a self-reinforcing feedback loop can cause fairness violations in the long term, even if the immediate outcomes are fair. In this paper, we propose a simulation-based framework called FairSense to detect and analyze long-term unfairness in ML-enabled systems. Given a fairness requirement, FairSense performs Monte-Carlo simulation to enumerate evolution traces for each system configuration. Then, FairSense performs sensitivity analysis on the space of possible configurations to understand the impact of design options and environmental factors on the long-term fairness of the system. We demonstrate FairSense's potential utility through three real-world case studies: Loan lending, opioids risk scoring, and predictive policing.	 | 近年来，机器学习（ML）模型的算法公平性引起了广泛关注。许多测试、验证和偏见缓解技术被提出，以识别和减少ML模型中的公平性问题。现有的方法主要是模型为中心的，旨在在静态环境中检测公平性问题。然而，许多依赖ML的系统在动态环境中运行，系统做出的预测决策会影响环境，反过来又影响未来的决策。这种自我强化的反馈循环可能导致公平性问题在长期内发生，即使即时结果是公平的。在本文中，我们提出了一种基于仿真的框架FairSense，用于检测和分析ML使能系统的长期不公平性。给定一个公平性要求，FairSense将执行蒙特卡洛模拟以枚举每种系统配置的演化轨迹。然后，FairSense在可能配置的空间上进行灵敏度分析，以理解设计选项和环境因素对系统长期公平性的影响。我们通过三个实际案例研究展示了FairSense的潜在用途：贷款借贷、阿片类药物风险评分和预测警务。
2501.01653	 | Look Back for More: Harnessing Historical Sequential Updates for Personalized Federated Adapter Tuning	 | Danni Peng,Yuan Wang,Huazhu Fu,Jinpeng Jiang,Yong Liu,Rick Siow Mong Goh,Qingsong Wei	 | 个人化联邦学习（pFedSeq）提出了一种新的框架，通过使用选择性状态空间模型（SSM）处理客户端的过去适配器更新序列，来生成高性能的个性化适配器，从而有效解决传统联邦学习中客户端数据异质性的问题，并在多个基准数据集上表现优越。	 | Personalized federated learning (PFL) studies effective model personalization to address the data heterogeneity issue among clients in traditional federated learning (FL). Existing PFL approaches mainly generate personalized models by relying solely on the clients' latest updated models while ignoring their previous updates, which may result in suboptimal personalized model learning. To bridge this gap, we propose a novel framework termed pFedSeq, designed for personalizing adapters to fine-tune a foundation model in FL. In pFedSeq, the server maintains and trains a sequential learner, which processes a sequence of past adapter updates from clients and generates calibrations for personalized adapters. To effectively capture the cross-client and cross-step relations hidden in previous updates and generate high-performing personalized adapters, pFedSeq adopts the powerful selective state space model (SSM) as the architecture of sequential learner. Through extensive experiments on four public benchmark datasets, we demonstrate the superiority of pFedSeq over state-of-the-art PFL methods.	 | 个人化联邦学习（pFedSeq）研究了有效模型个性化的方法，以解决传统联邦学习（FL）中客户端之间数据异质性的问题。现有的个人化联邦学习方法主要通过依赖客户端最新的更新模型来生成个性化模型，而忽略了它们之前的更新，这可能导致个性化模型学习的次优结果。为了解决这一问题，我们提出了一种新的框架pFedSeq，用于在FL中对适配器进行个性化处理以微调基础模型。在pFedSeq中，服务器维护并训练一个顺序学习器，该学习器处理来自客户端的过去适配器更新序列，并生成个性化适配器的校准。为了有效地捕捉以前更新中隐藏的跨客户端和跨步骤的关系并生成高性能的个性化适配器，pFedSeq采用了强大的选择性状态空间模型（SSM）作为顺序学习器的架构。通过在四个公开基准数据集上的广泛实验，我们证明了pFedSeq在最新的个人化联邦学习方法中具有优越性。
2501.01649	 | AVATAR: Adversarial Autoencoders with Autoregressive Refinement for Time Series Generation	 | MohammadReza EskandariNasab,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi	 | AVATAR框架结合了对抗自编码器和自回归学习，以同时学习时间序列数据的概率分布和条件分布，从而提升生成数据的质量和实用性。该框架通过引入新的监督损失和分布损失，并采用联合训练机制，有效解决了时间序列数据生成中的独特挑战。	 | Data augmentation can significantly enhance the performance of machine learning tasks by addressing data scarcity and improving generalization. However, generating time series data presents unique challenges. A model must not only learn a probability distribution that reflects the real data distribution but also capture the conditional distribution at each time step to preserve the inherent temporal dependencies. To address these challenges, we introduce AVATAR, a framework that combines Adversarial Autoencoders (AAE) with Autoregressive Learning to achieve both objectives. Specifically, our technique integrates the autoencoder with a supervisor and introduces a novel supervised loss to assist the decoder in learning the temporal dynamics of time series data. Additionally, we propose another innovative loss function, termed distribution loss, to guide the encoder in more efficiently aligning the aggregated posterior of the autoencoder's latent representation with a prior Gaussian distribution. Furthermore, our framework employs a joint training mechanism to simultaneously train all networks using a combined loss, thereby fulfilling the dual objectives of time series generation. We evaluate our technique across a variety of time series datasets with diverse characteristics. Our experiments demonstrate significant improvements in both the quality and practical utility of the generated data, as assessed by various qualitative and quantitative metrics.	 | 数据增强可以通过解决数据稀缺性和提高泛化能力显著提升机器学习任务的性能。然而，生成时间序列数据带来了独特的挑战。一个模型不仅要学习一个反映真实数据分布的概率分布，还要捕获每个时间步的条件分布，以保留内在的时间依赖关系。为了解决这些挑战，我们引入了AVATAR框架，该框架结合了对抗自编码器（AAE）和自回归学习，以同时实现这两个目标。具体而言，我们的方法将自编码器与监督器相结合，并引入了一种新的监督损失来协助解码器学习时间序列数据的时间动态。此外，我们还提出了一种新的损失函数，称为分布损失，以指导编码器更有效地将自编码器隐空间的聚合后验与先验高斯分布对齐。此外，我们的框架采用了联合训练机制，使用组合损失同时训练所有网络，从而满足时间序列生成的双重目标。我们使用多种具有不同特性的时间序列数据集评估了我们的技术。我们的实验结果显示，生成的数据在质量和实用性方面都取得了显著改进，这通过各种定性和定量指标得到验证。
2501.01630	 | A Probabilistic Model for Node Classification in Directed Graphs	 | Diego Huerta,Gerardo Arizmendi	 | 该研究提出了一种适用于有向图的概率模型，能够预测未见过节点的标签，并且预测结果具有高度可解释性；该模型在两个数据集中展现了竞争力甚至更优的预测性能，特别是首次应用于数学家谱项目数据集，提供了新的基准比较。	 | In this work, we present a probabilistic model for directed graphs where nodes have attributes and labels. This model serves as a generative classifier capable of predicting the labels of unseen nodes using either maximum likelihood or maximum a posteriori estimations. The predictions made by this model are highly interpretable, contrasting with some common methods for node classification, such as graph neural networks. We applied the model to two datasets, demonstrating predictive performance that is competitive with, and even superior to, state-of-the-art methods. One of the datasets considered is adapted from the Math Genealogy Project, which has not previously been utilized for this purpose. Consequently, we evaluated several classification algorithms on this dataset to compare the performance of our model and provide benchmarks for this new resource.	 | 在这项工作中，我们提出了一种适用于具有属性和标签节点的有向图的概率模型。该模型作为生成分类器，能够利用最大似然估计或最大后验估计预测未见过节点的标签。该模型的预测结果具有高度可解释性，与某些常见的节点分类方法（如图神经网络）形成鲜明对比。我们将该模型应用于两个数据集，展示了与其相比具有竞争力甚至更优的预测性能。其中一个数据集来自数学家谱项目，这是首次将其用于此目的。因此，我们在该数据集上评估了多种分类算法，以比较我们模型的性能，并为此新资源提供基准。
2501.01629	 | Crossing Language Borders: A Pipeline for Indonesian Manhwa Translation	 | Nithyasri Narasimhan,Sagarika Singh	 | 该项目开发了一种结合计算机视觉、文本识别和自然语言处理技术的自动化解决方案，用于将印尼语 manhwa（韩国漫画）翻译成英语，以简化传统的手工翻译流程并节省时间和精力。该方法在各步骤中表现良好，特别针对印尼语到英语的翻译挑战进行了优化。	 | In this project, we develop a practical and efficient solution for automating the Manhwa translation from Indonesian to English. Our approach combines computer vision, text recognition, and natural language processing techniques to streamline the traditionally manual process of Manhwa(Korean comics) translation. The pipeline includes fine-tuned YOLOv5xu for speech bubble detection, Tesseract for OCR and fine-tuned MarianMT for machine translation. By automating these steps, we aim to make Manhwa more accessible to a global audience while saving time and effort compared to manual translation methods. While most Manhwa translation efforts focus on Japanese-to-English, we focus on Indonesian-to-English translation to address the challenges of working with low-resource languages. Our model shows good results at each step and was able to translate from Indonesian to English efficiently.	 | 在这个项目中，我们开发了一种实用且高效的解决方案，用于自动化将印尼语翻译成英语的 manhwa（韩国漫画）翻译过程。我们的方法结合了计算机视觉、文本识别和自然语言处理技术，以简化传统的手工 manhwa 翻译流程。整个工作流程包括针对对话气泡检测微调的 YOLOv5xu、Tesseract 进行 OCR 操作以及针对机器翻译微调的 MarianMT。通过自动化这些步骤，我们旨在使 manhwa 更便于全球观众使用，并且相比手工翻译方法，在节省时间和精力方面具有优势。尽管大多数 manhwa 翻译工作集中在日语到英语的翻译上，但我们将重点放在印尼语到英语的翻译上，以解决使用低资源语言所带来的挑战。我们的模型在每个步骤上都表现良好，并且能够高效地将印尼语翻译成英语。
2501.01620	 | Adaptive Meta-learning-based Adversarial Training for Robust Automatic Modulation Classification	 | Amirmohammad Bamdad,Ali Owfi,Fatemeh Afghah	 | 本文提出了一种基于元学习的对抗性训练框架，以增强自动调制分类模型对未见过的对抗性攻击的鲁棒性，并能够在少量新样本支持下快速适应新攻击，从而提高模型的准确性和在线训练效率。	 | DL-based automatic modulation classification (AMC) models are highly susceptible to adversarial attacks, where even minimal input perturbations can cause severe misclassifications. While adversarially training an AMC model based on an adversarial attack significantly increases its robustness against that attack, the AMC model will still be defenseless against other adversarial attacks. The theoretically infinite possibilities for adversarial perturbations mean that an AMC model will inevitably encounter new unseen adversarial attacks if it is ever to be deployed to a real-world communication system. Moreover, the computational limitations and challenges of obtaining new data in real-time will not allow a full training process for the AMC model to adapt to the new attack when it is online. To this end, we propose a meta-learning-based adversarial training framework for AMC models that substantially enhances robustness against unseen adversarial attacks and enables fast adaptation to these attacks using just a few new training samples, if any are available. Our results demonstrate that this training framework provides superior robustness and accuracy with much less online training time than conventional adversarial training of AMC models, making it highly efficient for real-world deployment.	 | 基于深度学习的自动调制分类（AMC）模型极易受到对抗性攻击的影响，即使是微小的输入扰动也可能导致严重的误分类。虽然基于对抗性攻击对AMC模型进行对抗性训练可以显著提高其对该攻击的鲁棒性，但该AMC模型仍无法防御其他类型的对抗性攻击。理论上无限多的对抗性扰动可能性意味着，如果AMC模型要部署到实际通信系统中，它不可避免地会遇到新的未见过的对抗性攻击。此外，计算限制和实时获取新数据的挑战将无法让AMC模型在在线时适应新的攻击进行完整的训练过程。为此，我们提出了一种基于元学习的对抗性训练框架，该框架能够显著增强AMC模型对未见过的对抗性攻击的鲁棒性，并能够在仅使用少量新训练样本（如果有可用的话）的情况下快速适应这些攻击。我们的结果显示，与传统的AMC模型对抗性训练相比，该训练框架提供了更好的鲁棒性和准确性，并且在线训练时间大大减少，使其在实际部署中非常高效。
2501.01608	 | Online Meta-Learning Channel Autoencoder for Dynamic End-to-end Physical Layer Optimization	 | Ali Owfi,Jonathan Ashdown,Kurt Turck	 | 通道自动编码器（CAEs）在优化无线通信系统的特定信道方面表现出潜力，但大多数设计假定静态场景且需要大量探针信号作为训练样本，这在实际动态场景中难以实现。本文提出了一种在线元学习信道自动编码器（OML-CAE）框架，旨在解决这些挑战，通过少量探针信号就能适应新信道条件并动态调整，提高了CAE在现实场景中的可行性。	 | Channel Autoencoders (CAEs) have shown significant potential in optimizing the physical layer of a wireless communication system for a specific channel through joint end-to-end training. However, the practical implementation of CAEs faces several challenges, particularly in realistic and dynamic scenarios. Channels in communication systems are dynamic and change with time. Still, most proposed CAE designs assume stationary scenarios, meaning they are trained and tested for only one channel realization without regard for the dynamic nature of wireless communication systems. Moreover, conventional CAEs are designed based on the assumption of having access to a large number of pilot signals, which act as training samples in the context of CAEs. However, in real-world applications, it is not feasible for a CAE operating in real-time to acquire large amounts of training samples for each new channel realization. Hence, the CAE has to be deployable in few-shot learning scenarios where only limited training samples are available. Furthermore, most proposed conventional CAEs lack fast adaptability to new channel realizations, which becomes more pronounced when dealing with a limited number of pilots. To address these challenges, this paper proposes the Online Meta Learning channel AE (OML-CAE) framework for few-shot CAE scenarios with dynamic channels. The OML-CAE framework enhances adaptability to varying channel conditions in an online manner, allowing for dynamic adjustments in response to evolving communication scenarios. Moreover, it can adapt to new channel conditions using only a few pilots, drastically increasing pilot efficiency and making the CAE design feasible in realistic scenarios.	 | 通道自动编码器（CAEs）在通过联合端到端训练优化无线通信系统特定信道的物理层方面展现了显著的潜力。然而，在实际应用中，CAEs 面临着几个挑战，特别是在现实且动态的场景中。通信系统中的信道是动态的，并且随时间变化。然而，大多数提出的 CAE 设计假定为静态场景，这意味着它们仅针对单一信道实现进行训练和测试，而不考虑无线通信系统的动态性质。此外，传统的 CAEs 是基于能够访问大量探针信号的假设设计的，这些探针信号作为 CAEs 的训练样本来使用。然而，在实际应用中，实时运行的 CAE 为每个新信道实现采集大量训练样本并不现实。因此，CAE 必须能够在仅有限训练样本可用的少量样本学习场景中部署。此外，大多数传统 CAEs 缺乏对新信道实现的快速适应性，特别是在探针数量有限的情况下，这一问题更为明显。为了解决这些问题，本文提出了一种在线元学习信道自动编码器（OML-CAE）框架，用于具有动态信道的少量样本学习场景。OML-CAE 框架以在线方式增强对不同信道条件的适应性，使其能够根据不断变化的通信场景做出动态调整。此外，它能够仅使用少量探针来适应新的信道条件，大大提高了探针的效率，并使 CAE 设计在现实场景中更为可行。
2501.01591	 | Multivariate Time Series Anomaly Detection using DiffGAN Model	 | Guangqiang Wu,Fu Zhang	 | 近年来，扩散模型被应用于多变量时间序列异常检测，但常用的半扩散策略受扩散步数影响，导致重构效果不佳。为此，提出了一种名为DiffGAN的新方法，通过将生成对抗网络组件加入扩散模型的去噪器中，同时生成噪声数据和预测扩散步数，从而提高了异常检测性能。	 | In recent years, some researchers have applied diffusion models to multivariate time series anomaly detection. The partial diffusion strategy, which depends on the diffusion steps, is commonly used for anomaly detection in these models. However, different diffusion steps have an impact on the reconstruction of the original data, thereby impacting the effectiveness of anomaly detection. To address this issue, we propose a novel method named DiffGAN, which adds a generative adversarial network component to the denoiser of diffusion model. This addition allows for the simultaneous generation of noisy data and prediction of diffusion steps. Compared to multiple state-of-the-art reconstruction models, experimental results demonstrate that DiffGAN achieves superior performance in anomaly detection.	 | 近年来，一些研究人员将扩散模型应用于多变量时间序列异常检测。这些模型中常用的半扩散策略依赖于扩散步数来进行异常检测，但不同的扩散步数会影响原始数据的重构，从而影响异常检测的效果。为了解决这个问题，我们提出了一种名为DiffGAN的新方法，该方法将生成对抗网络（GAN）组件添加到了扩散模型的去噪器中。这种方法允许同时生成噪声数据和预测扩散步数。与多个最先进的重构模型相比，实验结果表明，DiffGAN在异常检测方面取得了更优的性能。
2501.01584	 | Stackelberg Game Based Performance Optimization in Digital Twin Assisted Federated Learning over NOMA Networks	 | Bibo Wu,Fang Fang,Xianbin Wang	 | 本文提出了一种利用数字孪生在非正交多址网络中协助联邦学习的方法，通过基于声誉的客户端选择方案和Stackelberg博弈框架来减轻恶意攻击并最小化系统延迟和能耗，仿真结果验证了其优越性。	 | Despite the advantage of preserving data privacy, federated learning (FL) still suffers from the straggler issue due to the limited computing resources of distributed clients and the unreliable wireless communication environment. By effectively imitating the distributed resources, digital twin (DT) shows great potential in alleviating this issue. In this paper, we leverage DT in the FL framework over non-orthogonal multiple access (NOMA) network to assist FL training process, considering malicious attacks on model updates from clients. A reputationbased client selection scheme is proposed, which accounts for client heterogeneity in multiple aspects and effectively mitigates the risks of poisoning attacks in FL systems. To minimize the total latency and energy consumption in the proposed system, we then formulate a Stackelberg game by considering clients and the server as the leader and the follower, respectively. Specifically, the leader aims to minimize the energy consumption while the objective of the follower is to minimize the total latency during FL training. The Stackelberg equilibrium is achieved to obtain the optimal solutions. We first derive the strategies for the followerlevel problem and include them in the leader-level problem which is then solved via problem decomposition. Simulation results verify the superior performance of the proposed scheme.	 | 尽管联邦学习（FL）在保留数据隐私方面具有优势，但由于分布式客户端的有限计算资源和不可靠的无线通信环境，它仍然面临“拖后腿”问题。通过有效模仿分布式资源，数字孪生（DT）显示了缓解这一问题的巨大潜力。在本文中，我们利用数字孪生在非正交多址（NOMA）网络的联邦学习框架中协助联邦学习训练过程，同时考虑了来自客户端的恶意攻击。我们提出了一种基于声誉的客户端选择方案，该方案综合考虑了客户端在多个方面的异质性，有效地减轻了联邦学习系统中中毒攻击的风险。为了最小化所提系统中的总延迟和能耗，我们通过将客户端视为领导者、服务器视为跟随者的Stackelberg博弈框架来建模。具体而言，领导者的目标是最小化能耗，而跟随者的目的是在联邦学习训练过程中最小化总延迟。通过求解Stackelberg均衡来获得最优解。我们首先推导了跟随者层面问题的策略，并将它们纳入领导者层面的问题中，然后通过问题分解的方法求解领导者层面的问题。仿真结果验证了所提方案的优越性能。
2501.01564	 | Semialgebraic Neural Networks: From roots to representations	 | S. David Mis,Matti Lassas,Maarten V. de Hoop	 | 本文提出了一种半代数神经网络（SANNs），它可以表示任意有界半代数函数并精确计算至指定精度，通过分段多项式和特定的同伦方法实现对半代数函数的评估，甚至可以表示不连续的函数。	 | Many numerical algorithms in scientific computing -- particularly in areas like numerical linear algebra, PDE simulation, and inverse problems -- produce outputs that can be represented by semialgebraic functions; that is, the graph of the computed function can be described by finitely many polynomial equalities and inequalities. In this work, we introduce Semialgebraic Neural Networks (SANNs), a neural network architecture capable of representing any bounded semialgebraic function, and computing such functions up to the accuracy of a numerical ODE solver chosen by the programmer. Conceptually, we encode the graph of the learned function as the kernel of a piecewise polynomial selected from a class of functions whose roots can be evaluated using a particular homotopy continuation method. We show by construction that the SANN architecture is able to execute this continuation method, thus evaluating the learned semialgebraic function. Furthermore, the architecture can exactly represent even discontinuous semialgebraic functions by executing a continuation method on each connected component of the target function. Lastly, we provide example applications of these networks and show they can be trained with traditional deep-learning techniques.	 | 许多科学计算中的数值算法，特别是在数值线性代数、偏微分方程模拟和逆问题等领域，产生的输出可以用半代数函数来表示；也就是说，计算函数的图可以由有限多个多项式方程和不等式来描述。在本文中，我们引入了半代数神经网络（SANNs），这是一种能够表示任意有界半代数函数，并将这些函数计算到由程序员选择的数值常微分方程求解器精度的神经网络架构。概念上，我们将所学习的函数的图编码为来自一类函数的分段多项式的核，这些函数的根可以通过特定的同伦 continuation 方法来评估。我们通过构造方法证明，SANN 架构能够执行这一 continuation 方法，从而评估所学习的半代数函数。此外，该架构可以通过在目标函数的每个连通组件上执行 continuation 方法来精确表示甚至不连续的半代数函数。最后，我们提供了一些这些网络的应用实例，并展示了它们可以通过传统的深度学习技术进行训练。
2501.01558	 | Predicting the Performance of Black-box LLMs through Self-Queries	 | Dylan Sam,Marc Finzi,J. Zico Kolter	 | 本文提出了一种以黑盒方式从大型语言模型（LLMs）中提取特征的方法，通过后续提示和不同响应的概率来训练预测模型，以可靠地预测模型行为并区分不同模型版本和架构，甚至超出基于模型内部表示的白盒方法的能力。	 | As large language models (LLMs) are increasingly relied on in AI systems, predicting when they make mistakes is crucial. While a great deal of work in the field uses internal representations to interpret model behavior, these representations are inaccessible when given solely black-box access through an API. In this paper, we extract features of LLMs in a black-box manner by using follow-up prompts and taking the probabilities of different responses as representations to train reliable predictors of model behavior. We demonstrate that training a linear model on these low-dimensional representations produces reliable and generalizable predictors of model performance at the instance level (e.g., if a particular generation correctly answers a question). Remarkably, these can often outperform white-box linear predictors that operate over a model's hidden state or the full distribution over its vocabulary. In addition, we demonstrate that these extracted features can be used to evaluate more nuanced aspects of a language model's state. For instance, they can be used to distinguish between a clean version of GPT-4o-mini and a version that has been influenced via an adversarial system prompt that answers question-answering tasks incorrectly or introduces bugs into generated code. Furthermore, they can reliably distinguish between different model architectures and sizes, enabling the detection of misrepresented models provided through an API (e.g., identifying if GPT-3.5 is supplied instead of GPT-4o-mini).	 | 随着大型语言模型（LLMs）在AI系统中的依赖程度不断增加，预测它们何时出错变得至关重要。尽管该领域大量的工作使用内部表示来解释模型行为，但在仅通过API提供黑盒访问的情况下，这些表示是不可访问的。在本文中，我们通过使用后续提示并以不同响应的概率作为表示来以黑盒方式提取LLMs的特征，从而训练可靠的预测模型来预测模型行为。我们证明，在这些低维度表示上训练线性模型可以产生可靠且可泛化的实例级模型性能预测器（例如，如果特定生成正确地回答了一个问题）。令人惊讶的是，这些预测器通常可以超越在模型隐藏状态或词汇表完整分布上工作的白盒线性预测器。此外，我们还证明这些提取的特征可以用于评估语言模型状态的更细微方面。例如，它们可以用来区分GPT-4o-mini的干净版本和一个通过对抗性系统提示被影响、错误地回答问题或引入代码生成错误的版本。此外，它们还可以可靠地区分不同的模型架构和规模，从而检测通过API提供的被误表示的模型（例如，识别是否提供了GPT-3.5而不是GPT-4o-mini）。
2501.01544	 | Many of Your DPOs are Secretly One: Attempting Unification Through Mutual Information	 | Rasul Tutnov,Antoine Grosnit,Haitham Bou-Ammar	 | 本文提出了一种基于互信息的统一框架，该框架提供了一种新的灵活先验损失函数，并能够推导出多种现有的直接偏好优化算法，从而简化了这些算法的复杂关系，有助于研究LLM对齐技术的进一步发展。	 | Post-alignment of large language models (LLMs) is critical in improving their utility, safety, and alignment with human intentions. Direct preference optimisation (DPO) has become one of the most widely used algorithms for achieving this alignment, given its ability to optimise models based on human feedback directly. However, the vast number of DPO variants in the literature has made it increasingly difficult for researchers to navigate and fully grasp the connections between these approaches. This paper introduces a unifying framework inspired by mutual information, which proposes a new loss function with flexible priors. By carefully specifying these priors, we demonstrate that many existing algorithms, such as SimPO, TDPO, SparsePO, and others, can be derived from our framework. This unification offers a clearer and more structured approach, allowing researchers to understand the relationships between different DPO variants better. We aim to simplify the landscape of DPO algorithms, making it easier for the research community to gain insights and foster further advancements in LLM alignment. Ultimately, we hope our framework can be a foundation for developing more robust and interpretable alignment techniques.	 | 大型语言模型（LLMs）的后调整对于提高其实用性和与人类意图的对齐至关重要。直接偏好优化（DPO）已成为实现这一对齐最广泛使用的算法之一，因为其能够根据人类反馈直接优化模型。然而，文献中众多的DPO变体使得研究人员越来越难以导航并完全理解这些方法之间的联系。本文介绍了一个基于互信息的统一框架，提出了一种具有灵活先验的新损失函数。通过仔细指定这些先验，我们展示了诸如SimPO、TDPO、SparsePO等许多现有算法都可以从我们的框架中推导出来。这种统一提供了一种更加清晰和结构化的途径，使研究人员能够更好地理解不同DPO变体之间的关系。我们旨在简化DPO算法的景观，使研究社区更容易获得洞察力并促进LLM对齐的进一步进步。最终，我们希望我们的框架能够成为开发更稳健和可解释对齐技术的基础。
2501.01540	 | BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery	 | Kanishk Gandhi,Michael Y. Li,Lyle Goodyear,Louise Li,Aditi Bhaskar,Mohammed Zaman,Noah D. Goodman	 | 该研究引入了BoxingGym，这是一个基准平台，用于评估大规模语言模型在实验设计和模型发现方面的能力，通过计算预期信息增益和基于模型解释的预测准确性来量化评估。尽管当前的大规模语言模型在这些任务上表现不佳，结合显式统计模型也无法显著改善其性能。	 | Understanding the world and explaining it with scientific theories is a central aspiration of artificial intelligence research. Proposing theories, designing experiments to test them, and then revising them based on data are fundamental to scientific discovery. Despite the significant promise of LLM-based scientific agents, no benchmarks systematically test LLM's ability to propose scientific models, collect experimental data, and revise them in light of new data. We introduce BoxingGym, a benchmark with 10 environments for systematically evaluating both experimental design (e.g. collecting data to test a scientific theory) and model discovery (e.g. proposing and revising scientific theories). To enable tractable and quantitative evaluation, we implement each environment as a generative probabilistic model with which a scientific agent can run interactive experiments. These probabilistic models are drawn from various real-world scientific domains ranging from psychology to ecology. To quantitatively evaluate a scientific agent's ability to collect informative experimental data, we compute the expected information gain (EIG), an information-theoretic quantity which measures how much an experiment reduces uncertainty about the parameters of a generative model. A good scientific theory is a concise and predictive explanation. Therefore, to quantitatively evaluate model discovery, we ask a scientific agent to explain their model and then assess whether this explanation enables another scientific agent to make reliable predictions about this environment. In addition to this explanation-based evaluation, we compute standard model evaluation metrics such as prediction errors. We find that current LLMs, such as GPT-4o, struggle with both experimental design and model discovery. We find that augmenting the LLM-based agent with an explicit statistical model does not reliably improve these results.	 | 理解世界并用科学理论对其进行解释是人工智能研究的核心追求。提出理论、设计实验来测试这些理论，然后根据数据进行修订，这是科学研究的基本过程。尽管基于大规模语言模型（LLM）的科学代理具有巨大的潜力，但目前还没有系统性地测试LLM提出科学模型、收集实验数据以及根据新数据修订模型的能力。我们引入了BoxingGym，这是一个包含10个环境的基准，用于系统性地评估实验设计（例如，收集数据来测试科学理论）和模型发现（例如，提出和修订科学理论）。为了确保评估的可行性和量化，我们将每个环境实现为一个生成概率模型，代理可以在其中运行交互式实验。这些概率模型来自心理学到生态学等多个实际科学领域。为了定量评估科学代理收集信息性实验数据的能力，我们计算了预期信息增益（EIG），这是一种信息论量度，衡量实验如何减少对生成模型参数的不确定性。一个好的科学理论是简洁且预测性的解释。因此，为了定量评估模型发现，我们要求科学代理解释其模型，并评估这种解释是否使另一科学代理能够对该环境进行可靠的预测。除了基于解释的评估之外，我们还计算了标准的模型评估指标，如预测误差。我们发现，当前的LLM（如GPT-4o）在实验设计和模型发现方面存在困难。我们发现，将显式统计模型与基于LLM的代理结合使用，并不能可靠地改善这些结果。
2501.01525	 | Transfer Neyman-Pearson Algorithm for Outlier Detection	 | Mohammadreza M. Kalan,Eitan J. Neugut,Samory Kpotufe	 | 本文探讨了在目标域异常数据稀少情况下的迁移学习问题，并提出了一种通用元算法，该算法能够提供对异常分布变化的强理论保证，且易于实现；实验证明基于该元算法的不同实例（如多层神经网络）在传统平衡分类设置的迁移方法上表现更优。	 | We consider the problem of transfer learning in outlier detection where target abnormal data is rare. While transfer learning has been considered extensively in traditional balanced classification, the problem of transfer in outlier detection and more generally in imbalanced classification settings has received less attention. We propose a general meta-algorithm which is shown theoretically to yield strong guarantees w.r.t. to a range of changes in abnormal distribution, and at the same time amenable to practical implementation. We then investigate different instantiations of this general meta-algorithm, e.g., based on multi-layer neural networks, and show empirically that they outperform natural extensions of transfer methods for traditional balanced classification settings (which are the only solutions available at the moment).	 | 我们考虑在目标异常数据稀少情况下的迁移学习问题。虽然在传统平衡分类中已经广泛研究了迁移学习，但在异常检测中的迁移问题，更广泛地说是在不平衡分类设置中的迁移问题，却较少受到关注。我们提出了一种通用元算法，该算法在理论上能够提供对异常分布变化的强保证，并且同时易于实际实现。然后，我们探讨了这种通用元算法的不同实例，例如基于多层神经网络，实验证明它们在传统平衡分类设置的迁移方法自然拓展之上表现更优（目前仅有的解决方案）。
2501.01516	 | Improving Robustness Estimates in Natural Language Explainable AI though Synonymity Weighted Similarity Measures	 | Christopher Burger	 | 该研究关注可解释的人工智能（XAI）的可靠性，发现常用度量标准在评估对抗性XAI时存在不足，并提出通过考虑扰动词语的同义性来改进这些度量标准，以更准确地揭示XAI方法的弱点。	 | Explainable AI (XAI) has seen a surge in recent interest with the proliferation of powerful but intractable black-box models. Moreover, XAI has come under fire for techniques that may not offer reliable explanations. As many of the methods in XAI are themselves models, adversarial examples have been prominent in the literature surrounding the effectiveness of XAI, with the objective of these examples being to alter the explanation while maintaining the output of the original model. For explanations in natural language, it is natural to use measures found in the domain of information retrieval for use with ranked lists to guide the adversarial XAI process. We show that the standard implementation of these measures are poorly suited for the comparison of explanations in adversarial XAI and amend them by using information that is discarded, the synonymity of perturbed words. This synonymity weighting produces more accurate estimates of the actual weakness of XAI methods to adversarial examples.	 | 可解释的人工智能（XAI）在最近得到了广泛关注，这主要是由于广泛应用了强大但难以解释的黑箱模型。此外，XAI的技术也受到了批评，因为这些技术可能并不提供可靠的解释。由于XAI中的许多方法本身也是模型，因此对抗性示例在关于XAI有效性的文献中占据了重要地位，这些对抗性示例的目标是改变解释结果，同时保持原模型的输出。对于自然语言解释，自然语言检索领域中用于排序列表的度量标准非常适合指导对抗性XAI过程。我们发现标准实现的这些度量标准不适合用于比较对抗性XAI中的解释，因此我们通过利用被丢弃的信息，即扰动词语的同义性，对其进行改进。这种同义性加权能够更准确地估计XAI方法对对抗性示例的实际弱点。
2501.01515	 | DiagrammaticLearning: A Graphical Language for Compositional Training Regimes	 | Mason Lary,Richard Samuelson,Alexander Wilentz,Alina Zare,Matthew Klawonn,James P. Fairbanks	 | 该研究受深度学习中相互独立但参数共享的模型组件启发，提出了一种称为“学习图”的新型表示方法，将参数化学习视作图形中的数据，实现多个模型组件的一致协同工作，并通过一个统一的损失函数进行训练。学习图不仅能够表示多种流行的训练设置，还提供了一个数学框架，使用户能够构建复杂的模型并进行操作。	 | Motivated by deep learning regimes with multiple interacting yet distinct model components, we introduce learning diagrams, graphical depictions of training setups that capture parameterized learning as data rather than code. A learning diagram compiles to a unique loss function on which component models are trained. The result of training on this loss is a collection of models whose predictions ``agree" with one another. We show that a number of popular learning setups such as few-shot multi-task learning, knowledge distillation, and multi-modal learning can be depicted as learning diagrams. We further implement learning diagrams in a library that allows users to build diagrams of PyTorch and Flux.jl models. By implementing some classic machine learning use cases, we demonstrate how learning diagrams allow practitioners to build complicated models as compositions of smaller components, identify relationships between workflows, and manipulate models during or after training. Leveraging a category theoretic framework, we introduce a rigorous semantics for learning diagrams that puts such operations on a firm mathematical foundation.	 | 受多种相互作用但又各自独立的模型组件的深度学习范式的启发，我们引入了学习图，这是一种将参数化学习作为一种数据而非代码的图形表示。学习图编译为一个唯一的损失函数，组件模型在此基础上进行训练。在通过此损失函数进行训练后，得到的一系列模型的预测结果“一致”地协同工作。我们展示了诸如少量样本多任务学习、知识蒸馏和多模态学习等许多流行的训练设置可以表示为学习图。我们进一步在一个库中实现了学习图，该库允许用户构建 PyTorch 和 Flux.jl 模型的图。通过实现一些经典的机器学习应用场景，我们展示了学习图如何使从业者能够将复杂的模型构建为较小组件的组合，识别工作流之间的关系，并在训练过程中或之后对模型进行操作。利用范畴论框架，我们引入了学习图的形式化语义，为这些操作奠定了坚实的数学基础。
2501.01511	 | TreeLUT: An Efficient Alternative to Deep Neural Networks for Inference Acceleration Using Gradient Boosted Decision Trees	 | Alireza Khataei,Kia Bazargan	 | 该研究提出了一种名为TreeLUT的开源工具，利用高效量化方案、硬件架构和流水线策略在FPGA上实现梯度提升决策树（GBDTs），展示了在多个分类数据集上显著提高的硬件利用率、延迟和吞吐量。	 | Accelerating machine learning inference has been an active research area in recent years. In this context, field-programmable gate arrays (FPGAs) have demonstrated compelling performance by providing massive parallelism in deep neural networks (DNNs). Neural networks (NNs) are computationally intensive during inference, as they require massive amounts of multiplication and addition, which makes their implementations costly. Numerous studies have recently addressed this challenge to some extent using a combination of sparsity induction, quantization, and transformation of neurons or sub-networks into lookup tables (LUTs) on FPGAs. Gradient boosted decision trees (GBDTs) are a high-accuracy alternative to DNNs in a wide range of regression and classification tasks, particularly for tabular datasets. The basic building block of GBDTs is a decision tree, which resembles the structure of binary decision diagrams. FPGA design flows are heavily optimized to implement such a structure efficiently. In addition to decision trees, GBDTs perform simple operations during inference, including comparison and addition. We present TreeLUT as an open-source tool for implementing GBDTs using an efficient quantization scheme, hardware architecture, and pipelining strategy. It primarily utilizes LUTs with no BRAMs or DSPs on FPGAs, resulting in high efficiency. We show the effectiveness of TreeLUT using multiple classification datasets, commonly used to evaluate ultra-low area and latency architectures. Using these benchmarks, we compare our implementation results with existing DNN and GBDT methods, such as DWN, PolyLUT-Add, NeuraLUT, LogicNets, FINN, hls4ml, and others. Our results show that TreeLUT significantly improves hardware utilization, latency, and throughput at competitive accuracy compared to previous works.	 | 近年来，加速机器学习推理一直是活跃的研究领域。在此背景下，现场可编程门阵列（FPGAs）通过在深度神经网络（DNNs）中提供巨大的并行性，展示了令人信服的性能。神经网络（NNs）在推理过程中计算密集，因为它们需要大量的乘法和加法操作，这使得它们的实现成本高昂。近年来，许多研究通过组合稀疏性诱导、量化以及将神经元或子网络转换为FPGAs上的查找表（LUTs）来部分解决这一挑战。  梯度提升决策树（GBDTs）在回归和分类任务中是DNNs的一种高精度替代方案，尤其是在表格数据集中。GBDT的基本构建块是决策树，其结构类似于二叉决策图。FPGA的设计流程高度优化，以便高效地实现这种结构。除了决策树外，GBDT在推理过程中还执行一些简单操作，包括比较和加法。我们提出TreeLUT作为一种开源工具，用于利用高效的量化方案、硬件架构和流水线策略来实现GBDTs。它主要使用LUTs，不使用FPGAs上的BRAMs或DSPs，从而实现高效性。我们使用多个分类数据集展示了TreeLUT的有效性，这些数据集常用于评估超低面积和延迟架构。通过这些基准，我们将我们的实现结果与现有的DNN和GBDT方法（如DWN、PolyLUT-Add、NeuraLUT、LogicNets、FINN、hls4ml等）进行了比较。我们的结果表明，与先前的工作相比，TreeLUT在保持竞争力的同时显著提高了硬件利用率、延迟和吞吐量。
2501.01510	 | Explainable Brain Age Gap Prediction in Neurodegenerative Conditions using coVariance Neural Networks	 | Saurabh Sihag,Gonzalo Mateos,Alejandro Ribeiro	 | 该研究使用共变异神经网络（VNN）方法分析皮层厚度特征，以评估不同类型神经退行性疾病（如阿尔茨海默病、前额颞叶痴呆和非典型帕金森病）的大脑年龄差距，并揭示了与解剖协方差矩阵特征相关的不同解剖模式，增强了结果的可解释性。	 | Brain age is the estimate of biological age derived from neuroimaging datasets using machine learning algorithms. Increasing \textit{brain age gap} characterized by an elevated brain age relative to the chronological age can reflect increased vulnerability to neurodegeneration and cognitive decline. Hence, brain age gap is a promising biomarker for monitoring brain health. However, black-box machine learning approaches to brain age gap prediction have limited practical utility. Recent studies on coVariance neural networks (VNN) have proposed a relatively transparent deep learning pipeline for neuroimaging data analyses, which possesses two key features: (i) inherent \textit{anatomically interpretablity} of derived biomarkers; and (ii) a methodologically interpretable perspective based on \textit{linkage with eigenvectors of anatomic covariance matrix}. In this paper, we apply the VNN-based approach to study brain age gap using cortical thickness features for various prevalent neurodegenerative conditions. Our results reveal distinct anatomic patterns for brain age gap in Alzheimer's disease, frontotemporal dementia, and atypical Parkinsonian disorders. Furthermore, we demonstrate that the distinct anatomic patterns of brain age gap are linked with the differences in how VNN leverages the eigenspectrum of the anatomic covariance matrix, thus lending explainability to the reported results.	 | 大脑年龄是通过使用机器学习算法从神经影像数据集中推断出的生物学年龄估计值。大脑年龄差距的增加，表现为大脑年龄相对于实际年龄升高，可以反映神经退行和认知下降的增加风险。因此，大脑年龄差距是一个监测脑健康很有前景的生物标志物。然而，用于大脑年龄差距预测的黑箱机器学习方法在实际应用中存在局限性。最近关于共变异神经网络（VNN）的研究提出了一种相对透明的深度学习管道，适用于神经影像数据分析，该方法具有两个关键特征：(i) 推导出的生物标志物的解剖可解释性；以及(ii) 基于与解剖协方差矩阵特征向量关联的方法可解释性视角。在本文中，我们应用基于VNN的方法，使用皮层厚度特征来研究各种常见神经退行性疾病的大脑年龄差距。我们的结果揭示了阿尔茨海默病、前额颞叶痴呆和非典型帕金森病等疾病中不同的解剖模式。此外，我们证明了大脑年龄差距的不同解剖模式与VNN如何利用解剖协方差矩阵特征谱有关，从而为所报告的结果提供了解释性。
2501.01509	 | AI-Enabled Operations at Fermi Complex: Multivariate Time Series Prediction for Outage Prediction and Diagnosis	 | Milan Jain,Burcu O. Mutlu,Caleb Stam,Jan Strube,Brian A. Schupbach,Jason M. St. John,William A. Pellico	 | 该研究提出了一种基于AI的框架，利用预测分析和自动标签来提高费米实验室加速器系统的运行效率，通过深度学习和随机森林等方法预测束流停机，并提供一致且带有置信度评分的停机注释，从而减少停机时间和优化决策过程。	 | The Main Control Room of the Fermilab accelerator complex continuously gathers extensive time-series data from thousands of sensors monitoring the beam. However, unplanned events such as trips or voltage fluctuations often result in beam outages, causing operational downtime. This downtime not only consumes operator effort in diagnosing and addressing the issue but also leads to unnecessary energy consumption by idle machines awaiting beam restoration. The current threshold-based alarm system is reactive and faces challenges including frequent false alarms and inconsistent outage-cause labeling. To address these limitations, we propose an AI-enabled framework that leverages predictive analytics and automated labeling. Using data from $2,703$ Linac devices and $80$ operator-labeled outages, we evaluate state-of-the-art deep learning architectures, including recurrent, attention-based, and linear models, for beam outage prediction. Additionally, we assess a Random Forest-based labeling system for providing consistent, confidence-scored outage annotations. Our findings highlight the strengths and weaknesses of these architectures for beam outage prediction and identify critical gaps that must be addressed to fully harness AI for transitioning downtime handling from reactive to predictive, ultimately reducing downtime and improving decision-making in accelerator management.	 | 费米实验室加速器复杂系统的主要控制室持续从数千个监控束流的传感器收集大量时间序列数据。然而，诸如跳闸或电压波动等突发事件往往会导致束流中断，引起操作停机时间。这些停机时间不仅消耗了操作员在诊断和解决问题上的努力，还导致闲置机器在束流恢复期间的不必要的能量消耗。当前基于阈值的报警系统是被动的，存在频繁误报和不一致的停机原因标签等挑战。为解决这些局限性，我们提出了一种基于AI的框架，利用预测分析和自动标签。通过来自2703个直线加速器设备和80个操作员标记的停机事件的数据，我们评估了最先进的深度学习架构，包括循环、注意力机制和线性模型，以预测束流停机。此外，我们还评估了一种基于随机森林的标签系统，以提供一致且带有置信度评分的停机注释。我们的研究结果突显了这些架构在束流停机预测中的优势和不足，并指出了必须解决的关键缺陷，以充分利用AI，将停机处理从被动转变为预测，最终减少停机时间并改善加速器管理中的决策。
2501.01480	 | Drift2Matrix: Kernel-Induced Self Representation for Concept Drift Adaptation in Co-evolving Time Series	 | Kunpeng Xu,Lifei Chen,Shengrui Wang	 | Drift2Matrix 是一种新型框架，利用核诱导的自我表示来适应时间序列中概念漂移的变化，通过生成捕捉共演化内在动态的表示矩阵来有效识别和适应概念漂移，提升分析模型的可靠性和准确性。	 | In the realm of time series analysis, tackling the phenomenon of concept drift poses a significant challenge. Concept drift -- characterized by the evolving statistical properties of time series data, affects the reliability and accuracy of conventional analysis models. This is particularly evident in co-evolving scenarios where interactions among variables are crucial. This paper presents Drift2Matrix, a novel framework that leverages kernel-induced self-representation for adaptive responses to concept drift in time series. Drift2Matrix employs a kernel-based learning mechanism to generate a representation matrix, encapsulating the inherent dynamics of co-evolving time series. This matrix serves as a key tool for identification and adaptation to concept drift by observing its temporal variations. Furthermore, Drift2Matrix effectively identifies prevailing patterns and offers insights into emerging trends through pattern evolution analysis. Our empirical evaluation of Drift2Matrix across various datasets demonstrates its effectiveness in handling the complexities of concept drift. This approach introduces a novel perspective in the theoretical domain of co-evolving time series analysis, enhancing adaptability and accuracy in the face of dynamic data environments.	 | 在时间序列分析领域，处理概念漂移现象提出了一个重大的挑战。概念漂移表现为时间序列数据统计特性的演变，这影响了传统分析模型的可靠性和准确性。特别是在变量相互作用至关重要的共演化场景中，这一影响尤为显著。本文提出了一种名为Drift2Matrix的新框架，该框架利用核诱导的自我表示来适应时间序列中概念漂移的变化。Drift2Matrix采用基于核的学习机制生成一个表示矩阵，该矩阵捕捉共演化时间序列的内在动态。该矩阵作为识别和适应概念漂移的关键工具，通过观察其时变特性来发挥作用。此外，Drift2Matrix能够有效识别存在的模式，并通过模式演化的分析提供新兴趋势的见解。我们在多种数据集上的实证研究表明，Drift2Matrix在处理概念漂移的复杂性方面表现出色。这种方法为共演化时间序列分析的理论领域引入了新的视角，提高了在动态数据环境中适应性和准确性。
2501.01473	 | Unraveling Indirect In-Context Learning Using Influence Functions	 | Hadi Askari,Shivanshu Gupta,Terry Tong,Fei Wang,Anshuman Chhabra,Muhao Chen	 | 本文提出了一种新的间接In-Context Learning (ICL)范式，并探讨了任务混合和嘈杂示范两种场景下的示范选择策略，通过使用影响函数作为选择工具，展示了在多个任务和嘈杂示范场景下的性能提升，证明了间接ICL的有效性和鲁棒性。	 | This work introduces a novel paradigm for generalized In-Context Learning (ICL), termed Indirect In-Context Learning. In Indirect ICL, we explore demonstration selection strategies tailored for two distinct real-world scenarios: Mixture of Tasks and Noisy Demonstrations. We systematically evaluate the effectiveness of Influence Functions (IFs) as a selection tool for these settings, highlighting the potential for IFs to better capture the informativeness of examples within the demonstration pool. For the Mixture of Tasks setting, demonstrations are drawn from 28 diverse tasks, including MMLU, BigBench, StrategyQA, and CommonsenseQA. We demonstrate that combining BertScore-Recall (BSR) with an IF surrogate model can significantly improve performance, leading to average absolute accuracy gains of 0.37\% and 1.45\% for 3-shot and 5-shot setups when compared to traditional ICL metrics. In the Noisy Demonstrations setting, we examine scenarios where demonstrations might be mislabeled. Our experiments show that reweighting traditional ICL selectors (BSR and Cosine Similarity) with IF-based selectors boosts accuracy by an average of 2.90\% for Cosine Similarity and 2.94\% for BSR on noisy GLUE benchmarks. In sum, we propose a robust framework for demonstration selection that generalizes beyond traditional ICL, offering valuable insights into the role of IFs for Indirect ICL.	 | 本文介绍了一种泛化的In-Context Learning (ICL)的新范式，称为间接In-Context Learning（Indirect ICL）。在间接ICL中，我们探讨了两种不同的实际场景下的示范选择策略：任务混合和嘈杂的示范。我们系统地评估了影响函数（IFs）作为这些设置下的选择工具的有效性，强调了IFs在捕获示范池中例证的信息性方面的潜力。对于任务混合设置，示范是从28个不同的任务中抽取的，包括MMLU、BigBench、StrategyQA和CommonsenseQA。我们展示了结合BertScore-Recall (BSR)和基于IF的代理模型可以显著提高性能，在3-shot和5-shot设置中，与传统的ICL指标相比，平均绝对准确率提高了0.37%和1.45%。在嘈杂的示范设置中，我们研究了示范可能被误标的情况。我们的实验表明，用基于IF的选择器重新加权传统的ICL选择器（BSR和余弦相似度）可以在嘈杂的GLUE基准上分别提高余弦相似度3.90%和BSR 2.94%的准确率。总之，我们提出了一种稳健的示范选择框架，超越了传统的ICL，为间接ICL提供了IFs角色的重要见解。
2501.01472	 | Augmented Contrastive Clustering with Uncertainty-Aware Prototyping for Time Series Test Time Adaptation	 | Peiliang Gong,Mohamed Ragab,Min Wu,Zhenghua Chen,Yongyi Su,Xiaoli Li,Daoqiang Zhang	 | 该研究提出了一种名为ACCUP的方法，通过增强对比聚类和不确定性感知原型等技术，实现对时间序列数据的测试时自适应，有效提升了在时间序列数据上的适应性能和泛化能力。	 | Test-time adaptation aims to adapt pre-trained deep neural networks using solely online unlabelled test data during inference. Although TTA has shown promise in visual applications, its potential in time series contexts remains largely unexplored. Existing TTA methods, originally designed for visual tasks, may not effectively handle the complex temporal dynamics of real-world time series data, resulting in suboptimal adaptation performance. To address this gap, we propose Augmented Contrastive Clustering with Uncertainty-aware Prototyping (ACCUP), a straightforward yet effective TTA method for time series data. Initially, our approach employs augmentation ensemble on the time series data to capture diverse temporal information and variations, incorporating uncertainty-aware prototypes to distill essential characteristics. Additionally, we introduce an entropy comparison scheme to selectively acquire more confident predictions, enhancing the reliability of pseudo labels. Furthermore, we utilize augmented contrastive clustering to enhance feature discriminability and mitigate error accumulation from noisy pseudo labels, promoting cohesive clustering within the same class while facilitating clear separation between different classes. Extensive experiments conducted on three real-world time series datasets and an additional visual dataset demonstrate the effectiveness and generalization potential of the proposed method, advancing the underexplored realm of TTA for time series data.	 | 测试时自适应的目标是在推断过程中仅使用在线未标记的测试数据来适应预训练的深度神经网络。尽管测试时自适应在视觉应用中显示出了一定的潜力，但在时间序列上下文中的应用潜力仍基本未被探索。现有的测试时自适应方法，最初是为视觉任务设计的，可能无法有效处理真实世界时间序列数据中的复杂时间动态，导致适应性能不佳。为了解决这一差距，我们提出了一种名为Augmented Contrastive Clustering with Uncertainty-aware Prototyping (ACCUP)的方法，这是一种简单且有效的测试时自适应方法，适用于时间序列数据。首先，我们的方法通过时间序列数据的增强组合，捕捉多样化的时序信息和变化，并结合不确定性感知原型来提炼关键特征。此外，我们引入了熵对比方案，以选择性地获取更具信心的预测，增强伪标签的可靠性。进一步地，我们使用增强对比聚类来增强特征的可区分性，减少来自嘈杂伪标签的误差累积，促进同一类别的凝聚聚类，并促进不同类别的清晰分离。在三个真实世界的时间序列数据集和一个额外的视觉数据集上进行的广泛实验表明，所提出的方法的有效性和泛化潜力，推进了时间序列数据中测试时自适应的未探索领域。
2501.01470	 | Balance-aware Sequence Sampling Makes Multi-modal Learning Better	 | Zhi-Hao Guan	 | 本文提出了一种名为平衡感知序列采样（BSS）的新方法，通过定义多视角测度器评估样本平衡性，并结合逐步学习和动态概率采样策略，重新平衡多模态学习中的模态不平衡问题，从而提高模型的鲁棒性。实验结果表明，该方法在多个数据集上优于现有的先进多模态学习方法。	 | To address the modality imbalance caused by data heterogeneity, existing multi-modal learning (MML) approaches primarily focus on balancing this difference from the perspective of optimization objectives. However, almost all existing methods ignore the impact of sample sequences, i.e., an inappropriate training order tends to trigger learning bias in the model, further exacerbating modality imbalance. In this paper, we propose Balance-aware Sequence Sampling (BSS) to enhance the robustness of MML. Specifically, we first define a multi-perspective measurer to evaluate the balance degree of each sample. Via the evaluation, we employ a heuristic scheduler based on curriculum learning (CL) that incrementally provides training subsets, progressing from balanced to imbalanced samples to rebalance MML. Moreover, considering that sample balance may evolve as the model capability increases, we propose a learning-based probabilistic sampling method to dynamically update the training sequences at the epoch level, further improving MML performance. Extensive experiments on widely used datasets demonstrate the superiority of our method compared with state-of-the-art (SOTA) MML approaches.	 | 为了应对由数据异质性引起的模态不平衡问题，现有的多模态学习（MML）方法主要从优化目标的角度来平衡这种差异。然而，几乎所有的现有方法都忽视了样本序列的影响，即不合适的训练顺序往往会引发模型的学习偏差，进一步加剧了模态不平衡的问题。本文我们提出了平衡感知序列采样（BSS），以增强MML的鲁棒性。具体来说，我们首先定义了一种多视角测度器来评估每个样本的平衡程度。通过这种评估，我们采用基于逐步学习（CL）的启发式调度器，逐步提供训练子集，从平衡样本过渡到不平衡样本，以重新平衡MML。此外，考虑到随着模型能力的提升，样本平衡可能会发生变化，我们提出了一种基于学习的概率采样方法，在每个epoch级别动态更新训练序列，进一步提高MML的性能。在广泛使用的数据集上进行的大量实验表明，我们的方法在与最先进的（SOTA）MML方法相比时具有优越性。
2501.01463	 | Goal Recognition using Actor-Critic Optimization	 | Ben Nageris,Felipe Meneguzzi,Reuth Mirsky	 | DRACO是一种基于深度强化学习的目标识别方法，通过学习策略网络和引入新的评价指标，能够在离散和连续设置中超越现有方法，提升目标识别性能并减少计算与内存成本。	 | Goal Recognition aims to infer an agent's goal from a sequence of observations. Existing approaches often rely on manually engineered domains and discrete representations. Deep Recognition using Actor-Critic Optimization (DRACO) is a novel approach based on deep reinforcement learning that overcomes these limitations by providing two key contributions. First, it is the first goal recognition algorithm that learns a set of policy networks from unstructured data and uses them for inference. Second, DRACO introduces new metrics for assessing goal hypotheses through continuous policy representations. DRACO achieves state-of-the-art performance for goal recognition in discrete settings while not using the structured inputs used by existing approaches. Moreover, it outperforms these approaches in more challenging, continuous settings at substantially reduced costs in both computing and memory. Together, these results showcase the robustness of the new algorithm, bridging traditional goal recognition and deep reinforcement learning.	 | 目标识别旨在从观测序列中推断出代理的目标。现有的方法通常依赖于手工设计的领域和离散表示。深度识别利用演员-评论家优化（DRACO）是一种基于深度强化学习的新方法，通过提供两个关键贡献克服了这些限制。首先，它是一类从非结构化数据中学习一组策略网络并用于推理的第一个目标识别算法。其次，DRACO引入了新的评估目标假设的指标，通过连续策略表示。在不使用现有方法使用的结构化输入的情况下，DRACO在离散设置中的目标识别性能达到了最先进的水平。此外，在更具挑战性的连续设置中，DRACO还能在计算和内存成本显著降低的情况下超越现有方法。这些结果共同展示了新算法的稳健性，将其传统的目标识别与深度强化学习联系起来。
2501.01462	 | Pan-infection Foundation Framework Enables Multiple Pathogen Prediction	 | Lingrui Zhang,Haonan Wu,Nana Jin,Chenqing Zheng,Jize Xie,Qitai Cai,Jun Wang,Qin Cao,Xubin Zheng,Jiankun Wang,Lixin Cheng	 | 该研究构建了一个基于大规模感染宿主反应转录组数据的泛感染诊断模型，并通过知识蒸馏技术开发了针对特定病原体的轻量化模型，提高了诊断准确性，并能在临床环境中灵活部署。	 | Host-response-based diagnostics can improve the accuracy of diagnosing bacterial and viral infections, thereby reducing inappropriate antibiotic prescriptions. However, the existing cohorts with limited sample size and coarse infections types are unable to support the exploration of an accurate and generalizable diagnostic model. Here, we curate the largest infection host-response transcriptome data, including 11,247 samples across 89 blood transcriptome datasets from 13 countries and 21 platforms. We build a diagnostic model for pathogen prediction starting from a pan-infection model as foundation (AUC = 0.97) based on the pan-infection dataset. Then, we utilize knowledge distillation to efficiently transfer the insights from this "teacher" model to four lightweight pathogen "student" models, i.e., staphylococcal infection (AUC = 0.99), streptococcal infection (AUC = 0.94), HIV infection (AUC = 0.93), and RSV infection (AUC = 0.94), as well as a sepsis "student" model (AUC = 0.99). The proposed knowledge distillation framework not only facilitates the diagnosis of pathogens using pan-infection data, but also enables an across-disease study from pan-infection to sepsis. Moreover, the framework enables high-degree lightweight design of diagnostic models, which is expected to be adaptively deployed in clinical settings.	 | 基于宿主反应的诊断可以提高对细菌和病毒感染的诊断准确性，从而减少不适当的抗生素处方。然而，现有的样本量有限且感染类型粗糙的群体无法支持对准确且可泛化的诊断模型的探索。在这里，我们整理了最大的感染宿主反应转录组数据集，包括来自13个国家和21个平台的89个血液转录组数据集中的11,247个样本。我们以泛感染模型为基础构建了一个预测病原体的诊断模型（AUC = 0.97）。然后，我们利用知识蒸馏高效地将“教师”模型中的洞察力转移到四个轻量级的“学生”模型中，即金黄色葡萄球菌感染（AUC = 0.99）、链球菌感染（AUC = 0.94）、HIV感染（AUC = 0.93）、RSV感染（AUC = 0.94），以及脓毒症“学生”模型（AUC = 0.99）。所提出的知识蒸馏框架不仅促进了使用泛感染数据进行病原体诊断，还能够从泛感染到脓毒症进行跨疾病研究。此外，该框架允许诊断模型的高度轻量化设计，这有望在临床环境中灵活部署。
2501.01458	 | GAN-TAT: A Novel Framework Using Protein Interaction Networks in Druggable Gene Identification	 | George Yuanji Wang,Srisharan Murugesan,Aditya Prince Rohatgi	 | 研究人员提出了一种名为 GAN-TAT 的新框架，利用先进的图嵌入技术直接集成蛋白质相互作用网络，从而提高可成药基因的推断精度；实验结果表明，GAN-TAT 在多个数据集上表现优异，且其预测得到了临床证据的支持。	 | Identifying druggable genes is essential for developing effective pharmaceuticals. With the availability of extensive, high-quality data, computational methods have become a significant asset. Protein Interaction Network (PIN) is valuable but challenging to implement due to its high dimensionality and sparsity. Previous methods relied on indirect integration, leading to resolution loss. This study proposes GAN-TAT, a framework utilizing an advanced graph embedding technology, ImGAGN, to directly integrate PIN for druggable gene inference work. Tested on three Pharos datasets, GAN-TAT achieved the highest AUC-ROC score of 0.951 on Tclin. Further evaluation shows that GAN-TAT's predictions are supported by clinical evidence, highlighting its potential practical applications in pharmacogenomics. This research represents a methodological attempt with the direct utilization of PIN, expanding potential new solutions for developing drug targets. The source code of GAN-TAT is available at (https://github.com/george-yuanji-wang/GAN-TAT).	 | 识别可成药基因是开发有效药物的关键。随着大量高质量数据的可用性，计算方法已成为重要的工具。蛋白质相互作用网络（PIN）虽然有价值，但由于其高维度和稀疏性，实现起来颇具挑战。以往的方法依赖于间接集成，导致分辨率损失。本研究提出了一种名为 GAN-TAT 的框架，利用先进的图嵌入技术 ImGAGN 直接集成 PIN，进行可成药基因推断工作。在三个 Pharos 数据集上测试后，GAN-TAT 在 Tclin 上获得了最高的 AUC-ROC 分数 0.951。进一步的评估显示，GAN-TAT 的预测得到了临床证据的支持，突显了其在药理基因组学中的潜在实际应用价值。这项研究代表了一种方法论上的尝试，通过直接利用 PIN 扩展了潜在的新解决方案，以开发药物靶标。GAN-TAT 的源代码可在以下链接获取：(https://github.com/george-yuanji-wang/GAN-TAT)。
2501.01457	 | Reinforcing Thinking through Reasoning-Enhanced Reward Models	 | Diji Yang,Linda Zeng,Kezhen Chen,Yi Zhang	 | 本文提出了“提炼-强化-推理”（DRR）框架，通过利用大型语言模型（LLMs）自身的推理过程生成合成行为数据，并使用轻量级的判别奖励模型辅助决策，从而解决了LLMs在复杂多步推理和停止策略上的挑战，实验表明该框架在多个基准测试中优于传统自我批判方法。	 | Large Language Models (LLMs) exhibit great potential in complex multi-step reasoning through inference-time thinking but still struggle with deciding when to stop thinking due to limited self-awareness about their knowledge boundaries. While human preference alignment has shown extraordinary opportunities, expensive labeling challenges adherence to scaling law. Language model self-critique, as an alternative to using human-labeled reasoning data, is questioned with its inherited biases. This work addresses these challenges by distilling the LLM's own reasoning processes into synthetic behavioral data, eliminating the need for manual labeling of intermediate steps. Building on this concept, we propose Distillation-Reinforcement-Reasoning (DRR), a three-step framework that leverages the LLM's inherent behaviors as external feedback by first generating behavioral data using the Reasoner (LLM) to reflect its reasoning capabilities, then training a lightweight discriminative reward model (DM) on behavioral data, and finally deploying the DM at inference time to assist the Reasoner's decision-making. Experiments on multiple benchmarks show that the DRR framework outperforms self-critique approaches without relying on additional complex data annotation. Benefiting from lightweight design, ease of replication, and adaptability, DRR is applicable to a wide range of LLM-centric tasks.	 | 大型语言模型（LLMs）在通过推理时展现出在复杂多步推理方面巨大的潜力，但在决定何时停止思考方面仍面临挑战，这主要是因为它们对于知识边界缺乏自我意识。虽然人类偏好对齐展示了巨大的潜力，但昂贵的标注工作却违背了规模效应定律。语言模型自我批判作为一种替代使用人类标注推理数据的方法，也因其固有的偏见而受到质疑。本文通过将LLM自身的推理过程提炼成合成行为数据，从而消除了中间步骤手动标注的需要，来应对这些挑战。在此概念的基础上，我们提出了“提炼-强化-推理”（DRR，Distillation-Reinforcement-Reasoning）框架，该框架利用LLM固有的行为作为外部反馈。具体步骤包括：首先使用Reasoner（LLM）生成行为数据以反映其推理能力；然后在行为数据上训练一个轻量级的判别奖励模型（DM）；最后在推理时部署DM来辅助Reasoner的决策。在多个基准测试上的实验表明，DRR框架在不依赖额外复杂数据标注的情况下优于自我批判方法。受益于其轻量级设计、易于复制和适应性，DRR适用于广泛的语言模型中心任务。
2501.01453	 | Geometry Matters: Benchmarking Scientific ML Approaches for Flow Prediction around Complex Geometries	 | Ali Rabeh,Ethan Herron,Aditya Balu,Soumik Sarkar,Chinmay Hegde,Adarsh Krishnamurthy,Baskar Ganapathysubramanian	 | 本研究通过对比神经算子和基于视觉变换器的基础模型，评估了几种科学机器学习方法在复杂几何形状流体流动预测中的表现，并引入了一种统一评分框架来综合评估模型的准确度、可扩展性和物理一致性。研究发现，基础模型在数据有限时表现更优，而SDF表示方法在充足数据下更优秀，但仍需解决模型的分布外泛化问题。	 | Rapid yet accurate simulations of fluid dynamics around complex geometries is critical in a variety of engineering and scientific applications, including aerodynamics and biomedical flows. However, while scientific machine learning (SciML) has shown promise, most studies are constrained to simple geometries, leaving complex, real-world scenarios underexplored. This study addresses this gap by benchmarking diverse SciML models, including neural operators and vision transformer-based foundation models, for fluid flow prediction over intricate geometries. Using a high-fidelity dataset of steady-state flows across various geometries, we evaluate the impact of geometric representations -- Signed Distance Fields (SDF) and binary masks -- on model accuracy, scalability, and generalization. Central to this effort is the introduction of a novel, unified scoring framework that integrates metrics for global accuracy, boundary layer fidelity, and physical consistency to enable a robust, comparative evaluation of model performance. Our findings demonstrate that foundation models significantly outperform neural operators, particularly in data-limited scenarios, and that SDF representations yield superior results with sufficient training data. Despite these advancements, all models struggle with out-of-distribution generalization, highlighting a critical challenge for future SciML applications. By advancing both evaluation methodologies and modeling capabilities, this work paves the way for robust and scalable ML solutions for fluid dynamics across complex geometries.	 | 在各种工程和科学应用中，如空气动力学和生物医学流体流动，快速而准确地模拟复杂几何形状周围的流体动力学至关重要。然而，尽管科学机器学习（SciML）显示出前景，大多数研究仍集中在简单几何形状上，导致复杂的真实世界场景未被充分探索。本研究通过对比多种SciML模型，包括神经算子和基于视觉变换器的基础模型，填补了这一空白，用于复杂几何形状上的流体流动预测。利用包含多种几何形状的高保真稳态流数据集，我们评估了几何表示方法——符号距离场（SDF）和二值掩码——对模型准确度、可扩展性和泛化能力的影响。本研究的核心是引入了一种新颖的统一评分框架，该框架结合了全局准确度、边界层保真度和物理一致性等度量，以实现对模型性能的稳健比较评估。研究发现，基础模型在数据有限的情况下显著优于神经算子，并且在充足的训练数据下SDF表示方法能取得更好的结果。尽管取得这些进展，所有模型在分布外泛化方面仍面临挑战，突显了未来SciML应用中的关键挑战。通过推进评估方法和建模能力的进步，本研究为复杂几何形状中的流体动力学提供了一种稳健且可扩展的机器学习解决方案奠定了基础。
2501.01908	 | Detecting and Mitigating Adversarial Attacks on Deep Learning-Based MRI Reconstruction Without Any Retraining	 | Mahdi Saberi,Chi Zhang,Mehmet Akcakaya	 | 本文提出了一种无需重新训练模型即可检测和减轻下采样磁共振成像（MRI）重建中对抗性攻击影响的新方法，该方法基于循环测量一致性理念，并通过设计新的目标函数来最小化攻击影响，实验结果表明该方法优于传统需要重新训练的缓解方法。	 | Deep learning (DL) methods, especially those based on physics-driven DL, have become the state-of-the-art for reconstructing sub-sampled magnetic resonance imaging (MRI) data. However, studies have shown that these methods are susceptible to small adversarial input perturbations, or attacks, resulting in major distortions in the output images. Various strategies have been proposed to reduce the effects of these attacks, but they require retraining and may lower reconstruction quality for non-perturbed/clean inputs. In this work, we propose a novel approach for detecting and mitigating adversarial attacks on MRI reconstruction models without any retraining. Our detection strategy is based on the idea of cyclic measurement consistency. The output of the model is mapped to another set of MRI measurements for a different sub-sampling pattern, and this synthesized data is reconstructed with the same model. Intuitively, without an attack, the second reconstruction is expected to be consistent with the first, while with an attack, disruptions are present. Subsequently, this idea is extended to devise a novel objective function, which is minimized within a small ball around the attack input for mitigation. Experimental results show that our method substantially reduces the impact of adversarial perturbations across different datasets, attack types/strengths and PD-DL networks, and qualitatively and quantitatively outperforms conventional mitigation methods that involve retraining.	 | 基于物理驱动的深度学习（Physics-driven Deep Learning, PD-DL）方法，尤其是用于重建下采样磁共振成像（MRI）数据，已成为最先进方法。然而，研究表明这些方法对小规模的对抗性输入扰动或攻击非常敏感，会导致输出图像出现重大失真。为了减少这些攻击的影响，已经提出了多种策略，但这些策略通常需要重新训练模型，从而在没有扰动/干净输入的情况下降低重建质量。在这项工作中，我们提出了一种无需重新训练即可检测和减轻MRI重建模型对抗性攻击的新型方法。我们的检测策略基于循环测量一致性（Cyclic Measurement Consistency）的理念。模型的输出被映射到另一组MRI测量数据，用于不同的下采样模式，并使用相同的模型对该合成数据进行重建。直觉上，如果没有攻击，第二次重建应与第一次重建一致；如果有攻击，则会出现破坏。随后，这一思想被扩展以设计一种新颖的目标函数，该函数在攻击输入附近的小球区域内最小化，用于减轻攻击的影响。实验结果表明，我们的方法显著减少了不同数据集、攻击类型/强度以及PD-DL网络中的对抗性扰动的影响，并在定性和定量上优于涉及重新训练的传统缓解方法。
2501.01895	 | EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation	 | Siyuan Huang,Liliang Chen,Pengfei Zhou,Shengcong Chen,Zhengkai Jiang,Yue Hu,Peng Gao,Hongsheng Li,Maoqing Yao,Guanghui Ren	 | EnerVerse是一个全面的框架，用于生成机器人操作任务中的未来空间，结合卷积和双向注意力机制以确保低级的一致性和连续性，并引入自由锚视图空间以增强观察和分析能力。该框架通过生成模型和4D高斯斑点化相结合的数据引擎管道，有效提高了机器人的泛化能力和适应性，特别是在远程机器人操作任务中显著提升了策略的预测能力。	 | We introduce EnerVerse, a comprehensive framework for embodied future space generation specifically designed for robotic manipulation tasks. EnerVerse seamlessly integrates convolutional and bidirectional attention mechanisms for inner-chunk space modeling, ensuring low-level consistency and continuity. Recognizing the inherent redundancy in video data, we propose a sparse memory context combined with a chunkwise unidirectional generative paradigm to enable the generation of infinitely long sequences. To further augment robotic capabilities, we introduce the Free Anchor View (FAV) space, which provides flexible perspectives to enhance observation and analysis. The FAV space mitigates motion modeling ambiguity, removes physical constraints in confined environments, and significantly improves the robot's generalization and adaptability across various tasks and settings. To address the prohibitive costs and labor intensity of acquiring multi-camera observations, we present a data engine pipeline that integrates a generative model with 4D Gaussian Splatting (4DGS). This pipeline leverages the generative model's robust generalization capabilities and the spatial constraints provided by 4DGS, enabling an iterative enhancement of data quality and diversity, thus creating a data flywheel effect that effectively narrows the sim-to-real gap. Finally, our experiments demonstrate that the embodied future space generation prior substantially enhances policy predictive capabilities, resulting in improved overall performance, particularly in long-range robotic manipulation tasks.	 | 我们介绍了EnerVerse，一个全面的框架，专门用于生成机器人操作任务中的未来空间，以实现身体化。EnerVerse无缝地结合了卷积和双向注意力机制，用于内部片段空间建模，确保低级的一致性和连续性。考虑到视频数据中的内在冗余性，我们提出了一种稀疏内存上下文结合片段级单向生成范式，以生成无限长的序列。为了进一步增强机器人的能力，我们引入了自由锚视图（FAV）空间，它可以提供灵活的视角，从而增强观察和分析。FAV空间减轻了运动建模的歧义性，消除了受限制环境中的物理约束，并大幅提高了机器人在各种任务和环境中的泛化能力和适应性。为了应对多摄像头观测获取的高成本和劳动密集性，我们提出了一种数据引擎管道，结合了生成模型和4D高斯斑点化（4DGS）。该管道利用生成模型的稳健泛化能力和4DGS提供的空间约束，实现数据质量与多样性的逐步增强，从而形成数据飞轮效应，有效缩小了仿真到现实的差距。最后，我们的实验表明，未来空间生成先验极大地提升了策略的预测能力，特别是在远程机器人操作任务中，总体性能得到了显著提升。
2501.01892	 | QuArch: A Question-Answering Dataset for AI Agents in Computer Architecture	 | Shvetank Prakash,Andrew Cheng,Jason Yik,Arya Tschand,Radhika Ghosal,Ikechukwu Uchendu,Jessica Quaye,Jeffrey Ma,Shreyas Grampurohit,Sofia Giannuzzi,Arnav Balyan,Fin Amin,Aadya Pipersenia,Yash Choudhary,Ankita Nayak,Amir Yazdanbakhsh,Vijay Janapa Reddi	 | QuArch 是一个包含1500个人工验证的问题-答案对的数据集，旨在评估和提升语言模型对计算机体系结构的理解能力，并揭示了闭源和开源模型在该领域的性能差距；通过微调，小型模型的准确性可提升8%，为相关研究提供了基础。	 | We introduce QuArch, a dataset of 1500 human-validated question-answer pairs designed to evaluate and enhance language models' understanding of computer architecture. The dataset covers areas including processor design, memory systems, and performance optimization. Our analysis highlights a significant performance gap: the best closed-source model achieves 84% accuracy, while the top small open-source model reaches 72%. We observe notable struggles in memory systems, interconnection networks, and benchmarking. Fine-tuning with QuArch improves small model accuracy by up to 8%, establishing a foundation for advancing AI-driven computer architecture research. The dataset and leaderboard are at https://harvard-edge.github.io/QuArch/.	 | 我们介绍了QuArch，这是一个包含1500个人工验证的问题-答案对的数据集，旨在评估和提高语言模型对计算机体系结构的理解能力。该数据集涵盖了处理器设计、存储系统和性能优化等领域。我们的分析揭示了一个显著的性能差距：最好的闭源模型准确率为84%，而领先的开源小型模型达到72%。我们注意到，在存储系统、互连网络和基准测试方面存在明显的挑战。通过使用QuArch进行微调，小型模型的准确性可以提高多达8%，为推进基于AI的计算机体系结构研究奠定了基础。数据集和排行榜可在https://harvard-edge.github.io/QuArch/获得。
2501.01840	 | Signal Recovery Using a Spiked Mixture Model	 | Paul-Louis Delacour,Sander Wahls,Jeffrey M. Spraggins,Lukasz Migas,Raf Van de Plas	 | 该研究提出了一种尖峰混合模型（SMM）及其对应的期望最大化（EM）算法，用于从噪声和随机缩放的观测中恢复信号，并证明了在低信噪比条件下的优越性能，尤其是在生物医学成像和超光谱成像等应用中能恢复传统方法未能恢复的信号。	 | We introduce the spiked mixture model (SMM) to address the problem of estimating a set of signals from many randomly scaled and noisy observations. Subsequently, we design a novel expectation-maximization (EM) algorithm to recover all parameters of the SMM. Numerical experiments show that in low signal-to-noise ratio regimes, and for data types where the SMM is relevant, SMM surpasses the more traditional Gaussian mixture model (GMM) in terms of signal recovery performance. The broad relevance of the SMM and its corresponding EM recovery algorithm is demonstrated by applying the technique to different data types. The first case study is a biomedical research application, utilizing an imaging mass spectrometry dataset to explore the molecular content of a rat brain tissue section at micrometer scale. The second case study demonstrates SMM performance in a computer vision application, segmenting a hyperspectral imaging dataset into underlying patterns. While the measurement modalities differ substantially, in both case studies SMM is shown to recover signals that were missed by traditional methods such as k-means clustering and GMM.	 | 我们引入了尖峰混合模型（SMM），以解决从大量随机缩放和噪声观测中估计信号集的问题。随后，我们设计了一个新颖的期望最大化（EM）算法来恢复SMM的所有参数。数值实验表明，在低信噪比条件下，以及在SMM适用的数据类型中，SMM在信号恢复性能方面优于传统的高斯混合模型（GMM）。SMM及其对应的EM恢复算法的广泛应用性通过将其应用于不同数据类型得到了证明。第一个案例研究是一个生物医学研究应用，利用成像质谱数据集来探索微米尺度鼠脑组织切片中的分子内容。第二个案例研究展示了SMM在计算机视觉应用中的性能，将其应用于超光谱成像数据集，将数据集分割为潜在模式。虽然测量模态有显著差异，但在两个案例研究中，SMM均能恢复传统方法（如k均值聚类和GMM）未能恢复的信号。
2501.01832	 | Time Series Language Model for Descriptive Caption Generation	 | Mohamed Trabelsi,Aidan Boyd,Jin Cao,Huseyin Uzunalioglu	 | TSLM是一种新型的时间序列语言模型，能够自动生成时间序列数据的自然语言描述，通过合成数据和跨模态检索解决数据稀缺问题，并在多种时间序列图注数据集上表现出色。	 | The automatic generation of representative natural language descriptions for observable patterns in time series data enhances interpretability, simplifies analysis and increases cross-domain utility of temporal data. While pre-trained foundation models have made considerable progress in natural language processing (NLP) and computer vision (CV), their application to time series analysis has been hindered by data scarcity. Although several large language model (LLM)-based methods have been proposed for time series forecasting, time series captioning is under-explored in the context of LLMs. In this paper, we introduce TSLM, a novel time series language model designed specifically for time series captioning. TSLM operates as an encoder-decoder model, leveraging both text prompts and time series data representations to capture subtle temporal patterns across multiple phases and generate precise textual descriptions of time series inputs. TSLM addresses the data scarcity problem in time series captioning by first leveraging an in-context prompting synthetic data generation, and second denoising the generated data via a novel cross-modal dense retrieval scoring applied to time series-caption pairs. Experimental findings on various time series captioning datasets demonstrate that TSLM outperforms existing state-of-the-art approaches from multiple data modalities by a significant margin.	 | 自动生成时间序列数据中可观察模式的代表性自然语言描述，增强了解释性，简化了分析，并提高了时间数据的跨域实用性。虽然预训练的基础模型在自然语言处理（NLP）和计算机视觉（CV）方面取得了显著进展，但它们在时间序列分析中的应用受到了数据稀缺性的阻碍。尽管已经提出了几种基于大型语言模型（LLM）的时间序列预测方法，但时间序列图注在LLM的背景下尚未得到充分探索。在本文中，我们介绍了TSLM，这是一种专为时间序列图注设计的新型时间序列语言模型。TSLM作为一种编码器-解码器模型，利用文本提示和时间序列数据表示来捕捉多个阶段中的细微时间模式，并生成时间序列输入的精确文本描述。TSLM通过首先利用上下文提示合成了合成数据，然后通过应用于时间序列-图注对的新型跨模态密集检索评分对生成数据进行去噪，解决了时间序列图注中的数据稀缺问题。在多种时间序列图注数据集上的实验结果表明，TSLM在多种数据模态的现有最佳方法中表现出显著的优势。
2501.01828	 | Age-Based Device Selection and Transmit Power Optimization in Over-the-Air Federated Learning	 | Jingyuan Liu,Zheng Chang,Ying-Chang Liang	 | 该研究提出了一种联合设备选择与传输功率优化框架，以提高空中联邦学习的性能，特别关注延时设备的参与，通过优化算法最小化加权峰年龄信息和均方误差，从而提升模型性能和公平性。	 | Recently, over-the-air federated learning (FL) has attracted significant attention for its ability to enhance communication efficiency. However, the performance of over-the-air FL is often constrained by device selection strategies and signal aggregation errors. In particular, neglecting straggler devices in FL can lead to a decline in the fairness of model updates and amplify the global model's bias toward certain devices' data, ultimately impacting the overall system performance. To address this issue, we propose a joint device selection and transmit power optimization framework that ensures the appropriate participation of straggler devices, maintains efficient training performance, and guarantees timely updates. First, we conduct a theoretical analysis to quantify the convergence upper bound of over-the-air FL under age-of-information (AoI)-based device selection. Our analysis further reveals that both the number of selected devices and the signal aggregation errors significantly influence the convergence upper bound. To minimize the expected weighted sum peak age of information, we calculate device priorities for each communication round using Lyapunov optimization and select the highest-priority devices via a greedy algorithm. Then, we formulate and solve a transmit power and normalizing factor optimization problem for selected devices to minimize the time-average mean squared error (MSE). Experimental results demonstrate that our proposed method offers two significant advantages: (1) it reduces MSE and improves model performance compared to baseline methods, and (2) it strikes a balance between fairness and training efficiency while maintaining satisfactory timeliness, ensuring stable model performance.	 | 最近，空中联邦学习（FL）因其提高通信效率的能力吸引了广泛关注。然而，空中联邦学习的性能往往受限于设备选择策略和信号聚合误差。特别是忽视了延时设备在联邦学习中的作用，可能导致模型更新的公平性下降，并加剧了全局模型对某些设备数据的偏差，最终影响整个系统性能。为了解决这一问题，我们提出了一种联合设备选择与传输功率优化框架，确保延时设备的适当参与，维持高效的训练性能，并保证及时更新。首先，我们进行理论分析以量化基于年龄信息（AoI）的设备选择下的空中联邦学习的收敛上界。我们的分析进一步揭示了选择设备的数量和信号聚合误差对收敛上界的影响。为了最小化期望加权峰年龄信息，我们使用Lyapunov优化计算每次通信轮次的设备优先级，并通过贪婪算法选择最高优先级的设备。然后，我们为选定设备制定并解决了传输功率和归一化因子优化问题，以最小化时间平均均方误差（MSE）。实验结果表明，我们的方法具有两个显著优势：（1）它减少了MSE并提高了模型性能，优于基线方法；（2）它在公平性和训练效率之间取得了平衡，同时保持了满意的及时性，确保了模型性能的稳定。
2501.01825	 | Unified Native Spaces in Kernel Methods	 | Xavier Emery,Emilio Porcu,Moreno Bevilacqua	 | 本文提出了一种统一的参数类别，将多个广为人知的正定内核纳入其中，并确定了与这些新内核相关的再生核希尔伯特空间等价的Sobolev空间，涵盖了许多特殊情形，如Matérn内核和Wendland内核及其具有孔效应的变种。	 | There exists a plethora of parametric models for positive definite kernels, and their use is ubiquitous in disciplines as diverse as statistics, machine learning, numerical analysis, and approximation theory. Usually, the kernel parameters index certain features of an associated process. Amongst those features, smoothness (in the sense of Sobolev spaces, mean square differentiability, and fractal dimensions), compact or global supports, and negative dependencies (hole effects) are of interest to several theoretical and applied disciplines. This paper unifies a wealth of well-known kernels into a single parametric class that encompasses them as special cases, attained either by exact parameterization or through parametric asymptotics. We furthermore characterize the Sobolev space that is norm equivalent to the RKHS associated with the new kernel. As a by-product, we infer the Sobolev spaces that are associated with existing classes of kernels. We illustrate the main properties of the new class, show how this class can switch from compact to global supports, and provide special cases for which the kernel attains negative values over nontrivial intervals. Hence, the proposed class of kernel is the reproducing kernel of a very rich Hilbert space that contains many special cases, including the celebrated Matérn and Wendland kernels, as well as their aliases with hole effects.	 | 存在大量参数化模型用于正定内核，这些模型在统计学、机器学习、数值分析和逼近理论等诸多学科中被广泛应用。通常，内核参数索引与相关过程相关的某些特征。这些特征中，包括Sobolev空间意义下的平滑性、均方可微性和分形维数，局部或全局支撑，以及负相关性（孔效应）等，引起了多个理论和应用学科的兴趣。本文将许多广为人知的内核统一到一个单一的参数类别中，这种类别可以将它们作为特殊情况通过精确参数化或参数渐近性获得。此外，我们还确定了与新内核相关的再生核希尔伯特空间（RKHS）等价的Sobolev空间。作为副产品，我们推断了与现有内核类别的相关Sobolev空间。我们展示了新类别的一些主要特性，展示了如何使此类别从局部支撑切换到全局支撑，并提供了内核在非平凡区间取负值的特殊情况。因此，所提出的内核类别是包含许多特殊情形的非常丰富的希尔伯特空间的再生核，其中包括著名的Matérn内核和Wendland内核及其具有孔效应的同义词。
2501.01818	 | Rerouting LLM Routers	 | Avital Shafran,Roei Schuster,Thomas Ristenpart,Vitaly Shmatikov	 | 该研究旨在评估LLM路由器在对抗攻击下的鲁棒性，发现攻击者可以通过生成特定的“混淆装置”序列，使得LLM路由器将任何查询都路由到强大的LLM，而不会影响生成质量，从而证明基于困惑度的过滤并非有效防御方法。	 | LLM routers aim to balance quality and cost of generation by classifying queries and routing them to a cheaper or more expensive LLM depending on their complexity. Routers represent one type of what we call LLM control planes: systems that orchestrate use of one or more LLMs. In this paper, we investigate routers' adversarial robustness.   We first define LLM control plane integrity, i.e., robustness of LLM orchestration to adversarial inputs, as a distinct problem in AI safety. Next, we demonstrate that an adversary can generate query-independent token sequences we call ``confounder gadgets'' that, when added to any query, cause LLM routers to send the query to a strong LLM.   Our quantitative evaluation shows that this attack is successful both in white-box and black-box settings against a variety of open-source and commercial routers, and that confounding queries do not affect the quality of LLM responses. Finally, we demonstrate that gadgets can be effective while maintaining low perplexity, thus perplexity-based filtering is not an effective defense. We finish by investigating alternative defenses.	 | 下面是该摘要的中文翻译：  LLM路由器旨在通过根据查询的复杂性将查询路由到较便宜或较昂贵的LLM来平衡生成的质量和成本。路由器代表我们称为LLM控制平面的一种类型：一种协调一个或多个LLM使用的系统。在本文中，我们研究了路由器的对抗鲁棒性。首先，我们将LLM控制平面的完整性定义为对抗输入下的LLM协调的鲁棒性，视为AI安全中的一个独立问题。接着，我们展示了攻击者可以生成查询无关的标记序列，我们称之为“混淆装置”，这些装置在添加到任何查询时，会导致LLM路由器将查询路由到一个强大的LLM。我们的定量评估表明，这种攻击在白盒和黑盒环境中都能成功地针对多种开源和商用路由器，并且混淆查询不会影响LLM的响应质量。最后，我们展示了这些装置可以在保持低困惑度的同时有效工作，因此基于困惑度的过滤并非有效的防御方法。最后，我们探讨了其他防御方案。
2501.01811	 | QuantumBind-RBFE: Accurate Relative Binding Free Energy Calculations Using Neural Network Potentials	 | Francesc Sabanés Zariquiey,Stephen E. Farr,Stefan Doerr,Gianni De Fabritiis	 | 本文利用一种基于TensorNet的新神经网络势模型AceForce 1.0，提高了药物发现中配体结合亲和力的预测准确性，并展示了比传统分子力学方法更好的性能，同时运行速度也显著提升。	 | Accurate prediction of protein-ligand binding affinities is crucial in drug discovery, particularly during hit-to-lead and lead optimization phases, however, limitations in ligand force fields continue to impact prediction accuracy. In this work, we validate relative binding free energy (RBFE) accuracy using neural network potentials (NNPs) for the ligands. We utilize a novel NNP model, AceForce 1.0, based on the TensorNet architecture for small molecules that broadens the applicability to diverse drug-like compounds, including all important chemical elements and supporting charged molecules. Using established benchmarks, we show overall improved accuracy and correlation in binding affinity predictions compared with GAFF2 for molecular mechanics and ANI2-x for NNPs. Slightly less accuracy but comparable correlations with OPLS4. We also show that we can run the NNP simulations at 2 fs timestep, at least two times larger than previous NNP models, providing significant speed gains. The results show promise for further evolutions of free energy calculations using NNPs while demonstrating its practical use already with the current generation. The code and NNP model are publicly available for research use.	 | 药物发现中的小分子配体结合亲和力的准确预测对于确定候选药物至关重要，尤其是在从“hit”到“lead”的阶段和后续优化阶段。然而，配体力场的限制继续影响预测准确性。本文中，我们利用神经网络势（NNP）验证配体的相对结合自由能（RBFE）的准确性。我们采用基于张量网络架构（TensorNet）的新型NNP模型AceForce 1.0，该模型适用于多种药物样化合物，包括所有重要化学元素，并支持带有电荷的分子。通过使用现有基准，我们展示了与GAFF2（分子力学）和ANI2-x（NNP）相比，结合亲和力预测的总体改进和更好的相关性。与OPLS4相比，虽然略有准确性降低，但相关性相当。我们还展示了可以将NNP模拟的时间步长设置为2 fs，是以往NNP模型的大约两倍，从而显著提高运行速度。结果表明，NNP在自由能计算中的进一步发展具有潜力，同时也展示了其在当前版本中的实用性。相关代码和NNP模型已公开供研究使用。
2501.01801	 | John Ellipsoids via Lazy Updates	 | David P. Woodruff,Taisuke Yasuda	 | 本文提出了一种更快的算法来近似计算$d$维空间中$n$个点的John椭球，通过采样延迟高精度杠杆得分的计算并使用快速矩形矩阵乘法加速过程，同时也提供了一种低空间流式计算John椭球的算法。	 | We give a faster algorithm for computing an approximate John ellipsoid around $n$ points in $d$ dimensions. The best known prior algorithms are based on repeatedly computing the leverage scores of the points and reweighting them by these scores [CCLY19]. We show that this algorithm can be substantially sped up by delaying the computation of high accuracy leverage scores by using sampling, and then later computing multiple batches of high accuracy leverage scores via fast rectangular matrix multiplication. We also give low-space streaming algorithms for John ellipsoids using similar ideas.	 | 我们提供了一种更快的算法，用于计算在 $d$ 维空间中有 $n$ 个点的 John 椭球的近似值。目前已知的最佳算法是反复计算点的杠杆得分并根据这些得分重新加权 [CCLY19]。我们通过使用采样延迟高精度杠杆得分的计算，并随后通过快速矩形矩阵乘法计算多个批次的高精度杠杆得分，证明了可以显著加速该算法。我们还使用类似的思想提供了低空间流式计算 John 椭球的算法。
2501.01727	 | Proposing Hierarchical Goal-Conditioned Policy Planning in Multi-Goal Reinforcement Learning	 | Gavin B. Rens	 | 该研究提出了一种结合强化学习和自动规划的方法，通过层次化的短目标条件策略（GCPs）和蒙特卡洛树搜索（MCTS）生成高阶动作（HLAs），以提高类人机器人的稀疏奖励任务学习效率。这种方法维护一个计划树，利用HLAs重用和预见未来动作来增强样本效率并加速推理。	 | Humanoid robots must master numerous tasks with sparse rewards, posing a challenge for reinforcement learning (RL). We propose a method combining RL and automated planning to address this. Our approach uses short goal-conditioned policies (GCPs) organized hierarchically, with Monte Carlo Tree Search (MCTS) planning using high-level actions (HLAs). Instead of primitive actions, the planning process generates HLAs. A single plan-tree, maintained during the agent's lifetime, holds knowledge about goal achievement. This hierarchy enhances sample efficiency and speeds up reasoning by reusing HLAs and anticipating future actions. Our Hierarchical Goal-Conditioned Policy Planning (HGCPP) framework uniquely integrates GCPs, MCTS, and hierarchical RL, potentially improving exploration and planning in complex tasks.	 | 类人机器人必须掌握大量具有稀疏奖励的任务，这对强化学习（RL）提出了挑战。我们提出了一种结合强化学习和自动规划的方法来解决这一问题。我们的方法使用层次化的短目标条件策略（GCPs），并通过高阶动作（HLAs）使用蒙特卡洛树搜索（MCTS）进行规划。规划过程生成的是HLAs，而不是基础动作。在整个代理的生命周期中维护一个计划树，其中包含了关于目标实现的知识。这种层次结构通过重用HLAs和预见未来的动作来增强样本效率并加快推理速度。我们的层次化目标条件策略规划框架（HGCPP）独特地结合了GCPs、MCTS和层次化RL，可能在复杂任务中的探索和规划方面有所改进。
2501.01716	 | Beyond Non-Degeneracy: Revisiting Certainty Equivalent Heuristic for Online Linear Programming	 | Yilun Chen,Wenjia Wang	 | 本文对确定等价启发式（CE）在在线线性规划中的性能进行了深入分析，证明了在轻微分布假设下，CE的遗憾几乎达到最优，并且无需依赖流体规律性条件，挑战了以往认为违反这些条件会导致性能下降的观点。通过开发新型算法分析技术，本文还为随机线性规划解建立了强集中分析，拓展了多秘书问题中的相关发现。	 | The Certainty Equivalent heuristic (CE) is a widely-used algorithm for various dynamic resource allocation problems in OR and OM. Despite its popularity, existing theoretical guarantees of CE are limited to settings satisfying restrictive fluid regularity conditions, particularly, the non-degeneracy conditions, under the widely held belief that the violation of such conditions leads to performance deterioration and necessitates algorithmic innovation beyond CE.   In this work, we conduct a refined performance analysis of CE within the general framework of online linear programming. We show that CE achieves uniformly near-optimal regret (up to a polylogarithmic factor in $T$) under only mild assumptions on the underlying distribution, without relying on any fluid regularity conditions. Our result implies that, contrary to prior belief, CE effectively beats the curse of degeneracy for a wide range of problem instances with continuous conditional reward distributions, highlighting the distinction of the problem's structure between discrete and non-discrete settings. Our explicit regret bound interpolates between the mild $(\log T)^2$ regime and the worst-case $\sqrt{T}$ regime with a parameter $β$ quantifying the minimal rate of probability accumulation of the conditional reward distributions, generalizing prior findings in the multisecretary setting.   To achieve these results, we develop novel algorithmic analytical techniques. Drawing tools from the empirical processes theory, we establish strong concentration analysis of the solutions to random linear programs, leading to improved regret analysis under significantly relaxed assumptions. These techniques may find potential applications in broader online decision-making contexts.	 | 确定等价启发式（CE）是一种在运筹学（OR）和运作管理（OM）中的各种动态资源分配问题中广泛使用的算法。尽管它非常流行，但目前对CE的理论保证仅限于满足严格流体规律性条件（尤其是非退化条件）的设置下。一直以来，人们普遍认为违反这些条件会导致性能下降，并需要超越CE的算法创新。  在本文中，我们对CE在在线线性规划的通用框架下的性能进行了精细的分析。我们证明，在仅对基础分布作出轻微假设的情况下，CE在时间$T$上的遗憾（遗憾损失，regret）几乎是最优的（至多在一个多项式对数因子中的差距），而不需要依赖任何流体规律性条件。我们的结果表明，与之前的信念相反，CE在一系列具有连续条件收益分布的问题实例中有效地克服了退化问题，突显了问题结构在离散和非离散设置之间的区别。我们的显式遗憾界在轻微的$(\log T)^2$区间和最坏情况$\sqrt{T}$区间之间进行插值，通过参数$β$量化条件收益分布的最小概率积累速率，从而扩展了在多秘书问题中先前的发现。  为了获得这些结果，我们开发了新型的算法分析技术。我们利用经验过程理论中的工具，建立了随机线性规划解的强集中分析，从而在显著放宽假设的情况下得到了改进的遗憾分析。这些技术可能在更广泛的在线决策场景中找到潜在的应用。
2501.01710	 | Enhancing Large Vision Model in Street Scene Semantic Understanding through Leveraging Posterior Optimization Trajectory	 | Wei-Bin Kou,Qingfeng Lin,Ming Tang,Shuai Wang,Rongguang Ye,Guangxu Zhu,Yik-Chung Wu	 | 为了提高自动驾驶感知模型的泛化能力，该研究提出使用预训练的大型视觉模型作为骨干网络，并结合下游感知头，同时引入后验优化轨迹引导的优化方案（POTGui）以加速收敛，实验结果表明这种方法在性能和收敛速度上显著优于现有方法。	 | To improve the generalization of the autonomous driving (AD) perception model, vehicles need to update the model over time based on the continuously collected data. As time progresses, the amount of data fitted by the AD model expands, which helps to improve the AD model generalization substantially. However, such ever-expanding data is a double-edged sword for the AD model. Specifically, as the fitted data volume grows to exceed the the AD model's fitting capacities, the AD model is prone to under-fitting. To address this issue, we propose to use a pretrained Large Vision Models (LVMs) as backbone coupled with downstream perception head to understand AD semantic information. This design can not only surmount the aforementioned under-fitting problem due to LVMs' powerful fitting capabilities, but also enhance the perception generalization thanks to LVMs' vast and diverse training data. On the other hand, to mitigate vehicles' computational burden of training the perception head while running LVM backbone, we introduce a Posterior Optimization Trajectory (POT)-Guided optimization scheme (POTGui) to accelerate the convergence. Concretely, we propose a POT Generator (POTGen) to generate posterior (future) optimization direction in advance to guide the current optimization iteration, through which the model can generally converge within 10 epochs. Extensive experiments demonstrate that the proposed method improves the performance by over 66.48\% and converges faster over 6 times, compared to the existing state-of-the-art approach.	 | 为了改善自动驾驶（AD）感知模型的泛化能力，车辆需要根据不断收集的数据定期更新模型。随着时间的推移，AD模型拟合的数据量不断扩展，这有助于显著提高AD模型的泛化能力。然而，这种不断增加的数据量对AD模型而言是一把双刃剑。具体来说，当拟合的数据量增长到超过AD模型的拟合能力时，AD模型容易出现欠拟合现象。为了解决这个问题，我们提出使用预训练的大型视觉模型（LVMs）作为骨干网络，并结合下游感知头来理解AD语义信息。这种设计不仅可以克服由于LVMs强大拟合能力带来的上述欠拟合问题，还能通过LVMs广泛且多样的训练数据增强感知泛化能力。另一方面，为了缓解车辆在运行LVM主干网络的同时训练感知头的计算负担，我们引入了一种后验优化轨迹（POT）引导的优化方案（POTGui）以加速收敛。具体而言，我们提出了一种POT生成器（POTGen）来提前生成后续的优化方向以引导当前的优化迭代，从而使模型一般在10个周期内即可收敛。广泛实验表明，与现有的最新方法相比，所提出的方法在性能上提高了66.48%以上，并且收敛速度提高了6倍以上。
2501.01696	 | Guaranteed Nonconvex Low-Rank Tensor Estimation via Scaled Gradient Descent	 | Tong Wu	 | 本文在张量-张量乘积和张量奇异值分解框架下，提出了一种名为 ScaledGD 的算法，用于从受污染的张量数据中高效地提取有意义的信息，并且在一定条件下能够实现线性收敛，特别适用于低秩张量的估计问题。	 | Tensors, which give a faithful and effective representation to deliver the intrinsic structure of multi-dimensional data, play a crucial role in an increasing number of signal processing and machine learning problems. However, tensor data are often accompanied by arbitrary signal corruptions, including missing entries and sparse noise. A fundamental challenge is to reliably extract the meaningful information from corrupted tensor data in a statistically and computationally efficient manner. This paper develops a scaled gradient descent (ScaledGD) algorithm to directly estimate the tensor factors with tailored spectral initializations under the tensor-tensor product (t-product) and tensor singular value decomposition (t-SVD) framework. In theory, ScaledGD achieves linear convergence at a constant rate that is independent of the condition number of the ground truth low-rank tensor for two canonical problems -- tensor robust principal component analysis and tensor completion -- as long as the level of corruptions is not too large and the sample size is sufficiently large, while maintaining the low per-iteration cost of gradient descent. To the best of our knowledge, ScaledGD is the first algorithm that provably has such properties for low-rank tensor estimation with the t-SVD decomposition. Finally, numerical examples are provided to demonstrate the efficacy of ScaledGD in accelerating the convergence rate of ill-conditioned low-rank tensor estimation in these two applications.	 | 张量为多维数据提供了一个忠实且有效的表示方式，能够传达其内在结构，在众多信号处理和机器学习问题中扮演着至关重要的角色。然而，张量数据常常伴随着任意的信号污染，包括缺失项和稀疏噪声。一个基本的挑战是如何以统计和计算效率的方式可靠地从受污染的张量数据中提取有意义的信息。本文在张量-张量乘积（t-乘积）和张量奇异值分解（t-SVD）框架下，开发了一种标度梯度下降（ScaledGD）算法，以自适应的光谱初始化来直接估计张量因子。理论上，对于两个典型的例子——张量鲁棒主成分分析和张量补全——在污染水平不太高且样本数量足够的情况下，只要真实低秩张量的条件数不是太大，ScaledGD 就能实现线性收敛，并且具有与梯度下降相当的每次迭代成本。据我们所知，ScaledGD 是第一个在 t-SVD 分解中具有此类性质的可用于低秩张量估计的算法。最后，通过数值实验展示了 ScaledGD 在这两个应用中加速条件不良低秩张量估计收敛速度的有效性。
2501.01642	 | iCBIR-Sli: Interpretable Content-Based Image Retrieval with 2D Slice Embeddings	 | Shuhei Tomoshige,Hayato Muraki,Kenichi Oishi,Hitoshi Iyatomi	 | 本文提出了一种新的脑部MRI图像内容基于图像检索（CBIR）方法，iCBIR-Sli，该方法通过使用一系列2D切片有效汇总信息，实现了高完备性、可用性、鲁棒性和互操作性，且在top-1检索性能上与现有深度学习模型相当，同时提供了高可解释性。	 | Current methods for searching brain MR images rely on text-based approaches, highlighting a significant need for content-based image retrieval (CBIR) systems. Directly applying 3D brain MR images to machine learning models offers the benefit of effectively learning the brain's structure; however, building the generalized model necessitates a large amount of training data. While models that consider depth direction and utilize continuous 2D slices have demonstrated success in segmentation and classification tasks involving 3D data, concerns remain. Specifically, using general 2D slices may lead to the oversight of pathological features and discontinuities in depth direction information. Furthermore, to the best of the authors' knowledge, there have been no attempts to develop a practical CBIR system that preserves the entire brain's structural information. In this study, we propose an interpretable CBIR method for brain MR images, named iCBIR-Sli (Interpretable CBIR with 2D Slice Embedding), which, for the first time globally, utilizes a series of 2D slices. iCBIR-Sli addresses the challenges associated with using 2D slices by effectively aggregating slice information, thereby achieving low-dimensional representations with high completeness, usability, robustness, and interoperability, which are qualities essential for effective CBIR. In retrieval evaluation experiments utilizing five publicly available brain MR datasets (ADNI2/3, OASIS3/4, AIBL) for Alzheimer's disease and cognitively normal, iCBIR-Sli demonstrated top-1 retrieval performance (macro F1 = 0.859), comparable to existing deep learning models explicitly designed for classification, without the need for an external classifier. Additionally, the method provided high interpretability by clearly identifying the brain regions indicative of the searched-for disease.	 | 当前用于搜索脑部MRI图像的方法主要依赖于基于文本的方法，突显了内容基于图像检索（CBIR）系统的显著需求。直接将3D脑部MRI图像应用于机器学习模型的优势在于能够有效地学习大脑结构；然而，构建通用模型需要大量的训练数据。虽然考虑深度方向并利用连续的2D切片的模型在涉及3D数据的分割和分类任务中已经显示出成功，但仍存在一些担忧。具体来说，使用一般2D切片可能导致病理特征和深度方向信息中的断点被忽略。此外，据作者所知，还没有尝试开发能够保留整个大脑结构信息的实用CBIR系统。在本研究中，我们提出了一种可解释的脑部MRI图像CBIR方法，命名为iCBIR-Sli（可解释的CBIR图像嵌入2D切片），这是全球首次采用一系列2D切片。iCBIR-Sli通过有效地汇总切片信息来应对使用2D切片的挑战，从而实现低维表示，具有高完备性、可用性、鲁棒性和互操作性，这些都是有效CBIR所需的重要特性。在使用公开的五个多发性痴呆和认知正常脑部MRI数据集（ADNI2/3、OASIS3/4、AIBL）进行检索评估实验中，iCBIR-Sli在top-1检索性能（宏F1 = 0.859）方面表现出色，与专门为分类设计的现有深度学习模型相当，无需外部分类器。此外，该方法通过明确标识与所搜索疾病相关的脑区提供了高可解释性。
2501.01638	 | A non-ergodic framework for understanding emergent capabilities in Large Language Models	 | Javier Marin	 | 该研究通过Stuart Kauffman的可能邻近理论（TAP）提供了一个数学框架，解释大型语言模型在资源约束下的非遍历特性如何导致其实现意外能力的相变现象，并通过实验验证了架构、训练和上下文约束在这一过程中的作用。这一框架为理解语言模型中的涌现提供了理论基础，并指导未来架构的设计。	 | Large language models have emergent capabilities that come unexpectedly at scale, but we need a theoretical framework to explain why and how they emerge. We prove that language models are actually non-ergodic systems while providing a mathematical framework based on Stuart Kauffman's theory of the adjacent possible (TAP) to explain capability emergence. Our resource-constrained TAP equation demonstrates how architectural, training, and contextual constraints interact to shape model capabilities through phase transitions in semantic space. We prove through experiments with three different language models that capacities emerge through discrete transitions guided by constraint interactions and path-dependent exploration. This framework provides a theoretical basis for understanding emergence in language models and guides the development of architectures that can guide capability emergence.	 | 大型语言模型在大规模下表现出意料之外的能力，但我们需要一个理论框架来解释它们为什么以及如何出现。我们证明语言模型实际上是非遍历系统，并基于Stuart Kauffman的可能邻近理论（TAP）提供了一个数学框架来解释能力的出现。我们的资源约束下TAP方程展示了架构、训练和上下文约束如何通过语义空间中的相变相互作用来塑造模型的能力。通过三种不同语言模型的实验，我们证明了能力通过受约束交互和路径依赖探索引导的离散转变而出现。该框架为理解语言模型中的涌现提供了理论基础，并指导能够引导能力涌现的架构的发展。
2501.01594	 | PSYCHE: A Multi-faceted Patient Simulation Framework for Evaluation of Psychiatric Assessment Conversational Agents	 | Jingoo Lee,Kyungho Lim,Young-Chul Jung,Byung-Hoon Kim	 | 本文提出了一种名为PSYCHE的新框架，旨在评估基于大型语言模型的心理评估对话代理（PACAs）在临床相关性、道德安全性、成本效益和定量评估方面的表现，通过多方面的心理健康构建来模拟患者的档案、历史和行为，并通过认证精神科医生的研究和模拟患者话语的分析进行了验证。	 | Recent advances in large language models (LLMs) have accelerated the development of conversational agents capable of generating human-like responses. Since psychiatric assessments typically involve complex conversational interactions between psychiatrists and patients, there is growing interest in developing LLM-based psychiatric assessment conversational agents (PACAs) that aim to simulate the role of psychiatrists in clinical evaluations. However, standardized methods for benchmarking the clinical appropriateness of PACAs' interaction with patients still remain underexplored. Here, we propose PSYCHE, a novel framework designed to enable the 1) clinically relevant, 2) ethically safe, 3) cost-efficient, and 4) quantitative evaluation of PACAs. This is achieved by simulating psychiatric patients based on a multi-faceted psychiatric construct that defines the simulated patients' profiles, histories, and behaviors, which PACAs are expected to assess. We validate the effectiveness of PSYCHE through a study with 10 board-certified psychiatrists, supported by an in-depth analysis of the simulated patient utterances.	 | 近年来，大型语言模型（LLMs）的进展加速了能够生成类人响应的对话代理的发展。由于心理评估通常涉及精神科医生和患者之间的复杂对话互动，因此在基于LLM的心理评估对话代理（PACAs）方面，即旨在模拟精神科医生在临床评估中的角色，出现了越来越大的兴趣。然而，量化评估PACAs与患者互动的临床适宜性的标准化方法仍然未被充分探索。在此，我们提出了一种名为PSYCHE的新框架，旨在实现PACAs的1）临床相关，2）道德安全，3）成本效益高，4）定量评估。这通过基于多方面的心理健康构建来实现，该构建定义了模拟患者的档案、历史和行为，PACAs需要对其进行评估。我们通过一项包含10名认证精神科医生的研究来验证PSYCHE的有效性，并通过模拟患者的话语的深入分析进一步支持这一研究。
2501.01593	 | BLAST: A Stealthy Backdoor Leverage Attack against Cooperative Multi-Agent Deep Reinforcement Learning based Systems	 | Yinbo Yu,Saihao Yan,Xueyu Yin,Jing Fang,Jiajia Liu	 | 本文提出了一种新颖的后门利用攻击BLAST，该攻击仅通过在单个智能体中嵌入后门来影响整个多智能体团队，通过引入对手时空行为模式作为触发器并采用单向指导方式破解奖励函数来注入后门，从而实现高攻击成功率且不影响系统性能。	 | Recent studies have shown that cooperative multi-agent deep reinforcement learning (c-MADRL) is under the threat of backdoor attacks. Once a backdoor trigger is observed, it will perform malicious actions leading to failures or malicious goals. However, existing backdoor attacks suffer from several issues, e.g., instant trigger patterns lack stealthiness, the backdoor is trained or activated by an additional network, or all agents are backdoored. To this end, in this paper, we propose a novel backdoor leverage attack against c-MADRL, BLAST, which attacks the entire multi-agent team by embedding the backdoor only in a single agent. Firstly, we introduce adversary spatiotemporal behavior patterns as the backdoor trigger rather than manual-injected fixed visual patterns or instant status and control the period to perform malicious actions. This method can guarantee the stealthiness and practicality of BLAST. Secondly, we hack the original reward function of the backdoor agent via unilateral guidance to inject BLAST, so as to achieve the \textit{leverage attack effect} that can pry open the entire multi-agent system via a single backdoor agent. We evaluate our BLAST against 3 classic c-MADRL algorithms (VDN, QMIX, and MAPPO) in 2 popular c-MADRL environments (SMAC and Pursuit), and 2 existing defense mechanisms. The experimental results demonstrate that BLAST can achieve a high attack success rate while maintaining a low clean performance variance rate.	 | 近年来的研究表明，合作多智能体深度强化学习（c-MADRL）面临着后门攻击的威胁。一旦检测到后门触发器，它将执行恶意操作，导致系统失败或实现恶意目标。然而，现有的后门攻击存在一些问题，例如，即时触发模式缺乏隐蔽性，后门通过附加网络进行训练或激活，或者所有智能体都被后门攻击。为此，在本文中，我们提出了一种针对c-MADRL的新颖后门利用攻击BLAST，它仅通过在单个智能体中嵌入后门来攻击整个多智能体团队。首先，我们引入对手时空行为模式作为后门触发器，而不是手动注入的固定视觉模式或即时状态，控制执行恶意操作的时间。这种方法可以保证BLAST的隐蔽性和实用性。其次，我们通过单向指导方式破解后门智能体的原始奖励函数来注入BLAST，以实现通过单个后门智能体撬开整个多智能体系统的效果。我们将在两种流行的c-MADRL环境中（SMAC和追捕）的3种经典c-MADRL算法（VDN、QMIX和MAPPO）以及两种现有防御机制上评估我们的BLAST。实验结果表明，BLAST能够在保持低清洁性能方差率的同时实现高攻击成功率。
2501.01579	 | Unsupervised learning for anticipating critical transitions	 | Shirin Panahi,Ling-Wei Kong,Bryan Glaz,Mulugeta Haile,Ying-Cheng Lai	 | 本文提出了一种结合变分自编码器（VAE）和循环计算的框架，用于预测复杂动力系统中的临界过渡，该方法能够从时间序列中无监督学习驱动因素并用于参数输入；该方案适用于目标系统由多个独立参数驱动或只有部分状态观测值的情况。	 | For anticipating critical transitions in complex dynamical systems, the recent approach of parameter-driven reservoir computing requires explicit knowledge of the bifurcation parameter. We articulate a framework combining a variational autoencoder (VAE) and reservoir computing to address this challenge. In particular, the driving factor is detected from time series using the VAE in an unsupervised-learning fashion and the extracted information is then used as the parameter input to the reservoir computer for anticipating the critical transition. We demonstrate the power of the unsupervised learning scheme using prototypical dynamical systems including the spatiotemporal Kuramoto-Sivashinsky system. The scheme can also be extended to scenarios where the target system is driven by several independent parameters or with partial state observations.	 | 为了预测复杂动力系统中的临界过渡，近期的参数驱动循环计算方法需要明确知道分岔参数。我们提出了一种结合变分自编码器（VAE）和循环计算的框架来应对这一挑战。具体来说，VAE 以无监督学习的方式从时间序列中检测驱动因素，提取的信息随后被用作循环计算的参数输入以预测临界过渡。我们使用时空库兹马诺维奇-西瓦什金斯基系统等原型动力系统来展示无监督学习方案的威力。该方案还可以扩展到目标系统由多个独立参数驱动或只有部分状态观测值的情况。
2501.01507	 | Transfer Learning Analysis of Variational Quantum Circuits	 | Huan-Hsin Tseng,Hsin-Yi Lin,Samuel Yen-Chi Chen,Shinjae Yoo	 | 该研究分析了变量子电路（VQC）的迁移学习能力，提出了一种从预训练VQC配置转换到新领域的方法，并通过损失边界分析研究了VQC的知识迁移机制。	 | This work analyzes transfer learning of the Variational Quantum Circuit (VQC). Our framework begins with a pretrained VQC configured in one domain and calculates the transition of 1-parameter unitary subgroups required for a new domain. A formalism is established to investigate the adaptability and capability of a VQC under the analysis of loss bounds. Our theory observes knowledge transfer in VQCs and provides a heuristic interpretation for the mechanism. An analytical fine-tuning method is derived to attain the optimal transition for adaptations of similar domains.	 | 本工作分析了变量子电路（VQC）的迁移学习。我们的框架从一个预训练的VQC配置在一个特定领域开始，计算出用于新领域所需的1参数酉子群的转换。我们建立了一种形式化的分析方法，以研究在损失边界分析下VQC的适应能力和能力。我们的理论观察了VQC中的知识迁移，并为机制提供了启发式的解释。我们推导出一种分析方法，以获得相似领域适应的最佳转换。
2501.01496	 | ORACLE: A Real-Time, Hierarchical, Deep-Learning Photometric Classifier for the LSST	 | Ved G. Shah,Alex Gagliano,Konstantin Malanchev,Gautham Narayan, TheLSST Dark Energy Science Collaboration	 | ORACLE 是一个分层深度学习模型，结合了门控递归单元和上下文信息，能够实时、高置信度地对瞬变和变星进行分类；经过训练后，ORACLE 在首次检测后的短时间内便能达到顶级分类精度，并在较长光曲线数据收集后进一步提升分类准确性。	 | We present ORACLE, the first hierarchical deep-learning model for real-time, context-aware classification of transient and variable astrophysical phenomena. ORACLE is a recurrent neural network with Gated Recurrent Units (GRUs), and has been trained using a custom hierarchical cross-entropy loss function to provide high-confidence classifications along an observationally-driven taxonomy with as little as a single photometric observation. Contextual information for each object, including host galaxy photometric redshift, offset, ellipticity and brightness, is concatenated to the light curve embedding and used to make a final prediction. Training on $\sim$0.5M events from the Extended LSST Astronomical Time-Series Classification Challenge, we achieve a top-level (Transient vs Variable) macro-averaged precision of 0.96 using only 1 day of photometric observations after the first detection in addition to contextual information, for each event; this increases to $>$0.99 once 64 days of the light curve has been obtained, and 0.83 at 1024 days after first detection for 19-way classification (including supernova sub-types, active galactic nuclei, variable stars, microlensing events, and kilonovae). We also compare ORACLE with other state-of-the-art classifiers and report comparable performance for the 19-way classification task, in addition to delivering accurate top-level classifications much earlier. The code and model weights used in this work are publicly available at our associated GitHub repository (https://github.com/uiucsn/ELAsTiCC-Classification).	 | 我们提出了ORACLE，这是首个用于实时、基于上下文分类瞬变和变星天体物理现象的分层深度学习模型。ORACLE 是一个带有门控递归单元（GRU）的递归神经网络，并通过自定义的分层交叉熵损失函数进行了训练，可以在仅使用一次光度观测的情况下提供高置信度分类。对于每个天体，其宿主星系的光度红移、偏移量、椭圆度和亮度等上下文信息都会被连接到光曲线嵌入中，用于做出最终预测。在使用约0.5百万事件（来自扩展的LSST天体时间序列分类挑战）进行训练后，我们仅在首次检测后的1天内使用光度观测数据加上下文信息便实现了顶级（瞬变 vs 变星）的宏平均精度为0.96；当获取64天的光曲线数据后，这一精度提升至超过0.99；而在首次检测后1024天，对于19类分类（包括超新星亚类型、活跃星系核、变星、微透镜事件和千新星），准确分类的精度为0.83。我们还将ORACLE与其他最先进的分类器进行了比较，在19类分类任务上表现出相当的性能，并且能够提前提供准确的顶级分类。我们在此工作中使用的代码和模型权重已在我们相关的GitHub仓库（https://github.com/uiucsn/ELAsTiCC-Classification）中公开。
2501.01484	 | Sequencing Silicates in the IRS Debris Disk Catalog I: Methodology for Unsupervised Clustering	 | Cicero X. Lu,Tushar Mittal,Christine H. Chen,Alexis Y. Li,Kadin Worthen,B. A. Sargent,Carey M. Lisse,G. C. Sloan,Dean C. Hines,Dan M. Watson,Isabel Rebollido,Bin B. Ren,Joel D. Green	 | 该研究介绍了CLUES（CLustering UnsupErvised with Sequencer）工具，这是一种新颖的非参数、可解释的机器学习方法，用于分析和分类尘埃盘的光谱数据，揭示了原行星盘中的矿物多样性和地质物理过程；通过Spitzer太空望远镜的观测数据，CLUES工具帮助探索了尘埃盘的广阔参数空间。	 | Debris disks, which consist of dust, planetesimals, planets, and gas, offer a unique window into the mineralogical composition of their parent bodies, especially during the critical phase of terrestrial planet formation spanning 10 to a few hundred million years. Observations from the $\textit{Spitzer}$ Space Telescope have unveiled thousands of debris disks, yet systematic studies remain scarce, let alone those with unsupervised clustering techniques. This study introduces $\texttt{CLUES}$ (CLustering UnsupErvised with Sequencer), a novel, non-parametric, fully-interpretable machine-learning spectral analysis tool designed to analyze and classify the spectral data of debris disks. $\texttt{CLUES}$ combines multiple unsupervised clustering methods with multi-scale distance measures to discern new groupings and trends, offering insights into compositional diversity and geophysical processes within these disks. Our analysis allows us to explore a vast parameter space in debris disk mineralogy and also offers broader applications in fields such as protoplanetary disks and solar system objects. This paper details the methodology, implementation, and initial results of $\texttt{CLUES}$, setting the stage for more detailed follow-up studies focusing on debris disk mineralogy and demographics.	 | 尘埃盘由尘埃、类行星体、行星和气体组成，提供了了解其母体矿物组成的独特窗口，特别是在持续约10到几亿年的类地行星形成关键阶段。Spitzer 空间望远镜的观测已经揭示了成千上万个尘埃盘，然而系统性的研究仍然很少，更不用说使用无监督聚类技术的研究了。本研究介绍了一种新颖的、非参数的、完全可解释的机器学习光谱分析工具——CLUES（CLustering UnsupErvised with Sequencer），用于分析和分类尘埃盘的光谱数据。CLUES 结合了多种无监督聚类方法和多尺度距离度量来识别新的分组和趋势，提供关于这些盘中成分多样性和地质物理过程的见解。我们的分析使我们能够探索尘埃盘矿物学的广阔参数空间，并为原行星盘和太阳系天体等领域提供了更广泛的应用。本文详细阐述了CLUES的方法、实现以及初步结果，为后续更详细的尘埃盘矿物学和统计研究奠定了基础。
2501.01482	 | An unsupervised method for MRI recovery: Deep image prior with structured sparsity	 | Muhammad Ahmad Sultan,Chong Chen,Yingmin Liu,Katarzyna Gil,Karolina Zareba,Rizwan Ahmad	 | 该研究提出了一种名为DISCUS的无监督MRI重建方法，通过引入结构稀疏性改进深度图像先验，能够在不完全采样k空间数据的情况下有效重建图像，并在多项模拟和实际数据测试中表现出色。	 | Objective: To propose and validate an unsupervised MRI reconstruction method that does not require fully sampled k-space data. Materials and Methods: The proposed method, deep image prior with structured sparsity (DISCUS), extends the deep image prior (DIP) by introducing group sparsity to frame-specific code vectors, enabling the discovery of a low-dimensional manifold for capturing temporal variations. \discus was validated using four studies: (I) simulation of a dynamic Shepp-Logan phantom to demonstrate its manifold discovery capabilities, (II) comparison with compressed sensing and DIP-based methods using simulated single-shot late gadolinium enhancement (LGE) image series from six distinct digital cardiac phantoms in terms of normalized mean square error (NMSE) and structural similarity index measure (SSIM), (III) evaluation on retrospectively undersampled single-shot LGE data from eight patients, and (IV) evaluation on prospectively undersampled single-shot LGE data from eight patients, assessed via blind scoring from two expert readers. Results: DISCUS outperformed competing methods, demonstrating superior reconstruction quality in terms of NMSE and SSIM (Studies I--III) and expert reader scoring (Study IV). Discussion: An unsupervised image reconstruction method is presented and validated on simulated and measured data. These developments can benefit applications where acquiring fully sampled data is challenging.	 | 目的：提出并验证一种无需完全采样 k-空间数据的无监督 MRI 重建方法。材料与方法：提出的方法为带结构稀疏性的深度图像先验（DISCUS），该方法在深度图像先验（DIP）的基础上引入组稀疏性，对特定框架代码向量进行约束，从而发现一个低维流形来捕捉时间变化。DISCUS 在四项研究中得到了验证：（I）通过模拟动态 Shepp-Logan 幻影以展示其流形发现能力，（II）通过比较压缩感知和基于 DIP 的方法，使用六种不同数字心脏幻影模拟的单次快照延迟钆增强（LGE）图像系列的归一化均方误差（NMSE）和结构相似性指数测量值（SSIM），（III）对八名患者的回顾性欠采样单次快照 LGE 数据进行评估，（IV）对八名患者的前瞻性欠采样单次快照 LGE 数据进行评估，通过两名专家读者的盲评分进行评估。结果：DISCUS 在 NMSE 和 SSIM（研究 I 至 III）以及专家读者评分（研究 IV）方面优于其他方法，显示出更高的重建质量。讨论：提出了一种无监督图像重建方法，并在模拟和实际数据上进行了验证。这些进展可以惠及那些难以获取完全采样数据的应用场景。
2501.01478	 | Enhancing Reasoning through Process Supervision with Monte Carlo Tree Search	 | Shuangtao Li,Shuaihao Dong,Kexin Luan,Xinhan Di,Chaofan Ding	 | 本文提出了一种使用蒙特卡罗树搜索生成过程监督数据的方法，通过LLMs自身抽样并评估推理步骤的“相对正确性”进行训练，从而提升其推理能力，并在数学推理任务上取得了显著效果，展示了增强的推理能力具有较好的可转移性。	 | Large language models (LLMs) have demonstrated their remarkable capacity across a variety of tasks. However, reasoning remains a challenge for LLMs. To improve LLMs' reasoning ability, process supervision has proven to be better than outcome supervision. In this work, we study using Monte Carlo Tree Search (MCTS) to generate process supervision data with LLMs themselves for training them. We sample reasoning steps with an LLM and assign each step a score that captures its "relative correctness," and the LLM is then trained by minimizing weighted log-likelihood of generating the reasoning steps. This generate-then-train process is repeated iteratively until convergence.Our experimental results demonstrate that the proposed methods considerably improve the performance of LLMs on two mathematical reasoning datasets. Furthermore, models trained on one dataset also exhibit improved performance on the other, showing the transferability of the enhanced reasoning ability.	 | 大型语言模型（LLMs）在多种任务中展现了其卓越的能力。然而，推理仍然是LLMs面临的挑战。为了提高LLMs的推理能力，过程监督被证明比结果监督更有效。在本文中，我们研究使用蒙特卡罗树搜索（MCTS）来生成过程监督数据，并利用LLMs本身进行训练。我们使用LLMs抽样推理步骤，并为每个步骤分配一个反映其“相对正确性”的得分，然后通过最小化生成推理步骤的加权对数似然来进行训练。这一生成然后训练的过程会迭代重复，直到收敛。我们的实验结果表明，所提出的方法显著提高了LLMs在两个数学推理数据集上的性能。此外，训练于一个数据集上的模型在另一个数据集上也表现出改进的性能，这表明增强的推理能力具有可转移性。
2501.01464	 | Estimation of 3T MR images from 1.5T images regularized with Physics based Constraint	 | Prabhjot Kaur,Atul Singh Minhas,Chirag Kamal Ahuja,Anil Kumar Sao	 | 本文提出了一种无监督框架，用于提高1.5T MRI图像质量，通过交替最小化估计未知的高场图像和线性变换，并引入物理约束的非线性函数以实现高对比度，实验结果表明该方法能获得优于现有方法的图像质量，并且在组织分割和体积量化方面表现出更好性能。	 | Limited accessibility to high field MRI scanners (such as 7T, 11T) has motivated the development of post-processing methods to improve low field images. Several existing post-processing methods have shown the feasibility to improve 3T images to produce 7T-like images [3,18]. It has been observed that improving lower field (LF, <=1.5T) images comes with additional challenges due to poor image quality such as the function mapping 1.5T and higher field (HF, 3T) images is more complex than the function relating 3T and 7T images [10]. Except for [10], no method has been addressed to improve <=1.5T MRI images. Further, most of the existing methods [3,18] including [10] require example images, and also often rely on pixel to pixel correspondences between LF and HF images which are usually inaccurate for <=1.5T images. The focus of this paper is to address the unsupervised framework for quality improvement of 1.5T images and avoid the expensive requirements of example images and associated image registration. The LF and HF images are assumed to be related by a linear transformation (LT). The unknown HF image and unknown LT are estimated in alternate minimization framework. Further, a physics based constraint is proposed that provides an additional non-linear function relating LF and HF images in order to achieve the desired high contrast in estimated HF image. The experimental results demonstrate that the proposed approach provides processed 1.5T images, i.e., estimated 3T-like images with improved image quality, and is comparably better than the existing methods addressing similar problems. The improvement in image quality is also shown to provide better tissue segmentation and volume quantification as compared to scanner acquired 1.5T images.	 | 有限的高场MRI扫描器（如7T、11T）可及性激发了开发后处理方法以提高低场图像质量的动力。已有的一些后处理方法已经证明，可以将3T图像改进为产生类似7T图像的效果 [3,18]。观察到，提升低于1.5T场强（LF, <=1.5T）图像面临额外挑战，因为1.5T及以上场强（HF, 3T）图像的功能映射比3T和7T图像之间的功能关系更为复杂 [10]。除了[10]，还没有方法专门用来改进1.5T MRI图像。此外，现有方法大多数（如[3,18] 及[10]）需要示例图像，并且通常依赖于低场和高场图像之间的像素对像素对应关系，对于低于1.5T的图像，这种对应关系往往是不准确的。本文的重点是提出一种无需示例图像和相关图像配准的无监督框架，用于提高1.5T图像的质量。假设低场和高场图像之间由线性变换（LT）相关。未知的高场图像和未知的LT在交替最小化框架中被估计。此外，提出了一种基于物理约束，提供了一种额外的非线性函数，用于低场和高场图像之间的关系，以实现所需的高对比度。实验结果表明，所提出的方法可以提供处理后的1.5T图像，即估计的3T类似图像，并且在图像质量方面优于解决类似问题的现有方法。图像质量的改善还显示出比扫描仪获取的1.5T图像更好的组织分割和体积量化结果。
2501.01460	 | GDSR: Global-Detail Integration through Dual-Branch Network with Wavelet Losses for Remote Sensing Image Super-Resolution	 | Qiwei Zhu,Kai Li,Guojing Zhang,Xiaoying Wang,Jianqiang Huang,Xilai Li	 | 该研究提出了一种新的全局-细节双分支结构（GDSR）和全局-细节重建模块（GDRM），结合Receptance Weighted Key Value（RWKV）和卷积操作，有效提高了遥感图像超分辨率的性能，同时降低了计算成本。实验结果表明，该方法在多个基准上显著优于基于变换器的方法，特别是在参数量和推理速度方面表现出色。	 | In recent years, deep neural networks, including Convolutional Neural Networks, Transformers, and State Space Models, have achieved significant progress in Remote Sensing Image (RSI) Super-Resolution (SR). However, existing SR methods typically overlook the complementary relationship between global and local dependencies. These methods either focus on capturing local information or prioritize global information, which results in models that are unable to effectively capture both global and local features simultaneously. Moreover, their computational cost becomes prohibitive when applied to large-scale RSIs. To address these challenges, we introduce the novel application of Receptance Weighted Key Value (RWKV) to RSI-SR, which captures long-range dependencies with linear complexity. To simultaneously model global and local features, we propose the Global-Detail dual-branch structure, GDSR, which performs SR reconstruction by paralleling RWKV and convolutional operations to handle large-scale RSIs. Furthermore, we introduce the Global-Detail Reconstruction Module (GDRM) as an intermediary between the two branches to bridge their complementary roles. In addition, we propose Wavelet Loss, a loss function that effectively captures high-frequency detail information in images, thereby enhancing the visual quality of SR, particularly in terms of detail reconstruction. Extensive experiments on several benchmarks, including AID, AID_CDM, RSSRD-QH, and RSSRD-QH_CDM, demonstrate that GSDR outperforms the state-of-the-art Transformer-based method HAT by an average of 0.05 dB in PSNR, while using only 63% of its parameters and 51% of its FLOPs, achieving an inference speed 2.9 times faster. Furthermore, the Wavelet Loss shows excellent generalization across various architectures, providing a novel perspective for RSI-SR enhancement.	 | 近年来，包括卷积神经网络（Convolutional Neural Networks, CNN）、变换器（Transformers）和状态空间模型（State Space Models）在内的深度神经网络在遥感图像（Remote Sensing Image, RSI）超分辨率（Super-Resolution, SR）方面取得了显著进展。然而，现有的SR方法通常忽视了全局和局部依赖性的互补关系。这些方法要么侧重于捕捉局部信息，要么优先处理全局信息，导致模型无法同时有效地捕捉全局和局部特征。此外，当应用于大规模RSI时，它们的计算成本变得难以承受。为解决这些挑战，我们引入了Receptance Weighted Key Value（RWKV）在RSI-SR中的新应用，通过线性复杂度捕获长距离依赖性。为了同时建模全局和局部特征，我们提出了全局-细节双分支结构（Global-Detail Dual-Branch Structure, GDSR），该结构通过并行RWKV和卷积操作进行SR重建，以处理大规模RSI。此外，我们引入了全局-细节重建模块（Global-Detail Reconstruction Module, GDRM）作为两个分支之间的中介，以弥合它们的功能差异。此外，我们提出了小波损失（Wavelet Loss），这是一种可以有效捕捉图像高频细节信息的损失函数，从而提高SR的视觉质量，特别是在细节重建方面。在AID、AID_CDM、RSSRD-QH和RSSRD-QH_CDM等几个基准上的广泛实验证明，GSDR在峰值信噪比（PSNR）上平均比基于变换器的方法HAT高出0.05 dB，同时仅使用其参数的63%和计算量的51%，推理速度提高了2.9倍。此外，小波损失在各种架构中表现出色，为RSI-SR增强提供了新的视角。
2501.01456	 | SS-CTML: Self-Supervised Cross-Task Mutual Learning for CT Image Reconstruction	 | Gaofeng Chen,Yaoduo Zhang,Li Huang,Pengfei Wang,Wenyu Zhang,Dong Zeng,Jianhua Ma,Ji He	 | 该研究提出了一种自我监督跨任务互学习（SS-CTML）框架，用于CT图像重建，通过利用全视图、稀疏视图和有限视图的sinogram数据，三个独立的重建任务相互学习以实现自我监督优化，实验结果表明该框架能够获得满意的CT图像重建性能。	 | Supervised deep-learning (SDL) techniques with paired training datasets have been widely studied for X-ray computed tomography (CT) image reconstruction. However, due to the difficulties of obtaining paired training datasets in clinical routine, the SDL methods are still away from common uses in clinical practices. In recent years, self-supervised deep-learning (SSDL) techniques have shown great potential for the studies of CT image reconstruction. In this work, we propose a self-supervised cross-task mutual learning (SS-CTML) framework for CT image reconstruction. Specifically, a sparse-view scanned and a limited-view scanned sinogram data are first extracted from a full-view scanned sinogram data, which results in three individual reconstruction tasks, i.e., the full-view CT (FVCT) reconstruction, the sparse-view CT (SVCT) reconstruction, and limited-view CT (LVCT) reconstruction. Then, three neural networks are constructed for the three reconstruction tasks. Considering that the ultimate goals of the three tasks are all to reconstruct high-quality CT images, we therefore construct a set of cross-task mutual learning objectives for the three tasks, in which way, the three neural networks can be self-supervised optimized by learning from each other. Clinical datasets are adopted to evaluate the effectiveness of the proposed framework. Experimental results demonstrate that the SS-CTML framework can obtain promising CT image reconstruction performance in terms of both quantitative and qualitative measurements.	 | 以下是对给定摘要的中文翻译：  监督深度学习（SDL）技术在使用配对训练数据集方面已经被广泛研究用于X射线计算机断层扫描（CT）图像重建。然而，由于在临床常规中获取配对训练数据集的难度，SDL方法仍然未能在临床实践中普遍应用。近年来，自我监督深度学习（SSDL）技术在CT图像重建研究中展现了巨大的潜力。在这项工作中，我们提出了一种自我监督跨任务互学习（SS-CTML）框架用于CT图像重建。具体地，从全视图扫描的sinogram数据中，首先提取出稀疏视图扫描和有限视图扫描的sinogram数据，这导致了三个独立的重建任务，即全视图CT（FVCT）重建、稀疏视图CT（SVCT）重建和有限视图CT（LVCT）重建。然后，为这三个重建任务构建了三个神经网络。考虑到这三个任务的最终目标都是重建高质量的CT图像，我们因此为这三个任务构建了一组跨任务互学习目标，这样，三个神经网络可以通过相互学习实现自我监督优化。临床数据集被用于评估所提出框架的有效性。实验结果表明，SS-CTML框架在定量和定性测量方面均能获得令人满意的CT图像重建性能。
2501.01447	 | Analyzing Country-Level Vaccination Rates and Determinants of Practical Capacity to Administer COVID-19 Vaccines	 | Sharika J. Hegde,Max T.M. Ng,Marcos Rios,Hani S. Mahmassani,Ying Chen,Karen Smilowitz	 | 通过排队理论和回归分析，研究发现提高基础设施和卫生设施、改善运输可达性以及参与国际合作如COVAX计划有助于提高新冠疫苗接种率，特别是在低收入和高收入国家中面临特定挑战的群体。	 | The COVID-19 vaccine development, manufacturing, transportation, and administration proved an extreme logistics operation of global magnitude. Global vaccination levels, however, remain a key concern in preventing the emergence of new strains and minimizing the impact of the pandemic's disruption of daily life. In this paper, country-level vaccination rates are analyzed through a queuing framework to extract service rates that represent the practical capacity of a country to administer vaccines. These rates are further characterized through regression and interpretable machine learning methods with country-level demographic, governmental, and socio-economic variates. Model results show that participation in multi-governmental collaborations such as COVAX may improve the ability to vaccinate. Similarly, improved transportation and accessibility variates such as roads per area for low-income countries and rail lines per area for high-income countries can improve rates. It was also found that for low-income countries specifically, improvements in basic and health infrastructure (as measured through spending on healthcare, number of doctors and hospital beds per 100k, population percent with access to electricity, life expectancy, and vehicles per 1000 people) resulted in higher vaccination rates. Of the high-income countries, those with larger 65-plus populations struggled to vaccinate at high rates, indicating potential accessibility issues for the elderly. This study finds that improving basic and health infrastructure, focusing on accessibility in the last mile, particularly for the elderly, and fostering global partnerships can improve logistical operations of such a scale. Such structural impediments and inequities in global health care must be addressed in preparation for future global public health crises.	 | 以下是该摘要的中文翻译：  新冠疫苗的研发、生产、运输和接种证明了一场具有全球规模的极端物流运作。然而，全球的疫苗接种水平仍然是防止新变种出现和减轻大流行对日常生活影响的关键问题。在本文中，通过排队理论分析了国家层面的疫苗接种率，从中提取出代表国家实际疫苗接种能力的服务率。进一步通过回归分析和可解释的机器学习方法，使用了国家层面的人口统计、政府和经济指标来描述这些服务率。模型结果显示，参与多国政府合作如COVAX可能提高接种能力。类似地，改善运输和可达性指标，如低收入国家的每平方公里道路数和高收入国家的每平方公里铁路线数，能够提高接种率。此外，对于低收入国家而言，提高基础建设和卫生设施（通过每10万人的医疗卫生支出、医生和病床数量、电力覆盖率、预期寿命以及每千人口的车辆数来衡量）能够提高接种率。对于高收入国家而言，65岁及以上人口较多的国家难以实现高接种率，这表明老年人可能面临潜在的可达性问题。本研究发现，改善基础建设和卫生设施、关注最后一公里的可达性，尤其是对老年人，以及促进全球伙伴关系可以改善这种规模的物流操作。这些结构性障碍和全球卫生保健中的不平等现象必须在未来全球公共卫生危机中予以解决。
2501.01431	 | CSI Compression using Channel Charting	 | Baptiste Chatelier(IETR, INSA Rennes, MERCE-France),Vincent Corlay(MERCE-France),Matthieu Crussière(INSA Rennes, IETR),Luc Le Magoarou(INSA Rennes, IETR)	 | 本文研究了信道图在基于任务的CSI压缩应用中的性能，并通过与基准方法的对比，展示了其良好的应用前景。	 | Reaping the benefits of multi-antenna communication systems in frequency division duplex (FDD) requires channel state information (CSI) reporting from mobile users to the base station (BS). Over the last decades, the amount of CSI to be collected has become very challenging owing to the dramatic increase of the number of antennas at BSs. To mitigate the overhead associated with CSI reporting, compressed CSI techniques have been proposed with the idea of recovering the original CSI at the BS from its compressed version sent by the mobile users. Channel charting is an unsupervised dimensionality reduction method that consists in building a radio-environment map from CSIs. Such a method can be considered in the context of the CSI compression problem, since a chart location is, by definition, a low-dimensional representation of the CSI. In this paper, the performance of channel charting for a task-based CSI compression application is studied. A comparison of the proposed method against baselines on realistic synthetic data is proposed, showing promising results.	 | 在频分双工（FDD）中利用多天线通信系统的益处需要移动用户向基站（BS）报告信道状态信息（CSI）。在过去几十年中，由于基站天线数量的急剧增加，需要收集的CSI数量变得非常具有挑战性。为了减轻与CSI报告相关的工作量，已经提出了压缩CSI技术，其想法是从移动用户发送的压缩版本中在基站端恢复原始的CSI。信道图是一种无监督的降维方法，其目的是从CSI构建无线电环境图。这种方法可以考虑在CSI压缩问题的背景下，因为信道图位置本质上是CSI的低维度表示。本文研究了信道图在基于任务的CSI压缩应用中的性能。提出了一种基于现实合成数据与基准方法的对比，结果显示有很好的前景。
