| ID | Title | Authors | Summary (zh) | Abstract (en) | Abstract (zh) | 
| --- | --- | --- | --- | --- | --- |
2501.09006	 | Improving Stability Estimates in Adversarial Explainable AI through Alternate Search Methods	 | Christopher Burger,Charles Walter	 | 该研究探讨了使用替代搜索方法来找到最小有效的扰动，以评估解释方法在对抗性攻击下的稳定性，从而提供对复杂模型解释方法脆弱性的更细微和优越的比较。	 | Advances in the effectiveness of machine learning models have come at the cost of enormous complexity resulting in a poor understanding of how they function. Local surrogate methods have been used to approximate the workings of these complex models, but recent work has revealed their vulnerability to adversarial attacks where the explanation produced is appreciably different while the meaning and structure of the complex model's output remains similar. This prior work has focused on the existence of these weaknesses but not on their magnitude. Here we explore using an alternate search method with the goal of finding minimum viable perturbations, the fewest perturbations necessary to achieve a fixed similarity value between the original and altered text's explanation. Intuitively, a method that requires fewer perturbations to expose a given level of instability is inferior to one which requires more. This nuance allows for superior comparisons of the stability of explainability methods.	 | 机器学习模型效果的提升是以巨大的复杂性为代价的，这导致人们对它们的工作原理缺乏理解。局部替代方法被用于近似这些复杂模型的工作原理，但最近的研究揭示了它们在对抗性攻击下的脆弱性：在解释结果显著不同的情况下，复杂模型输出的意义和结构保持相似。之前的研究主要关注这些弱点的存在，但没有关注它们的程度。在这里，我们探讨使用一种替代搜索方法，旨在找到最小有效的扰动——即实现原始文本和修改后文本解释固定相似度所需最少的扰动数量。直觉上，需要更少扰动就能暴露给定水平不稳定的解释方法是劣于需要更多扰动的方法的。这种细微差别允许对解释方法稳定性的比较更加优越。
2501.08995	 | VECT-GAN: A variationally encoded generative model for overcoming data scarcity in pharmaceutical science	 | Youssef Abdalla,Marrisa Taub,Eleanor Hilton,Priya Akkaraju,Alexander Milanovic,Mine Orlu,Abdul W. Basit,Michael T Cook,Tapabrata Chakraborty,David Shorthouse	 | 该研究开发了一种变分编码条件表生成对抗网络（VECT GAN），以扩充小规模和噪声药物数据集，并证明该方法在多个药物数据集上显著提高了回归模型的性能。通过在ChEMBL数据库上的预训练和知识蒸馏，这种方法为药物研究提供了强大的合成数据生成工具，展示了在实际药物开发中的应用潜力。	 | Data scarcity in pharmaceutical research has led to reliance on labour-intensive trial and error approaches for development rather than data driven methods. While Machine Learning offers a solution, existing datasets are often small and noisy, limiting their utility. To address this, we developed a Variationally Encoded Conditional Tabular Generative Adversarial Network (VECT GAN), a novel generative model specifically designed for augmenting small, noisy datasets. We introduce a pipeline where data is augmented before regression model development and demonstrate that this consistently and significantly improves performance over other state of the art tabular generative models. We apply this pipeline across six pharmaceutical datasets, and highlight its real-world applicability by developing novel polymers with medically desirable mucoadhesive properties, which we made and experimentally characterised. Additionally, we pre-train the model on the ChEMBL database of drug-like molecules, leveraging knowledge distillation to enhance its generalisability, making it readily available for use on pharmaceutical datasets containing small molecules, which is an extremely common pharmaceutical task. We demonstrate the power of synthetic data for regularising small tabular datasets, highlighting its potential to become standard practice in pharmaceutical model development, and make our method, including VECT GAN pretrained on ChEMBL available as a pip package.	 | 药物研究中数据稀缺导致了依赖于耗时的试错方法来开发药物，而不是依赖数据驱动的方法。尽管机器学习提供了解决方案，但现有的数据集通常较小且存在噪声，限制了它们的效用。为了解决这一问题，我们开发了一种变分编码条件表生成对抗网络（VECT GAN），这是一种专为扩充小规模和噪声数据集而设计的生成模型。我们提出了一种管道，其中数据在构建回归模型之前进行扩充，并证明这种方法在其他最先进的表生成模型中始终能显著提高性能。我们在此管道上应用了六个药物数据集，并通过开发具有医学上所需的黏附性质的新型聚合物，展示了其在实际应用中的可行性，我们还对这些聚合物进行了实验表征。此外，我们预先在ChEMBL药物样分子数据库上对模型进行了训练，并利用知识蒸馏来增强其泛化能力，使其能够方便地应用于包含小分子的药物数据集，这是非常常见的药物研究任务。我们展示了合成数据对规整小型表数据集的强大作用，并强调其在药物模型开发中的潜在标准应用前景。我们还提供了该方法的实现，包括在ChEMBL上预训练的VECT GAN，作为pip包提供。
2501.08963	 | Training-Aware Risk Control for Intensity Modulated Radiation Therapies Quality Assurance with Conformal Prediction	 | Kevin He,David Adam,Sarah Han-Oh,Anqi Liu	 | 该研究提出了一种新的基于训练的统一风险控制方法，旨在通过减少需要测量的计划数量来提高强度调制放射治疗（IMRT）的质量保证过程效率，同时保持高敏感性和特异性。这种方法结合了统一风险控制和统一训练的优点，并显著降低了工作量，而不会增加置信区间。	 | Measurement quality assurance (QA) practices play a key role in the safe use of Intensity Modulated Radiation Therapies (IMRT) for cancer treatment. These practices have reduced measurement-based IMRT QA failure below 1%. However, these practices are time and labor intensive which can lead to delays in patient care. In this study, we examine how conformal prediction methodologies can be used to robustly triage plans. We propose a new training-aware conformal risk control method by combining the benefit of conformal risk control and conformal training. We incorporate the decision making thresholds based on the gamma passing rate, along with the risk functions used in clinical evaluation, into the design of the risk control framework. Our method achieves high sensitivity and specificity and significantly reduces the number of plans needing measurement without generating a huge confidence interval. Our results demonstrate the validity and applicability of conformal prediction methods for improving efficiency and reducing the workload of the IMRT QA process.	 | 测量质量保证（QA）实践在确保强度调制放射治疗（IMRT）用于癌症治疗的安全使用方面起着关键作用。这些实践已将基于测量的IMRT QA失败率降低到1%以下。然而，这些实践耗时且劳动密集，可能导致患者护理延迟。在本研究中，我们探讨了如何利用一致预测方法来稳健地分拣计划。我们提出了一种新的基于训练的统一风险控制方法，结合了统一风险控制和统一训练的优点。我们根据γ通过率确定决策阈值，并将临床评估中使用的风险函数整合到风险控制框架的设计中。我们的方法实现了高敏感性和高特异性，显著减少了需要测量的计划数量，同时没有产生巨大的置信区间。我们的结果证明了统一预测方法在提高IMRT QA过程效率和减少工作量方面的有效性和适用性。
2501.08958	 | Kolmogorov-Arnold Networks for Time Series Granger Causality Inference	 | Meiliang Liu,Yunfang Xu,Zijin Li,Zhengye Si,Xiaoxiao Yang,Xinyue Yang,Zhiwen Zhao	 | GCKAN是一种将柯尔莫哥洛夫-阿诺德网络扩展到因果推理领域的创新架构，能够在时间序列中推断Granger因果性并自动选择时间滞后；结合稀疏性诱导惩罚和岭正则化，GCKAN还能通过反向时间序列的Granger因果性提升推断准确性，并在多种数据集上表现出竞争力。	 | We introduce Granger Causality Kolmogorov-Arnold Networks (GCKAN), an innovative architecture that extends the recently proposed Kolmogorov-Arnold Networks (KAN) to the domain of causal inference. By extracting base weights from KAN layers and incorporating the sparsity-inducing penalty along with ridge regularization, GCKAN infers the Granger causality from time series while enabling automatic time lag selection. Additionally, we propose an algorithm leveraging time-reversed Granger causality to enhance inference accuracy. The algorithm compares prediction and sparse-inducing losses derived from the original and time-reversed series, automatically selecting the casual relationship with the higher score or integrating the results to mitigate spurious connectivities. Comprehensive experiments conducted on Lorenz-96, gene regulatory networks, fMRI BOLD signals, and VAR datasets demonstrate that the proposed model achieves competitive performance to state-of-the-art methods in inferring Granger causality from nonlinear, high-dimensional, and limited-sample time series.	 | 我们提出了Granger因果性柯尔莫哥洛夫-阿诺德网络（GCKAN），这是一项创新的架构，将近期提出的柯尔莫哥洛夫-阿诺德网络（KAN）扩展到了因果推理领域。通过从KAN层中提取基础权重，并结合稀疏性诱导的惩罚和岭正则化，GCKAN能够在时间序列中推断Granger因果性的同时，实现自动时间滞后选择。此外，我们提出了一种算法，通过利用时间反向的Granger因果性来提升推断准确性。该算法比较了原始序列和时间反向序列中衍生的预测损失和稀疏性诱导损失，自动选择得分更高的因果关系，或者整合结果以减轻虚假连接性。在洛伦兹-96系统、基因调控网络、fMRI BOLD信号和向量自回归（VAR）数据集上的全面实验表明，所提出的模型在推断非线性、高维和样本有限的时间序列中的Granger因果性方面，与最先进的方法相比表现出了竞争力。
2501.08925	 | Disentangling Exploration of Large Language Models by Optimal Exploitation	 | Tim Grams,Patrick Betz,Christian Bartelt	 | 这项研究评估了大型语言模型在探索状态空间方面的能力，发现大多数模型在探索方面存在困难，且模型规模与探索性能正相关。研究还提出了一种新方法，将缺失的奖励分解为探索和利用的组成部分，以更好地理解代理行为的差异并改进其性能。	 | Exploration is a crucial skill for self-improvement and open-ended problem-solving. However, it remains uncertain whether large language models can effectively explore the state-space. Existing evaluations predominantly focus on the trade-off between exploration and exploitation, often assessed in multi-armed bandit problems. In contrast, this work isolates exploration as the sole objective, tasking the agent with delivering information that enhances future returns. For the evaluation, we propose to decompose missing rewards into exploration and exploitation components by measuring the optimal achievable return for the states already explored. Our experiments with various LLMs reveal that most models struggle to sufficiently explore the state-space and that weak exploration is insufficient. We observe a positive correlation between model size and exploration performance, with larger models demonstrating superior capabilities. Furthermore, we show that our decomposition provides insights into differences in behaviors driven by agent instructions during prompt engineering, offering a valuable tool for refining LLM performance in exploratory tasks.	 | 探索是自我提升和开放性问题解决的关键技能。然而，尚不清楚大型语言模型是否能够有效地探索状态空间。现有的评估主要集中在探索与利用之间的权衡，通常是在多臂老虎机问题中进行评估。相比之下，本工作将探索作为唯一目标，要求代理提供能够增强未来收益的信息。为进行评估，我们提出通过测量已探索状态的最优可实现回报来将缺失的奖励分解为探索和利用的组成部分。我们的实验表明，大多数模型在探索状态空间方面存在困难，弱探索是不够的。我们观察到模型规模与探索性能之间存在正相关关系，即更大规模的模型表现出更强的能力。此外，我们展示了我们的分解方法为理解由代理指令在提示工程过程中引起的行为差异提供了洞察，从而为改进大型语言模型在探索性任务中的性能提供了有价值的工具。
2501.08922	 | Modeling Melt Pool Features and Spatter Using Symbolic Regression and Machine Learning	 | Olabode T. Ajenifujah,Amir Barati Farimani	 | 本研究开发了一个基于机器学习和多项式符号回归的框架，用于预测激光粉末床熔化过程中的熔池尺寸和几何特征，从而提高增材制造的质量控制和工艺稳定性。通过实验验证，该方法在预测熔池参数方面取得了高精度，R²值超过95%。	 | Additive manufacturing (AM) is a rapidly evolving technology that has attracted applications across a wide range of fields due to its ability to fabricate complex geometries. However, one of the key challenges in AM is achieving consistent print quality. This inconsistency is often attributed to uncontrolled melt pool dynamics, partly caused by spatter which can lead to defects. Therefore, capturing and controlling the evolution of the melt pool is crucial for enhancing process stability and part quality. In this study, we developed a framework to support decision-making in AM operations, facilitating quality control and minimizing defects via machine learning (ML) and polynomial symbolic regression models. We implemented experimentally validated computational tools as a cost-effective approach to collect large datasets from laser powder bed fusion (LPBF) processes. For a dataset consisting of 281 process conditions, parameters such as melt pool dimensions (length, width, depth), melt pool geometry (area, volume), and volume indicated as spatter were extracted. Using machine learning (ML) and polynomial symbolic regression models, a high R2 of over 95 % was achieved in predicting the melt pool dimensions and geometry features for both the training and testing datasets, with either process conditions (power and velocity) or melt pool dimensions as the model inputs. In the case of volume indicated as spatter, R2 improved after logarithmic transforming the model inputs, which was either the process conditions or the melt pool dimensions. Among the investigated ML models, the ExtraTree model achieved the highest R2 values of 96.7 % and 87.5 %.	 | 增材制造（AM）是一种迅速发展的技术，由于其能够制造复杂几何形状的能力，已在众多领域得到了广泛应用。然而，AM的一个关键挑战是如何实现一致的打印质量。这种不一致性常常归因于不受控制的熔池动力学，部分原因是飞溅导致的缺陷。因此，捕捉和控制熔池的演化对于提高工艺稳定性和零件质量至关重要。在本研究中，我们开发了一个框架来支持AM操作中的决策制定，通过机器学习（ML）和多项式符号回归模型促进质量控制并减少缺陷。我们实施了实验验证的计算工具，作为一种成本效益高的方法，从激光粉末床熔化（LPBF）过程中收集大量数据集。对于包含281种工艺条件的数据集，我们提取了熔池尺寸（长度、宽度、深度）、熔池几何形状（面积、体积）以及表示飞溅的体积等参数。通过机器学习（ML）和多项式符号回归模型，我们在训练和测试数据集上分别实现了超过95%的高R²值来预测熔池尺寸和几何特征，输入变量为工艺条件（功率和速度）或熔池尺寸。对于表示飞溅的体积，通过将模型输入进行对数变换，R²值有所提高，输入变量为工艺条件或熔池尺寸。在研究的ML模型中，ExtraTree模型达到了最高的R²值，分别为96.7%和87.5%。
2501.08907	 | Projection Implicit Q-Learning with Support Constraint for Offline Reinforcement Learning	 | Xinchen Han,Hossam Afifi,Michel Marot	 | 本文提出了一种名为Proj-IQL的增强版隐式Q学习算法，通过引入支持约束并改进策略评估和改进阶段的方法，有效解决了离线强化学习中的外推误差问题。实验结果表明，Proj-IQL在D4RL基准测试中特别是在具有挑战性的导航领域表现优异。	 | Offline Reinforcement Learning (RL) faces a critical challenge of extrapolation errors caused by out-of-distribution (OOD) actions. Implicit Q-Learning (IQL) algorithm employs expectile regression to achieve in-sample learning, effectively mitigating the risks associated with OOD actions. However, the fixed hyperparameter in policy evaluation and density-based policy improvement method limit its overall efficiency. In this paper, we propose Proj-IQL, a projective IQL algorithm enhanced with the support constraint. In the policy evaluation phase, Proj-IQL generalizes the one-step approach to a multi-step approach through vector projection, while maintaining in-sample learning and expectile regression framework. In the policy improvement phase, Proj-IQL introduces support constraint that is more aligned with the policy evaluation approach. Furthermore, we theoretically demonstrate that Proj-IQL guarantees monotonic policy improvement and enjoys a progressively more rigorous criterion for superior actions. Empirical results demonstrate the Proj-IQL achieves state-of-the-art performance on D4RL benchmarks, especially in challenging navigation domains.	 | 离线强化学习（Offline Reinforcement Learning, RL）面临一个关键挑战，即由离分布（out-of-distribution, OOD）动作引起的外推误差。隐式Q学习（Implicit Q-Learning, IQL）算法利用分位回归实现样本内学习，有效缓解了与OOD动作相关的风险。然而，IQL在策略评估中的固定超参数以及基于密度的策略改进方法限制了其整体效率。本文中，我们提出了一种名为Proj-IQL的增强版IQL算法，加入了支持约束。在策略评估阶段，Proj-IQL通过向量投影将单步方法推广为多步方法，同时保留了样本内学习和分位回归框架。在策略改进阶段，Proj-IQL引入了与策略评估方法更一致的支持约束。此外，我们从理论上证明了Proj-IQL确保了单调的策略改进，并且具有更为严格的优秀行动评判标准。实验结果表明，Proj-IQL在D4RL基准测试中表现优异，特别是在具有挑战性的导航领域。
2501.08888	 | A Two-Stage Pretraining-Finetuning Framework for Treatment Effect Estimation with Unmeasured Confounding	 | Chuan Zhou,Yaxuan Li,Chunyuan Zheng,Haiteng Zhang,Min Zhang,Haoxuan Li,Mingming Gong	 | 本文提出了一种两阶段预训练-微调（TSPF）框架，结合大规模观察数据和小型随机对照试验（RCT）数据，以估计存在未测量混杂变量情况下的条件平均治疗效果（CATE）。该方法通过实验验证了在电商、医疗保健和经济学等领域的优越性。	 | Estimating the conditional average treatment effect (CATE) from observational data plays a crucial role in areas such as e-commerce, healthcare, and economics. Existing studies mainly rely on the strong ignorability assumption that there are no unmeasured confounders, whose presence cannot be tested from observational data and can invalidate any causal conclusion. In contrast, data collected from randomized controlled trials (RCT) do not suffer from confounding, but are usually limited by a small sample size. In this paper, we propose a two-stage pretraining-finetuning (TSPF) framework using both large-scale observational data and small-scale RCT data to estimate the CATE in the presence of unmeasured confounding. In the first stage, a foundational representation of covariates is trained to estimate counterfactual outcomes through large-scale observational data. In the second stage, we propose to train an augmented representation of the covariates, which is concatenated to the foundational representation obtained in the first stage to adjust for the unmeasured confounding. To avoid overfitting caused by the small-scale RCT data in the second stage, we further propose a partial parameter initialization approach, rather than training a separate network. The superiority of our approach is validated on two public datasets with extensive experiments. The code is available at https://github.com/zhouchuanCN/KDD25-TSPF.	 | 从观察数据中估计条件平均治疗效果（CATE）在电商、医疗保健和经济学等领域扮演着至关重要的角色。现有研究主要依赖于强可忽略性假设，即不存在未测量的混杂变量，这些变量的存在无法通过观察数据进行检验，会使得任何因果结论无效。相比之下，随机对照试验（RCT）收集的数据不受到混杂因素的影响，但通常样本量较小。本文提出了一种结合大规模观察数据和小型RCT数据的两阶段预训练-微调（TSPF）框架，以在存在未测量混杂变量的情况下估计CATE。在第一阶段，通过大规模观察数据训练基础特征表示，以估计反事实结果。在第二阶段，我们提出了一种增强特征表示的训练方法，即将第一阶段获得的基础特征表示与增强的特征表示进行拼接，以调整未测量的混杂因素。为避免在第二阶段由于小规模RCT数据导致的过拟合问题，我们进一步提出了一种部分参数初始化方法，而不是训练独立的网络。我们的方法通过两个公开数据集上广泛实验的验证，证明了其优越性。源代码可在https://github.com/zhouchuanCN/KDD25-TSPF获取。
2501.08887	 | PAC Learnability of Scenario Decision-Making Algorithms: Necessary and Sufficient Conditions	 | Guillaume O. Berger,Raphaël M. Jungers	 | 该研究探讨了场景决策算法的PAC性质，发现相关充分条件（如有限的VC维和压缩方案）并非必要条件，且即使在稳定性与一致性等额外假设下，这一结论依然成立，最终基于VC维和无免费午餐定理推导出PAC的必要条件。	 | We study the PAC property of scenario decision-making algorithms, that is, the ability to make a decision that has an arbitrarily low risk of violating an unknown safety constraint, provided sufficiently many realizations (called scenarios) of the safety constraint are sampled. Sufficient conditions for scenario decision-making algorithms to be PAC are available in the literature, such as finiteness of the VC dimension of its associated classifier and existence of a compression scheme. We study the question of whether these sufficient conditions are also necessary. We show with counterexamples that this is not the case in general. This contrasts with binary classification learning, for which the analogous conditions are sufficient and necessary. Popular scenario decision-making algorithms, such as scenario optimization, enjoy additional properties, such as stability and consistency. We show that even under these additional assumptions the above conclusions hold. Finally, we derive a necessary condition for scenario decision-making algorithms to be PAC, inspired by the VC dimension and the so-called no-free-lunch theorem.	 | 我们研究场景决策算法的PAC性质，即在足够多的安全约束样本（称为场景）被采样的情况下，能够做出一个违反未知安全约束的风险极低的决策的能力。文献中有关于场景决策算法满足PAC条件的充分条件，例如其关联分类器的VC维有限以及存在压缩方案。我们探讨了这些充分条件是否也是必要条件的问题。我们通过反例表明，一般来说这些条件并不是必要的。这与二元分类学习形成了对比，其中相应的条件既是充分的又是必要的。流行的场景决策算法，如场景优化，还具有稳定性与一致性等额外性质。我们证明即使在这些附加假设下，上述结论仍然成立。最后，我们基于VC维及所谓的无免费午餐定理，推导出场景决策算法满足PAC的必要条件。
2501.08883	 | Increasing Batch Size Improves Convergence of Stochastic Gradient Descent with Momentum	 | Keisuke Kamo,Hideaki Iiduka	 | 本文证明了在训练深度神经网络时，使用恒定批量大小的批量随机梯度下降法（mini-batch SGDM）并不总是能最小化经验损失的全梯度范数的期望值，而逐渐增大的批量大小则可以提高收敛性，实验结果支持了这一观点。	 | Stochastic gradient descent with momentum (SGDM), which is defined by adding a momentum term to SGD, has been well studied in both theory and practice. Theoretically investigated results showed that the settings of the learning rate and momentum weight affect the convergence of SGDM. Meanwhile, practical results showed that the setting of batch size strongly depends on the performance of SGDM. In this paper, we focus on mini-batch SGDM with constant learning rate and constant momentum weight, which is frequently used to train deep neural networks in practice. The contribution of this paper is showing theoretically that using a constant batch size does not always minimize the expectation of the full gradient norm of the empirical loss in training a deep neural network, whereas using an increasing batch size definitely minimizes it, that is, increasing batch size improves convergence of mini-batch SGDM. We also provide numerical results supporting our analyses, indicating specifically that mini-batch SGDM with an increasing batch size converges to stationary points faster than with a constant batch size. Python implementations of the optimizers used in the numerical experiments are available at https://anonymous.4open.science/r/momentum-increasing-batch-size-888C/.	 | 随机梯度下降法（SGD）带有动量项（SGDM）已经在理论和实践中得到了广泛的研究。理论研究结果表明，学习率和动量权重的设置会影响SGDM的收敛性。同时，实践结果表明，批量大小的设置对SGDM的性能有着重要影响。本文重点关注在训练深度神经网络时常用的具有恒定学习率和恒定动量权重的批量随机梯度下降法（mini-batch SGDM）。本文的贡献在于证明了使用恒定批量大小并不总是最小化训练深度神经网络时经验损失的全梯度范数的期望值，而使用逐渐增大的批量大小则可以最小化这一值，也就是说，增大批量大小可以提高mini-batch SGDM的收敛性。我们还提供了支持这些分析的数值结果，具体表明，使用逐渐增大的批量大小比使用恒定批量大小更快地收敛到稳定点。用于数值实验的优化器的Python实现可以在此处获取：https://anonymous.4open.science/r/momentum-increasing-batch-size-888C/。
2501.08878	 | Incrementally Learning Multiple Diverse Data Domains via Multi-Source Dynamic Expansion Model	 | Runqing Wu,Fei Ye,Qihe Liu,Guoxi Huang,Jinyu Guo,Rongyao Hu	 | 本文提出了一种名为多源动态扩展模型（MSDEM）的新方法，该方法能够在多个数据域中有效进行持续学习，并通过动态可扩展注意力机制和动态图权重路由器加速新任务的学习，从而提升了泛化性能。实验结果表明，MSDEM达到了最先进的性能。	 | Continual Learning seeks to develop a model capable of incrementally assimilating new information while retaining prior knowledge. However, current research predominantly addresses a straightforward learning context, wherein all data samples originate from a singular data domain. This paper shifts focus to a more complex and realistic learning environment, characterized by data samples sourced from multiple distinct domains. We tackle this intricate learning challenge by introducing a novel methodology, termed the Multi-Source Dynamic Expansion Model (MSDEM), which leverages various pre-trained models as backbones and progressively establishes new experts based on them to adapt to emerging tasks. Additionally, we propose an innovative dynamic expandable attention mechanism designed to selectively harness knowledge from multiple backbones, thereby accelerating the new task learning. Moreover, we introduce a dynamic graph weight router that strategically reuses all previously acquired parameters and representations for new task learning, maximizing the positive knowledge transfer effect, which further improves generalization performance. We conduct a comprehensive series of experiments, and the empirical findings indicate that our proposed approach achieves state-of-the-art performance.	 | 持续学习旨在开发能够在不断吸收新信息的同时保留先前知识的模型。然而，当前的研究主要集中在简单的学习场景中，其中所有数据样本都来自单一数据域。本文将焦点转向一个更加复杂和现实的学习环境，在这种环境中，数据样本来自多个不同的域。我们通过引入一种名为多源动态扩展模型（MSDEM）的新方法来应对这一复杂的学习挑战，该方法利用多种预训练模型作为基础架构，并逐步基于它们建立新的专家模型以适应新兴任务。此外，我们提出了一个创新的动态可扩展注意力机制，能够有选择地从多个基础架构中获取知识，从而加速新任务的学习。另外，我们引入了一个动态图权重路由器，能够在重新利用之前所有获取的参数和表示的基础上，战略性地为新任务学习优化，最大化正向知识迁移的效果，进一步提升泛化性能。我们进行了全面的实验，实证结果表明，我们提出的方法达到了最先进的性能。
2501.08862	 | ARMOR: Shielding Unlearnable Examples against Data Augmentation	 | Xueluan Gong,Yuji Wang,Yanjiao Chen,Haocheng Dong,Yiming Li,Mengyuan Sun,Shuaike Li,Qian Wang,Chen Chen	 | 该研究揭示了数据增强技术在提升模型性能的同时，会破坏不可学习样本的隐私保护效果，并提出了一种名为ARMOR的防御框架，通过非局部模块辅助的替代模型和优化的数据增强选择策略，有效恢复数据的不可学习性，从而保护隐私。	 | Private data, when published online, may be collected by unauthorized parties to train deep neural networks (DNNs). To protect privacy, defensive noises can be added to original samples to degrade their learnability by DNNs. Recently, unlearnable examples are proposed to minimize the training loss such that the model learns almost nothing. However, raw data are often pre-processed before being used for training, which may restore the private information of protected data. In this paper, we reveal the data privacy violation induced by data augmentation, a commonly used data pre-processing technique to improve model generalization capability, which is the first of its kind as far as we are concerned. We demonstrate that data augmentation can significantly raise the accuracy of the model trained on unlearnable examples from 21.3% to 66.1%. To address this issue, we propose a defense framework, dubbed ARMOR, to protect data privacy from potential breaches of data augmentation. To overcome the difficulty of having no access to the model training process, we design a non-local module-assisted surrogate model that better captures the effect of data augmentation. In addition, we design a surrogate augmentation selection strategy that maximizes distribution alignment between augmented and non-augmented samples, to choose the optimal augmentation strategy for each class. We also use a dynamic step size adjustment algorithm to enhance the defensive noise generation process. Extensive experiments are conducted on 4 datasets and 5 data augmentation methods to verify the performance of ARMOR. Comparisons with 6 state-of-the-art defense methods have demonstrated that ARMOR can preserve the unlearnability of protected private data under data augmentation. ARMOR reduces the test accuracy of the model trained on augmented protected samples by as much as 60% more than baselines.	 | 当私人数据在线发布时，可能会被未经授权的第三方收集以训练深度神经网络（DNNs）。为了保护隐私，在原始样本上添加防御噪声可以降低DNNs的学习能力。最近，提出了不可学习样本来最小化训练损失，使得模型几乎不学习任何内容。然而，原始数据在用于训练之前通常会经过预处理，这可能会恢复保护数据中的私人信息。在本文中，我们揭示了数据增强引发的数据隐私侵犯问题，这是一种常用的数据预处理技术，用于提高模型的泛化能力，这是我们所知的首个此类研究。我们证明了数据增强可以显著提高在不可学习样本上训练的模型的准确性，从21.3%提高到66.1%。为了解决这个问题，我们提出了一种名为ARMOR的防御框架，以防止数据增强可能引发的数据隐私侵犯。为了克服无法访问模型训练过程的困难，我们设计了一个非局部模块辅助的替代模型，以更好地捕捉数据增强的效果。此外，我们设计了一种替代数据增强选择策略，最大化增强样本与未增强样本之间的分布对齐，以为每个类别选择最佳的数据增强策略。我们还使用动态步长调整算法来增强防御噪声生成过程。我们在4个数据集和5种数据增强方法上进行了广泛的实验，以验证ARMOR的效果。与6种最先进的防御方法的比较表明，ARMOR可以在数据增强情况下保护私有数据的不可学习性。与基线相比，ARMOR可以将训练在增强保护样本上模型的测试精度降低高达60%。
2501.08851	 | Digital Phenotyping for Adolescent Mental Health: A Feasibility Study Employing Machine Learning to Predict Mental Health Risk From Active and Passive Smartphone Data	 | Balasundaram Kadirvelu,Teresa Bellido Bel,Aglaia Freccero,Martina Di Simplicio,Dasha Nicholls,A Aldo Faisal	 | 本研究利用新型机器学习框架整合主动和被动智能手机数据，评估了预测非临床青少年精神疾病（包括内部化和外部化障碍、进食障碍、失眠和自杀倾向）的可行性，并发现综合数据源的性能优于单独数据源。对比学习框架稳定了日常行为表示，提高了预测的稳健性，展示了利用智能手机进行早期干预的潜力。	 | Background: Adolescents are particularly vulnerable to mental disorders, with over 75% of cases manifesting before the age of 25. Research indicates that only 18 to 34% of young people experiencing high levels of depression or anxiety symptoms seek support. Digital tools leveraging smartphones offer scalable and early intervention opportunities. Objective: Using a novel machine learning framework, this study evaluated the feasibility of integrating active and passive smartphone data to predict mental disorders in non-clinical adolescents. Specifically, we investigated the utility of the Mindcraft app in predicting risks for internalising and externalising disorders, eating disorders, insomnia and suicidal ideation. Methods: Participants (N=103; mean age 16.1 years) were recruited from three London schools. Participants completed the Strengths and Difficulties Questionnaire, the Eating Disorders-15 Questionnaire, Sleep Condition Indicator Questionnaire and indicated the presence/absence of suicidal ideation. They used the Mindcraft app for 14 days, contributing active data via self-reports and passive data from smartphone sensors. A contrastive pretraining phase was applied to enhance user-specific feature stability, followed by supervised fine-tuning. The model evaluation employed leave-one-subject-out cross-validation using balanced accuracy as the primary metric. Results: The integration of active and passive data achieved superior performance compared to individual data sources, with mean balanced accuracies of 0.71 for SDQ-High risk, 0.67 for insomnia, 0.77 for suicidal ideation and 0.70 for eating disorders. The contrastive learning framework stabilised daily behavioural representations, enhancing predictive robustness. This study demonstrates the potential of integrating active and passive smartphone data with advanced machine-learning techniques for predicting mental health risks.	 | 背景：青少年特别容易患精神疾病，超过75%的病例在25岁之前就会出现。研究表明，在经历高水平抑郁或焦虑症状的年轻人中，只有18%到34%的人会寻求帮助。利用智能手机的数字工具提供了可扩展且早期干预的机会。目的：利用一种新型的机器学习框架，本研究评估了将主动和被动智能手机数据整合起来预测非临床青少年精神疾病的可行性。具体而言，我们研究了Mindcraft应用在预测内部化和外部化障碍、进食障碍、失眠和自杀倾向方面的效用。方法：参与者（N=103，平均年龄16.1岁）来自三所伦敦学校。参与者完成了《优势与困难问卷》（SDQ）、《进食障碍-15问卷》（ED-15）和《睡眠条件指标问卷》（SCIQ），并报告了自杀念头的存在与否。他们使用Mindcraft应用14天，通过自我报告提供主动数据，并通过智能手机传感器提供被动数据。采用对比预训练阶段来增强用户特定特征的稳定性，随后进行监督微调。模型评估采用了保留参与者交叉验证的方法，平衡准确率为主要评价指标。结果：整合主动和被动数据的性能优于单独的数据源，SDQ-High风险的平均平衡准确率为0.71，失眠为0.67，自杀倾向为0.77，进食障碍为0.70。对比学习框架稳定了日常行为表示，提高了预测的稳健性。本研究展示了将主动和被动智能手机数据与高级机器学习技术结合以预测心理健康风险的潜力。
2501.08850	 | Graph Counterfactual Explainable AI via Latent Space Traversal	 | Andreas Abildtrup Hansen,Paraskevas Pegios,Anna Calissano,Aasa Feragen	 | 本文提出了一种生成可微黑盒图形分类器反事实解释的方法，通过使用置换不变图形变分自编码器跨越潜在空间，从而无缝整合图形的离散结构和连续属性。实验结果显示，该方法在性能上优于基线模型，并且更加稳健。	 | Explaining the predictions of a deep neural network is a nontrivial task, yet high-quality explanations for predictions are often a prerequisite for practitioners to trust these models. Counterfactual explanations aim to explain predictions by finding the ''nearest'' in-distribution alternative input whose prediction changes in a pre-specified way. However, it remains an open question how to define this nearest alternative input, whose solution depends on both the domain (e.g. images, graphs, tabular data, etc.) and the specific application considered. For graphs, this problem is complicated i) by their discrete nature, as opposed to the continuous nature of state-of-the-art graph classifiers; and ii) by the node permutation group acting on the graphs. We propose a method to generate counterfactual explanations for any differentiable black-box graph classifier, utilizing a case-specific permutation equivariant graph variational autoencoder. We generate counterfactual explanations in a continuous fashion by traversing the latent space of the autoencoder across the classification boundary of the classifier, allowing for seamless integration of discrete graph structure and continuous graph attributes. We empirically validate the approach on three graph datasets, showing that our model is consistently high-performing and more robust than the baselines.	 | 解释深度神经网络的预测结果是一项非平凡的任务，然而高质量的预测解释往往是从业者信任这些模型的前提。反事实解释旨在通过找到一种“最接近”的在分布内替代输入来解释预测，这种替代输入的预测在预先指定的方式上发生变化。然而，如何定义这种最接近的替代输入仍然是一个开放问题，其解决方案取决于领域（例如图像、图形、表格数据等）和具体的应用场景。对于图形数据，这个问题被进一步复杂化：一是图形的离散性质与最先进的图形分类器的连续性质相对立；二是图形上的节点置换群作用。我们提出了一种方法，用于生成任何可微的黑盒图形分类器的反事实解释，利用一种针对具体案例的置换不变图形变分自编码器。我们通过穿越自编码器的潜在空间跨越分类器的分类边界，以连续的方式生成反事实解释，从而无缝地整合离散的图形结构和连续的图形属性。我们在三个图形数据集上进行实证验证，结果显示，我们的模型在性能上始终优于基线模型，并且更加稳健。
2501.08821	 | A Closer Look at the Learnability of Out-of-Distribution (OOD) Detection	 | Konstantin Garov,Kamalika Chaudhuri	 | 本文通过PAC学习理论探讨了分布外（OOD）检测的可学习性，区分了均匀和非均匀两种情况，并提供了OOD检测在某些条件下的具体算法和样本复杂性分析，展示了非均匀可学习如何将一些负面结果转化为正面结果。	 | Machine learning algorithms often encounter different or "out-of-distribution" (OOD) data at deployment time, and OOD detection is frequently employed to detect these examples. While it works reasonably well in practice, existing theoretical results on OOD detection are highly pessimistic. In this work, we take a closer look at this problem, and make a distinction between uniform and non-uniform learnability, following PAC learning theory. We characterize under what conditions OOD detection is uniformly and non-uniformly learnable, and we show that in several cases, non-uniform learnability turns a number of negative results into positive. In all cases where OOD detection is learnable, we provide concrete learning algorithms and a sample-complexity analysis.	 | 机器学习算法在部署时往往会遇到不同或“分布外”（OOD）的数据，而OOD检测通常被用来检测这些数据。虽然在实践中效果还算不错，但现有的OOD检测理论结果却非常悲观。在本文中，我们更仔细地研究了这个问题，并根据PAC学习理论将学习性分为均匀和非均匀两种情况。我们刻画了在什么条件下OOD检测是均匀和非均匀可学习的，并且显示在几种情况下，非均匀可学习将一些负面结果转化为正面结果。在所有OOD检测是可学习的情况下，我们提供了具体的算法和样本复杂性分析。
2501.08780	 | Deep learning for temporal super-resolution 4D Flow MRI	 | Pia Callmer,Mia Bonini,Edward Ferdian,David Nordsletten,Daniel Giese,Alistair A. Young,Alexander Fyrdahl,David Marlevi	 | 该研究通过重新设计4DFlowNet并优化其内部层结构，实现了时间域超分辨率4D流磁共振成像的技术，显著提高了流速定量的准确性，且不增加扫描时间，平均绝对误差降低至1.0 cm/s，优于传统插值方法。	 | 4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasive technique for volumetric, time-resolved blood flow quantification. However, apparent trade-offs between acquisition time, image noise, and resolution limit clinical applicability. In particular, in regions of highly transient flow, coarse temporal resolution can hinder accurate capture of physiologically relevant flow variations. To overcome these issues, post-processing techniques using deep learning have shown promising results to enhance resolution post-scan using so-called super-resolution networks. However, while super-resolution has been focusing on spatial upsampling, temporal super-resolution remains largely unexplored. The aim of this study was therefore to implement and evaluate a residual network for temporal super-resolution 4D Flow MRI. To achieve this, an existing spatial network (4DFlowNet) was re-designed for temporal upsampling, adapting input dimensions, and optimizing internal layer structures. Training and testing were performed using synthetic 4D Flow MRI data originating from patient-specific in-silico models, as well as using in-vivo datasets. Overall, excellent performance was achieved with input velocities effectively denoised and temporally upsampled, with a mean absolute error (MAE) of 1.0 cm/s in an unseen in-silico setting, outperforming deterministic alternatives (linear interpolation MAE = 2.3 cm/s, sinc interpolation MAE = 2.6 cm/s). Further, the network synthesized high-resolution temporal information from unseen low-resolution in-vivo data, with strong correlation observed at peak flow frames. As such, our results highlight the potential of utilizing data-driven neural networks for temporal super-resolution 4D Flow MRI, enabling high-frame-rate flow quantification without extending acquisition times beyond clinically acceptable limits.	 | 4D流磁共振成像（4D Flow MRI）是一种无创技术，用于体积、时间分辨的血液流速定量。然而，获取时间、图像噪声和分辨率之间的显性权衡限制了其临床应用。特别是在高度瞬态流区域，较低的时域分辨率可能会妨碍生理相关流速变化的准确捕捉。为解决这些问题，使用深度学习的后处理技术已经显示出增强扫描后分辨率的前景，特别是被称为超分辨率网络的技术。然而，尽管超分辨率主要集中在空间上采样，但时间域上的超分辨率仍然几乎没有被探索。本研究的目的是实现并评估残差网络在时间域超分辨率4D流MRI中的应用。为此，我们重新设计了一个现有的空间网络（4DFlowNet），适应输入维度，并优化内部层结构，以进行时间上采样。训练和测试使用来自患者特定的在硅模型的合成4D流MRI数据以及体内的实际数据进行。总体而言，取得了出色的性能，在未知的在硅环境中，输入速度有效降噪并进行时间上采样，平均绝对误差（MAE）为1.0 cm/s，优于确定性替代方案（线性插值MAE = 2.3 cm/s，sinc插值MAE = 2.6 cm/s）。此外，该网络可以从未知低分辨率的体内数据中合成高分辨率的时间信息，在峰值流速帧中观察到强相关性。因此，我们的结果强调了利用数据驱动的神经网络进行时间域超分辨率4D流MRI的潜力，使其能够在不延长获取时间的情况下实现高帧率流速定量。
2501.08778	 | Networked Agents in the Dark: Team Value Learning under Partial Observability	 | Guilherme S. Varela,Alberto Sardinha,Francisco S. Melo	 | 本文提出了一种在网络化代理的协同多智能体强化学习（MARL）中，代理在部分可观测性条件下通过局部通信和共识机制实现协同行为的新方法，并通过基准场景评估显示了其优越性。	 | We propose a novel cooperative multi-agent reinforcement learning (MARL) approach for networked agents. In contrast to previous methods that rely on complete state information or joint observations, our agents must learn how to reach shared objectives under partial observability. During training, they collect individual rewards and approximate a team value function through local communication, resulting in cooperative behavior. To describe our problem, we introduce the networked dynamic partially observable Markov game framework, where agents communicate over a switching topology communication network. Our distributed method, DNA-MARL, uses a consensus mechanism for local communication and gradient descent for local computation. DNA-MARL increases the range of the possible applications of networked agents, being well-suited for real world domains that impose privacy and where the messages may not reach their recipients. We evaluate DNA-MARL across benchmark MARL scenarios. Our results highlight the superior performance of DNA-MARL over previous methods.	 | 我们提出了一种新颖的网络化代理的协同多智能体强化学习（MARL）方法。与之前依赖完整状态信息或联合观测的方法不同，我们的代理必须在部分可观测性下学习如何达成共享的目标。在训练过程中，它们收集各自的奖励，并通过局部通信近似一个团队价值函数，从而实现协同行为。为了描述我们的问题，我们引入了网络动态部分可观测马尔可夫游戏框架，其中代理在切换拓扑的通信网络上进行通信。我们的分布式方法DNA-MARL 使用共识机制进行局部通信，并使用梯度下降进行局部计算。DNA-MARL 扩展了网络化代理的应用范围，特别适合那些具有隐私要求且信息可能无法到达接收方的真实世界领域。我们对DNA-MARL进行了基准MARL场景的评估。我们的结果突显了DNA-MARL相较于之前方法的优越性能。
2501.08738	 | MeshMask: Physics-Based Simulations with Masked Graph Neural Networks	 | Paul Garnier,Vincent Lannelongue,Jonathan Viquerat,Elie Hachem	 | 该研究提出了一种针对CFD问题的新型GNN掩码预训练技术，通过随机遮掩高达40%的输入网格节点，结合不对称编码解码架构和门控多层感知机，实现了在多个CFD数据集上的最佳性能，并显著提高了模型的训练效率和长期预测准确性。	 | We introduce a novel masked pre-training technique for graph neural networks (GNNs) applied to computational fluid dynamics (CFD) problems. By randomly masking up to 40\% of input mesh nodes during pre-training, we force the model to learn robust representations of complex fluid dynamics. We pair this masking strategy with an asymmetric encoder-decoder architecture and gated multi-layer perceptrons to further enhance performance. The proposed method achieves state-of-the-art results on seven CFD datasets, including a new challenging dataset of 3D intracranial aneurysm simulations with over 250,000 nodes per mesh. Moreover, it significantly improves model performance and training efficiency across such diverse range of fluid simulation tasks. We demonstrate improvements of up to 60\% in long-term prediction accuracy compared to previous best models, while maintaining similar computational costs. Notably, our approach enables effective pre-training on multiple datasets simultaneously, significantly reducing the time and data required to achieve high performance on new tasks. Through extensive ablation studies, we provide insights into the optimal masking ratio, architectural choices, and training strategies.	 | 我们提出了一种针对计算流体力学（CFD）问题的图神经网络（GNN）的新型掩码预训练技术。在预训练过程中，通过随机遮掩输入网格节点的高达40%，迫使模型学习复杂的流体力学的稳健表示。我们将这种遮掩策略与不对称编码解码架构和门控多层感知机相结合，进一步提升性能。所提出的方法在七个CFD数据集上达到了最先进的成果，包括一个具有超过25万个网格节点的新挑战性数据集，3D颅内动脉瘤模拟数据集。此外，它在各种不同的流体模拟任务上显著提高了模型性能和训练效率。与之前的最佳模型相比，我们展示了在长期预测准确性方面提高了高达60%，同时保持了类似的计算成本。值得注意的是，我们的方法能够同时有效地在多个数据集上进行预训练，显著减少了实现高性能所需的时间和数据。通过广泛的消融研究，我们提供了关于最佳遮掩比例、架构选择和训练策略的见解。
2501.08737	 | Resource-Constrained Federated Continual Learning: What Does Matter?	 | Yichen Li,Yuying Wang,Jiahua Dong,Haozhao Wang,Yining Qi,Rui Zhang,Ruixuan Li	 | 该研究重新审视了联邦持续学习（FCL）在资源受限条件下的表现，通过大规模基准测试发现，当前最先进的FCL方法在存储、计算预算和标签速率等资源受限条件下均无法达到预期性能，强调了未来研究应关注资源效率的提升。	 | Federated Continual Learning (FCL) aims to enable sequentially privacy-preserving model training on streams of incoming data that vary in edge devices by preserving previous knowledge while adapting to new data. Current FCL literature focuses on restricted data privacy and access to previously seen data while imposing no constraints on the training overhead. This is unreasonable for FCL applications in real-world scenarios, where edge devices are primarily constrained by resources such as storage, computational budget, and label rate. We revisit this problem with a large-scale benchmark and analyze the performance of state-of-the-art FCL approaches under different resource-constrained settings. Various typical FCL techniques and six datasets in two incremental learning scenarios (Class-IL and Domain-IL) are involved in our experiments. Through extensive experiments amounting to a total of over 1,000+ GPU hours, we find that, under limited resource-constrained settings, existing FCL approaches, with no exception, fail to achieve the expected performance. Our conclusions are consistent in the sensitivity analysis. This suggests that most existing FCL methods are particularly too resource-dependent for real-world deployment. Moreover, we study the performance of typical FCL techniques with resource constraints and shed light on future research directions in FCL.	 | 联邦持续学习（Federated Continual Learning, FCL）旨在通过在边缘设备上逐步隐私保护地训练模型来处理不断变化的数据流，同时保持先前的知识并适应新数据。现有的FCL文献主要关注数据隐私限制和先前见过的数据的访问，但并未对训练开销施加任何约束。在现实世界的应用场景中，这种做法是不合理的，因为边缘设备主要受限于诸如存储、计算预算和标签速率等资源。我们重新审视了这一问题，使用大规模基准测试，并分析了在不同的资源受限条件下，最先进的FCL方法的表现。我们的实验中涉及多种典型的FCL技术以及两个增量学习场景（Class-IL和Domain-IL）下的六个数据集。通过总计超过1000多个GPU小时的大量实验，我们发现，在资源受限的条件下，现有的FCL方法无一例外地无法达到预期性能。在敏感性分析中，我们的结论是一致的。这表明大多数现有的FCL方法在实际部署中特别依赖于资源。此外，我们研究了典型FCL技术在资源受限条件下的表现，并为FCL的未来研究方向提供了指导。
2501.08729	 | GRAPPA - A Hybrid Graph Neural Network for Predicting Pure Component Vapor Pressures	 | Marco Hoffmann,Hans Hasse,Fabian Jirasek	 | 研究人员开发了一种名为GRAPPA的混合图神经网络，用于准确预测有机分子的蒸气压，并取得了优于现有方法的性能；该模型已开源，用户可通过交互式网站直接使用。	 | Although the pure component vapor pressure is one of the most important properties for designing chemical processes, no broadly applicable, sufficiently accurate, and open-source prediction method has been available. To overcome this, we have developed GRAPPA - a hybrid graph neural network for predicting vapor pressures of pure components. GRAPPA enables the prediction of the vapor pressure curve of basically any organic molecule, requiring only the molecular structure as input. The new model consists of three parts: A graph attention network for the message passing step, a pooling function that captures long-range interactions, and a prediction head that yields the component-specific parameters of the Antoine equation, from which the vapor pressure can readily and consistently be calculated for any temperature. We have trained and evaluated GRAPPA on experimental vapor pressure data of almost 25,000 pure components. We found excellent prediction accuracy for unseen components, outperforming state-of-the-art group contribution methods and other machine learning approaches in applicability and accuracy. The trained model and its code are fully disclosed, and GRAPPA is directly applicable via the interactive website ml-prop.mv.rptu.de.	 | 尽管纯组分的蒸气压是设计化工过程最重要的性质之一，但目前还没有一种广泛适用、足够准确且开源的预测方法。为了解决这一问题，我们开发了GRAPPA——一种混合图神经网络，用于预测纯组分的蒸气压。GRAPPA 可以根据分子结构输入，预测几乎所有有机分子的蒸气压曲线。该新模型由三个部分组成：用于信息传递步骤的图注意力网络、捕捉长程相互作用的池化函数，以及用于从安托万方程推导出组分特定参数的预测头部，从而可以轻松且一致地计算任何温度下的蒸气压。我们使用近25,000种纯组分的实验蒸气压数据对GRAPPA进行了训练和评估。我们发现，对于未见过的组分，预测精度非常出色，优于最先进的组分贡献方法和其他机器学习方法的适用性和准确性。训练好的模型及其代码完全公开，用户可以通过交互式网站 ml-prop.mv.rptu.de 直接使用GRAPPA。
2501.08727	 | Transformed Low-rank Adaptation via Tensor Decomposition and Its Applications to Text-to-image Models	 | Zerui Tao,Yuhta Takida,Naoki Murata,Qibin Zhao,Yuki Mitsufuji	 | 该研究提出了一种新的参数高效微调（PEFT）方法，结合了变换和残差适应，以提高低秩适应（LoRA）的性能和效率；实验结果表明，该方法在主题驱动和可控生成任务中优于LoRA及一些基线方法，显示出更高的性能和参数效率。	 | Parameter-Efficient Fine-Tuning (PEFT) of text-to-image models has become an increasingly popular technique with many applications. Among the various PEFT methods, Low-Rank Adaptation (LoRA) and its variants have gained significant attention due to their effectiveness, enabling users to fine-tune models with limited computational resources. However, the approximation gap between the low-rank assumption and desired fine-tuning weights prevents the simultaneous acquisition of ultra-parameter-efficiency and better performance. To reduce this gap and further improve the power of LoRA, we propose a new PEFT method that combines two classes of adaptations, namely, transform and residual adaptations. In specific, we first apply a full-rank and dense transform to the pre-trained weight. This learnable transform is expected to align the pre-trained weight as closely as possible to the desired weight, thereby reducing the rank of the residual weight. Then, the residual part can be effectively approximated by more compact and parameter-efficient structures, with a smaller approximation error. To achieve ultra-parameter-efficiency in practice, we design highly flexible and effective tensor decompositions for both the transform and residual adaptations. Additionally, popular PEFT methods such as DoRA can be summarized under this transform plus residual adaptation scheme. Experiments are conducted on fine-tuning Stable Diffusion models in subject-driven and controllable generation. The results manifest that our method can achieve better performances and parameter efficiency compared to LoRA and several baselines.	 | 文本到图像模型的参数高效微调（PEFT）已成为一种越来越受欢迎的技术，具有广泛的应用。在各种PEFT方法中，低秩适应（LoRA）及其变体因其有效性和能够使用户在有限的计算资源下进行模型微调而备受关注。然而，低秩假设与期望的微调权重之间的近似差距限制了同时获得超参数效率和更好性能的可能性。为了减小这一差距并进一步提高LoRA的性能，我们提出了一种新的PEFT方法，该方法结合了两种类型的适应，即变换和残差适应。具体而言，我们首先对预训练权重应用一个全秩和密集变换。这种可学习的变换期望能够尽可能地将预训练权重与期望权重对齐，从而减少残差权重的秩。然后，残差部分可以通过更紧凑和参数高效的结构来有效近似，且具有较小的近似误差。为了在实践中实现超参数效率，我们设计了灵活且有效的张量分解方案，以适应变换和残差适应。此外，流行的PEFT方法如DoRA也可以概括为这种变换加残差适应的方案。我们在主题驱动和可控生成中对Stable Diffusion模型进行了微调实验。结果表明，我们的方法在性能和参数效率方面优于LoRA以及一些基线方法。
2501.08710	 | Disentangled Interleaving Variational Encoding	 | Noelle Y. L. Wong,Eng Yeow Cheu,Zhonglin Chiam	 | DeepDIVE（深度解耦交错变分编码）通过将输入分解为边缘概率分布和条件概率分布，在变分自编码器中实现多任务学习的解耦特征表示，从而在保持任务间独立性的同时提高预测准确性。通过理论证明和实验证明，DeepDIVE在多个数据集上展示了优于传统VAE和其他基线方法的性能。	 | Conflicting objectives present a considerable challenge in interleaving multi-task learning, necessitating the need for meticulous design and balance to ensure effective learning of a representative latent data space across all tasks without mutual negative impact. Drawing inspiration from the concept of marginal and conditional probability distributions in probability theory, we design a principled and well-founded approach to disentangle the original input into marginal and conditional probability distributions in the latent space of a variational autoencoder. Our proposed model, Deep Disentangled Interleaving Variational Encoding (DeepDIVE) learns disentangled features from the original input to form clusters in the embedding space and unifies these features via the cross-attention mechanism in the fusion stage. We theoretically prove that combining the objectives for reconstruction and forecasting fully captures the lower bound and mathematically derive a loss function for disentanglement using Naïve Bayes. Under the assumption that the prior is a mixture of log-concave distributions, we also establish that the Kullback-Leibler divergence between the prior and the posterior is upper bounded by a function minimized by the minimizer of the cross entropy loss, informing our adoption of radial basis functions (RBF) and cross entropy with interleaving training for DeepDIVE to provide a justified basis for convergence. Experiments on two public datasets show that DeepDIVE disentangles the original input and yields forecast accuracies better than the original VAE and comparable to existing state-of-the-art baselines.	 | 冲突的目标在交错多任务学习中构成了显著的挑战，需要精心的设计和平衡，以确保所有任务中都能有效地学习一个代表性的潜在数据空间，而不会产生相互负面影响。受到概率论中边缘概率分布和条件概率分布概念的启发，我们设计了一种有原则且可靠的方法，在变分自编码器的潜在空间中将原始输入分解为边缘概率分布和条件概率分布。我们提出的方法，深度解耦交错变分编码（DeepDIVE），从原始输入中学习解耦特征，形成嵌入空间中的聚类，并在融合阶段通过交叉注意力机制统一这些特征。我们从理论上证明，结合重建和预测的目标能够完全捕捉到下界，并使用朴素贝叶斯方法数学地推导出解耦损失函数。在假设先验是混合对数凹分布的情况下，我们还证明了先验和后验之间的克里金-莱布勒散度由交叉熵损失极小化器所最小化的函数上界确定，这为我们采用径向基函数（RBF）和交错训练的交叉熵损失提供了合理性基础，以确保DeepDIVE的收敛性。在两个公开数据集上的实验显示，DeepDIVE能够解耦原始输入，并在预测准确性上优于原始的VAE，并与现有最先进的基线相当。
2501.08679	 | Diagonal Over-parameterization in Reproducing Kernel Hilbert Spaces as an Adaptive Feature Model: Generalization and Adaptivity	 | Yicheng Li,Qian Lin	 | 本文提出了一种对角自适应核模型，在训练过程中动态学习核特征值和输出系数，从而提高泛化能力，特别在初始核与目标不匹配时表现优异。通过增加模型深度，进一步增强了其适应性和泛化能力，结合了特征学习和隐式正则化的优点。	 | This paper introduces a diagonal adaptive kernel model that dynamically learns kernel eigenvalues and output coefficients simultaneously during training. Unlike fixed-kernel methods tied to the neural tangent kernel theory, the diagonal adaptive kernel model adapts to the structure of the truth function, significantly improving generalization over fixed-kernel methods, especially when the initial kernel is misaligned with the target. Moreover, we show that the adaptivity comes from learning the right eigenvalues during training, showing a feature learning behavior. By extending to deeper parameterization, we further show how extra depth enhances adaptability and generalization. This study combines the insights from feature learning and implicit regularization and provides new perspective into the adaptivity and generalization potential of neural networks beyond the kernel regime.	 | 本文介绍了一种对角自适应核模型，该模型在训练过程中动态学习核特征值和输出系数。与固定核方法（受限于神经切线核理论）不同，对角自适应核模型能够适应真实函数的结构，显著提高泛化能力，尤其是在初始核与目标不匹配的情况下。此外，我们证明这种适应性来自于学习正确的特征值，显示出一种特征学习行为。通过进一步扩展到更深的参数化，我们展示了额外的深度如何增强适应性和泛化能力。这项研究结合了特征学习和隐式正则化的见解，为神经网络在核域之外的适应性和泛化潜力提供了新的视角。
2501.08678	 | Investigating Parameter-Efficiency of Hybrid QuGANs Based on Geometric Properties of Generated Sea Route Graphs	 | Tobias Rohe,Florian Burger,Michael Kölle,Sebastian Wölckert,Maximilian Zorn,Claudia Linnhoff-Popien	 | 研究使用量子-经典混合生成对抗网络（QuGANs）生成航运路线图数据，结果显示QuGANs能够快速学习并表示潜在的几何属性和分布，与经典GANs相比在结果质量上表现出相似水平，展示了量子计算在生成型人工智能领域的应用潜力。	 | The demand for artificially generated data for the development, training and testing of new algorithms is omnipresent. Quantum computing (QC), does offer the hope that its inherent probabilistic functionality can be utilised in this field of generative artificial intelligence. In this study, we use quantum-classical hybrid generative adversarial networks (QuGANs) to artificially generate graphs of shipping routes. We create a training dataset based on real shipping data and investigate to what extent QuGANs are able to learn and reproduce inherent distributions and geometric features of this data. We compare hybrid QuGANs with classical Generative Adversarial Networks (GANs), with a special focus on their parameter efficiency. Our results indicate that QuGANs are indeed able to quickly learn and represent underlying geometric properties and distributions, although they seem to have difficulties in introducing variance into the sampled data. Compared to classical GANs of greater size, measured in the number of parameters used, some QuGANs show similar result quality. Our reference to concrete use cases, such as the generation of shipping data, provides an illustrative example and demonstrate the potential and diversity in which QC can be used.	 | 对于开发、训练和测试新算法所需的人工生成数据的需求无处不在。量子计算（QC）确实提供了希望，即其固有的概率功能可以在生成型人工智能领域得到利用。在这项研究中，我们使用量子-经典混合生成对抗网络（QuGANs）来人工生成航运路线图。我们基于实际的航运数据创建了一个训练数据集，并研究QuGANs在多大程度上能够学习并重现该数据的固有分布和几何特征。我们将混合QuGANs与经典的生成对抗网络（GANs）进行比较，特别关注其参数效率。我们的结果显示，QuGANs确实能够快速学习并表示潜在的几何属性和分布，尽管在引入样本数据中的变异性方面似乎存在困难。与在参数数量上更大的经典GANs相比，一些QuGANs在结果质量方面表现出相似的水平。我们对具体应用场景（如生成航运数据）的参考提供了一个说明性的例子，并展示了QC在多种方式中的应用潜力和多样性。
2501.08669	 | SPEQ: Stabilization Phases for Efficient Q-Learning in High Update-To-Data Ratio Reinforcement Learning	 | Carlo Romeo,Girolamo Macaluso,Alessandro Sestini,Andrew D. Bagdanov	 | 本文提出了一种提高深度强化学习样本效率的方法，通过将训练分为在线低更新到数据比率的训练阶段和离线稳定化阶段，从而减少计算成本并保持与高更新到数据比率的最新算法相同的样本效率。实验结果表明，该方法仅需56%的梯度更新和50%的训练时间即可达到与最新算法相当的性能。	 | A key challenge in Deep Reinforcement Learning is sample efficiency, especially in real-world applications where collecting environment interactions is expensive or risky. Recent off-policy algorithms improve sample efficiency by increasing the Update-To-Data (UTD) ratio and performing more gradient updates per environment interaction. While this improves sample efficiency, it significantly increases computational cost due to the higher number of gradient updates required. In this paper we propose a sample-efficient method to improve computational efficiency by separating training into distinct learning phases in order to exploit gradient updates more effectively. Our approach builds on top of the Dropout Q-Functions (DroQ) algorithm and alternates between an online, low UTD ratio training phase, and an offline stabilization phase. During the stabilization phase, we fine-tune the Q-functions without collecting new environment interactions. This process improves the effectiveness of the replay buffer and reduces computational overhead. Our experimental results on continuous control problems show that our method achieves results comparable to state-of-the-art, high UTD ratio algorithms while requiring 56\% fewer gradient updates and 50\% less training time than DroQ. Our approach offers an effective and computationally economical solution while maintaining the same sample efficiency as the more costly, high UTD ratio state-of-the-art.	 | 深度强化学习中的一个关键挑战是样本效率，特别是在实际应用中，收集环境交互往往既昂贵又具有风险。近年来，离策略算法通过增加更新到数据（Update-To-Data, UTD）的比例和每次环境交互执行更多梯度更新来提高样本效率。虽然这提高了样本效率，但这也显著增加了计算成本，因为需要进行更多的梯度更新。在本文中，我们提出了一种样本高效的计算效率改进方法，通过将训练分为不同的学习阶段来更有效地利用梯度更新。我们的方法基于Dropout Q-函数（DroQ）算法，并交替进行在线的低UTD比率训练阶段和离线稳定化阶段。在稳定化阶段，我们不收集新的环境交互数据来微调Q-函数。这一过程提高了经验回放存储器的有效性，并减少了计算开销。我们的实验结果表明，与高UTD比率的最新算法相比，我们的方法在完成与最新算法相当的结果的同时，只需要56%的梯度更新和50%的训练时间。我们的方法提供了一种既有效又计算成本较低的解决方案，同时保持与更昂贵的高UTD比率最新算法相同的样本效率。
2501.08653	 | Fine-grained Spatio-temporal Event Prediction with Self-adaptive Anchor Graph	 | Wang-Tao Zhou,Zhao Kang,Sicong Liu,Lizong Zhang,Ling Tian	 | 本文提出了一种新的图时空点过程（GSTPP）模型，通过编码器-解码器架构结合神经常微分方程和自适应锚图来精细化事件预测，显著提高了时空事件预测的准确性。	 | Event prediction tasks often handle spatio-temporal data distributed in a large spatial area. Different regions in the area exhibit different characteristics while having latent correlations. This spatial heterogeneity and correlations greatly affect the spatio-temporal distributions of event occurrences, which has not been addressed by state-of-the-art models. Learning spatial dependencies of events in a continuous space is challenging due to its fine granularity and a lack of prior knowledge. In this work, we propose a novel Graph Spatio-Temporal Point Process (GSTPP) model for fine-grained event prediction. It adopts an encoder-decoder architecture that jointly models the state dynamics of spatially localized regions using neural Ordinary Differential Equations (ODEs). The state evolution is built on the foundation of a novel Self-Adaptive Anchor Graph (SAAG) that captures spatial dependencies. By adaptively localizing the anchor nodes in the space and jointly constructing the correlation edges between them, the SAAG enhances the model's ability of learning complex spatial event patterns. The proposed GSTPP model greatly improves the accuracy of fine-grained event prediction. Extensive experimental results show that our method greatly improves the prediction accuracy over existing spatio-temporal event prediction approaches.	 | 事件预测任务经常处理分布在大面积空间中的时空数据。不同区域在该区域内表现出不同的特征，同时隐藏着潜在的相关性。这种空间异质性和相关性极大地影响了事件发生的时空分布，而现有的先进模型尚未解决这一问题。在连续空间中学习事件的空间依赖关系具有挑战性，因为缺乏精细粒度的先验知识。在本文中，我们提出了一种新的图时空点过程（Graph Spatio-Temporal Point Process，GSTPP）模型，用于精细化事件预测。该模型采用编码器-解码器架构，通过神经常微分方程（ODEs）联合建模空间局部区域的状态动力学。状态演化基于一种新颖的自适应锚图（Self-Adaptive Anchor Graph，SAAG）来捕捉空间依赖关系。通过在空间中自适应地定位锚节点，并联合构建它们之间的相关边，SAAG增强了模型学习复杂空间事件模式的能力。所提出的GSTPP模型显著提高了精细化事件预测的准确性。广泛实验结果表明，我们的方法显著提高了时空事件预测方法的预测准确性。
2501.08640	 | Quantum Reservoir Computing and Risk Bounds	 | Naomi Mona Chmielewski(L2S),Nina Amini(L2S, CNRS),Joseph Mikael	 | 该研究利用Rademacher复杂性分析了几类量子回路的泛化误差，并提供了参数依赖的具体误差边界，发现随着qubit数量增加，风险边界在大训练样本数量时会收敛，同时强调这些边界与qubit数量呈指数级增长。	 | We propose a way to bound the generalisation errors of several classes of quantum reservoirs using the Rademacher complexity. We give specific, parameter-dependent bounds for two particular quantum reservoir classes. We analyse how the generalisation bounds scale with growing numbers of qubits. Applying our results to classes with polynomial readout functions, we find that the risk bounds converge in the number of training samples. The explicit dependence on the quantum reservoir and readout parameters in our bounds can be used to control the generalisation error to a certain extent. It should be noted that the bounds scale exponentially with the number of qubits $n$. The upper bounds on the Rademacher complexity can be applied to other reservoir classes that fulfill a few hypotheses on the quantum dynamics and the readout function.	 | 我们提出了一种使用Rademacher复杂性来限制若干类量子回路的泛化误差的方法。我们为两类特定的量子回路提供了具体的、参数依赖的误差边界。我们分析了随着qubit数量的增加，泛化误差边界如何变化。将我们的结果应用于具有多项式读出函数的类，我们发现风险边界在训练样本数量增加时会收敛。我们在边界中显式地依赖于量子回路和读出参数，这可以用来在一定程度上控制泛化误差。需要注意的是，这些边界与qubit数量n呈指数级增长。Rademacher复杂性的上界可以应用于满足某些量子动力学和读出函数假设的其他回路类。
2501.08631	 | SWSC: Shared Weight for Similar Channel in LLM	 | Binrui Zeng,Yongtao Tang,Xiaodong Liu,Xiaopeng Li	 | 该研究提出了一种名为SWSC的模型压缩方法，通过共享相似通道权重并使用K-Means聚类和奇异值分解来减少大规模语言模型的参数数量，同时保持模型性能。	 | Large language models (LLMs) have spurred development in multiple industries. However, the growing number of their parameters brings substantial storage and computing burdens, making it essential to explore model compression techniques for parameter reduction and easier deployment. We propose SWSC, an LLM compression method based on the concept of Shared Weight for Similar Channel. It uses the K-Means clustering algorithm to cluster model weights channel-by-channel, generating clusters with highly similar vectors within each. A representative vector from each cluster is selected to approximately replace all vectors in the cluster, significantly reducing the number of model weight parameters. However, approximate restoration will inevitably cause damage to the performance of the model. To tackle this issue, we perform singular value decomposition on the weight error values before and after compression and retain the larger singular values and their corresponding singular vectors to compensate for the accuracy. The experimental results show that our method can effectively ensure the performance of the compressed LLM even under low-precision conditions.	 | 大规模语言模型（LLMs）在多个行业中推动了技术的发展。然而，其参数数量的快速增长带来了显著的存储和计算负担，因此探索模型压缩技术以减少参数并便于部署变得至关重要。我们提出了SWSC（Shared Weight for Similar Channel），这是一种基于共享相似通道权重的概念的LLM压缩方法。它使用K-Means聚类算法按通道对模型权重进行聚类，在每个聚类中生成具有高度相似向量的簇。从每个聚类中选择一个代表向量来近似替换该聚类中的所有向量，从而显著减少了模型权重参数的数量。然而，这种近似恢复不可避免地会对模型性能造成损害。为解决这一问题，我们在压缩前后对权重误差值进行奇异值分解，并保留较大的奇异值及其对应的奇异向量，以弥补精度损失。实验结果表明，即使在低精度条件下，我们的方法也能有效确保压缩后的LLM的性能。
2501.08628	 | Transformer-based Multivariate Time Series Anomaly Localization	 | Charalampos Shimillas,Kleanthis Malialis,Konstantinos Fokianos,Marios M. Polycarpou	 | 本文提出了一种基于变压器的无监督异常诊断模型，通过引入时空异常评分（STAS）和统计特征异常评分（SFAS）来提高多变量时间序列数据中异常的定位性能，并在实验中展示了其优越性。	 | With the growing complexity of Cyber-Physical Systems (CPS) and the integration of Internet of Things (IoT), the use of sensors for online monitoring generates large volume of multivariate time series (MTS) data. Consequently, the need for robust anomaly diagnosis in MTS is paramount to maintaining system reliability and safety. While significant advancements have been made in anomaly detection, localization remains a largely underexplored area, though crucial for intelligent decision-making. This paper introduces a novel transformer-based model for unsupervised anomaly diagnosis in MTS, with a focus on improving localization performance, through an in-depth analysis of the self-attention mechanism's learning behavior under both normal and anomalous conditions. We formulate the anomaly localization problem as a three-stage process: time-step, window, and segment-based. This leads to the development of the Space-Time Anomaly Score (STAS), a new metric inspired by the connection between transformer latent representations and space-time statistical models. STAS is designed to capture individual anomaly behaviors and inter-series dependencies, delivering enhanced localization performance. Additionally, the Statistical Feature Anomaly Score (SFAS) complements STAS by analyzing statistical features around anomalies, with their combination helping to reduce false alarms. Experiments on real world and synthetic datasets illustrate the model's superiority over state-of-the-art methods in both detection and localization tasks.	 | 随着网络物理系统（CPS）的复杂性不断增加以及物联网（IoT）的集成，使用传感器进行在线监测会产生大量的多变量时间序列（MTS）数据。因此，对MTS进行稳健的异常诊断对于确保系统的可靠性和安全性至关重要。尽管在异常检测方面已经取得了显著进展，但在异常定位方面仍是一个很大程度上未被探索的领域，但对智能决策至关重要。本文提出了一种基于变压器的新型无监督异常诊断模型，旨在通过深入分析自注意力机制在正常和异常条件下的学习行为，提高定位性能。我们将异常定位问题定义为三个阶段的过程：时间步、窗口和段基。这导致了时空异常评分（STAS）这一新度量准则的提出，STAS受变压器潜在表示与时空统计模型之间联系的启发而形成。STAS旨在捕捉个体异常行为和系列间的依赖关系，从而提高定位性能。此外，统计特征异常评分（SFAS）通过分析异常周围的统计特征补充了STAS，它们的结合有助于减少误报。在真实世界和合成数据集上的实验表明，该模型在检测和定位任务中均优于现有方法。
2501.08620	 | CT-PatchTST: Channel-Time Patch Time-Series Transformer for Long-Term Renewable Energy Forecasting	 | Menghao Huo,Kuan Lu,Yuxiao Li,Qiang Zhu	 | 该研究开发了一种名为通道-时间通道时间序列变换器（CT-PatchTST）的先进深度学习模型，用于预测丹麦的光伏发电和风能系统的功率输出，证明了其在提供精确可靠的能源预测方面的有效性。	 | Accurately predicting renewable energy output is crucial for the efficient integration of solar and wind power into modern energy systems. This study develops and evaluates an advanced deep learning model, Channel-Time Patch Time-Series Transformer (CT-PatchTST), to forecast the power output of photovoltaic and wind energy systems using annual offshore wind power, onshore wind power, and solar power generation data from Denmark. While the original Patch Time-Series Transformer(PatchTST) model employs a channel-independent (CI) approach, it tends to overlook inter-channel relationships during training, potentially leading to a loss of critical information. To address this limitation and further leverage the benefits of increased data granularity brought by CI, we propose CT-PatchTST. This enhanced model improves the processing of inter-channel information while maintaining the advantages of the channel-independent approach. The predictive performance of CT-PatchTST is rigorously analyzed, demonstrating its ability to provide precise and reliable energy forecasts. This work contributes to improving the predictability of renewable energy systems, supporting their broader adoption and integration into energy grids.	 | 准确预测可再生能源输出对于将太阳能和风能高效集成到现代能源系统中至关重要。本研究开发并评估了一种先进的深度学习模型——通道-时间通道时间序列变换器（CT-PatchTST），利用丹麦的年度离岸风电、陆上风电和光伏发电数据来预测光伏发电和风能系统的功率输出。虽然原始的Patch Time-Series Transformer（PatchTST）模型采用的是通道独立（CI）方法，但在训练过程中容易忽略通道间的关联性，可能丢失关键信息。为解决这一局限性，进一步利用CI带来的数据粒度增加的优势，我们提出了CT-PatchTST模型。该增强模型在保持通道独立方法优点的同时，改进了对通道间信息的处理。CT-PatchTST的预测性能得到了严格分析，证实了其提供精确可靠的能源预测的能力。这项工作有助于提高可再生能源系统的可预测性，支持其更广泛的采用和集成到能源系统中。
2501.08617	 | RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation	 | Kaiqu Liang,Haimin Hu,Ryan Liu,Thomas L. Griffiths,Jaime Fernández Fisac	 | 该研究指出，现有的基于人类反馈的强化学习（RLHF）方法主要依赖即时反馈，这可能导致模型行为不一致和欺骗性。为解决这一问题，研究提出了一种新的方法——基于后见模拟的强化学习（RLHS），该方法通过模拟未来结果并根据这些模拟结果提供反馈，从而减少不一致并提高模型的长期效用。	 | Generative AI systems like foundation models (FMs) must align well with human values to ensure their behavior is helpful and trustworthy. While Reinforcement Learning from Human Feedback (RLHF) has shown promise for optimizing model performance using human judgments, existing RLHF pipelines predominantly rely on immediate feedback, which can fail to accurately reflect the downstream impact of an interaction on users' utility. We demonstrate that feedback based on evaluators' foresight estimates of downstream consequences systematically induces Goodhart's Law dynamics, incentivizing misaligned behaviors like sycophancy and deception and ultimately degrading user outcomes. To alleviate this, we propose decoupling evaluation from prediction by refocusing RLHF on hindsight feedback. Our theoretical analysis reveals that conditioning evaluator feedback on downstream observations mitigates misalignment and improves expected human utility, even when these observations are simulated by the AI system itself. To leverage this insight in a practical alignment algorithm, we introduce Reinforcement Learning from Hindsight Simulation (RLHS), which first simulates plausible consequences and then elicits feedback to assess what behaviors were genuinely beneficial in hindsight. We apply RLHS to two widely-employed online and offline preference optimization methods -- Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) -- and show empirically that misalignment is significantly reduced with both methods. Through an online human user study, we show that RLHS consistently outperforms RLHF in helping users achieve their goals and earns higher satisfaction ratings, despite being trained solely with simulated hindsight feedback. These results underscore the importance of focusing on long-term consequences, even simulated ones, to mitigate misalignment in RLHF.	 | 生成型人工智能系统，如基础模型（FMs），必须与人类价值观很好地对齐，以确保其行为是有帮助且值得信赖的。虽然基于人类反馈的强化学习（RLHF）展示了通过人类判断优化模型性能的潜力，但现有的RLHF管道主要依赖即时反馈，这可能导致无法准确反映交互对用户效用的下游影响。我们证明，基于评估者对下游后果的前瞻性估计的反馈会系统地引起Goodhart定律动态，激励诸如奉承和欺骗等不一致的行为，并最终降低用户的结果。为了解决这一问题，我们建议通过重新聚焦RLHF于反馈而非预见性评估来解耦评估和预测。我们的理论分析表明，根据下游观察调整评估反馈可以减少不一致并提高预期的人类效用，即使这些观察是由AI系统自身模拟的。为了利用这一见解在实际对齐算法中，我们引入了基于后见模拟的强化学习（RLHS），该算法首先模拟可能的结果，然后获取反馈以评估哪些行为在后见中是真正有益的。我们用两种广泛使用的在线和离线偏好优化方法——近端策略优化（PPO）和直接偏好优化（DPO）——应用了RLHS，并证明了两种方法都能显著减少不一致。通过一项在线的人类用户研究，我们展示了RLHS在帮助用户实现目标方面一直优于RLHF，并获得了更高的满意度评分，尽管它仅使用模拟的后见反馈进行训练。这些结果突显了在RLHF中关注长期后果（即使这些后果是模拟的）的重要性，以减少不一致。
2501.08615	 | Towards Aligned Data Forgetting via Twin Machine Unlearning	 | Zhenxing Niu,Haoxuan Ji,Yuyao Sun,Zheng Lin,Fei Gao,Yuhang Wang,Haichao Gao	 | 本文提出了一种双胞胎机器遗忘（Twin Machine Unlearning, TMU）方法，通过定义一个与原始遗忘问题对应的双胞胎遗忘问题，促进未学习模型与金模型之间的对齐，从而提高了数据遗忘的效果。	 | Modern privacy regulations have spurred the evolution of machine unlearning, a technique enabling a trained model to efficiently forget specific training data. In prior unlearning methods, the concept of "data forgetting" is often interpreted and implemented as achieving zero classification accuracy on such data. Nevertheless, the authentic aim of machine unlearning is to achieve alignment between the unlearned model and the gold model, i.e., encouraging them to have identical classification accuracy. On the other hand, the gold model often exhibits non-zero classification accuracy due to its generalization ability. To achieve aligned data forgetting, we propose a Twin Machine Unlearning (TMU) approach, where a twin unlearning problem is defined corresponding to the original unlearning problem. Consequently, the generalization-label predictor trained on the twin problem can be transferred to the original problem, facilitating aligned data forgetting. Comprehensive empirical experiments illustrate that our approach significantly enhances the alignment between the unlearned model and the gold model.	 | 现代隐私法规推动了机器遗忘技术的发展，该技术使得训练好的模型能够高效地忘记特定的训练数据。在之前的遗忘方法中，数据遗忘的概念通常被解释和实现为在这些数据上的分类准确率达到零。然而，机器遗忘的真实目的是实现未学习模型与金模型之间的对齐，即鼓励它们在分类准确度上保持一致。另一方面，金模型由于其泛化能力经常表现有非零的分类准确度。为了实现对齐的数据遗忘，我们提出了一种双胞胎机器遗忘（Twin Machine Unlearning, TMU）方法，其中定义了一个与原始遗忘问题对应的双胞胎遗忘问题。因此，可以在双胞胎问题上训练的泛化标签预测器可以转移到原始问题上，促进对齐的数据遗忘。广泛的经验实验表明，我们的方法显著提高了未学习模型与金模型之间的对齐程度。
2501.08612	 | Neural Risk-sensitive Satisficing in Contextual Bandits	 | Shogo Ito,Tatsuji Takahashi,Yu Kono	 | 上下文臂问题是一种用于解决推荐系统挑战的有效强化学习框架，但面对大规模状态-动作空间和处理上的顺序限制等问题，需要改进。为此，研究提出了神经风险敏感满意算法（NeuralRS），通过整合神经网络来应对非线性关系，从而提高性能。	 | The contextual bandit problem, which is a type of reinforcement learning tasks, provides an effective framework for solving challenges in recommendation systems, such as satisfying real-time requirements, enabling personalization, addressing cold-start problems. However, contextual bandit algorithms face challenges since they need to handle large state-action spaces sequentially. These challenges include the high costs for learning and balancing exploration and exploitation, as well as large variations in performance that depend on the domain of application. To address these challenges, Tsuboya et~al. proposed the Regional Linear Risk-sensitive Satisficing (RegLinRS) algorithm. RegLinRS switches between exploration and exploitation based on how well the agent has achieved the target. However, the reward expectations in RegLinRS are linearly approximated based on features, which limits its applicability when the relationship between features and reward expectations is non-linear. To handle more complex environments, we proposed Neural Risk-sensitive Satisficing (NeuralRS), which incorporates neural networks into RegLinRS, and demonstrated its utility.	 | 上下文臂问题是一种强化学习任务，为解决推荐系统中的挑战，如满足实时要求、实现个性化、解决冷启动问题提供了有效的框架。然而，上下文臂算法面临着挑战，因为它们需要处理大规模的状态-动作空间，并且必须顺序进行处理。这些挑战包括学习成本高、探索与利用之间的平衡困难，以及性能的大幅波动，这取决于应用领域。为了应对这些挑战，Tsuboya等人提出了区域线性风险敏感满意算法（RegLinRS）。RegLinRS根据代理实现目标的情况在探索和利用之间切换。然而，在RegLinRS中，奖励期望基于特征进行线性逼近，这限制了其在特征与奖励期望之间的关系为非线性时的应用。为了应对更复杂的环境，我们提出了神经风险敏感满意算法（NeuralRS），该算法将神经网络整合到RegLinRS中，并展示了其实用性。
2501.08589	 | Molecular Graph Contrastive Learning with Line Graph	 | Xueyuan Chen,Shangzhe Li,Ruomei Liu,Bowen Shi,Jiaheng Liu,Junran Wu,Ke Xu	 | LEMON是一种新的图对比学习方法，通过对比分子图与其线图来自由编码分子语义，同时引入新的补丁和损失函数以增强信息传递和处理负样本，从而在分子性质预测中表现出优越性能。	 | Trapped by the label scarcity in molecular property prediction and drug design, graph contrastive learning (GCL) came forward. Leading contrastive learning works show two kinds of view generators, that is, random or learnable data corruption and domain knowledge incorporation. While effective, the two ways also lead to molecular semantics altering and limited generalization capability, respectively. To this end, we relate the \textbf{L}in\textbf{E} graph with \textbf{MO}lecular graph co\textbf{N}trastive learning and propose a novel method termed \textit{LEMON}. Specifically, by contrasting the given graph with the corresponding line graph, the graph encoder can freely encode the molecular semantics without omission. Furthermore, we present a new patch with edge attribute fusion and two local contrastive losses enhance information transmission and tackle hard negative samples. Compared with state-of-the-art (SOTA) methods for view generation, superior performance on molecular property prediction suggests the effectiveness of our proposed framework.	 | 受限于分子性质预测和药物设计中的标签稀缺性，图对比学习（GCL）应运而生。领先的对比学习工作展示了两种视图生成器，即随机或可学习的数据损坏和领域知识融合。这两种方法虽然有效，但也分别导致了分子语义改变和有限的泛化能力。针对此问题，我们将**L**in**E**图与**MO**分子图的**C**ontrastive学习联系起来，并提出了一种名为**LEMON**的新方法。具体而言，通过对比给定的图与其相应的线图，图编码器可以自由地编码分子语义而不会遗漏。此外，我们提出了一种新的补丁，结合边属性融合，并引入了两种局部对比损失，以增强信息传递并处理难以区分的负样本。与用于视图生成的最新技术(SOTA)方法相比，在分子性质预测上的优越性能表明我们提出的框架的有效性。
2501.08581	 | Normalize Then Propagate: Efficient Homophilous Regularization for Few-shot Semi-Supervised Node Classification	 | Baoming Zhang,MingCai Chen,Jianqing Song,Shuangjie Li,Jie Zhang,Chongjun Wang	 | 本文分析了图神经网络在半监督节点分类中的泛化限制，并提出了一种名为NormProp的新算法，该算法通过利用未标记节点的同质性假设生成额外的监督信号，从而在标注数据稀缺的情况下提高泛化能力。NormProp通过解耦节点表示的方向和欧几里得范数，有效捕捉类信息和聚合一致性，并通过同质性正则化方法进一步优化。	 | Graph Neural Networks (GNNs) have demonstrated remarkable ability in semi-supervised node classification. However, most existing GNNs rely heavily on a large amount of labeled data for training, which is labor-intensive and requires extensive domain knowledge. In this paper, we first analyze the restrictions of GNNs generalization from the perspective of supervision signals in the context of few-shot semi-supervised node classification. To address these challenges, we propose a novel algorithm named NormProp, which utilizes the homophily assumption of unlabeled nodes to generate additional supervision signals, thereby enhancing the generalization against label scarcity. The key idea is to efficiently capture both the class information and the consistency of aggregation during message passing, via decoupling the direction and Euclidean norm of node representations. Moreover, we conduct a theoretical analysis to determine the upper bound of Euclidean norm, and then propose homophilous regularization to constraint the consistency of unlabeled nodes. Extensive experiments demonstrate that NormProp achieve state-of-the-art performance under low-label rate scenarios with low computational complexity.	 | 图神经网络（GNNs）在半监督节点分类中展现出了显著的能力。然而，大多数现有的GNNs在训练过程中依赖大量的标记数据，这既费时又需要大量的领域知识。在本文中，我们首先从少量标记数据的半监督节点分类监督信号的角度分析了GNNs的泛化限制。为了解决这些问题，我们提出了一种名为NormProp的新算法，该算法利用未标记节点的同质性假设生成额外的监督信号，从而增强在标注数据稀缺情况下的泛化能力。核心思想是通过解耦节点表示的方向和欧几里得范数，有效地捕捉消息传递过程中的类信息和聚合一致性。此外，我们进行了理论分析以确定欧几里得范数的上限，并提出了一种同质性正则化方法来约束未标记节点的一致性。大量的实验表明，NormProp在低标注率情况下实现了最先进的性能，且计算复杂度较低。
2501.08572	 | DNMDR: Dynamic Networks and Multi-view Drug Representations for Safe Medication Recommendation	 | Guanlin Liu,Xiaomei Yu,Zihao Liu,Xue Li,Xingxu Fan,Xiangwei Zheng	 | 本文提出了一种新的药品推荐（MR）方法——多视图药物表示（DNMDR），通过结合动态网络和多视图药物表示，利用加权快照序列构建动态异质网络，以捕获医疗事件的时间依赖性和药物的交互信息，从而提高了MR系统的性能。实验结果表明，该方法在多项指标上优于现有最先进的基线模型。	 | Medication Recommendation (MR) is a promising research topic which booms diverse applications in the healthcare and clinical domains. However, existing methods mainly rely on sequential modeling and static graphs for representation learning, which ignore the dynamic correlations in diverse medical events of a patient's temporal visits, leading to insufficient global structural exploration on nodes. Additionally, mitigating drug-drug interactions (DDIs) is another issue determining the utility of the MR systems. To address the challenges mentioned above, this paper proposes a novel MR method with the integration of dynamic networks and multi-view drug representations (DNMDR). Specifically, weighted snapshot sequences for dynamic heterogeneous networks are constructed based on discrete visits in temporal EHRs, and all the dynamic networks are jointly trained to gain both structural correlations in diverse medical events and temporal dependency in historical health conditions, for achieving comprehensive patient representations with both semantic features and structural relationships. Moreover, combining the drug co-occurrences and adverse drug-drug interactions (DDIs) in internal view of drug molecule structure and interactive view of drug pairs, the safe drug representations are available to obtain high-quality medication combination recommendation. Finally, extensive experiments on real world datasets are conducted for performance evaluation, and the experimental results demonstrate that the proposed DNMDR method outperforms the state-of-the-art baseline models with a large margin on various metrics such as PRAUC, Jaccard, DDI rates and so on.	 | 药品推荐（MR）是一个充满前景的研究领域，在医疗和临床应用中展现出多样化的用途。然而，现有的方法主要依赖于序列建模和静态图进行表示学习，这忽略了患者随访过程中多样化医疗事件的动态关联，导致节点上的全球结构探索不足。此外，减少药物-药物相互作用（DDIs）是决定MR系统实用性的另一个问题。为了应对上述挑战，本文提出了一种结合动态网络和多视图药物表示（DNMDR）的新颖MR方法。具体地，基于时间EHR中的离散访问，构建加权快照序列以构建动态异质网络，并将所有动态网络联合训练以获得不同医疗事件中的结构关联以及历史健康状况的时间依赖性，从而实现包含语义特征和结构关系的综合患者表示。此外，结合药物分子结构内部视图中的共现药物和交互视图中的不良药物-药物相互作用（DDIs），可以得到安全的药物表示，以获得高质量的药物组合推荐。最后，对真实世界数据集进行了广泛的实验证明性能，实验结果表明，提出的DNMDR方法在PRAUC、Jaccard、DDI率等各项指标上显著优于最先进的基线模型。
2501.08563	 | Adaptive Sampled Softmax with Inverted Multi-Index: Methods, Theory and Applications	 | Jin Chen,Jin Zhang,Xu huang,Yi Yang,Defu Lian,Enhong Chen	 | MIDX采样器通过将softmax概率分解为多项式概率并结合自适应采样策略，显著降低了多类分类中的计算复杂度，同时保持了较低的偏差和高效的梯度计算，适用于大规模和极端多类分类任务。	 | The softmax function is a cornerstone of multi-class classification, integral to a wide range of machine learning applications, from large-scale retrieval and ranking models to advanced large language models. However, its computational cost grows linearly with the number of classes, which becomes prohibitively expensive in scenarios with millions or even billions of classes. The sampled softmax, which relies on self-normalized importance sampling, has emerged as a powerful alternative, significantly reducing computational complexity. Yet, its estimator remains unbiased only when the sampling distribution matches the true softmax distribution. To improve both approximation accuracy and sampling efficiency, we propose the MIDX Sampler, a novel adaptive sampling strategy based on an inverted multi-index approach. Concretely, we decompose the softmax probability into several multinomial probabilities, each associated with a specific set of codewords and the last associated with the residual score of queries, thus reducing time complexity to the number of codewords instead of the number of classes. To further boost efficiency, we replace the query-specific residual probability with a simple uniform distribution, simplifying the computation while retaining high performance. Our method is backed by rigorous theoretical analysis, addressing key concerns such as sampling bias, gradient bias, convergence rates, and generalization error bounds. The results demonstrate that a smaller divergence from the ideal softmax distribution leads to faster convergence and improved generalization. Extensive experiments on large-scale language models, sequential recommenders, and extreme multi-class classification tasks confirm that the MIDX-Sampler delivers superior effectiveness and efficiency compared to existing approaches.	 | softmax函数是多类分类的核心，广泛应用于各种机器学习应用，从大规模检索和排名模型到先进的大型语言模型。然而，softmax函数的计算成本随着类别的数量线性增长，在有数百万甚至数十亿个类别的场景中变得极其昂贵。自规范化重要性采样的随机采样softmax函数作为一种有力的替代方案，显著降低了计算复杂度。不过，其估计器仅在采样分布与真实softmax分布匹配时保持无偏性。为了提高近似精度和采样效率，我们提出了MIDX采样器，这是一种基于倒置多索引方法的新型自适应采样策略。具体来说，我们将softmax概率分解为几个多项式概率，每个概率与一组特定的代码关联，最后一个与查询的剩余分数关联，从而将时间复杂度从类别数量降低到代码数量。为了进一步提高效率，我们用简单的均匀分布替换查询特定的剩余概率，简化了计算过程，同时保持高性能。我们的方法得到了严格的理论分析支持，解决了采样偏差、梯度偏差、收敛速率和泛化误差边界等关键问题。实验结果表明，与理想softmax分布的较小偏差会导致更快的收敛和更好的泛化性能。在大规模语言模型、序列推荐和极端多类分类任务中进行的广泛实验表明，MIDX-Sampler相比现有方法在有效性和效率方面更具优势。
2501.08538	 | Homophily-aware Heterogeneous Graph Contrastive Learning	 | Haosen Wang,Chenglong Shi,Can Xu,Surong Yan,Pan Tang	 | 该研究提出了一个新颖的异质图对比学习框架HGMS，通过利用连接强度和多视图自我表达来学习同质节点表示，并通过异质边删除增强策略和多视图自我表达学习方法提高同质性，实验结果表明HGMS在多个下游任务中表现优越。	 | Heterogeneous graph pre-training (HGP) has demonstrated remarkable performance across various domains. However, the issue of heterophily in real-world heterogeneous graphs (HGs) has been largely overlooked. To bridge this research gap, we proposed a novel heterogeneous graph contrastive learning framework, termed HGMS, which leverages connection strength and multi-view self-expression to learn homophilous node representations. Specifically, we design a heterogeneous edge dropping augmentation strategy that enhances the homophily of augmented views. Moreover, we introduce a multi-view self-expressive learning method to infer the homophily between nodes. In practice, we develop two approaches to solve the self-expressive matrix. The solved self-expressive matrix serves as an additional augmented view to provide homophilous information and is used to identify false negatives in contrastive loss. Extensive experimental results demonstrate the superiority of HGMS across different downstream tasks.	 | 异质图预训练（HGP）在各个领域中都展现出了显著的性能。然而，真实世界异质图（HG）中的异质性问题在现有研究中被忽视了。为解决这一研究空白，我们提出了一种新颖的异质图对比学习框架，称为HGMS，该框架利用连接强度和多视图自我表达来学习同质节点表示。具体而言，我们设计了一种异质边删除增强策略，以增强增强视图中的同质性。此外，我们引入了一种多视图自我表达学习方法来推断节点之间的同质性。在实践中，我们开发了两种方法来解决自我表达矩阵。解决后的自我表达矩阵用作附加增强视图，提供同质信息，并用于在对比损失中识别假阴性。大量实验结果表明，HGMS在不同下游任务中具有优越性。
2501.08521	 | Mitigating Domain Shift in Federated Learning via Intra- and Inter-Domain Prototypes	 | Huy Q. Le,Ye Lin Tun,Yu Qiao,Minh N. H. Nguyen,Keon Oh Kim,Choong Seon Hong	 | 联邦原型学习方法I²PFL结合了同域和跨域原型，通过特征对齐和跨域原型的重新加权机制，有效减轻了域偏移问题，提升了联邦学习中全局模型的性能。	 | Federated Learning (FL) has emerged as a decentralized machine learning technique, allowing clients to train a global model collaboratively without sharing private data. However, most FL studies ignore the crucial challenge of heterogeneous domains where each client has a distinct feature distribution, which is common in real-world scenarios. Prototype learning, which leverages the mean feature vectors within the same classes, has become a prominent solution for federated learning under domain skew. However, existing federated prototype learning methods only consider inter-domain prototypes on the server and overlook intra-domain characteristics. In this work, we introduce a novel federated prototype learning method, namely I$^2$PFL, which incorporates $\textbf{I}$ntra-domain and $\textbf{I}$nter-domain $\textbf{P}$rototypes, to mitigate domain shifts and learn a generalized global model across multiple domains in federated learning. To construct intra-domain prototypes, we propose feature alignment with MixUp-based augmented prototypes to capture the diversity of local domains and enhance the generalization of local features. Additionally, we introduce a reweighting mechanism for inter-domain prototypes to generate generalized prototypes to provide inter-domain knowledge and reduce domain skew across multiple clients. Extensive experiments on the Digits, Office-10, and PACS datasets illustrate the superior performance of our method compared to other baselines.	 | 联邦学习（FL）作为一种去中心化的机器学习技术 emerged 作为去中心化的机器学习技术，允许客户端在不共享私人数据的情况下协作训练全局模型。然而，大多数FL研究忽视了不同域（每个客户端具有独特的特征分布）这一关键挑战，这在现实场景中非常常见。原型学习，即利用同一类内均特征向量的方法，已成为解决联邦学习中域偏移问题的主流解决方案。然而，现有的联邦原型学习方法仅在服务器上考虑跨域原型，而忽略了同域特性。本文介绍了新的联邦原型学习方法，即 I²PFL（Intra-domain and Inter-domain Prototypes Federated Learning），该方法结合了同域和跨域原型，以减轻域偏移并跨多个域学习一个普适的全局模型。为了构建同域原型，我们提出了基于MixUp增强原型的特征对齐方法，以捕捉本地域的多样性并增强局部特征的泛化能力。此外，我们引入了跨域原型的重新加权机制，以生成普适的原型来提供跨域知识并减少多个客户端之间的域偏移。在 Digits、Office-10 和 PACS 数据集上的广泛实验表明，与其它基准方法相比，我们的方法具有更优的性能。
2501.08515	 | Learning Hyperplane Tree: A Piecewise Linear and Fully Interpretable Decision-making Framework	 | Hongyi Li,Jun Xu,William Ward Armstrong	 | 本文提出了一种名为学习超平面树（LHT）的新模型，该模型在多个公开数据集上的分类任务中表现优于现有最先进的树模型，通过使用多个超平面逐步区分样本并结合分段线性隶属函数进行分类，LHT 提供了高度透明和可解释的分类过程。	 | This paper introduces a novel tree-based model, Learning Hyperplane Tree (LHT), which outperforms state-of-the-art (SOTA) tree models for classification tasks on several public datasets. The structure of LHT is simple and efficient: it partitions the data using several hyperplanes to progressively distinguish between target and non-target class samples. Although the separation is not perfect at each stage, LHT effectively improves the distinction through successive partitions. During testing, a sample is classified by evaluating the hyperplanes defined in the branching blocks and traversing down the tree until it reaches the corresponding leaf block. The class of the test sample is then determined using the piecewise linear membership function defined in the leaf blocks, which is derived through least-squares fitting and fuzzy logic. LHT is highly transparent and interpretable--at each branching block, the contribution of each feature to the classification can be clearly observed.	 | 本文介绍了一种新型树型模型——学习超平面树（LHT），该模型在多个公开数据集上的分类任务中表现优于现有的最先进的（SOTA）树模型。LHT 的结构简单高效：它使用多个超平面逐步区分目标类和非目标类样本。虽然在每个阶段的分离可能不完全完美，但 LHT 通过连续的分区逐步提高了区分能力。在测试过程中，样本通过评估分支块中定义的超平面并沿树向下遍历，直到到达相应的叶块，从而进行分类。然后，测试样本的类别是通过在叶块中定义的分段线性隶属函数来确定的，该函数是通过最小二乘拟合和模糊逻辑推导出来的。LHT 非常透明和可解释，在每个分支块中，每个特征对分类的贡献可以清楚地观察到。
2501.08508	 | Score-based 3D molecule generation with neural fields	 | Matthieu Kirchmeyer,Pedro O. Pinheiro,Saeed Saremi	 | 研究人员提出了一种基于连续原子密度场的新3D分子表示方法，并开发了名为FuncMol的模型，该模型能够在连续空间中无条件生成3D分子，适用于各种分子大小且采样速度快，尤其适用于宏环肽的生成。	 | We introduce a new representation for 3D molecules based on their continuous atomic density fields. Using this representation, we propose a new model based on walk-jump sampling for unconditional 3D molecule generation in the continuous space using neural fields. Our model, FuncMol, encodes molecular fields into latent codes using a conditional neural field, samples noisy codes from a Gaussian-smoothed distribution with Langevin MCMC (walk), denoises these samples in a single step (jump), and finally decodes them into molecular fields. FuncMol performs all-atom generation of 3D molecules without assumptions on the molecular structure and scales well with the size of molecules, unlike most approaches. Our method achieves competitive results on drug-like molecules and easily scales to macro-cyclic peptides, with at least one order of magnitude faster sampling. The code is available at https://github.com/prescient-design/funcmol.	 | 我们提出了一种基于连续原子密度场的新3D分子表示方法。利用这种表示方法，我们提出了一种新的模型，该模型使用神经场中的走跳采样，在连续空间中无条件生成3D分子。我们的模型FuncMol 使用条件神经场将分子场编码为潜在代码，从平滑的高斯分布中以拉普拉斯MCMC（走）的方式采样噪声代码，然后在一个步骤中清除这些样本的噪点（跳），最后将它们解码回分子场。FuncMol 能在不假设分子结构的情况下生成所有的原子，并且在分子大小增加时也能很好地扩展，这与大多数方法不同。我们的方法在药物样分子上取得了具有竞争力的结果，并且能够轻松扩展到宏环肽，同时采样速度至少快了一个数量级。代码可在 https://github.com/prescient-design/funcmol 获取。
2501.08506	 | Exploring the Efficacy of Meta-Learning: Unveiling Superior Data Diversity Utilization of MAML Over Pre-training	 | Kavita Selva,Satita Vittayaareekul,Brando Miranda	 | 研究发现，测试集准确率与数据集多样性之间存在中等到较强的正相关关系，表明数据集的多样性对视觉模型的性能有重要影响。该研究通过分析多种模型配置在多个视觉数据集上的表现，展示了数据多样性作为衡量指标的潜力，并强调了在大规模学习领域中理解数据集的重要性。	 | Currently, data and model size dominate the narrative in the training of super-large, powerful models. However, there has been a lack of exploration on the effect of other attributes of the training dataset on model performance. We hypothesize that dataset diversity can impact the performance of vision models. Our study shows positive correlations between test set accuracy and data diversity, providing an argument for furthering the research of dataset attributes beyond size. We analyzed pre-training and model-agnostic meta-learning methods on twelve popular visual datasets (e.g., Omniglot, CIFAR-FS, Aircraft) and five model configurations, including MAML variants with different numbers of inner gradient steps and supervised learning. We show moderate to strong positive correlations (R-squared: 0.15-0.42) between accuracy and data diversity and weaker but significant correlations (R-squared: ~0.2) between loss and diversity. These findings support our hypothesis and demonstrate a promising way for a deeper exploration of how formal data diversity influences model performance. This initial study highlights the potential of (Task2Vec) data diversity as a valuable measure in the rapidly evolving field of large-scale learning and emphasizes that understanding the dataset is key to building more powerful and generalizable models.	 | 目前，数据和模型大小主导着超级大型、强大模型训练的叙事。然而，对于训练数据集的其他属性对模型性能的影响，研究相对缺乏。我们假设数据集多样性可以影响视觉模型的性能。我们的研究显示了测试集准确率与数据多样性之间的正相关关系，这为超越数据集规模的研究提供了论据。我们分析了预训练和模型泛化元学习方法在十二个流行的视觉数据集（例如，Omniglot、CIFAR-FS、Aircraft）和五种模型配置上的效果，包括带有不同内部梯度步数的MAML变体以及监督学习。我们展示了准确率与数据多样性之间中等到较强的正相关关系（决定系数：0.15-0.42），以及损失与多样性之间较弱但显著的相关关系（决定系数：约0.2）。这些发现支持了我们的假设，并表明了进一步探索正式数据多样性如何影响模型性能的有希望途径。这项初步研究突显了（Task2Vec）数据多样性作为一种有价值度量的潜力，在快速发展的大规模学习领域中，理解数据集是构建更强大、更具泛化能力模型的关键。
2501.08464	 | Time series forecasting for multidimensional telemetry data using GAN and BiLSTM in a Digital Twin	 | Joao Carmo de Almeida Neto,Claudio Miceli de Farias,Leandro Santiago de Araujo,Leopoldo Andre Dutra Lusquino Filho	 | 近年来，研究重点不仅在于构建数字孪生，还在于提供与物理数据相关的服务，如利用时间序列模型预测物理部件的未来行为。为提高预测精度，本研究提出结合双向LSTM层与生成对抗网络生成的时间序列，以更好地处理多变量环境中的时间序列数据。	 | The research related to digital twins has been increasing in recent years. Besides the mirroring of the physical word into the digital, there is the need of providing services related to the data collected and transferred to the virtual world. One of these services is the forecasting of physical part future behavior, that could lead to applications, like preventing harmful events or designing improvements to get better performance. One strategy used to predict any system operation it is the use of time series models like ARIMA or LSTM, and improvements were implemented using these algorithms. Recently, deep learning techniques based on generative models such as Generative Adversarial Networks (GANs) have been proposed to create time series and the use of LSTM has gained more relevance in time series forecasting, but both have limitations that restrict the forecasting results. Another issue found in the literature is the challenge of handling multivariate environments/applications in time series generation. Therefore, new methods need to be studied in order to fill these gaps and, consequently, provide better resources for creating useful digital twins. In this proposal, it is going to be studied the integration of a BiLSTM layer with a time series obtained by GAN in order to improve the forecasting of all the features provided by the dataset in terms of accuracy and, consequently, improving behaviour prediction.	 | 近年来，关于数字孪生的研究一直在增加。除了将物理世界映射到数字世界之外，还需要提供与收集和传输到虚拟世界的数据相关的服务。其中一种服务是预测物理部件的未来行为，这可能导致诸如预防有害事件或设计改进以获得更好性能的应用。为了预测任何系统的操作，常使用时间序列模型如ARIMA或LSTM，并通过这些算法实施改进。最近，基于生成模型的深度学习技术，如生成对抗网络（GANs），被提出用于生成时间序列，LSTM在时间序列预测中的应用也越来越普遍，但两者都存在限制，这限制了预测结果。文献中还发现的问题之一是如何处理时间序列生成中的多变量环境/应用。因此，需要研究新的方法来填补这些空白，从而提供更好的资源来创建有用的数字孪生。在这项提案中，将研究将双向LSTM（BiLSTM）层与由GAN生成的时间序列相结合的方法，以提高对数据集所提供的所有特征的预测精度，从而相应地提高行为预测。
2501.08455	 | Keras Sig: Efficient Path Signature Computation on GPU in Keras 3	 | Rémi Genet,Hugo Inzirillo	 | Keras Sig 是一个基于 Keras 3 的高性能 Python 库，用于为深度学习应用计算路径签名，通过利用 GPU 并行性将训练时间减少 55%，同时保持类似 CPU 性能，相比现有方法速度提升了 5 到 10 倍。	 | In this paper we introduce Keras Sig a high-performance pythonic library designed to compute path signature for deep learning applications. Entirely built in Keras 3, \textit{Keras Sig} leverages the seamless integration with the mostly used deep learning backends such as PyTorch, JAX and TensorFlow. Inspired by Kidger and Lyons (2021),we proposed a novel approach reshaping signature calculations to leverage GPU parallelism. This adjustment allows us to reduce the training time by 55\% and 5 to 10-fold improvements in direct signature computation compared to existing methods, while maintaining similar CPU performance. Relying on high-level tensor operations instead of low-level C++ code, Keras Sig significantly reduces the versioning and compatibility issues commonly encountered in deep learning libraries, while delivering superior or comparable performance across various hardware configurations. We demonstrate through extensive benchmarking that our approach scales efficiently with the length of input sequences and maintains competitive performance across various signature parameters, though bounded by memory constraints for very large signature dimensions.	 | 在本文中，我们介绍了Keras Sig，这是一个高性能的Python库，旨在为深度学习应用计算路径签名。整个库完全基于Keras 3构建，并利用与主流深度学习后端（如PyTorch、JAX和TensorFlow）的无缝集成。受到Kidger和Lyons（2021）的启发，我们提出了一种新颖的方法，将签名计算重塑为利用GPU并行性。这种方法使得训练时间减少了55%，在直接计算签名方面，与现有方法相比提高了5到10倍的速度，同时保持类似的CPU性能。通过高层张量操作而不是低级C++代码，Keras Sig 显著减少了在深度学习库中常见的版本管理和兼容性问题，同时在各种硬件配置下提供卓越或可比的性能。我们通过广泛的基准测试表明，我们的方法能够有效地扩展到输入序列的长度，并在各种路径参数下保持竞争力，尽管在非常大的路径维度下受到内存限制的约束。
2501.08430	 | Physics-informed neural networks for phase-resolved data assimilation and prediction of nonlinear ocean waves	 | Svenja Ehlers,Norbert Hoffmann,Tianning Tang,Adrian H. Callaghan,Rui Cao,Enrique M. Padilla,Yuxin Fang,Merten Stender	 | 本文提出了一种利用物理信息神经网络（PINNs）将潜在流理论（PFT）解参数化的新型方法，旨在提高海洋科学和工程中相位分辨表面重力波的预测和数据同化效率，能够准确捕捉非线性波浪动态并从表面高度测量中推断流体速度。	 | The assimilation and prediction of phase-resolved surface gravity waves are critical challenges in ocean science and engineering. Potential flow theory (PFT) has been widely employed to develop wave models and numerical techniques for wave prediction. However, traditional wave prediction methods are often limited. For example, most simplified wave models have a limited ability to capture strong wave nonlinearity, while fully nonlinear PFT solvers often fail to meet the speed requirements of engineering applications. This computational inefficiency also hinders the development of effective data assimilation techniques, which are required to reconstruct spatial wave information from sparse measurements to initialize the wave prediction. To address these challenges, we propose a novel solver method that leverages physics-informed neural networks (PINNs) that parameterize PFT solutions as neural networks. This provides a computationally inexpensive way to assimilate and predict wave data. The proposed PINN framework is validated through comparisons with analytical linear PFT solutions and experimental data collected in a laboratory wave flume. The results demonstrate that our approach accurately captures and predicts irregular, nonlinear, and dispersive wave surface dynamics. Moreover, the PINN can infer the fully nonlinear velocity potential throughout the entire fluid volume solely from surface elevation measurements, enabling the calculation of fluid velocities that are difficult to measure experimentally.	 | 海洋科学和工程中，相位分辨表面重力波的同化和预测是关键挑战。潜在流理论（PFT）广泛应用于波浪模型和波浪预测的数值方法的开发。然而，传统的波浪预测方法往往受到限制。例如，大多数简化波浪模型难以捕捉强波非线性效应，而全非线性PFT求解器则往往无法满足工程应用的速度要求。这种计算效率低下也妨碍了有效数据同化技术的发展，这些技术对于从稀疏测量中重构空间波浪信息以初始化波浪预测是必要的。为了解决这些挑战，我们提出了一种新型求解方法，利用物理信息神经网络（PINNs）将PFT解参数化为神经网络。这种方法提供了一种计算成本低的方式来进行波浪数据的同化和预测。我们提出的PINN框架通过与解析线性PFT解和实验室波槽中收集的实际数据进行比较进行验证。结果表明，我们的方法能够准确捕捉和预测不规则、非线性和色散的波浪表面动态。此外，PINN可以从表面高度测量中推断出整个流体体积内的全非线性速度势，从而计算出难以通过实验测量的流体速度。
2501.08428	 | Physics-Informed Latent Neural Operator for Real-time Predictions of Complex Physical Systems	 | Sharmila Karumuri,Lori Graham-Brady,Somdatta Goswami	 | 本文提出了一种基于物理的潜空间DeepONet（PI-Latent-NO），该方法通过结合物理信息和端到端训练，解决了传统DeepONet在大规模和复杂系统中的泛化问题和数据依赖问题，同时保持高精度和计算效率。	 | Deep operator network (DeepONet) has shown great promise as a surrogate model for systems governed by partial differential equations (PDEs), learning mappings between infinite-dimensional function spaces with high accuracy. However, achieving low generalization errors often requires highly overparameterized networks, posing significant challenges for large-scale, complex systems. To address these challenges, latent DeepONet was proposed, introducing a two-step approach: first, a reduced-order model is used to learn a low-dimensional latent space, followed by operator learning on this latent space. While effective, this method is inherently data-driven, relying on large datasets and making it difficult to incorporate governing physics into the framework. Additionally, the decoupled nature of these steps prevents end-to-end optimization and the ability to handle data scarcity. This work introduces PI-Latent-NO, a physics-informed latent operator learning framework that overcomes these limitations. Our architecture employs two coupled DeepONets in an end-to-end training scheme: the first, termed Latent-DeepONet, identifies and learns the low-dimensional latent space, while the second, Reconstruction-DeepONet, maps the latent representations back to the original physical space. By integrating governing physics directly into the training process, our approach requires significantly fewer data samples while achieving high accuracy. Furthermore, the framework is computationally and memory efficient, exhibiting nearly constant scaling behavior on a single GPU and demonstrating the potential for further efficiency gains with distributed training. We validate the proposed method on high-dimensional parametric PDEs, demonstrating its effectiveness as a proof of concept and its potential scalability for large-scale systems.	 | 深嵌入网络（DeepONet）在作为偏微分方程（PDEs）所支配系统的代理模型方面表现出巨大的潜力，能够以高精度学习无限维函数空间之间的映射。然而，要实现低泛化误差通常需要高度过参数化的网络，这为大规模和复杂系统带来了显著挑战。为应对这些挑战，提出了潜空间DeepONet，引入了两步方法：首先，使用降阶模型来学习低维潜空间，然后在潜空间上进行算子学习。虽然这种方法效果显著，但它本质上是数据驱动的，依赖于大数据集，难以将支配物理纳入框架中。此外，这些步骤的解耦性质妨碍了端到端优化和处理数据稀缺的能力。本文引入了PI-Latent-NO，这是一种基于物理的潜空间算子学习框架，克服了这些限制。我们的架构采用两个耦合的DeepONet进行端到端训练：第一个称为潜空间DeepONet，用于识别并学习低维潜空间；第二个称为重建DeepONet，用于将潜空间表示映射回原始物理空间。通过直接将支配物理纳入训练过程，我们的方法可以显著减少所需的数据样本量，同时保持高精度。此外，该框架在计算和内存效率方面表现出色，在单个GPU上具有几乎恒定的标度行为，并且显示出分布式训练进一步提高效率的潜力。我们通过高维参数化PDEs验证了所提方法的有效性，证明了其作为概念证明的有效性及其在大规模系统中的潜在可扩展性。
2501.08426	 | Causal vs. Anticausal merging of predictors	 | Sergio Hernan Garrido Mejia,Patrick Blöbaum,Bernhard Schölkopf,Dominik Janzing	 | 该研究探讨了在使用因果最大熵（CMAXENT）合并预测器时，由于因果方向和反因果方向的不对称性导致的差异，发现全数据情况下CMAXENT解分别对应逻辑回归和线性判别分析，部分数据情况下两种解的决策边界不同，影响泛化能力。	 | We study the differences arising from merging predictors in the causal and anticausal directions using the same data. In particular we study the asymmetries that arise in a simple model where we merge the predictors using one binary variable as target and two continuous variables as predictors. We use Causal Maximum Entropy (CMAXENT) as inductive bias to merge the predictors, however, we expect similar differences to hold also when we use other merging methods that take into account asymmetries between cause and effect. We show that if we observe all bivariate distributions, the CMAXENT solution reduces to a logistic regression in the causal direction and Linear Discriminant Analysis (LDA) in the anticausal direction. Furthermore, we study how the decision boundaries of these two solutions differ whenever we observe only some of the bivariate distributions implications for Out-Of-Variable (OOV) generalisation.	 | 我们研究使用相同数据在因果方向和反因果方向合并预测器时产生的差异。特别是，我们研究在一种简单模型中的不对称性，该模型使用一个二元变量作为目标变量，且有两个连续变量作为预测器。我们使用因果最大熵（CMAXENT）作为归纳偏见来合并预测器，然而，我们预计当使用考虑因果与效应之间不对称性的其他合并方法时，也会出现类似差异。我们表明，如果观察到所有二元分布，CMAXENT 解决方案在因果方向上会简化为逻辑回归，在反因果方向上会简化为线性判别分析（LDA）。此外，我们研究了仅观察部分二元分布时这两种解决方案的决策边界如何不同，这对变量之外的一般化（OOV generalization）有何影响。
2501.08425	 | Is Stochastic Gradient Descent Effective? A PDE Perspective on Machine Learning processes	 | Davide Barbieri,Matteo Bonforte,Peio Ibarrondo	 | 本文分析了随机梯度下降（SGD）在优化神经网络权重过程中的行为，特别是在非凸损失函数和退化扩散矩阵条件下的动态，通过Fokker-Planck方程揭示了SGD的两个主要阶段：漂移阶段和扩散阶段，并探讨了其渐近收敛性和平均退出时间。	 | In this paper we analyze the behaviour of the stochastic gradient descent (SGD), a widely used method in supervised learning for optimizing neural network weights via a minimization of non-convex loss functions. Since the pioneering work of E, Li and Tai (2017), the underlying structure of such processes can be understood via parabolic PDEs of Fokker-Planck type, which are at the core of our analysis. Even if Fokker-Planck equations have a long history and a extensive literature, almost nothing is known when the potential is non-convex or when the diffusion matrix is degenerate, and this is the main difficulty that we face in our analysis.   We identify two different regimes: in the initial phase of SGD, the loss function drives the weights to concentrate around the nearest local minimum. We refer to this phase as the drift regime and we provide quantitative estimates on this concentration phenomenon. Next, we introduce the diffusion regime, where stochastic fluctuations help the learning process to escape suboptimal local minima. We analyze the Mean Exit Time (MET) and prove upper and lower bounds of the MET. Finally, we address the asymptotic convergence of SGD, for a non-convex cost function and a degenerate diffusion matrix, that do not allow to use the standard approaches, and require new techniques. For this purpose, we exploit two different methods: duality and entropy methods.   We provide new results about the dynamics and effectiveness of SGD, offering a deep connection between stochastic optimization and PDE theory, and some answers and insights to basic questions in the Machine Learning processes: How long does SGD take to escape from a bad minimum? Do neural network parameters converge using SGD? How do parameters evolve in the first stage of training with SGD?	 | 在本文中，我们分析了随机梯度下降（SGD）的行为，这是一种在监督学习中广泛用于通过最小化非凸损失函数来优化神经网络权重的方法。自E, Li和Tai（2017）的开创性工作以来，这类过程的结构可以通过Fokker-Planck型抛物型偏微分方程来理解，这些方程构成了我们分析的核心。尽管Fokker-Planck方程有着悠久的历史和广泛的文献，但在势函数非凸或扩散矩阵退化的条件下，几乎没有什么已知的信息，这也是我们在分析中面临的最大困难。   我们确定了两个不同的阶段：在SGD的初期阶段，损失函数促使权重集中在最近的局部极小值附近。我们将这一阶段称为漂移阶段，并提供关于这一集中现象的定量估计。接着，我们引入了扩散阶段，在这一阶段中，随机波动有助于学习过程从劣质局部极小值中逃脱。我们分析了平均退出时间（MET），并证明了MET的上界和下界。最后，我们探讨了在非凸成本函数和退化扩散矩阵下SGD的渐近收敛性，这些情况不允许使用标准方法，需要新的技术。为了这一目的，我们利用了两种不同的方法：对偶性和熵方法。   我们提供了关于SGD动态和有效性的新结果，建立了随机优化与偏微分方程理论之间的深刻联系，并对机器学习过程中的基本问题提供了新的见解和答案：SGD需要多长时间才能从一个糟糕的极小值中逃脱？使用SGD时，神经网络参数是否会收敛？在使用SGD进行训练的初期阶段，参数是如何演化的？
2501.08418	 | CVaR-Based Variational Quantum Optimization for User Association in Handoff-Aware Vehicular Networks	 | Zijiang Yan,Hao Zhou,Jianhua Pei,Aryan Kaushik,Hina Tabassum,Ping Wang	 | 本文提出了一种基于条件价值风险的变分量子本征值求解器框架，用于解决车联网中的广义分配问题，并通过在嘈杂的中间尺度量子设备上优化收敛性和鲁棒性，实现了优于深度神经网络方法的性能。	 | Efficient resource allocation is essential for optimizing various tasks in wireless networks, which are usually formulated as generalized assignment problems (GAP). GAP, as a generalized version of the linear sum assignment problem, involves both equality and inequality constraints that add computational challenges. In this work, we present a novel Conditional Value at Risk (CVaR)-based Variational Quantum Eigensolver (VQE) framework to address GAP in vehicular networks (VNets). Our approach leverages a hybrid quantum-classical structure, integrating a tailored cost function that balances both objective and constraint-specific penalties to improve solution quality and stability. Using the CVaR-VQE model, we handle the GAP efficiently by focusing optimization on the lower tail of the solution space, enhancing both convergence and resilience on noisy intermediate-scale quantum (NISQ) devices. We apply this framework to a user-association problem in VNets, where our method achieves 23.5% improvement compared to the deep neural network (DNN) approach.	 | 无线网络中各种任务的高效资源分配对于优化至关重要，这通常被形式化为广义分配问题（Generalized Assignment Problem, GAP）。GAP 是线性和分配问题的广义版本，包含等式和不等式约束，增加了计算上的难度。本文提出了一种基于条件价值风险（Conditional Value at Risk, CVaR）的变分量子本征值求解器（Variational Quantum Eigensolver, VQE）框架，用于解决车联网（Vehicular Networks, VNets）中的GAP问题。我们的方法利用了混合量子-经典结构，结合了一个定制的成本函数，该函数平衡了目标和约束特定的惩罚，以提高解决方案的质量和稳定性。通过CVaR-VQE模型，我们通过专注于解决方案空间的下尾部进行优化，从而在嘈杂的中间尺度量子（Noisy Intermediate-Scale Quantum, NISQ）设备上提高了收敛性和鲁棒性。我们将此框架应用于VNets中的用户关联问题，在与深度神经网络（Deep Neural Network, DNN）方法相比时，我们的方法实现了23.5%的改进。
2501.08411	 | BiDepth Multimodal Neural Network: Bidirectional Depth Deep Learning Arcitecture for Spatial-Temporal Prediction	 | Sina Ehsani,Fenglian Pan,Qingpei Hu,Jian Liu	 | 本文提出了一种双向深度调控的双深度多模态神经网络（BDMNN），以解决动态系统中时空信息准确预测的挑战，该网络能够同时处理长期季节性和短期波动，适应复杂的时空上下文。实验结果表明，BDMNN在城市交通预测和雨量预报中分别降低了12%的均方误差和提高了15%的预测准确性。	 | Accurate prediction of spatial-temporal (ST) information in dynamic systems, such as urban mobility and weather patterns, is a crucial yet challenging problem. The complexity stems from the intricate interplay between spatial proximity and temporal relevance, where both long-term trends and short-term fluctuations are present in convoluted patterns. Existing approaches, including traditional statistical methods and conventional neural networks, may provide inaccurate results due to the lack of an effective mechanism that simultaneously incorporates information at variable temporal depths while maintaining spatial context, resulting in a trade-off between comprehensive long-term historical analysis and responsiveness to short-term new information. To bridge this gap, this paper proposes the BiDepth Multimodal Neural Network (BDMNN) with bidirectional depth modulation that enables a comprehensive understanding of both long-term seasonality and short-term fluctuations, adapting to the complex ST context. Case studies with real-world public data demonstrate significant improvements in prediction accuracy, with a 12% reduction in Mean Squared Error for urban traffic prediction and a 15% improvement in rain precipitation forecasting compared to state-of-the-art benchmarks, without demanding extra computational resources.	 | 动态系统（如城市流动性和天气模式）中的时空（ST）信息准确预测是一个关键但具有挑战性的问题。这种复杂性源于空间接近性与时间相关性之间的微妙互动，其中既存在长期趋势也存在短期波动，这些都在复杂的模式中交织在一起。现有的方法，包括传统的统计方法和传统的神经网络，由于缺乏一种能够同时有效结合变量时间深度信息并保持空间上下文的机制，可能导致不准确的结果。这种机制会导致对长期历史分析的全面性与对短期新信息的反应之间的权衡。为了解决这一差距，本文提出了一种双向深度调控的双深度多模态神经网络（BDMNN），该网络能够全面理解长期季节性和短期波动，适应复杂的时空上下文。通过使用真实世界公开数据的实际案例研究，证明了预测准确性的显著提高，与最先进的基准相比，在城市交通预测中降低了12%的均方误差，在雨量预报中提高了15%，且无需额外的计算资源。
2501.08397	 | Predict Confidently, Predict Right: Abstention in Dynamic Graph Learning	 | Jayadratha Gayen,Himanshu Pal,Naresh Manwani,Charu Sharma	 | 该研究将拒绝选项策略引入时间图神经网络，通过在高不确定性和低信心情况下避免预测，减少关键分类错误，提高了动态图中链接预测和节点分类的可靠性和准确性。实验结果表明，该方法有效改善了预测评分，并在处理不平衡数据集时表现出色。	 | Many real-world systems can be modeled as dynamic graphs, where nodes and edges evolve over time, requiring specialized models to capture their evolving dynamics in risk-sensitive applications effectively. Temporal graph neural networks (GNNs) are one such category of specialized models. For the first time, our approach integrates a reject option strategy within the framework of GNNs for continuous-time dynamic graphs. This allows the model to strategically abstain from making predictions when the uncertainty is high and confidence is low, thus minimizing the risk of critical misclassification and enhancing the results and reliability. We propose a coverage-based abstention prediction model to implement the reject option that maximizes prediction within a specified coverage. It improves the prediction score for link prediction and node classification tasks. Temporal GNNs deal with extremely skewed datasets for the next state prediction or node classification task. In the case of class imbalance, our method can be further tuned to provide a higher weightage to the minority class. Exhaustive experiments are presented on four datasets for dynamic link prediction and two datasets for dynamic node classification tasks. This demonstrates the effectiveness of our approach in improving the reliability and area under the curve (AUC)/ average precision (AP) scores for predictions in dynamic graph scenarios. The results highlight our model's ability to efficiently handle the trade-offs between prediction confidence and coverage, making it a dependable solution for applications requiring high precision in dynamic and uncertain environments.	 | 许多实际系统可以被建模为动态图，其中节点和边会随时间演化，需要专门的模型来有效地捕捉其在风险敏感应用中的动态变化。时间图神经网络（Temporal Graph Neural Networks, TGNNs）就是这类专门模型的一种。我们首次将拒绝选项策略融入图神经网络框架中，应用于连续时间的动态图。这使得模型能够在不确定性高、信心低时有策略地避免做出预测，从而减少关键分类错误的风险，提高预测结果和可靠性。我们提出了一种基于覆盖率的拒绝预测模型，该模型通过最大化指定覆盖率内的预测来实现拒绝选项，从而在链接预测和节点分类任务中改善预测评分。时间图神经网络处理下一状态预测或节点分类任务中极度不平衡的数据集。在类不平衡的情况下，我们的方法可以通过增加少数类的权重进一步优化。我们在四个动态链接预测数据集和两个动态节点分类数据集上进行了详尽的实验。这些实验展示了我们的方法在动态图场景中提高预测可靠性和面积下曲线（AUC）/平均精度（AP）评分的有效性。结果突显了我们的模型在处理预测信心和覆盖率之间的权衡方面的能力，使其成为需要在动态和不确定环境中实现高精度的应用的可靠解决方案。
2501.09009	 | Towards Fast, Specialized Machine Learning Force Fields: Distilling Foundation Models via Energy Hessians	 | Ishan Amin,Sanjeev Raja,Aditi Krishnapriyan	 | 该研究提出了一种将基础模型（FM）的一般表示转移到针对特定化学空间区域进行专业化的小型化分子力场（MLFF）的方法，从而实现更快的推理速度同时保持或提升性能。通过知识蒸馏过程，这种方法不仅加速了计算，还在测试时的分子动力学模拟中保持了能量守恒。	 | The foundation model (FM) paradigm is transforming Machine Learning Force Fields (MLFFs), leveraging general-purpose representations and scalable training to perform a variety of computational chemistry tasks. Although MLFF FMs have begun to close the accuracy gap relative to first-principles methods, there is still a strong need for faster inference speed. Additionally, while research is increasingly focused on general-purpose models which transfer across chemical space, practitioners typically only study a small subset of systems at a given time. This underscores the need for fast, specialized MLFFs relevant to specific downstream applications, which preserve test-time physical soundness while maintaining train-time scalability. In this work, we introduce a method for transferring general-purpose representations from MLFF foundation models to smaller, faster MLFFs specialized to specific regions of chemical space. We formulate our approach as a knowledge distillation procedure, where the smaller "student" MLFF is trained to match the Hessians of the energy predictions of the "teacher" foundation model. Our specialized MLFFs can be up to 20 $\times$ faster than the original foundation model, while retaining, and in some cases exceeding, its performance and that of undistilled models. We also show that distilling from a teacher model with a direct force parameterization into a student model trained with conservative forces (i.e., computed as derivatives of the potential energy) successfully leverages the representations from the large-scale teacher for improved accuracy, while maintaining energy conservation during test-time molecular dynamics simulations. More broadly, our work suggests a new paradigm for MLFF development, in which foundation models are released along with smaller, specialized simulation "engines" for common chemical subsets.	 | 基础模型（Foundation Model，FM）范式正在重塑机器学习力场（Machine Learning Force Fields，MLFFs），通过利用通用表示和可扩展训练来执行各种计算化学任务。尽管MLFF的FM已经开始缩小与第一性原理方法的准确性差距，但仍然需要更快的推理速度。此外，尽管研究越来越多地关注能够跨越化学空间通用化的模型，但实践者通常仅在同一时间内研究一小部分系统。这凸显了对特定下游应用相关的快速且专业化的MLFFs的需求，这些MLFFs在测试时保持物理合理性的同时，还保留了训练时的可扩展性。在这项工作中，我们介绍了一种方法，通过将MLFF基础模型的一般表示转移到针对特定化学空间区域进行了专门化的、更小且更快的MLFFs上。我们将我们的方法表述为一种知识蒸馏过程，其中较小的“学生”MLFF被训练为匹配“教师”基础模型的能量预测的哈密顿量。我们的专门化MLFFs在某些情况下可以比原始基础模型快20倍，同时保留或超越其性能以及未经蒸馏模型的性能。我们还展示了从具有直接力参数化的教师模型蒸馏到用保守力（即作为势能导数计算）训练的学生模型的成功案例，这不仅提高了准确性，还在测试时分子动力学模拟中保持了能量守恒。更广泛地说，我们的工作建议了一种新的MLFF开发范式，在这种范式中，基础模型将与针对常见化学子集的小型专业化模拟“引擎”一起发布。
2501.08998	 | CrystalGRW: Generative Modeling of Crystal Structures with Targeted Properties via Geodesic Random Walks	 | Krit Tangsongcharoen,Teerachote Pakornchote,Chayanon Atthapak,Natthaphon Choomphon-anomakhun,Annop Ektarawong,Björn Alling,Christopher Sutton,Thiti Bovornratanaraks,Thiparat Chotibut	 | CrystalGRW是一种基于扩散的生成模型，能够在黎曼流形上生成具有周期性的晶体结构，并通过密度泛函理论验证其稳定性，同时支持条件控制以指定所需的晶体学点群，有助于加速材料发现和逆向设计。	 | Determining whether a candidate crystalline material is thermodynamically stable depends on identifying its true ground-state structure, a central challenge in computational materials science. We introduce CrystalGRW, a diffusion-based generative model on Riemannian manifolds that proposes novel crystal configurations and can predict stable phases validated by density functional theory. The crystal properties, such as fractional coordinates, atomic types, and lattice matrices, are represented on suitable Riemannian manifolds, ensuring that new predictions generated through the diffusion process preserve the periodicity of crystal structures. We incorporate an equivariant graph neural network to also account for rotational and translational symmetries during the generation process. CrystalGRW demonstrates the ability to generate realistic crystal structures that are close to their ground states with accuracy comparable to existing models, while also enabling conditional control, such as specifying a desired crystallographic point group. These features help accelerate materials discovery and inverse design by offering stable, symmetry-consistent crystal candidates for experimental validation.	 | 确定候选晶体材料是否具有热力学稳定性，取决于识别其真正的基态结构，这是计算材料科学中的一个核心挑战。我们引入了一种基于扩散的生成模型CrystalGRW，该模型在黎曼流形上提出新的晶体构型，并能通过密度泛函理论验证预测的稳定相。晶体的性质，如分数坐标、原子类型和晶格矩阵，在合适的黎曼流形上表示，确保通过扩散过程生成的新预测保留了晶体结构的周期性。我们还引入了一种不变图神经网络，以在生成过程中考虑旋转和平移对称性。CrystalGRW展示了生成与基态接近且与现有模型具有相似准确性的现实晶体结构的能力，同时还能实现条件控制，例如指定所需的晶体学点群。这些特性有助于加速材料的发现和逆向设计，通过提供具有热力学稳定性和对称性的晶体候选物，以便进行实验验证。
2501.08970	 | Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography	 | Ilia Shumailov,Daniel Ramage,Sarah Meiklejohn,Peter Kairouz,Florian Hartmann,Borja Balle,Eugene Bagdasarian	 | 本文提出了一种新的方法，即利用有能力的机器学习模型作为可信的第三方，以实现隐私优先条件下的安全计算，这种方法被称为可信有能力模型环境（TCMEs），旨在平衡隐私和计算效率，适用于经典加密方法难以处理的复杂应用规模。	 | We often interact with untrusted parties. Prioritization of privacy can limit the effectiveness of these interactions, as achieving certain goals necessitates sharing private data. Traditionally, addressing this challenge has involved either seeking trusted intermediaries or constructing cryptographic protocols that restrict how much data is revealed, such as multi-party computations or zero-knowledge proofs. While significant advances have been made in scaling cryptographic approaches, they remain limited in terms of the size and complexity of applications they can be used for. In this paper, we argue that capable machine learning models can fulfill the role of a trusted third party, thus enabling secure computations for applications that were previously infeasible. In particular, we describe Trusted Capable Model Environments (TCMEs) as an alternative approach for scaling secure computation, where capable machine learning model(s) interact under input/output constraints, with explicit information flow control and explicit statelessness. This approach aims to achieve a balance between privacy and computational efficiency, enabling private inference where classical cryptographic solutions are currently infeasible. We describe a number of use cases that are enabled by TCME, and show that even some simple classic cryptographic problems can already be solved with TCME. Finally, we outline current limitations and discuss the path forward in implementing them.	 | 我们经常与不可信的第三方进行交互。隐私优先考虑可能会限制这些交互的有效性，因为实现某些目标需要分享私人数据。传统上，解决这一挑战的方法要么是寻找可信的中介，要么是构建限制数据揭示程度的加密协议，例如多方计算或零知识证明。尽管在扩展加密方法方面取得了重大进展，但它们在可以使用的应用程序规模和复杂性方面仍然受到限制。在本文中，我们主张有能力的机器学习模型可以担任可信的第三方角色，从而为以前不可行的应用程序提供安全计算。具体而言，我们描述了可信有能力模型环境（TCMEs）作为扩展安全计算的替代方法，在这种方法中，有能力的机器学习模型在输入/输出约束下交互，并具有显式的信息流控制和显式的无状态性。这种方法旨在在隐私和计算效率之间取得平衡，从而在经典加密解决方案当前不可行的情况下实现私密推理。我们描述了由TCME启用的各种用例，并表明即使是某些简单的经典加密问题，也可以使用TCME解决。最后，我们概述了当前的限制，并讨论了实施它们的道路。
2501.08950	 | Computing Approximated Fixpoints via Dampened Mann Iteration	 | Paolo Baldan,Sebastian Gurke,Barbara König,Tommaso Padoan,Florian Wittbold	 | 本文提出了一种带有减缓因子的曼恩迭代方案，用于近似单调且非扩张函数的最小固定点，特别是在函数由一系列逼近函数给出的情况下。该方案在适当条件下能够保证收敛到所需的最小固定点，并证明其在基于模型的强化学习，特别是马尔可夫决策过程中的应用，能够实现对最佳预期回报的收敛。	 | Fixpoints are ubiquitous in computer science and when dealing with quantitative semantics and verification one is commonly led to consider least fixpoints of (higher-dimensional) functions over the nonnegative reals. We show how to approximate the least fixpoint of such functions, focusing on the case in which they are not known precisely, but represented by a sequence of approximating functions that converge to them. We concentrate on monotone and non-expansive functions, for which uniqueness of fixpoints is not guaranteed and standard fixpoint iteration schemes might get stuck at a fixpoint that is not the least. Our main contribution is the identification of an iteration scheme, a variation of Mann iteration with a dampening factor, which, under suitable conditions, is shown to guarantee convergence to the least fixpoint of the function of interest. We then argue that these results are relevant in the context of model-based reinforcement learning for Markov decision processes (MDPs), showing that the proposed iteration scheme instantiates to MDPs and allows us to derive convergence to the optimal expected return. More generally, we show that our results can be used to iterate to the least fixpoint almost surely for systems where the function of interest can be approximated with given probabilistic error bounds, as it happens for probabilistic systems, such as simple stochastic games, that can be explored via sampling.	 | 固定点在计算机科学中无处不在，特别是在处理定量语义和验证时，人们通常需要考虑非负实数上的函数的最小固定点。我们展示了如何近似这类函数的最小固定点，特别是在这些函数并非完全已知，而是由一系列逼近函数逼近的情况下。我们集中在单调且非扩张的函数上，对于这类函数，固定点的唯一性并不保证，标准的固定点迭代方案可能会停留在一个非最小的固定点上。我们的主要贡献是识别了一种迭代方案，这是一种带有减缓因子的曼恩迭代方案，该方案在适当条件下能够保证收敛到所关心的函数的最小固定点。然后我们论证这些结果在基于模型的强化学习（特别是马尔可夫决策过程（MDP））的背景下是相关的。我们展示了所提出的迭代方案如何适用于MDP，并且可以实现收敛到最佳预期回报。更广泛地说，我们证明了我们的结果可以用于在函数可以使用给定的概率误差边界近似的情况下，通过抽样等方式探索概率系统（如简单随机游戏），几乎确定地迭代到最小固定点。
2501.08941	 | A Reinforcement Learning Approach to Quiet and Safe UAM Traffic Management	 | Surya Murthy,John-Paul Clarke,Ufuk Topcu,Zhenyu Gao	 | 该研究提出了一种多代理强化学习方法来管理城市空中交通(UAM)，以同时确保系统的安全间隔和减少噪音，并展示了通过高度调整来平衡这两项关键目标的潜力。	 | Urban air mobility (UAM) is a transformative system that operates various small aerial vehicles in urban environments to reshape urban transportation. However, integrating UAM into existing urban environments presents a variety of complex challenges. Recent analyses of UAM's operational constraints highlight aircraft noise and system safety as key hurdles to UAM system implementation. Future UAM air traffic management schemes must ensure that the system is both quiet and safe. We propose a multi-agent reinforcement learning approach to manage UAM traffic, aiming at both vertical separation assurance and noise mitigation. Through extensive training, the reinforcement learning agent learns to balance the two primary objectives by employing altitude adjustments in a multi-layer UAM network. The results reveal the tradeoffs among noise impact, traffic congestion, and separation. Overall, our findings demonstrate the potential of reinforcement learning in mitigating UAM's noise impact while maintaining safe separation using altitude adjustments	 | 城市空中交通(UAM)是一种革命性的系统，能够在城市环境中操作各种小型飞行器，重塑城市交通。然而，将UAM集成到现有的城市环境中带来了多种复杂挑战。最近对UAM运营限制的分析强调了飞机噪音和系统安全性是主要障碍。未来UAM空中交通管理方案必须确保系统既安静又安全。我们提出了一个多代理强化学习方法来管理UAM交通，旨在同时确保垂直间隔保障和减少噪音。通过大量的训练，强化学习代理学会在多层UAM网络中通过高度调整来平衡两个主要目标。结果揭示了噪音影响、交通拥堵和间隔之间的权衡。总体而言，我们的研究结果展示了通过高度调整利用强化学习减轻UAM噪音影响、维持安全间隔的潜力。
2501.08913	 | GenAI Content Detection Task 3: Cross-Domain Machine-Generated Text Detection Challenge	 | Liam Dugan,Andrew Zhu,Firoj Alam,Preslav Nakov,Marianna Apidianaki,Chris Callison-Burch	 | 该研究利用新发布的RAID基准，评估了多个团队在检测多种常见领域的生成文本方面的性能，发现多个参与者能够在保持高准确率的同时控制误报率，表明检测器具有较好的稳健性。	 | Recently there have been many shared tasks targeting the detection of generated text from Large Language Models (LLMs). However, these shared tasks tend to focus either on cases where text is limited to one particular domain or cases where text can be from many domains, some of which may not be seen during test time. In this shared task, using the newly released RAID benchmark, we aim to answer whether or not models can detect generated text from a large, yet fixed, number of domains and LLMs, all of which are seen during training. Over the course of three months, our task was attempted by 9 teams with 23 detector submissions. We find that multiple participants were able to obtain accuracies of over 99% on machine-generated text from RAID while maintaining a 5% False Positive Rate -- suggesting that detectors are able to robustly detect text from many domains and models simultaneously. We discuss potential interpretations of this result and provide directions for future research.	 | 最近，有许多共享任务专注于检测来自大型语言模型（LLMs）的生成文本。然而，这些共享任务往往要么侧重于文本仅限于某个特定领域的情况，要么侧重于文本可以来自多个领域的情况，其中一些领域可能在测试时未见到。在本次共享任务中，利用新发布的RAID基准，我们旨在回答是否可以检测来自众多且固定数量领域的生成文本，这些领域和模型在训练期间都能见到。在三个月的时间里，共有9个团队提交了23个检测器，参与了我们的任务。我们发现，有多名参与者能够将RAID生成文本的准确率保持在99%以上，同时保持5%的误报率——这表明检测器能够在同时检测来自多个领域和模型的情况下展现出稳健性。我们讨论了这一结果的潜在解释，并提供了未来研究的方向。
2501.08902	 | Multi-View Transformers for Airway-To-Lung Ratio Inference on Cardiac CT Scans: The C4R Study	 | Sneha N. Naik,Elsa D. Angelini,Eric A. Hoffman,Elizabeth C. Oelsner,R. Graham Barr,Benjamin M. Smith,Andrew F. Laine	 | 本研究提出了一种基于注意力机制的多视图Swin变换器，用于从心脏CT图像中推断全肺气道树管腔与肺体积比（ALR），并展示了其在准确性和可重复性方面优于现有方法。	 | The ratio of airway tree lumen to lung size (ALR), assessed at full inspiration on high resolution full-lung computed tomography (CT), is a major risk factor for chronic obstructive pulmonary disease (COPD). There is growing interest to infer ALR from cardiac CT images, which are widely available in epidemiological cohorts, to investigate the relationship of ALR to severe COVID-19 and post-acute sequelae of SARS-CoV-2 infection (PASC). Previously, cardiac scans included approximately 2/3 of the total lung volume with 5-6x greater slice thickness than high-resolution (HR) full-lung (FL) CT. In this study, we present a novel attention-based Multi-view Swin Transformer to infer FL ALR values from segmented cardiac CT scans. For the supervised training we exploit paired full-lung and cardiac CTs acquired in the Multi-Ethnic Study of Atherosclerosis (MESA). Our network significantly outperforms a proxy direct ALR inference on segmented cardiac CT scans and achieves accuracy and reproducibility comparable with a scan-rescan reproducibility of the FL ALR ground-truth.	 | 在全肺高分辨率计算机断层扫描（HR-CT）中评估的气道树管腔与肺体积比（ALR），在全呼气状态下进行评估，是慢性阻塞性肺疾病（COPD）的主要风险因素。人们对从心脏CT图像中推断ALR产生了越来越大的兴趣，因为心脏CT图像在流行病学队列中广泛可用，用于研究ALR与严重COVID-19及新冠病毒（SARS-CoV-2）感染后遗症（PASC）之间的关系。此前，心脏扫描包括大约2/3的总肺体积，并且其切片厚度比高分辨率（HR）全肺（FL）CT大5-6倍。在本研究中，我们提出了一种基于注意力机制的多视图Swin变换器，用于从分割的心脏CT扫描中推断全肺ALR值。在监督训练中，我们利用多族动脉硬化研究（MESA）中同时获得的全肺和心脏CT图像进行利用。我们的网络在分割心脏CT图像上直接推断ALR的替代方法上表现显著更优，并且其准确性和可重复性与全肺ALR真实值的扫描-再扫描可重复性相当。
2501.08884	 | Improved Compression Bounds for Scenario Decision Making	 | Guillaume O. Berger,Raphaël M. Jungers	 | 情景决策方法通过抽取不确定性情景样本来提供决策时的失败风险概率保证，这种保证以抽样情景数量、最大可容忍风险和问题的压缩大小为函数，并且提出的新界限在无需更强假设的情况下改进了现有界限。	 | Scenario decision making offers a flexible way of making decision in an uncertain environment while obtaining probabilistic guarantees on the risk of failure of the decision. The idea of this approach is to draw samples of the uncertainty and make a decision based on the samples, called "scenarios". The probabilistic guarantees take the form of a bound on the probability of sampling a set of scenarios that will lead to a decision whose risk of failure is above a given maximum tolerance. This bound can be expressed as a function of the number of sampled scenarios, the maximum tolerated risk, and some intrinsic property of the problem called the "compression size". Several such bounds have been proposed in the literature under various assumptions on the problem. We propose new bounds that improve upon the existing ones without requiring stronger assumptions on the problem.	 | 在不确定环境中做出决策时，情景决策方法提供了一种灵活的方式来获得失败风险的概率保证。这种方法的核心思想是通过抽取不确定性的情景样本，并基于这些样本做出决策。这种概率保证表现为一个界限，即在抽取的一组情景中，导致决策失败风险超过给定最大容忍值的概率上限。该界限可以表示为抽样情景的数量、最大可容忍风险以及问题的固有属性（称为“压缩大小”）的函数。文献中已经提出了多种这样的界限，基于对问题的不同假设。我们提出了新的界限，可以在不需要更强假设的情况下改进现有界限。
2501.08848	 | RouteNet-Gauss: Hardware-Enhanced Network Modeling with Machine Learning	 | Carlos Güemes-Palau,Miquel Ferriol-Galmés,Jordi Paillisse-Vilanova,Albert López-Brescó,Pere Barlet-Ros,Albert Cabellos-Aparicio	 | RouteNet-Gauss是一种将测试床网络与机器学习模型结合的新方法，能够以高保真度快速生成训练数据集并模拟网络场景，显著降低预测误差并大幅加快推理时间。该方法通过模块化架构适应不同网络配置，支持时间聚合性能估计，提供可配置的时间粒度和高准确性。	 | Network simulation is pivotal in network modeling, assisting with tasks ranging from capacity planning to performance estimation. Traditional approaches such as Discrete Event Simulation (DES) face limitations in terms of computational cost and accuracy. This paper introduces RouteNet-Gauss, a novel integration of a testbed network with a Machine Learning (ML) model to address these challenges. By using the testbed as a hardware accelerator, RouteNet-Gauss generates training datasets rapidly and simulates network scenarios with high fidelity to real-world conditions. Experimental results show that RouteNet-Gauss significantly reduces prediction errors by up to 95% and achieves a 488x speedup in inference time compared to state-of-the-art DES-based methods. RouteNet-Gauss's modular architecture is dynamically constructed based on the specific characteristics of the network scenario, such as topology and routing. This enables it to understand and generalize to different network configurations beyond those seen during training, including networks up to 10x larger. Additionally, it supports Temporal Aggregated Performance Estimation (TAPE), providing configurable temporal granularity and maintaining high accuracy in flow performance metrics. This approach shows promise in improving both simulation efficiency and accuracy, offering a valuable tool for network operators.	 | 网络仿真在网络建模中至关重要，有助于从容量规划到性能评估等一系列任务。传统的离散事件仿真（DES）方法在计算成本和准确性方面存在局限性。本文介绍了一种名为RouteNet-Gauss的新颖方法，这是一种将测试床网络与机器学习（ML）模型相结合的新颖集成，以解决这些挑战。通过将测试床用作硬件加速器，RouteNet-Gauss可以快速生成训练数据集，并以高保真度模拟网络场景，接近真实世界的条件。实验结果表明，RouteNet-Gauss将预测误差显著降低高达95%，并在推理时间上比最先进的基于DES的方法快488倍。RouteNet-Gauss的模块化架构是根据网络场景的具体特征（如拓扑和路由）动态构建的。这使其能够理解并泛化到训练过程中未见过的不同网络配置，包括规模多达10倍更大的网络。此外，它还支持时间聚合性能估计（TAPE），提供可配置的时间粒度，并在流量性能指标中保持高准确性。这种方法在提高仿真效率和准确性方面显示出潜力，为网络运营商提供了一个有价值的工具。
2501.08822	 | Deep Learning Meets Queue-Reactive: A Framework for Realistic Limit Order Book Simulation	 | Hamza Bodor,Laurent Carlier	 | 多维深度队列反应（MDQR）模型在黄等（2015）的队列反应模型基础上进行了扩展，通过引入复杂的神经网络架构来学习多层次价格之间的依赖关系，并融入市场特征和订单规模分布，从而更好地捕捉市场特性。该模型在保持模型可解释性的同时，能够有效再现沪债期货市场的关键市场特征，并保持计算效率，适用于策略开发的应用场景。	 | The Queue-Reactive model introduced by Huang et al. (2015) has become a standard tool for limit order book modeling, widely adopted by both researchers and practitioners for its simplicity and effectiveness. We present the Multidimensional Deep Queue-Reactive (MDQR) model, which extends this framework in three ways: it relaxes the assumption of queue independence, enriches the state space with market features, and models the distribution of order sizes. Through a neural network architecture, the model learns complex dependencies between different price levels and adapts to varying market conditions, while preserving the interpretable point-process foundation of the original framework. Using data from the Bund futures market, we show that MDQR captures key market properties including the square-root law of market impact, cross-queue correlations, and realistic order size patterns. The model demonstrates particular strength in reproducing both conditional and stationary distributions of order sizes, as well as various stylized facts of market microstructure. The model achieves this while maintaining the computational efficiency needed for practical applications such as strategy development through reinforcement learning or realistic backtesting.	 | 黄等（2015）提出的队列反应模型已成为限价订单簿建模的标准工具，研究人员和从业者广泛采用该模型，因其简单性和有效性。我们提出了多维深度队列反应（MDQR）模型，该模型在三个方面扩展了这一框架：它放宽了队列独立性的假设，扩展了状态空间以融入市场特征，并建模了订单规模的分布。通过神经网络架构，该模型学习不同价格层次之间的复杂依赖关系，并适应不同的市场条件，同时保持原始框架的可解释性点过程基础。使用沪债期货市场的数据，我们表明MDQR能够捕捉到关键的市场特性，包括市场的平方根法则、队列间的相关性以及现实中的订单规模模式。该模型在再现订单规模的条件分布和静态分布、以及市场微观结构的各种特征方面表现尤为突出。同时，该模型保持了在策略开发通过强化学习或现实回测等实际应用中所需的计算效率。
2501.08816	 | IDEA: Image Description Enhanced CLIP-Adapter	 | Zhipeng Ye,Feng Jiang,Qiufeng Wang,Kaizhu Huang,Jiaqi Huang	 | 本文提出了一种图像描述增强的CLIP适配器（IDEA）方法，通过结合视觉特征和文本描述来提升少样本图像分类任务的性能，并在多个任务上达到了或超过了现有最佳模型；进一步引入的可训练IDEA（T-IDEA）在11个数据集上取得了SOTA结果，同时提供了一个包含1,637,795个图像-文本对的“IMD-11”数据集。	 | CLIP (Contrastive Language-Image Pre-training) has attained great success in pattern recognition and computer vision. Transferring CLIP to downstream tasks (e.g. zero- or few-shot classification) is a hot topic in multimodal learning. However, current studies primarily focus on either prompt learning for text or adapter tuning for vision, without fully exploiting the complementary information and correlations among image-text pairs. In this paper, we propose an Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to few-shot image classification tasks. This method captures fine-grained features by leveraging both visual features and textual descriptions of images. IDEA is a training-free method for CLIP, and it can be comparable to or even exceeds state-of-the-art models on multiple tasks. Furthermore, we introduce Trainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable components (i.e., a projector and a learnable latent space), further enhancing the model's performance and achieving SOTA results on 11 datasets. As one important contribution, we employ the Llama model and design a comprehensive pipeline to generate textual descriptions for images of 11 datasets, resulting in a total of 1,637,795 image-text pairs, named "IMD-11". Our code and data are released at https://github.com/FourierAI/IDEA.	 | CLIP（对比语言-图像预训练）在模式识别和计算机视觉领域取得了巨大成功。将CLIP应用到下游任务（例如零样本或少样本分类）是多模态学习领域的热门话题。然而，现有研究主要集中在文本的提示学习或视觉适配调优上，未能充分利用图像-文本对之间的互补信息和关联。本文提出了一个图像描述增强的CLIP-适配器（IDEA）方法，以适应少样本图像分类任务。该方法通过利用图像的视觉特征和文本描述来捕捉细粒度特征。IDEA是CLIP的无训练方法，并且在多个任务上可以与或超过当前最先进的模型。此外，我们引入了可训练的IDEA（T-IDEA），通过添加两个轻量级可学习组件（即投影器和可学习的潜在空间）进一步增强了模型性能，并在11个数据集上取得了SOTA结果。作为一个重要贡献，我们使用Llama模型并设计了一个全面的管道来为11个数据集生成图像文本描述，生成了总计1,637,795个图像-文本对，命名为“IMD-11”。我们的代码和数据已发布在https://github.com/FourierAI/IDEA。
2501.08779	 | Nesterov Acceleration for Ensemble Kalman Inversion and Variants	 | Sydney Vernon,Eviatar Bach,Oliver R. A. Dunbar	 | 本文展示了如何在 Ensemble Kalman 反演（EKI）中应用 Nesterov 加速来加快成本函数的减少，并为两种 EKI 变体实现了这一加速方法，其形式为粒子级的微调，无需增加额外计算成本或超参数调优。	 | Ensemble Kalman inversion (EKI) is a derivative-free, particle-based optimization method for solving inverse problems. It can be shown that EKI approximates a gradient flow, which allows the application of methods for accelerating gradient descent. Here, we show that Nesterov acceleration is effective in speeding up the reduction of the EKI cost function on a variety of inverse problems. We also implement Nesterov acceleration for two EKI variants, unscented Kalman inversion and ensemble transform Kalman inversion. Our specific implementation takes the form of a particle-level nudge that is demonstrably simple to couple in a black-box fashion with any existing EKI variant algorithms, comes with no additional computational expense, and with no additional tuning hyperparameters. This work shows a pathway for future research to translate advances in gradient-based optimization into advances in gradient-free Kalman optimization.	 | Ensemble Kalman 反演（EKI）是一种无导数的粒子基优化方法，用于解决反问题。可以证明，EKI 近似于梯度流动，这使得应用加速梯度下降的方法成为可能。在这里，我们展示了 Nesterov 加速在多种反问题中有效加快 EKI 成本函数的减少。我们还为两种 EKI 变体——无散Kalman反演和集合转换Kalman 反演——实现了 Nesterov 加速。我们的具体实现形式为粒子级的微调，这种微调以黑盒方式与任何现有的 EKI 变体算法结合是显而易见且简单的，不增加额外的计算成本，并且不需要额外的调优超参数。这项工作为未来研究指明了一条途径，即将基于梯度的优化进步转化为无导数的 Kalman 优化进步。
2501.08760	 | Leveraging LLM Agents for Translating Network Configurations	 | Yunze Wei,Xiaohui Xie,Yiwei Zuo,Tianshuo Hu,Xinyi Chen,Kaiwen Chi,Yong Cui	 | 本文提出了一种基于意图的网络配置翻译框架，利用大规模语言模型进行自动翻译，并通过两阶段验证确保翻译的正确性，实验结果显示其在语法和翻译准确性上均优于现有方法。	 | Configuration translation is a critical and frequent task in network operations. When a network device is damaged or outdated, administrators need to replace it to maintain service continuity. The replacement devices may originate from different vendors, necessitating configuration translation to ensure seamless network operation. However, translating configurations manually is a labor-intensive and error-prone process. In this paper, we propose an intent-based framework for translating network configuration with Large Language Model (LLM) Agents. The core of our approach is an Intent-based Retrieval Augmented Generation (IRAG) module that systematically splits a configuration file into fragments, extracts intents, and generates accurate translations. We also design a two-stage verification method to validate the syntax and semantics correctness of the translated configurations. We implement and evaluate the proposed method on real-world network configurations. Experimental results show that our method achieves 97.74% syntax correctness, outperforming state-of-the-art methods in translation accuracy.	 | 配置翻译在网络运维中是一项关键且频繁的任务。当网络设备损坏或过时，管理员需要更换以保持服务连续性。替换设备可能来自不同的厂商，因此需要进行配置翻译以确保网络操作无缝进行。然而，手动翻译配置是一个劳动密集且容易出错的过程。本文提出了一种基于意图的框架，利用大规模语言模型（LLM）代理进行网络配置翻译。我们方法的核心是基于意图的检索增强生成（IRAG）模块，该模块系统地将配置文件拆分成片段，提取意图，并生成准确的翻译。我们还设计了一种两阶段验证方法，用于验证翻译配置的语法和语义正确性。我们在实际网络配置上实现并评估了该方法。实验结果显示，我们的方法在语法正确性方面达到了97.74%，在翻译准确性方面优于现有最先进的方法。
2501.08717	 | $\texttt{InfoHier}$: Hierarchical Information Extraction via Encoding and Embedding	 | Tianru Zhang,Li Ju,Prashant Singh,Salman Toor	 | InfoHier 是一个结合自监督学习（SSL）和层次聚类（HC）的框架，旨在学习鲁棒的潜在表示和层次结构，以提高复杂数据集的聚类和表示学习性能。该方法通过利用 SSL 提供自适应表示并结合 HC 捕获复杂模式，从而生成更符合潜在信息层次的表示。	 | Analyzing large-scale datasets, especially involving complex and high-dimensional data like images, is particularly challenging. While self-supervised learning (SSL) has proven effective for learning representations from unlabelled data, it typically focuses on flat, non-hierarchical structures, missing the multi-level relationships present in many real-world datasets. Hierarchical clustering (HC) can uncover these relationships by organizing data into a tree-like structure, but it often relies on rigid similarity metrics that struggle to capture the complexity of diverse data types. To address these we envision $\texttt{InfoHier}$, a framework that combines SSL with HC to jointly learn robust latent representations and hierarchical structures. This approach leverages SSL to provide adaptive representations, enhancing HC's ability to capture complex patterns. Simultaneously, it integrates HC loss to refine SSL training, resulting in representations that are more attuned to the underlying information hierarchy. $\texttt{InfoHier}$ has the potential to improve the expressiveness and performance of both clustering and representation learning, offering significant benefits for data analysis, management, and information retrieval.	 | 分析大规模数据集，尤其是涉及复杂和高维数据（如图像）的情况，尤其具有挑战性。虽然自监督学习（SSL）已被证明在从未标记数据中学习表示方面是有效的，但它通常侧重于平坦而非层次结构的结构，未能捕捉到许多真实世界数据集中存在的多级关系。层次聚类（HC）可以通过将数据组织成树状结构来发现这些关系，但通常依赖于刚性的相似度度量，难以捕捉多种数据类型中的复杂性。为了解决这些问题，我们设想了一个叫作 $\texttt{InfoHier}$ 的框架，它将SSL与HC结合起来，以联合学习鲁棒的潜在表示和层次结构。这种方法利用SSL提供自适应表示，增强HC捕获复杂模式的能力。同时，它结合了HC损失以改进SSL训练，从而生成更符合潜在信息层次的表示。$\texttt{InfoHier}$ 有可能提高聚类和表示学习的表达能力和性能，为数据的分析、管理和信息检索提供显著的好处。
2501.08712	 | Self-supervised Transformation Learning for Equivariant Representations	 | Jaemyung Yu,Jaehyun Choi,Dong-Jae Lee,HyeongGwon Hong,Junmo Kim	 | 自我监督变换学习（STL）通过使用图像对的变换表示来替代变换标签，从而在多种分类和检测任务中提升了性能，并在11个基准中的7个中超越了现有方法。该方法能够学习并处理复杂的变换，提升了模型的适应性和鲁棒性，同时也与多种基础模型兼容。	 | Unsupervised representation learning has significantly advanced various machine learning tasks. In the computer vision domain, state-of-the-art approaches utilize transformations like random crop and color jitter to achieve invariant representations, embedding semantically the same inputs despite transformations. However, this can degrade performance in tasks requiring precise features, such as localization or flower classification. To address this, recent research incorporates equivariant representation learning, which captures transformation-sensitive information. However, current methods depend on transformation labels and thus struggle with interdependency and complex transformations. We propose Self-supervised Transformation Learning (STL), replacing transformation labels with transformation representations derived from image pairs. The proposed method ensures transformation representation is image-invariant and learns corresponding equivariant transformations, enhancing performance without increased batch complexity. We demonstrate the approach's effectiveness across diverse classification and detection tasks, outperforming existing methods in 7 out of 11 benchmarks and excelling in detection. By integrating complex transformations like AugMix, unusable by prior equivariant methods, this approach enhances performance across tasks, underscoring its adaptability and resilience. Additionally, its compatibility with various base models highlights its flexibility and broad applicability. The code is available at https://github.com/jaemyung-u/stl.	 | 无监督表示学习在各种机器学习任务中取得了显著进展。在计算机视觉领域，最新的方法通过使用随机裁剪和颜色抖动等变换手段，实现了对不变表示的构建，即使在变换后也能嵌入语义上相同的输入。然而，这种方法在需要精确特征的任务（如定位或花卉分类）中可能会降低性能。为了解决这个问题，最近的研究引入了可变形表示学习，它可以捕捉变换敏感的信息。然而，当前的方法依赖于变换标签，因此在处理复杂的依赖关系和变换时存在困难。我们提出了一种自我监督变换学习（Self-supervised Transformation Learning, STL），用来自图像对的变换表示替代了变换标签。所提出的方法确保变换表示是图像不变的，并且能够学习相应的可变形变换，这在不增加批处理复杂度的情况下提升了性能。我们在多种分类和检测任务中验证了该方法的有效性，在11个基准中的7个中超越了现有方法，并且在检测任务中表现出色。通过集成以前的可变形方法无法处理的复杂变换（如AugMix），这种方法在任务中都提升了性能，突显了其适应性和鲁棒性。此外，与各种基础模型的兼容性显示了其灵活性和广泛的应用范围。代码可以在https://github.com/jaemyung-u/stl获取。
2501.08662	 | Product of Gaussian Mixture Diffusion Model for non-linear MRI Inversion	 | Laurenz Nagler,Martin Zach,Thomas Pock	 | 该研究提出了一种新的扩散模型，通过联合重建图像和线圈灵敏度，使用参数效率高的高斯混合模型作为先验，并采用经典平滑先验来估计线圈灵敏度，从而克服了传统方法在可解释性和计算效率方面的限制，展示了对异常数据和采样轨迹的良好鲁棒性。	 | Diffusion models have recently shown remarkable results in magnetic resonance imaging reconstruction. However, the employed networks typically are black-box estimators of the (smoothed) prior score with tens of millions of parameters, restricting interpretability and increasing reconstruction time. Furthermore, parallel imaging reconstruction algorithms either rely on off-line coil sensitivity estimation, which is prone to misalignment and restricting sampling trajectories, or perform per-coil reconstruction, making the computational cost proportional to the number of coils. To overcome this, we jointly reconstruct the image and the coil sensitivities using the lightweight, parameter-efficient, and interpretable product of Gaussian mixture diffusion model as an image prior and a classical smoothness priors on the coil sensitivities. The proposed method delivers promising results while allowing for fast inference and demonstrating robustness to contrast out-of-distribution data and sampling trajectories, comparable to classical variational penalties such as total variation. Finally, the probabilistic formulation allows the calculation of the posterior expectation and pixel-wise variance.	 | 扩散模型最近在磁共振成像（MRI）重建中取得了显著成果。然而，常用的网络通常是具有数以千万计参数的黑盒估计器，这限制了可解释性并增加了重建时间。此外，平行成像重建算法要么依赖于离线线圈灵敏度估计，这容易导致对齐错误并限制采样轨迹，要么对每个线圈进行独立重建，使得计算成本与线圈数量成正比。为克服这些问题，我们联合重建图像和线圈灵敏度，使用轻量级、参数效率高且可解释的高斯混合模型的乘积作为图像先验，并对线圈灵敏度采用经典的平滑先验。所提出的方法在允许快速推断的同时展示了对对比度异常数据和采样轨迹的鲁棒性，与经典的变分惩罚（如全变差）方法相当。最后，概率性表述允许计算后验期望和像素级方差。
2501.08649	 | Joint Learning of Depth and Appearance for Portrait Image Animation	 | Xinya Ji,Gaspard Zoss,Prashanth Chandran,Lingchen Yang,Xun Cao,Barbara Solenthaler,Derek Bradley	 | 本文提出了一种新的基于扩散的方法，能够在生成高质量2D肖像图像的同时学习并生成对应的深度信息，从而实现面部深度到图像和图像到深度生成、肖像重光照以及音频驱动的头部动画等多种应用。	 | 2D portrait animation has experienced significant advancements in recent years. Much research has utilized the prior knowledge embedded in large generative diffusion models to enhance high-quality image manipulation. However, most methods only focus on generating RGB images as output, and the co-generation of consistent visual plus 3D output remains largely under-explored. In our work, we propose to jointly learn the visual appearance and depth simultaneously in a diffusion-based portrait image generator. Our method embraces the end-to-end diffusion paradigm and introduces a new architecture suitable for learning this conditional joint distribution, consisting of a reference network and a channel-expanded diffusion backbone. Once trained, our framework can be efficiently adapted to various downstream applications, such as facial depth-to-image and image-to-depth generation, portrait relighting, and audio-driven talking head animation with consistent 3D output.	 | 近年来，2D肖像动画经历了显著的进步。许多研究利用大型生成扩散模型中嵌入的先验知识来提升高质量图像操作的效果。然而，大多数方法仅专注于生成RGB图像作为输出，而一致的视觉和3D输出的联合生成则 largely 仍然未被充分探索。在我们的工作中，我们提出了一种在基于扩散的肖像图像生成器中同时学习视觉外观和深度的方法。我们的方法采用了端到端的扩散范式，并引入了一种新的架构，适用于学习这种条件联合分布，包括一个参考网络和一个通道扩展的扩散骨干。经过训练后，我们的框架可以高效地适应各种下游应用，例如面部深度到图像和图像到深度生成、肖像重光照以及与一致的3D输出相结合的音频驱动的头部动画。
2501.08626	 | A Learning Algorithm That Attains the Human Optimum in a Repeated Human-Machine Interaction Game	 | Jason T. Isa,Lillian J. Ratliff,Samuel A. Burden	 | 该研究提出了一种基于博弈论的学习算法，能够通过观察人类行为直接找到成本最小值，避免了解逆问题的复杂性，并在多种实验中证实了其有效性。	 | When humans interact with learning-based control systems, a common goal is to minimize a cost function known only to the human. For instance, an exoskeleton may adapt its assistance in an effort to minimize the human's metabolic cost-of-transport. Conventional approaches to synthesizing the learning algorithm solve an inverse problem to infer the human's cost. However, these problems can be ill-posed, hard to solve, or sensitive to problem data. Here we show a game-theoretic learning algorithm that works solely by observing human actions to find the cost minimum, avoiding the need to solve an inverse problem. We evaluate the performance of our algorithm in an extensive set of human subjects experiments, demonstrating consistent convergence to the minimum of a prescribed human cost function in scalar and multidimensional instantiations of the game. We conclude by outlining future directions for theoretical and empirical extensions of our results.	 | 当人类与基于学习的控制系统进行交互时，一个共同的目标是将人类未知的成本函数最小化。例如，外骨骼可能会调整其辅助方式，以尽量减少人类的代谢成本。传统上，合成学习算法的方法是通过求解逆问题来推断人类的成本。然而，这些方法可能会导致问题不明确、难以解决或对问题数据敏感。在这里，我们展示了一个基于博弈论的学习算法，该算法仅通过观察人类的行为来找到成本最小值，从而避免了解逆问题的需要。我们在大量的人类受试者实验中评估了我们算法的表现，证明了在单变量和多变量实例中，该算法能够一致地收敛到指定的人类成本函数的最小值。最后，我们概述了未来在理论和实证方面扩展我们结果的方向。
2501.08591	 | OpenMLDB: A Real-Time Relational Data Feature Computation System for Online ML	 | Xuanhe Zhou,Wei Zhou,Liguo Qi,Hao Zhang,Dihao Chen,Bingsheng He,Mian Lu,Guoliang Li,Fan Wu,Yuqiang Chen	 | OpenMLDB 是4Paradigm公司为在线机器学习应用设计的一个特征计算系统，通过统一查询计划生成器和特殊的执行引擎解决了离线和在线阶段的一致性问题，并提高了长窗口计算和多表窗口合并的性能。该系统已经在超过100个实际场景中部署，并在性能和资源利用率方面显著优于现有数据处理系统。	 | Efficient and consistent feature computation is crucial for a wide range of online ML applications. Typically, feature computation is divided into two distinct phases, i.e., offline stage for model training and online stage for model serving. These phases often rely on execution engines with different interface languages and function implementations, causing significant inconsistencies. Moreover, many online ML features involve complex time-series computations (e.g., functions over varied-length table windows) that differ from standard streaming and analytical queries. Existing data processing systems (e.g., Spark, Flink, DuckDB) often incur multi-second latencies for these computations, making them unsuitable for real-time online ML applications that demand timely feature updates.   This paper presents OpenMLDB, a feature computation system deployed in 4Paradigm's SageOne platform and over 100 real scenarios. Technically, OpenMLDB first employs a unified query plan generator for consistent computation results across the offline and online stages, significantly reducing feature deployment overhead. Second, OpenMLDB provides an online execution engine that resolves performance bottlenecks caused by long window computations (via pre-aggregation) and multi-table window unions (via data self-adjusting). It also provides a high-performance offline execution engine with window parallel optimization and time-aware data skew resolving. Third, OpenMLDB features a compact data format and stream-focused indexing to maximize memory usage and accelerate data access. Evaluations in testing and real workloads reveal significant performance improvements and resource savings compared to the baseline systems. The open community of OpenMLDB now has over 150 contributors and gained 1.6k stars on GitHub.	 | 在线机器学习（ML）应用中，高效且一致的特征计算至关重要。特征计算通常分为两个独立阶段：离线阶段用于模型训练和在线阶段用于模型服务。这两个阶段通常依赖于不同接口语言和函数实现的执行引擎，导致显著的不一致性。此外，许多在线ML特征涉及复杂的时序计算（例如，不同长度表格窗口上的函数），这些计算与标准流式和分析查询不同。现有的数据处理系统（例如，Spark、Flink、DuckDB）在这些计算上往往会产生多秒的延迟，使得它们不适合需要及时特征更新的实时在线ML应用。  本文介绍了OpenMLDB，这是4Paradigm公司SageOne平台上的一个特征计算系统，已经在超过100个实际场景中部署。从技术角度来看，OpenMLDB 首先采用统一的查询计划生成器，以确保离线和在线阶段计算结果的一致性，大幅减少特征部署的开销。其次，OpenMLDB 提供了一个在线执行引擎，通过预聚合解决长窗口计算性能瓶颈，并通过数据自我调整解决多表窗口合并带来的问题。它还提供了一个高性能的离线执行引擎，该引擎包含窗口并行优化和时间感知数据倾斜解决。第三，OpenMLDB 具备紧凑的数据格式和流式焦点索引，以最大化内存使用并加速数据访问。在测试和实际工作负载中的评估显示，与基准系统相比，OpenMLDB 在性能和资源利用率方面都有显著提升。OpenMLDB 的开源社区已有超过150名贡献者，并在GitHub上获得了1600多个星标。
2501.08585	 | A Systematic Review of Machine Learning Methods for Multimodal EEG Data in Clinical Application	 | Siqi Zhao(1),Wangyang Li(1),Xiru Wang(1),Stevie Foglia(2),Hongzhao Tan(1),Bohan Zhang(1),Ameer Hamoodi(2),Aimee Nelson(2 and 3),Zhen Gao(1 and 2) ((1) WBooth School of Engineering Practice and Technology, McMaster University, Hamilton, Ontario Canada, (2) School of Biomedical Engineering, McMaster University, Hamilton, Ontario, Canada, (3) Department of Kinesiology, McMaster University, Hamilton, Ontario, Canada)	 | 本综述探讨了机器学习和深度学习模型在整合多模态脑电图数据方面的应用，这些应用旨在提高临床诊断的准确性，涵盖神经精神疾病、癫痫检测、自闭症谱系障碍和睡眠阶段分类等领域。研究表明，多模态数据融合能够显著提高模型的准确性，特别是在支持向量机和决策树模型中表现突出。	 | Machine learning (ML) and deep learning (DL) techniques have been widely applied to analyze electroencephalography (EEG) signals for disease diagnosis and brain-computer interfaces (BCI). The integration of multimodal data has been shown to enhance the accuracy of ML and DL models. Combining EEG with other modalities can improve clinical decision-making by addressing complex tasks in clinical populations. This systematic literature review explores the use of multimodal EEG data in ML and DL models for clinical applications. A comprehensive search was conducted across PubMed, Web of Science, and Google Scholar, yielding 16 relevant studies after three rounds of filtering. These studies demonstrate the application of multimodal EEG data in addressing clinical challenges, including neuropsychiatric disorders, neurological conditions (e.g., seizure detection), neurodevelopmental disorders (e.g., autism spectrum disorder), and sleep stage classification. Data fusion occurred at three levels: signal, feature, and decision levels. The most commonly used ML models were support vector machines (SVM) and decision trees. Notably, 11 out of the 16 studies reported improvements in model accuracy with multimodal EEG data. This review highlights the potential of multimodal EEG-based ML models in enhancing clinical diagnostics and problem-solving.	 | 机器学习（ML）和深度学习（DL）技术已被广泛应用于分析脑电图（EEG）信号，用于疾病诊断和脑-机接口（BCI）。多模态数据的整合已被证明可以提高ML和DL模型的准确性。将EEG与其他模态数据相结合，可以通过解决临床人群中的复杂任务来改善临床决策。本系统文献综述探讨了在临床应用中使用多模态EEG数据的ML和DL模型。我们在PubMed、Web of Science和Google Scholar中进行了全面搜索，在三轮筛选后获得了16篇相关研究。这些研究展示了使用多模态EEG数据解决临床挑战的应用，包括神经精神疾病、神经学状况（如癫痫检测）、神经发育障碍（如自闭症谱系障碍）和睡眠阶段分类。数据融合发生在三个层面：信号级、特征级和决策级。最常使用的ML模型是支持向量机（SVM）和决策树。值得注意的是，16篇研究中有11篇报告了使用多模态EEG数据提高了模型的准确性。本综述强调了基于多模态EEG的ML模型在增强临床诊断和问题解决方面的潜力。
2501.08561	 | ANSR-DT: An Adaptive Neuro-Symbolic Learning and Reasoning Framework for Digital Twins	 | Safayat Bin Hakim,Muhammad Adil,Alvaro Velasquez,Houbing Herbert Song	 | 本文提出了一种名为ANSR-DT的适应性神经-符号学习框架，结合模式识别、强化学习和符号推理，以实现实时学习和自适应智能，提高决策准确性与可解释性。尽管该框架在复杂环境下的符号规则提取和集成存在挑战，但其开源实现促进了未来的深入研究和改进。	 | In this paper, we propose an Adaptive Neuro-Symbolic Learning Framework for digital twin technology called ``ANSR-DT." Our approach combines pattern recognition algorithms with reinforcement learning and symbolic reasoning to enable real-time learning and adaptive intelligence. This integration enhances the understanding of the environment and promotes continuous learning, leading to better and more effective decision-making in real-time for applications that require human-machine collaboration. We evaluated the \textit{ANSR-DT} framework for its ability to learn and adapt to dynamic patterns, observing significant improvements in decision accuracy, reliability, and interpretability when compared to existing state-of-the-art methods. However, challenges still exist in extracting and integrating symbolic rules in complex environments, which limits the full potential of our framework in heterogeneous settings. Moreover, our ongoing research aims to address this issue in the future by ensuring seamless integration of neural models at large. In addition, our open-source implementation promotes reproducibility and encourages future research to build on our foundational work.	 | 在本文中，我们提出了一种适应性神经-符号学习框架，称为“ANSR-DT”，用于数字孪生技术。我们的方法将模式识别算法与强化学习和符号推理相结合，以实现实时学习和自适应智能。这种集成增强了对环境的理解，并促进了持续学习，从而在需要人机协作的应用中实现更好的实时决策。我们评估了ANSR-DT框架在学习和适应动态模式方面的能力，与现有的先进方法相比，观察到在决策准确性、可靠性和可解释性方面的显著改进。然而，在复杂环境中提取和集成符号规则仍存在挑战，这限制了框架在异构环境中的全部潜力。此外，我们正在进行的研究旨在未来通过确保大型神经模型的无缝集成来解决这一问题。另外，我们的开源实现促进了可重复性，并鼓励未来的研究在此基础上进行更深入的研究。
2501.08558	 | LAMS: LLM-Driven Automatic Mode Switching for Assistive Teleoperation	 | Yiran Tao,Jehan Yang,Dan Ding,Zackory Erickson	 | 本文提出了一种由大规模语言模型驱动的自动模式切换（LAMS）方法，能够在不需要事先任务演示的情况下，根据任务上下文自动切换控制模式，从而减少手动模式切换，提高远程操作高自由度机器人的效率。用户研究显示，LAMS 在复杂任务中表现优异，性能随时间提升，优于现有方法。	 | Teleoperating high degrees-of-freedom (DoF) robotic manipulators via low-DoF controllers like joysticks often requires frequent switching between control modes, where each mode maps controller movements to specific robot actions. Manually performing this frequent switching can make teleoperation cumbersome and inefficient. On the other hand, existing automatic mode-switching solutions, such as heuristic-based or learning-based methods, are often task-specific and lack generalizability. In this paper, we introduce LLM-Driven Automatic Mode Switching (LAMS), a novel approach that leverages Large Language Models (LLMs) to automatically switch control modes based on task context. Unlike existing methods, LAMS requires no prior task demonstrations and incrementally improves by integrating user-generated mode-switching examples. We validate LAMS through an ablation study and a user study with 10 participants on complex, long-horizon tasks, demonstrating that LAMS effectively reduces manual mode switches, is preferred over alternative methods, and improves performance over time. The project website with supplementary materials is at https://lams-assistance.github.io/.	 | 通过低自由度（DoF）控制器（如游戏手柄）远程操作高自由度机器人 manipulators 的过程常常需要频繁切换控制模式，每个模式将控制器的运动映射到特定的机器人动作。手动执行这种频繁切换会使得远程操作变得繁琐且效率低下。另一方面，现有的自动模式切换解决方案，如基于启发式的方法或基于学习的方法，通常是针对特定任务的，缺乏通用性。在这篇论文中，我们引入了由大规模语言模型（LLMs）驱动的自动模式切换（LAMS），这是一种新方法，利用大规模语言模型自动根据任务上下文切换控制模式。与现有方法不同，LAMS 不需要事先进行任务演示，并通过集成用户生成的模式切换示例逐步改进。我们通过消融研究和用户研究（涉及10名参与者）验证了 LAMS，该研究使用了复杂的、长时间的任务，结果显示 LAMS 有效地减少了手动模式切换，优于其他方法，并且随着时间的推移提高了性能。项目的网站及补充材料可以在 https://lams-assistance.github.io/ 找到。
2501.08552	 | Reinforcement Learning-Enhanced Procedural Generation for Dynamic Narrative-Driven AR Experiences	 | Aniruddha Srinivas Joshi	 | 本文提出了一种结合增强学习（RL）的WFC框架，用于生成动态且上下文一致的环境，特别适用于移动AR游戏，提升叙事驱动AR游戏的沉浸感，并具有广泛的应用前景，如教育和模拟训练。	 | Procedural Content Generation (PCG) is widely used to create scalable and diverse environments in games. However, existing methods, such as the Wave Function Collapse (WFC) algorithm, are often limited to static scenarios and lack the adaptability required for dynamic, narrative-driven applications, particularly in augmented reality (AR) games. This paper presents a reinforcement learning-enhanced WFC framework designed for mobile AR environments. By integrating environment-specific rules and dynamic tile weight adjustments informed by reinforcement learning (RL), the proposed method generates maps that are both contextually coherent and responsive to gameplay needs. Comparative evaluations and user studies demonstrate that the framework achieves superior map quality and delivers immersive experiences, making it well-suited for narrative-driven AR games. Additionally, the method holds promise for broader applications in education, simulation training, and immersive extended reality (XR) experiences, where dynamic and adaptive environments are critical.	 | 程序化内容生成（PCG）广泛用于在游戏创建可扩展和多样化的环境。然而，现有的方法，如波函数坍缩（WFC）算法，往往局限于静态场景，并且缺乏适应动态、叙事驱动应用所需的变化能力，特别是在增强现实（AR）游戏中。本文提出了一种增强学习（RL）增强的WFC框架，旨在用于移动AR环境。通过结合特定环境规则和由强化学习（RL）驱动的动态瓷砖权重调整，所提出的方法生成的地图既具有上下文一致性，又能响应游戏需求。比较评估和用户研究显示，该框架能够生成高质量的地图并提供沉浸式的体验，使其非常适用于叙事驱动的AR游戏。此外，该方法还具有更广泛的应用前景，包括教育、模拟训练和沉浸式扩展现实（XR）体验，其中动态和适应性环境至关重要。
2501.08551	 | A Theory of Optimistically Universal Online Learnability for General Concept Classes	 | Steve Hanneke,Hongao Wang	 | 该研究提供了在乐观普遍在线学习框架下具有 \(\{0, 1\}\) 标签的概念类的完整描述，探讨了实现在线学习所需的最小数据过程假设，并设计了相应的学习算法；此外，研究还将这些结果扩展到了泛化情况，展示了数据过程假设的等价性，并证明了乐观普遍学习的等价性。	 | We provide a full characterization of the concept classes that are optimistically universally online learnable with $\{0, 1\}$ labels. The notion of optimistically universal online learning was defined in [Hanneke, 2021] in order to understand learnability under minimal assumptions. In this paper, following the philosophy behind that work, we investigate two questions, namely, for every concept class: (1) What are the minimal assumptions on the data process admitting online learnability? (2) Is there a learning algorithm which succeeds under every data process satisfying the minimal assumptions? Such an algorithm is said to be optimistically universal for the given concept class. We resolve both of these questions for all concept classes, and moreover, as part of our solution, we design general learning algorithms for each case. Finally, we extend these algorithms and results to the agnostic case, showing an equivalence between the minimal assumptions on the data process for learnability in the agnostic and realizable cases, for every concept class, as well as the equivalence of optimistically universal learnability.	 | 以下是该摘要的中文翻译：  我们提供了具有 $\{0, 1\}$ 标签的概念类在乐观普遍在线学习方面的完整描述。乐观普遍在线学习的概念在 [Hanneke, 2021] 中被定义，目的是在尽可能少的假设下理解可学习性。在这篇论文中，我们遵循该工作中所体现的思想，探讨两个问题：(1) 每个概念类别能实现在线学习的最小数据过程假设是什么？(2) 是否存在能够在满足最小数据过程假设的每一个数据条件下成功的学习算法？这样的算法被称为给定概念类的乐观普遍学习算法。我们对所有概念类别都解决了这两个问题，并且作为解决方案的一部分，我们为每种情况设计了一般的学习算法。最后，我们将这些算法和结果扩展到泛化情况，展示了泛化情况和可实现情况下的数据过程最小假设之间的等价性，对每一个概念类别都是如此，同时也展示了乐观普遍学习的等价性。
2501.08547	 | OMEGA: A Low-Latency GNN Serving System for Large Graphs	 | Geon-Woo Kim,Donghyun Kim,Jeongyoon Moon,Henry Liu,Tarannum Khan,Anand Iyer,Daehyeok Kim,Aditya Akella	 | OMEGA系统通过选择性重计算和计算图并行性，能够在保持低延迟的同时最小化准确率损失，为大型图上的GNN服务提供高效解决方案。	 | Graph Neural Networks (GNNs) have been widely adopted for their ability to compute expressive node representations in graph datasets. However, serving GNNs on large graphs is challenging due to the high communication, computation, and memory overheads of constructing and executing computation graphs, which represent information flow across large neighborhoods. Existing approximation techniques in training can mitigate the overheads but, in serving, still lead to high latency and/or accuracy loss. To this end, we propose OMEGA, a system that enables low-latency GNN serving for large graphs with minimal accuracy loss through two key ideas. First, OMEGA employs selective recomputation of precomputed embeddings, which allows for reusing precomputed computation subgraphs while selectively recomputing a small fraction to minimize accuracy loss. Second, we develop computation graph parallelism, which reduces communication overhead by parallelizing the creation and execution of computation graphs across machines. Our evaluation with large graph datasets and GNN models shows that OMEGA significantly outperforms state-of-the-art techniques.	 | 图神经网络（GNNs）因其在图数据集中计算表达性节点表示的能力而被广泛采用。然而，在大型图上提供GNN服务具有挑战性，因为构造和执行表示大型邻域间信息流的计算图会产生高通信、计算和内存开销。现有的训练中近似技术可以缓解这些开销，但在服务阶段仍然会导致高延迟和/或准确率损失。为了解决这一问题，我们提出了一种名为OMEGA的系统，该系统通过两个关键思想实现了在大型图上以最小准确率损失提供低延迟的GNN服务。首先，OMEGA采用选择性重计算预先计算的嵌入，这允许重用预先计算的计算子图，同时选择性地重计算一小部分以最小化准确率损失。其次，我们开发了计算图并行性，通过在机器之间并行化计算图的创建和执行来减少通信开销。我们对大型图数据集和GNN模型的评估表明，OMEGA显著优于现有技术。
2501.08537	 | Complexity Control Facilitates Reasoning-Based Compositional Generalization in Transformers	 | Zhongwang Zhang,Pengxiao Lin,Zhiwei Wang,Yaoyu Zhang,Zhi-Qin John Xu	 | 研究发现，复杂性控制策略显著影响transformer在组合任务中是学习普适规则还是依赖记忆映射的能力，并通过不同掩码策略揭示了基于推理的解决方案具有较低的复杂性偏差，这种偏差使得模型能够更好地学习推理规则。研究结果在图像生成和自然语言处理任务中得到验证，展示了广泛的应用潜力。	 | Transformers have demonstrated impressive capabilities across various tasks, yet their performance on compositional problems remains a subject of debate. In this study, we investigate the internal mechanisms underlying Transformers' behavior in compositional tasks. We find that complexity control strategies significantly influence whether the model learns primitive-level rules that generalize out-of-distribution (reasoning-based solutions) or relies solely on memorized mappings (memory-based solutions). By applying masking strategies to the model's information circuits and employing multiple complexity metrics, we reveal distinct internal working mechanisms associated with different solution types. Further analysis reveals that reasoning-based solutions exhibit a lower complexity bias, which aligns with the well-studied neuron condensation phenomenon. This lower complexity bias is hypothesized to be the key factor enabling these solutions to learn reasoning rules. We validate these conclusions across multiple real-world datasets, including image generation and natural language processing tasks, confirming the broad applicability of our findings.	 |  transformer在各种任务中展现了令人印象深刻的性能，但在组合问题上的表现仍然存在争议。在本研究中，我们探讨了 transformer 在组合任务中的内部运作机制。我们发现，复杂性控制策略显著影响模型是学习普适规则（基于推理的解决方案）还是依赖记忆映射（基于记忆的解决方案）的能力。通过在模型的信息电路中应用掩码策略并采用多种复杂性指标，我们揭示了不同解决方案类型相关的不同内部运作机制。进一步的分析表明，基于推理的解决方案表现出较低的复杂性偏差，这与广泛研究的神经元凝聚现象相一致。这种较低的复杂性偏差被假设是这些解决方案能够学习推理规则的关键因素。我们在多个实际数据集上验证了这些结论，包括图像生成和自然语言处理任务，确认了我们的发现具有广泛的应用性。
2501.08504	 | SuperSAM: Crafting a SAM Supernetwork via Structured Pruning and Unstructured Parameter Prioritization	 | Waqwoya Abebe,Sadegh Jafari,Sixing Yu,Akash Dutta,Jan Strube,Nathan R. Tallent,Luanzheng Guo,Pablo Munoz,Ali Jannesari	 | 本文提出了一种针对Vision Transformer架构的单次神经架构搜索方法，通过将Segment Anything Model（SAM）转换为共享权重的超级网络SuperSAM，并结合逐层结构化剪枝和参数优先级化，从而生成更高效、体积更小且性能优于原始预训练模型的子网络。	 | Neural Architecture Search (NAS) is a powerful approach of automating the design of efficient neural architectures. In contrast to traditional NAS methods, recently proposed one-shot NAS methods prove to be more efficient in performing NAS. One-shot NAS works by generating a singular weight-sharing supernetwork that acts as a search space (container) of subnetworks. Despite its achievements, designing the one-shot search space remains a major challenge. In this work we propose a search space design strategy for Vision Transformer (ViT)-based architectures. In particular, we convert the Segment Anything Model (SAM) into a weight-sharing supernetwork called SuperSAM. Our approach involves automating the search space design via layer-wise structured pruning and parameter prioritization. While the structured pruning applies probabilistic removal of certain transformer layers, parameter prioritization performs weight reordering and slicing of MLP-blocks in the remaining layers. We train supernetworks on several datasets using the sandwich rule. For deployment, we enhance subnetwork discovery by utilizing a program autotuner to identify efficient subnetworks within the search space. The resulting subnetworks are 30-70% smaller in size compared to the original pre-trained SAM ViT-B, yet outperform the pretrained model. Our work introduces a new and effective method for ViT NAS search-space design.	 | 神经架构搜索（NAS）是一种自动化设计高效神经架构的强大方法。与传统的NAS方法相比，最近提出的单次NAS方法在执行NAS方面证明更加高效。单次NAS方法通过生成一个共享权重的超级网络来工作，该超级网络充当子网络的搜索空间（容器）。尽管取得了这些成就，设计单次搜索空间仍然是一项重大挑战。在本文中，我们提出了一种针对基于Vision Transformer（ViT）架构的搜索空间设计策略。特别是，我们将Segment Anything Model（SAM）转换为一个共享权重的超级网络，称为SuperSAM。我们的方法涉及通过逐层结构化剪枝和参数优先级化来自动化搜索空间设计。结构化剪枝通过概率性移除某些变压器层来应用，而参数优先级化则对剩余层中的MLP块进行权重重排和切片。我们使用三明治规则在多个数据集上训练超级网络。在部署时，我们利用程序自动调优工具来识别搜索空间中的高效子网络。生成的子网络比原始预训练的SAM ViT-B小30-70%，但却优于预训练模型。我们的工作引入了一种新的且有效的ViT NAS搜索空间设计方法。
2501.08501	 | Scalable Bayesian Physics-Informed Kolmogorov-Arnold Networks	 | Zhiwei Gao,George Em Karniadakis	 | 该研究提出了一种结合dropout Tikhonov 集卡尔曼反演（DTEKI）与Chebyshev柯尔莫哥洛夫-阿诺尔德网络（KANs）的新方法，以解决多层感知机过拟合问题，并通过活动子空间方法降低参数空间维度，从而提高预测准确性和计算效率。	 | Uncertainty quantification (UQ) plays a pivotal role in scientific machine learning, especially when surrogate models are used to approximate complex systems. Although multilayer perceptions (MLPs) are commonly employed as surrogates, they often suffer from overfitting due to their large number of parameters. Kolmogorov-Arnold networks (KANs) offer an alternative solution with fewer parameters. However, gradient-based inference methods, such as Hamiltonian Monte Carlo (HMC), may result in computational inefficiency when applied to KANs, especially for large-scale datasets, due to the high cost of back-propagation.To address these challenges, we propose a novel approach, combining the dropout Tikhonov ensemble Kalman inversion (DTEKI) with Chebyshev KANs. This gradient-free method effectively mitigates overfitting and enhances numerical stability. Additionally, we incorporate the active subspace method to reduce the parameter-space dimensionality, allowing us to improve the accuracy of predictions and obtain more reliable uncertainty estimates.Extensive experiments demonstrate the efficacy of our approach in various test cases, including scenarios with large datasets and high noise levels. Our results show that the new method achieves comparable or better accuracy, much higher efficiency as well as stability compared to HMC, in addition to scalability. Moreover, by leveraging the low-dimensional parameter subspace, our method preserves prediction accuracy while substantially reducing further the computational cost.	 | 不确定性量化（UQ）在科学机器学习中扮演着至关重要的角色，尤其是在使用代理模型逼近复杂系统时。尽管多层感知机（MLPs）常被用作代理模型，但由于其参数众多，它们往往容易过拟合。柯尔莫哥洛夫-阿诺尔德网络（KANs）提供了一种具有较少参数的替代方案。然而，基于梯度的推断方法，如哈密尔顿蒙特卡洛（HMC），在应用于KANs时，特别是对于大规模数据集，由于反向传播的成本较高，可能会导致计算效率低下。为了解决这些挑战，我们提出了一种新的方法，将dropout Tikhonov 集合卡尔曼反演（DTEKI）与Chebyshev KANs结合使用。这种方法是一种无梯度方法，能够有效缓解过拟合并提高数值稳定性。此外，我们还引入了活动子空间方法来降低参数空间的维度，从而提高预测的准确性并获得更可靠的影响估计。广泛的实验结果表明，我们的方法在各种测试案例中都表现出色，包括大数据集和高噪声水平的场景。我们的结果显示，新方法在准确性和效率方面优于HMC，具有更高的稳定性和可扩展性。此外，通过利用低维参数子空间，我们的方法在保持预测准确性的同时，进一步降低了计算成本。
2501.08496	 | Quantifying the Importance of Data Alignment in Downstream Model Performance	 | Krrish Chawla,Aryan Sahai,Mario DePavia,Sudharsan Sundar,Brando Miranda	 | 该研究探讨了数据对齐在训练大型语言模型中的作用，发现训练数据与评估数据之间的对齐程度与模型在下游任务上的性能呈强烈负相关，特别是在特定领域任务如自动形式化中尤为重要。通过使用Task2Vec对齐系数，研究展示了即使在数据集规模相似的情况下，数据对齐的质量对模型性能的影响更为关键。	 | Contrary to the conventional emphasis on dataset size, we explore the role of data alignment -- an often overlooked aspect of data quality -- in training capable Large Language Models (LLMs). To do so, we use the Task2Vec-based alignment coefficient, a quantitative measure of the similarity between two datasets, to quantify the impact of alignment between training data and evaluation data on downstream performance. In particular, we conduct controlled \textit{interventional} experiments for two settings: 1. the impact of increased alignment coefficients between various pre-training (pt) against evaluation datasets, and 2. the impact of increased alignment coefficients between domain specific fine-tuning (ft) against domain specific evaluation. The domain specific task we explore is Autoformalization -- the machine translation task between natural language and code for formal verification. In both settings, we find a strong, predictable negative correlation between the alignment coefficient of a model's training and evaluation data and the model's loss/perplexity on the respective downstream task. These findings suggest a re-evaluation of LLM training approaches, demonstrating the relevance of data alignment compared to data quantity, especially in specialized downstream tasks such as Autoformalization.	 | 与通常重视数据集规模的做法相反，我们探讨了数据对齐——这一经常被忽视的数据质量方面——在训练强大的大型语言模型（LLMs）中的作用。为此，我们使用基于Task2Vec的对齐系数，这是一种衡量两个数据集之间相似性的定量指标，来量化训练数据与评估数据之间对齐程度对下游性能的影响。具体来说，我们在两个设置中进行了受控的干预性实验：1. 各种预训练（pt）数据集与评估数据集之间对齐系数的增加对下游任务性能的影响；2. 针对特定领域微调（ft）数据集与特定领域评估数据集之间对齐系数的增加对下游任务性能的影响。我们研究的具体领域任务是自动形式化——自然语言与代码之间的机器翻译任务，用于形式验证。在两个设置中，我们发现模型的训练和评估数据之间对齐系数与模型在相应下游任务上的损失/困惑度之间存在强烈且可预测的负相关关系。这些发现表明，在与特定下游任务（如自动形式化）相关的训练方法中重新评估数据对齐的重要性，显示出数据对齐相比于数据量的相关性，尤其是在特定下游任务中尤为重要。
2501.08490	 | FLAVARS: A Multimodal Foundational Language and Vision Alignment Model for Remote Sensing	 | Isaac Corley,Simone Fobi Nsutezo,Anthony Ortiz,Caleb Robinson,Rahul Dodhia,Juan M. Lavista Ferres,Peyman Najafirad	 | 本文提出了一种名为FLAVARS的新预训练方法，结合对比学习和蒙版建模，并通过对比位置编码实现地理空间对齐，该方法在基于视觉的任务（如KNN分类和语义分割）上显著优于SkyCLIP基线，并保留了零样本分类能力。	 | Remote sensing imagery is dense with objects and contextual visual information. There is a recent trend to combine paired satellite images and text captions for pretraining performant encoders for downstream tasks. However, while contrastive image-text methods like CLIP enable vision-language alignment and zero-shot classification ability, vision-only downstream performance tends to degrade compared to image-only pretraining, such as MAE. In this paper, we propose FLAVARS, a pretraining method that combines the best of both contrastive learning and masked modeling, along with geospatial alignment via contrastive location encoding. We find that FLAVARS significantly outperforms a baseline of SkyCLIP for vision-only tasks such as KNN classification and semantic segmentation, +6\% mIOU on SpaceNet1, while retaining the ability to perform zero-shot classification, unlike MAE pretrained methods.	 | 遥感图像富含大量物体和上下文视觉信息。最近的趋势是结合配对的卫星图像和文本说明来预训练高性能的编码器，以用于下游任务。然而，尽管像CLIP这样的对比式图像-文本方法能够实现视觉-语言对齐和零样本分类能力，但仅基于图像的下游视觉性能往往会比仅基于图像的预训练方法（如MAE）下降。在本文中，我们提出了一种名为FLAVARS的预训练方法，该方法结合了对比学习和蒙版建模的最佳实践，并通过对比位置编码实现了地理空间对齐。我们发现，FLAVARS在如KNN分类和语义分割等仅基于视觉的任务上显著优于SkyCLIPbaseline，SpaceNet1上的mIOU提高了6%。同时，FLAVARS保留了零样本分类的能力，与MAE预训练方法不同。
2501.08459	 | Head Motion Degrades Machine Learning Classification of Alzheimer's Disease from Positron Emission Tomography	 | Eléonore V. Lieffrig,Takuya Toyonaga,Jiazhen Zhang,John A. Onofrey	 | 本研究展示了头部运动对基于正电子发射断层扫描(PET)的阿尔茨海默病(AD)分类准确性的影响，发现缺乏运动校正会显著降低分类准确性，强调了在PET图像分析中采用有效运动校正方法的重要性。	 | Brain positron emission tomography (PET) imaging is broadly used in research and clinical routines to study, diagnose, and stage Alzheimer's disease (AD). However, its potential cannot be fully exploited yet due to the lack of portable motion correction solutions, especially in clinical settings. Head motion during data acquisition has indeed been shown to degrade image quality and induces tracer uptake quantification error. In this study, we demonstrate that it also biases machine learning-based AD classification. We start by proposing a binary classification algorithm solely based on PET images. We find that it reaches a high accuracy in classifying motion corrected images into cognitive normal or AD. We demonstrate that the classification accuracy substantially decreases when images lack motion correction, thereby limiting the algorithm's effectiveness and biasing image interpretation. We validate these findings in cohorts of 128 $^{11}$C-UCB-J and 173 $^{18}$F-FDG scans, two tracers highly relevant to the study of AD. Classification accuracies decreased by 10% and 5% on 20 $^{18}$F-FDG and 20 $^{11}$C-UCB-J testing cases, respectively. Our findings underscore the critical need for efficient motion correction methods to make the most of the diagnostic capabilities of PET-based machine learning.	 | 脑正电子发射断层扫描（PET）成像在研究和临床中广泛用于阿尔茨海默病（AD）的研究、诊断和分期。然而，由于缺乏便携式的运动校正解决方案，尤其是在临床环境中，其潜力尚未得到充分利用。在数据采集过程中，头部运动确实已被证明会降低图像质量，并导致示踪剂摄取的定量误差。在本研究中，我们展示了它也会偏倚基于机器学习的AD分类。我们首先提出了一种仅基于PET图像的二分类算法。我们发现，该算法在将经过运动校正的图像分类为认知正常或AD方面达到了很高的准确性。我们证明，当图像缺乏运动校正时，分类准确性显著降低，从而限制了算法的效果并偏倚了图像解释。我们使用128例$^{11}$C-UCB-J和173例$^{18}$F-FDG扫描数据，对这一发现进行了验证，这两种示踪剂对AD研究非常重要。在20例$^{18}$F-FDG和20例$^{11}$C-UCB-J测试病例中，分类准确率分别下降了10%和5%。我们的研究结果强调了高效运动校正方法的必要性，以便充分利用基于PET的机器学习的诊断能力。
2501.08457	 | Large Language Models For Text Classification: Case Study And Comprehensive Review	 | Arina Kostina,Marios D. Dikaiakos,Dimosthenis Stefanidis,George Pallis	 | 该研究评估了不同大小、量化方式和架构的语言模型在多分类和二分类任务中的性能，并发现大型语言模型如Llama3和GPT-4在复杂任务中超越传统方法，尽管推理时间较长，而简单机器学习模型在简单任务中则能提供更好的性能与时间权衡。	 | Unlocking the potential of Large Language Models (LLMs) in data classification represents a promising frontier in natural language processing. In this work, we evaluate the performance of different LLMs in comparison with state-of-the-art deep-learning and machine-learning models, in two different classification scenarios: i) the classification of employees' working locations based on job reviews posted online (multiclass classification), and 2) the classification of news articles as fake or not (binary classification). Our analysis encompasses a diverse range of language models differentiating in size, quantization, and architecture. We explore the impact of alternative prompting techniques and evaluate the models based on the weighted F1-score. Also, we examine the trade-off between performance (F1-score) and time (inference response time) for each language model to provide a more nuanced understanding of each model's practical applicability. Our work reveals significant variations in model responses based on the prompting strategies. We find that LLMs, particularly Llama3 and GPT-4, can outperform traditional methods in complex classification tasks, such as multiclass classification, though at the cost of longer inference times. In contrast, simpler ML models offer better performance-to-time trade-offs in simpler binary classification tasks.	 | 利用大型语言模型（LLMs）在数据分类中的潜力，在自然语言处理领域代表了一个有前景的研究前沿。在本文中，我们评估了不同LLM在两种不同分类场景下的性能，这些场景与当前最先进的人工智能和机器学习模型进行了比较：i）基于在线发布的职位评价分类员工的工作地点（多分类任务），和 ii）将新闻文章分类为假新闻或真新闻（二分类任务）。我们的分析涵盖了不同大小、量化方式和架构的多种语言模型。我们探讨了替代提示技术的影响，并根据加权F1分数对模型进行评估。此外，我们研究了每种语言模型性能（F1分数）与时间（推断响应时间）之间的权衡，以提供对每种模型实际应用的更全面理解。我们的研究揭示了根据提示策略的不同，模型响应存在显著差异。我们发现，特别是在复杂分类任务如多分类任务中，LLM，尤其是Llama3和GPT-4，可以超越传统方法，尽管需要更长的推理时间。相比之下，简单的机器学习模型在简单的二分类任务中提供了更好的性能与时间权衡。
2501.08453	 | Vchitect-2.0: Parallel Transformer for Scaling Up Video Diffusion Models	 | Weichen Fan,Chenyang Si,Junhao Song,Zhenyu Yang,Yinan He,Long Zhuo,Ziqi Huang,Ziyue Dong,Jingwen He,Dongwei Pan,Yi Wang,Yuming Jiang,Yaohui Wang,Peng Gao,Xinyuan Chen,Hengjie Li,Dahua Lin,Yu Qiao,Ziwei Liu	 | Vchitect-2.0 是一种并行变压器架构，旨在扩展视频扩散模型以实现大规模文本到视频生成，通过引入多模态扩散块、高效训练框架和高质量数据集，实现视频质量、训练效率和可扩展性的显著提升。	 | We present Vchitect-2.0, a parallel transformer architecture designed to scale up video diffusion models for large-scale text-to-video generation. The overall Vchitect-2.0 system has several key designs. (1) By introducing a novel Multimodal Diffusion Block, our approach achieves consistent alignment between text descriptions and generated video frames, while maintaining temporal coherence across sequences. (2) To overcome memory and computational bottlenecks, we propose a Memory-efficient Training framework that incorporates hybrid parallelism and other memory reduction techniques, enabling efficient training of long video sequences on distributed systems. (3) Additionally, our enhanced data processing pipeline ensures the creation of Vchitect T2V DataVerse, a high-quality million-scale training dataset through rigorous annotation and aesthetic evaluation. Extensive benchmarking demonstrates that Vchitect-2.0 outperforms existing methods in video quality, training efficiency, and scalability, serving as a suitable base for high-fidelity video generation.	 | 我们介绍了Vchitect-2.0，这是一种并行变压器架构，旨在扩展视频扩散模型，以实现大规模的文本到视频生成。Vchitect-2.0系统具有几个关键设计。具体来说，（1）通过引入一种新颖的多模态扩散块，我们的方法可以实现文本描述与生成视频帧之间的一致对齐，并在序列中保持时间连贯性。 （2）为了解决内存和计算瓶颈问题，我们提出了一种高效训练框架，结合了混合并行性和其他内存缩减技术，使分布式系统能够高效训练长视频序列。 （3）此外，我们增强的数据处理流水线通过严格的注释和美学评估，确保了Vchitect T2V DataVerse的创建，这是一个高质量的百万级训练数据集。广泛的基准测试表明，Vchitect-2.0在视频质量、训练效率和可扩展性方面优于现有方法，适合作为高保真度视频生成的基础。
2501.08443	 | Instruction-Guided Fusion of Multi-Layer Visual Features in Large Vision-Language Models	 | Xu Li,Yi Zheng,Haotian Chen,Xiaolei Chen,Yuxuan Liang,Chenghang Lai	 | 该研究分析了不同编码器层的视觉特征在18个基准和6个任务类别中的贡献，并提出了一种基于指令的视觉聚合器，可以根据文本指令动态整合多层特征，提升多模态模型的性能。研究发现，中到高层特征在语义任务中起主导作用，而低层特征在精细感知中至关重要。	 | Large Vision-Language Models (LVLMs) have achieved significant success in multimodal tasks by combining pre-trained vision encoders and large language models. However, current LVLMs mainly rely on features from the final layers of the vision encoder, neglecting complementary information in shallower layers. While recent methods have explored multi-layer features, they are often task-agnostic. We investigate the contributions of visual features from different encoder layers across 18 benchmarks and 6 task categories. Our results show that multi-layer features provide complementary strengths with varying task dependencies, and uniform fusion performs suboptimally. Based on these findings, we propose an instruction-guided vision aggregator that dynamically integrates multi-layer features based on textual instructions, without increasing the number of visual tokens. Extensive evaluations show superior performance, and analysis reveals the dominance of mid-to-high-level features in semantic tasks and the critical role of low-level features in fine-grained perception. This work provides valuable insights into the adaptive use of hierarchical visual features in LVLMs, advancing more flexible multimodal systems.	 | 大型多模态视-语言模型（LVLMs）通过结合预训练的视觉编码器和大规模语言模型，在多模态任务中取得了显著的成功。然而，当前的LVLMs主要依赖于视觉编码器最终层的特征，忽略了较浅层中的补充信息。虽然最近的方法探索了多层特征，但它们往往缺乏特定任务的指导。我们研究了不同编码器层的视觉特征在18个基准和6个任务类别中的贡献。结果表明，多层特征在不同任务中有不同的补充作用，均匀融合效果不佳。基于这些发现，我们提出了一种基于指令的视觉聚合器，该聚合器可以根据文本指令动态整合多层特征，而无需增加视觉标记的数量。广泛的研究表明，该方法在性能上表现出色，并且分析揭示了中到高层特征在语义任务中的主导作用以及低层特征在精细感知中的关键作用。本工作为LVLMs中层次视觉特征的适应性使用提供了有价值的见解，推动了更加灵活的多模态系统的进步。
2501.08440	 | FARE: A Deep Learning-Based Framework for Radar-based Face Recognition and Out-of-distribution Detection	 | Sabri Mustafa Kahya,Boran Hamdi Sivrikaya,Muhammet Sami Yavuz,Eckehard Steinbach	 | 该研究提出了一种基于短距离FMCW雷达的面部识别和异常分布检测系统，通过Range-Doppler和微Range-Doppler图像实现，第一阶段使用三元损失训练识别路径以优化面部分类，第二阶段固定识别路径并训练专门用于异常检测的中间路径。该方法在自建数据集上达到了99.30%的面部分类准确率和96.91%的异常检测AUROC。	 | In this work, we propose a novel pipeline for face recognition and out-of-distribution (OOD) detection using short-range FMCW radar. The proposed system utilizes Range-Doppler and micro Range-Doppler Images. The architecture features a primary path (PP) responsible for the classification of in-distribution (ID) faces, complemented by intermediate paths (IPs) dedicated to OOD detection. The network is trained in two stages: first, the PP is trained using triplet loss to optimize ID face classification. In the second stage, the PP is frozen, and the IPs-comprising simple linear autoencoder networks-are trained specifically for OOD detection. Using our dataset generated with a 60 GHz FMCW radar, our method achieves an ID classification accuracy of 99.30% and an OOD detection AUROC of 96.91%.	 | 在本工作中，我们提出了一种基于短距离FMCW雷达的面部识别和异常分布（OOD）检测的新管道。所提出的系统利用了Range-Doppler和微Range-Doppler图像。该架构的特点是一个主要路径（PP），负责ID面部分类，以及专门用于OOD检测的中间路径（IPs）。该网络在两个阶段进行训练：首先，使用三元损失训练PP以优化ID面部分类。在第二阶段，固定PP，并训练IPs（包括简单的线性自编码网络），以专门进行OOD检测。使用我们的数据集，该数据集是使用60 GHz FMCW雷达生成的，我们的方法实现了99.30%的ID分类准确率和96.91%的OOD检测AUROC。
2501.08423	 | A Constant Velocity Latent Dynamics Approach for Accelerating Simulation of Stiff Nonlinear Systems	 | William Cole Nockolds,C. G. Krishnanunni,Tan Bui-Thanh	 | 该研究提出了一种新颖的方法来解析刚性常微分方程（Stiff ODEs），通过学习潜在的动力学系统并将解编码为一系列直线，从而避免了传统数值积分的计算负担，并且在处理不同时间尺度时表现出色。这种方法在数值实验中证明优于现有的基于机器学习的解决方案。	 | Solving stiff ordinary differential equations (StODEs) requires sophisticated numerical solvers, which are often computationally expensive. In particular, StODE's often cannot be solved with traditional explicit time integration schemes and one must resort to costly implicit methods to compute solutions. On the other hand, state-of-the-art machine learning (ML) based methods such as Neural ODE (NODE) poorly handle the timescale separation of various elements of the solutions to StODEs and require expensive implicit solvers for integration at inference time. In this work, we embark on a different path which involves learning a latent dynamics for StODEs, in which one completely avoids numerical integration. To that end, we consider a constant velocity latent dynamical system whose solution is a sequence of straight lines. Given the initial condition and parameters of the ODE, the encoder networks learn the slope (i.e the constant velocity) and the initial condition for the latent dynamics. In other words, the solution of the original dynamics is encoded into a sequence of straight lines which can be decoded back to retrieve the actual solution as and when required. Another key idea in our approach is a nonlinear transformation of time, which allows for the "stretching/squeezing" of time in the latent space, thereby allowing for varying levels of attention to different temporal regions in the solution. Additionally, we provide a simple universal-approximation-type proof showing that our approach can approximate the solution of stiff nonlinear system on a compact set to any degree of accuracy, ε. We show that the dimension of the latent dynamical system in our approach is independent of ε. Numerical investigation on prototype StODEs suggest that our method outperforms state-of-the art machine learning approaches for handling StODEs.	 | 解析刚性常微分方程（Stiff ODEs）通常需要复杂的数值求解器，这往往非常耗计算资源。特别是，Stiff ODEs 往往不能用传统的显式时间积分方案来求解，必须使用昂贵的隐式方法来计算解。另一方面，最先进的基于机器学习（ML）的方法，如神经常微分方程（Neural ODE, NODE），在处理 Stiff ODEs 解的不同时间尺度时表现不佳，并且在推理时需要昂贵的隐式求解器。在这项工作中，我们采取了一条不同的路径，即学习 Stiff ODEs 的潜在动力学，从而完全避免了数值积分。为此，我们考虑了一个恒定速度的潜在动力学系统，其解是一系列直线。给定初始条件和 ODE 参数，编码器网络学习潜在动力学的斜率（即恒定速度）和初始条件。换句话说，原始动力学的解被编码成一系列直线，可在需要时解码回实际解。我们方法的另一个关键思想是对时间的非线性变换，这使我们在潜在空间中能够“拉伸/压缩”时间，从而允许对解决方案的不同时间区域给予不同级别的注意力。此外，我们提供了一个简单的通用逼近证明，表明我们的方法在紧致集上可以将任何精度ε的刚性非线性系统的解进行逼近。我们证明了我们方法中的潜在动力学系统的维度与ε无关。对原型Stiff ODEs 的数值研究表明，我们的方法在处理 Stiff ODEs 方面优于最先进的机器学习方法。
2501.08421	 | SEAL: Speaker Error Correction using Acoustic-conditioned Large Language Models	 | Anurag Kumar,Rohit Paturi,Amber Afshan,Sundararajan Srinivasan	 | 该研究提出了一种新的声学条件化方法，结合大语言模型进行端到端的发言者分段，通过利用声学分段器的信息减少发言者错误，并在多个数据集上显著降低了24-43%的发言者错误率。	 | Speaker Diarization (SD) is a crucial component of modern end-to-end ASR pipelines. Traditional SD systems, which are typically audio-based and operate independently of ASR, often introduce speaker errors, particularly during speaker transitions and overlapping speech. Recently, language models including fine-tuned large language models (LLMs) have shown to be effective as a second-pass speaker error corrector by leveraging lexical context in the transcribed output. In this work, we introduce a novel acoustic conditioning approach to provide more fine-grained information from the acoustic diarizer to the LLM. We also show that a simpler constrained decoding strategy reduces LLM hallucinations, while avoiding complicated post-processing. Our approach significantly reduces the speaker error rates by 24-43% across Fisher, Callhome, and RT03-CTS datasets, compared to the first-pass Acoustic SD.	 | 发言者分段（SD）是现代端到端ASR管道中的关键组成部分。传统的SD系统通常基于音频，且独立于ASR运行，往往在发言转换和重叠语音期间引入了发言者错误。最近，包括微调的大语言模型（LLMs）已经显示出通过利用转录输出中的词汇上下文作为第二次发言者错误修正的有效工具。在本文中，我们介绍了一种新颖的声学条件化方法，以提供来自声学分段器的更细粒度的信息给LLM。我们还展示了简单受约束解码策略可以减少LLM的幻觉，同时避免复杂的后处理。我们的方法在Fisher、Callhome和RT03-CTS数据集上将发言者错误率显著降低了24-43%，相较于第一次通过声学SD。
2501.08408	 | Leveraging 2D Masked Reconstruction for Domain Adaptation of 3D Pose Estimation	 | Hansoo Park,Chanwoo Kim,Jihyeon Kim,Hoseong Cho,Nhat Nguyen Bao Truong,Taehwan Kim,Seungryul Baek	 | 本文提出了一种无监督域适应框架，利用掩码图像建模（MIM）结合前景为中心的重建和注意力正则化，提高了RGB图像在不同分布场景下的三维姿态估计精度，并在多个数据集上取得了最佳性能。	 | RGB-based 3D pose estimation methods have been successful with the development of deep learning and the emergence of high-quality 3D pose datasets. However, most existing methods do not operate well for testing images whose distribution is far from that of training data. However, most existing methods do not operate well for testing images whose distribution is far from that of training data. This problem might be alleviated by involving diverse data during training, however it is non-trivial to collect such diverse data with corresponding labels (i.e. 3D pose). In this paper, we introduced an unsupervised domain adaptation framework for 3D pose estimation that utilizes the unlabeled data in addition to labeled data via masked image modeling (MIM) framework. Foreground-centric reconstruction and attention regularization are further proposed to increase the effectiveness of unlabeled data usage. Experiments are conducted on the various datasets in human and hand pose estimation tasks, especially using the cross-domain scenario. We demonstrated the effectiveness of ours by achieving the state-of-the-art accuracy on all datasets.	 | 基于RGB的三维姿态估计方法随着深度学习的发展和高质量三维姿态数据集的出现而取得了成功。然而，大多数现有方法在测试图像的分布与训练数据分布相差较大时表现不佳。这个问题可以通过在训练过程中引入多样化的数据来缓解，但是收集附有相应标签（例如3D姿态）的多样数据并非易事。在本文中，我们提出了一种无监督域适应框架，通过掩码图像建模（MIM）框架利用标记数据和未标记数据。进一步提出了前景为中心的重建和注意力正则化，以提高未标记数据使用的有效性。我们分别在人类姿态估计和手部姿态估计任务的各种数据集上进行了实验，特别是使用跨域场景。通过在所有数据集上取得最先进的精度，我们证明了该方法的有效性。
2501.08406	 | OptiChat: Bridging Optimization Models and Practitioners with Large Language Models	 | Hao Chen,Gonzalo Esteban Constante-Flores,Krishna Sri Ipsit Mantri,Sai Madhukiran Kompalli,Akshdeep Singh Ahluwalia,Can Li	 | OptiChat 是一个自然语言对话系统，结合了大型语言模型和优化模型的功能调用，旨在帮助非优化背景的从业者更好地理解、使用和解释优化模型；实验表明，OptiChat 能够有效减少从业者与优化模型之间的差距，提供准确和即时的响应。	 | Optimization models have been applied to solve a wide variety of decision-making problems. These models are usually developed by optimization experts but are used by practitioners without optimization expertise in various application domains. As a result, practitioners often struggle to interact with and draw useful conclusions from optimization models independently. To fill this gap, we introduce OptiChat, a natural language dialogue system designed to help practitioners interpret model formulation, diagnose infeasibility, analyze sensitivity, retrieve information, evaluate modifications, and provide counterfactual explanations. By augmenting large language models (LLMs) with functional calls and code generation tailored for optimization models, we enable seamless interaction and minimize the risk of hallucinations in OptiChat. We develop a new dataset to evaluate OptiChat's performance in explaining optimization models. Experiments demonstrate that OptiChat effectively bridges the gap between optimization models and practitioners, delivering autonomous, accurate, and instant responses.	 | 优化模型已被应用于解决各种决策问题。这些模型通常由优化专家开发，但被没有优化背景的从业者在各个应用领域使用。因此，从业者常常难以独立与优化模型交互并从模型中得出有用的结论。为了解决这一问题，我们引入了OptiChat，这是一个自然语言对话系统，旨在帮助从业者解释模型公式、诊断不可行性、分析敏感性、检索信息、评估修改并提供反事实解释。通过将大型语言模型（LLMs）与针对优化模型的功能调用和代码生成相结合，我们使OptiChat能够无缝交互并最大限度地减少妄想的风险。我们开发了一个新的数据集来评估OptiChat在解释优化模型方面的性能。实验表明，OptiChat有效地弥合了优化模型与从业者之间的差距，提供了自主、准确和即时的响应。
2501.08393	 | Empathetic Conversational Agents: Utilizing Neural and Physiological Signals for Enhanced Empathetic Interactions	 | Nastaran Saffaryazdi,Tamil Selvan Gunasekaran,Kate Laveys,Elizabeth Broadbent,Mark Billinghurst	 | 本文探讨了将神经和生理信号集成到对话代理中，以增强共情交互，并通过用户研究验证了实时情绪识别的有效性，虽然研究也发现了准确性、转换速度和个人差异等挑战。这项工作推进了人机交互的发展，并展示了共情数字人类的潜力。	 | Conversational agents (CAs) are revolutionizing human-computer interaction by evolving from text-based chatbots to empathetic digital humans (DHs) capable of rich emotional expressions. This paper explores the integration of neural and physiological signals into the perception module of CAs to enhance empathetic interactions. By leveraging these cues, the study aims to detect emotions in real-time and generate empathetic responses and expressions. We conducted a user study where participants engaged in conversations with a DH about emotional topics. The DH responded and displayed expressions by mirroring detected emotions in real-time using neural and physiological cues. The results indicate that participants experienced stronger emotions and greater engagement during interactions with the Empathetic DH, demonstrating the effectiveness of incorporating neural and physiological signals for real-time emotion recognition. However, several challenges were identified, including recognition accuracy, emotional transition speeds, individual personality effects, and limitations in voice tone modulation. Addressing these challenges is crucial for further refining Empathetic DHs and fostering meaningful connections between humans and artificial entities. Overall, this research advances human-agent interaction and highlights the potential of real-time neural and physiological emotion recognition in creating empathetic DHs.	 | 对话代理（CAs）正在通过从基于文本的聊天机器人进化为能够表达丰富情感的共情数字人类（DHs）来革命性地改变人机交互方式。本文探讨了将神经和生理信号集成到CAs的感知模块中，以增强共情交互。通过利用这些线索，研究旨在实时检测情绪并生成共情响应和表达。我们在一项用户研究中让参与者与DH就情绪话题进行对话，DH通过实时模拟检测到的情绪来回应并显示表情。研究结果显示，参与者在与共情DH互动时体验到了更强烈的情绪和更高的参与度，证明了集成神经和生理信号进行实时情绪识别的有效性。然而，研究也发现了几个挑战，包括情绪识别准确性、情绪转换速度、个体个性影响以及声音音调调节的限制。解决这些挑战对于进一步完善共情DH并促进人类与虚拟实体之间的有意义连接至关重要。总体而言，这项研究推进了人机交互的发展，并突显了实时神经和生理情绪识别在创造共情DH方面的潜力。
2501.08365	 | Towards Best Practices for Open Datasets for LLM Training	 | Stefan Baack,Stella Biderman,Kasia Odrozek,Aviya Skowron,Ayah Bdeir,Jillian Bommarito,Jennifer Ding,Maximilian Gahntz,Paul Keller,Pierre-Carl Langlais,Greg Lindahl,Sebastian Majstorovic,Nik Marda,Guilherme Penedo,Maarten Van Segbroeck,Jennifer Wang,Leandro von Werra,Mitchell Baker,Julie Belião,Kasia Chmielinski,Marzieh Fadaee,Lisa Gutermuth,Hynek Kydlíček,Greg Leppert,EM Lewis-Jong,Solana Larsen,Shayne Longpre,Angela Oduor Lungati,Cullen Miller,Victor Miller,Max Ryabinin,Kathleen Siminyu,Andrew Strait,Mark Surman,Anna Tumadóttir,Maurice Weber,Rebecca Weiss,Lee White,Thomas Wolf	 | 许多AI公司在未经版权拥有者许可的情况下训练大型语言模型，虽然在某些国家法律上可能存在一定的灰色地带，但这种做法已导致版权诉讼增加和数据共享减少，损害了透明度和创新；为了解决这一问题，需要在法律、技术和政策层面进行合作，投资于相关标准和开放文化的发展。	 | Many AI companies are training their large language models (LLMs) on data without the permission of the copyright owners. The permissibility of doing so varies by jurisdiction: in countries like the EU and Japan, this is allowed under certain restrictions, while in the United States, the legal landscape is more ambiguous. Regardless of the legal status, concerns from creative producers have led to several high-profile copyright lawsuits, and the threat of litigation is commonly cited as a reason for the recent trend towards minimizing the information shared about training datasets by both corporate and public interest actors. This trend in limiting data information causes harm by hindering transparency, accountability, and innovation in the broader ecosystem by denying researchers, auditors, and impacted individuals access to the information needed to understand AI models.   While this could be mitigated by training language models on open access and public domain data, at the time of writing, there are no such models (trained at a meaningful scale) due to the substantial technical and sociological challenges in assembling the necessary corpus. These challenges include incomplete and unreliable metadata, the cost and complexity of digitizing physical records, and the diverse set of legal and technical skills required to ensure relevance and responsibility in a quickly changing landscape. Building towards a future where AI systems can be trained on openly licensed data that is responsibly curated and governed requires collaboration across legal, technical, and policy domains, along with investments in metadata standards, digitization, and fostering a culture of openness.	 | 许多AI公司正在未经版权拥有者许可的情况下训练他们的大型语言模型（LLMs）。这种行为在不同司法管辖区的合法性有所不同：在欧盟和日本等国家，这种做法在某些限制下是允许的，而在美国，法律环境则更加模糊。无论从法律角度如何界定，创作者的担忧已经导致了多个高调的版权诉讼案例，诉讼威胁也常被用来解释企业及公共利益方最近减少分享训练数据信息的趋势。这种限制数据信息的做法会损害透明度、问责制和创新，因为它剥夺了研究人员、审计员和受影响个人理解AI模型所需的必要信息。  尽管可以通过使用开放访问和公共领域数据来训练语言模型来缓解这一问题，但到目前为止，由于构建适合训练大规模语言模型的开放许可数据集存在大量技术和社会挑战，还没有这样的模型。这些挑战包括不完整和不稳定的元数据、物理记录数字化的成本和技术复杂性，以及确保快速变化环境下相关性和责任性的多样化的法律和技术技能要求。要朝着一个未来迈进，在这个未来中，AI系统可以基于负责任地编目和管理的开放许可数据训练，需要在法律、技术和政策领域进行合作，并投资于元数据标准、数字化和促进开放文化的发展。
2501.08361	 | Weight Averaging for Out-of-Distribution Generalization and Few-Shot Domain Adaptation	 | Shijian Xu	 | 本文通过引入梯度相似性作为损失正则化项来增强权重平均（WA）方法中的模型多样性，并结合锐度感知最小化（SAM）以提高离分布外推性能，特别是在少量样本域适配任务上取得了显著效果。	 | Empirical risk minimization (ERM) is not robust to changes in the distribution of data. When the distribution of test data is different from that of training data, the problem is known as out-of-distribution generalization. Recently, two techniques have been developed for addressing out-of-distribution generalization in computer vision: weight averaging (WA) and sharpness-aware minimization (SAM). WA involves training multiple models with different hyperparameters and then averaging the weights of these models, which can significantly improve out-of-distribution generalization performance. SAM optimizes a neural network to find minima in flat regions, which have been proven to perform well under distribution shifts. While these techniques have made great progress, there is still room for improvement and further exploration. In this thesis, we propose increasing the model diversity in WA explicitly by introducing gradient similarity as a loss regularizer to further improve out-of-distribution generalization performance. We also propose combining WA and SAM to solve the problem of few-shot domain adaptation. Our extensive experiments on digits datasets (MNIST, SVHN, USPS, MNIST-M) and other domain adaptation datasets (VLCS, PACS) show that combining WA and SAM leads to improved out-of-distribution generalization performance and significantly increases few-shot domain adaptation accuracy.	 | 经验风险最小化（ERM）对数据分布的变化不具有鲁棒性。当测试数据的分布与训练数据的分布不同时，这种情况被称为离分布外推。近年来，计算机视觉领域开发了两种技术来解决离分布外推问题：权重平均（WA）和锐度感知最小化（SAM）。WA涉及使用不同的超参数训练多个模型，然后平均这些模型的权重，这可以显著提高离分布外推性能。SAM通过优化神经网络以找到平坦区域的极小值，这些极小值在分布偏移下表现良好。尽管这些技术已经取得了显著的进步，但仍存在改进和进一步探索的空间。在本论文中，我们提出通过引入梯度相似性作为损失正则化项来显式增加WA中的模型多样性，以进一步提高离分布外推性能。我们还提出将WA和SAM相结合，解决少量样本域适配问题。我们在手写数字数据集（MNIST、SVHN、USPS、MNIST-M）和其他域适配数据集（VLCS、PACS）上的广泛实验表明，结合WA和SAM可以提高离分布外推性能，并显著提高少量样本域适配的准确性。
2501.08341	 | Dissecting a Small Artificial Neural Network	 | Xiguang Yang,Krish Arora,Michael Bachmann	 | 该研究探讨了简单人工神经网络在逻辑异或门问题上的损失景观和反向传播特性，发现通过非随机和随机批次获得的横截面形状差异显著，并引入微观可逆熵来表征网络相行为，揭示了随着隐藏神经元数量增加，损失景观简化从而消除熵障碍的过程。	 | We investigate the loss landscape and backpropagation dynamics of convergence for the simplest possible artificial neural network representing the logical exclusive-OR (XOR) gate. Cross-sections of the loss landscape in the nine-dimensional parameter space are found to exhibit distinct features, which help understand why backpropagation efficiently achieves convergence toward zero loss, whereas values of weights and biases keep drifting. Differences in shapes of cross-sections obtained by nonrandomized and randomized batches are discussed. In reference to statistical physics we introduce the microcanonical entropy as a unique quantity that allows to characterize the phase behavior of the network. Learning in neural networks can thus be thought of as an annealing process that experiences the analogue of phase transitions known from thermodynamic systems. It also reveals how the loss landscape simplifies as more hidden neurons are added to the network, eliminating entropic barriers caused by finite-size effects.	 | 我们研究了最简单的代表逻辑异或门（XOR门）的人工神经网络在损失景观和反向传播收敛动力学方面的特性。在九维参数空间中找到的损失景观的横截面显示出不同的特征，这些特征有助于理解为什么反向传播能够高效地将损失降至零，但权重和偏置值却持续波动。我们讨论了通过非随机批次和随机批次获得的横截面形状的差异。参考统计物理学，我们引入了微观可逆熵作为唯一能够表征网络相行为的量。因此，神经网络的学习可以被视为一种退火过程，该过程经历了类似于热力学系统中所知的相变。此外，它还揭示了随着网络中隐藏神经元数量的增加，损失景观如何简化，从而消除由有限尺寸效应引起的熵障碍。
2501.08334	 | High-throughput digital twin framework for predicting neurite deterioration using MetaFormer attention	 | Kuanren Qian,Genesis Omana Suarez,Toshihiko Nambara,Takahisa Kanekiyo,Yongjie Jessica Zhang	 | 该研究提出了一种高通量数字孪生框架，结合合成数据生成、实验图像和机器学习模型，用于模拟神经发育障碍（NDDs）相关的神经突变，能够有效捕捉神经突变的多样性和复杂性，辅助研究人员作出明智的实验决策，降低成本并促进治疗方案的发展。	 | Neurodevelopmental disorders (NDDs) cover a variety of conditions, including autism spectrum disorder, attention-deficit/hyperactivity disorder, and epilepsy, which impair the central and peripheral nervous systems. Their high comorbidity and complex etiologies present significant challenges for accurate diagnosis and effective treatments. Conventional clinical and experimental studies are time-intensive, burdening research progress considerably. This paper introduces a high-throughput digital twin framework for modeling neurite deteriorations associated with NDDs, integrating synthetic data generation, experimental images, and machine learning (ML) models. The synthetic data generator utilizes an isogeometric analysis (IGA)-based phase field model to capture diverse neurite deterioration patterns such as neurite retraction, atrophy, and fragmentation while mitigating the limitations of scarce experimental data. The ML model utilizes MetaFormer-based gated spatiotemporal attention architecture with deep temporal layers and provides fast predictions. The framework effectively captures long-range temporal dependencies and intricate morphological transformations with average errors of 1.9641% and 6.0339% for synthetic and experimental neurite deterioration, respectively. Seamlessly integrating simulations, experiments, and ML, the digital twin framework can guide researchers to make informed experimental decisions by predicting potential experimental outcomes, significantly reducing costs and saving valuable time. It can also advance our understanding of neurite deterioration and provide a scalable solution for exploring complex neurological mechanisms, contributing to the development of targeted treatments.	 | 神经发育障碍（NDDs）涵盖多种状况，包括自闭症谱系障碍、注意缺陷多动障碍和癫痫症，这些病症会损害中枢和外周神经系统。它们的高共病率和复杂的病因给准确诊断和有效治疗带来了巨大挑战。传统的临床和实验研究耗时较长，严重影响了研究进展。本文介绍了一种高通量数字孪生框架，用于模拟与NDDs相关的神经突变，该框架整合了合成数据生成、实验图像和机器学习（ML）模型。合成数据生成器使用基于等几何分析（IGA）的相场模型来捕捉神经突变化的多样性模式，如神经突的退缩、萎缩和碎片化，同时缓解实验数据稀缺的限制。ML模型利用基于MetaFormer的门控空间-时间注意力架构，具有深层时间层，并提供快速预测。该框架有效地捕捉了长时间依赖关系和复杂的形态学变化，对于合成和实验神经突变化的平均错误率分别为1.9641%和6.0339%。通过无缝整合模拟、实验和ML，数字孪生框架可以指导研究人员通过预测潜在实验结果来做出明智的实验决策，显著降低成本并节省宝贵的时间。它还可以推动对神经突变化的理解，并提供一种可扩展的解决方案，用于探讨复杂的神经机制，从而促进针对性治疗方法的发展。
