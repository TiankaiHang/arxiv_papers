| ID | Title | Authors | Summary (zh) | Abstract (en) | Abstract (zh) | 
| --- | --- | --- | --- | --- | --- |
2501.01402	 | Best Transition Matrix Esitimation or Best Label Noise Robustness Classifier? Two Possible Methods to Enhance the Performance of T-revision	 | Haixu Liu,Zerui Tao,Naihui Zhang,Sixing Liu	 | 本文探讨了在标签噪声环境下构建鲁棒深度学习分类器的方法，提出了基于已知和未知噪声转移矩阵的正向校正、重要性加权以及T-Revision系列方法，并通过FashionMNIST和CIFAR-10数据集评估了这些方法的效果。	 | Label noise refers to incorrect labels in a dataset caused by human errors or collection defects, which is common in real-world applications and can significantly reduce the accuracy of models. This report explores how to estimate noise transition matrices and construct deep learning classifiers that are robust against label noise. In cases where the transition matrix is known, we apply forward correction and importance reweighting methods to correct the impact of label noise using the transition matrix. When the transition matrix is unknown or inaccurate, we use the anchor point assumption and T-Revision series methods to estimate or correct the noise matrix. In this study, we further improved the T-Revision method by developing T-Revision-Alpha and T-Revision-Softmax to enhance stability and robustness. Additionally, we designed and implemented two baseline classifiers, a Multi-Layer Perceptron (MLP) and ResNet-18, based on the cross-entropy loss function. We compared the performance of these methods on predicting clean labels and estimating transition matrices using the FashionMINIST dataset with known noise transition matrices. For the CIFAR-10 dataset, where the noise transition matrix is unknown, we estimated the noise matrix and evaluated the ability of the methods to predict clean labels.	 | 标签噪声是指数据集中因人为错误或收集缺陷造成的错误标签，这种现象在实际应用中很常见，会显著降低模型的准确性。本报告探讨了如何估计噪声转移矩阵，并构建能够在标签噪声下保持鲁棒性的深度学习分类器。当转移矩阵已知时，我们应用正向校正和重要性加权方法，利用转移矩阵校正标签噪声的影响。当转移矩阵未知或不准确时，我们利用锚点假设和T-Revision系列方法来估计或校正噪声矩阵。在本研究中，我们进一步改进了T-Revision方法，开发了T-Revision-Alpha和T-Revision-Softmax，以增强稳定性和鲁棒性。此外，我们基于交叉熵损失函数设计并实现了两种基线分类器——多层感知机（MLP）和ResNet-18。我们使用FashionMINIST数据集（其中噪声转移矩阵已知）来比较这些方法在预测干净标签和估计转移矩阵方面的性能。对于噪声转移矩阵未知的CIFAR-10数据集，我们估计了噪声矩阵，并评估了这些方法预测干净标签的能力。
2501.01394	 | A Unified Hyperparameter Optimization Pipeline for Transformer-Based Time Series Forecasting Models	 | Jingjing Xu,Caesar Wu,Yuan-Fang Li,Grégoire Danoy,Pascal Bouvry	 | 本文提出了一种统一的超参数优化管道，用于Transformer基时序预测模型，并在多个最先进的模型上进行实验，展示了其在标准基准数据集上的性能，旨在为工业实践者和学术研究人员提供指导。	 | Transformer-based models for time series forecasting (TSF) have attracted significant attention in recent years due to their effectiveness and versatility. However, these models often require extensive hyperparameter optimization (HPO) to achieve the best possible performance, and a unified pipeline for HPO in transformer-based TSF remains lacking. In this paper, we present one such pipeline and conduct extensive experiments on several state-of-the-art (SOTA) transformer-based TSF models. These experiments are conducted on standard benchmark datasets to evaluate and compare the performance of different models, generating practical insights and examples. Our pipeline is generalizable beyond transformer-based architectures and can be applied to other SOTA models, such as Mamba and TimeMixer, as demonstrated in our experiments. The goal of this work is to provide valuable guidance to both industry practitioners and academic researchers in efficiently identifying optimal hyperparameters suited to their specific domain applications. The code and complete experimental results are available on GitHub.	 | 基于Transformer的模型在时序预测（TSF）领域近年来引起了广泛关注，由于其有效性和灵活性。然而，这些模型通常需要进行大量的超参数优化（HPO）以实现最佳性能，而在Transformer基时序预测中的统一HPO管道仍然缺乏。本文中，我们提出了一种这样的管道，并在几种最先进的（SOTA）Transformer基时序预测模型上进行了广泛实验。这些实验在标准基准数据集上进行，以评估和比较不同模型的表现，产生实际见解和案例。我们的管道不仅适用于Transformer架构，还可以推广到其他SOTA模型，如Mamba和TimeMixer，这在我们的实验中得到了证明。本文的目标是为工业实践者和学术研究人员提供有价值的指导，帮助他们高效地识别适合其特定领域应用的最佳超参数。代码和完整的实验结果已发布在GitHub上。
2501.01370	 | Embedding-based Approaches to Hyperpartisan News Detection	 | Karthik Mohan,Pengyu Chen	 | 本文介绍了一种系统，用于判断新闻文章是否属于具有极端政治立场、旨在制造公众政治分歧的超党派新闻，该系统结合了n-克gram、情感分析和预训练ELMo与双向LSTM的方法，在10折交叉验证中达到了83%的准确率。	 | In this paper, we describe our systems in which the objective is to determine whether a given news article could be considered as hyperpartisan. Hyperpartisan news is news that takes an extremely polarized political standpoint with an intention of creating political divide among the public. We attempted several approaches, including n-grams, sentiment analysis, as well as sentence and document representation using pre-tained ELMo. Our best system using pre-trained ELMo with Bidirectional LSTM achieved an accuracy of 83% through 10-fold cross-validation without much hyperparameter tuning.	 | 在本文中，我们描述了我们的系统，目标是确定给定的新闻文章是否可以被认为是超党派新闻。超党派新闻是指那些采取极端政治立场，旨在制造公众政治分歧的新闻。我们尝试了多种方法，包括n-克gram、情感分析，以及使用预训练ELMo的句子和文档表示。使用预训练ELMo与双向LSTM的最佳系统，在10折交叉验证中未进行大量超参数调整的情况下，取得了83%的准确率。
2501.01344	 | Machine Learning for Modeling Wireless Radio Metrics with Crowdsourced Data and Local Environment Features	 | Yifeng Qiu,Alexis Bose	 | 本文介绍了一套名为CRC-ML-Radio Metrics的机器学习模型，该模型利用众包数据和环境特征，能够准确预测4G环境中的RSRP、RSRQ和RSSI指标，在多伦多、蒙特利尔和温哥华地区的测试中表现出色，RMSE性能达到了较低的值，证明其在复杂城市环境中的稳健性和适应性。	 | This paper presents a suite of machine learning models, CRC-ML-Radio Metrics, designed for modeling RSRP, RSRQ, and RSSI wireless radio metrics in 4G environments. These models utilize crowdsourced data with local environmental features to enhance prediction accuracy across both indoor at elevation and outdoor urban settings. They achieve RMSE performance of 9.76 to 11.69 dB for RSRP, 2.90 to 3.23 dB for RSRQ, and 9.50 to 10.36 dB for RSSI, evaluated on over 300,000 data points in the Toronto, Montreal, and Vancouver areas. These results demonstrate the robustness and adaptability of the models, supporting precise network planning and quality of service optimization in complex Canadian urban environments.	 | 本文介绍了用于建模4G环境中的RSRP、RSRQ和RSSI无线射频指标的一套机器学习模型——CRC-ML-Radio Metrics。这些模型利用众包数据和本地环境特征，以提高在室内和室外城市环境中的预测精度。在多伦多、蒙特利尔和温哥华地区的超过30万个数据点上，这些模型实现了RSRP的RMSE性能在9.76至11.69 dB之间、RSRQ的RMSE性能在2.90至3.23 dB之间以及RSSI的RMSE性能在9.50至10.36 dB之间。这些结果表明，这些模型具有很强的稳健性和适应性，支持在复杂的加拿大城市环境中进行精确的网络规划和服务质量优化。
2501.01339	 | Simultaneous Latent State Estimation and Latent Linear Dynamics Discovery from Image Observations	 | Nikita Kostin	 | 本报告总结了状态估计领域的历史进展，特别是针对解析推导或近似给定噪声观测值后的先验滤波分布的算法，并提出了一种基于图像观测值的新颖潜在状态估计解决方案。	 | The problem of state estimation has a long history with many successful algorithms that allow analytical derivation or approximation of posterior filtering distribution given the noisy observations. This report tries to conclude previous works to resolve the problem of latent state estimation given image-based observations and also suggests a new solution to this problem.	 | 状态估计问题有着悠久的历史，许多成功的算法使得我们能够对给定噪声观测值后先验滤波分布进行解析推导或近似。本报告旨在总结前人的工作，解决基于图像观测值的潜在状态估计问题，并提出一个新的解决方案。
2501.01326	 | Domain-invariant feature learning in brain MR imaging for content-based image retrieval	 | Shuya Tobari,Shuhei Tomoshige,Hayato Muraki,Kenichi Oishi,Hitoshi Iyatomi	 | 本研究提出了一种名为样式编码对抗领域适应（SE-ADA）的新方法，通过将领域特定信息从低维表示中分离并使用对抗学习来最小化领域差异，从而提高脑部MRI图像的内容基于图像检索性能，并在多个公开数据集上展示了最佳的疾病搜索准确性。	 | When conducting large-scale studies that collect brain MR images from multiple facilities, the impact of differences in imaging equipment and protocols at each site cannot be ignored, and this domain gap has become a significant issue in recent years. In this study, we propose a new low-dimensional representation (LDR) acquisition method called style encoder adversarial domain adaptation (SE-ADA) to realize content-based image retrieval (CBIR) of brain MR images. SE-ADA reduces domain differences while preserving pathological features by separating domain-specific information from LDR and minimizing domain differences using adversarial learning.   In evaluation experiments comparing SE-ADA with recent domain harmonization methods on eight public brain MR datasets (ADNI1/2/3, OASIS1/2/3/4, PPMI), SE-ADA effectively removed domain information while preserving key aspects of the original brain structure and demonstrated the highest disease search accuracy.	 | 在进行大规模研究并从多个设施收集脑部MRI图像时，每个场所使用的成像设备和协议的差异不能被忽视，这种领域差异已成为近年来的重要问题。在本研究中，我们提出了一种新的低维表示（LDR）获取方法——样式编码对抗领域适应（SE-ADA），以实现脑部MRI图像的内容基于图像检索（CBIR）。SE-ADA通过将领域特定信息从LDR中分离出来，并使用对抗学习最小化领域差异，来减少领域差异同时保留病理特征。在使用对抗领域适应方法（SE-ADA）与最近的领域谐调方法在八个公开的脑部MRI数据集（ADNI1/2/3、OASIS1/2/3/4、PPMI）上的评估实验中，SE-ADA有效去除了领域信息，同时保留了原始脑结构的关键方面，并展示了最高的疾病搜索准确性。
2501.01317	 | Understanding Difficult-to-learn Examples in Contrastive Learning: A Theoretical Framework for Spectral Contrastive Learning	 | Yi-Ge Zhang,Jingyi Cui,Qiran Li,Yisen Wang	 | 无监督对比学习在各种任务中表现出显著性能提升，但难以学习的样本在无监督设置中贡献有限，去除这些样本并通过调整边界和温度缩放技术能够提升模型的泛化能力和分类性能。研究发展了一个理论框架并进行了详细分析，证实了去除难以学习样本的有效性，并提出了一种简单高效的机制来选择这些样本。	 | Unsupervised contrastive learning has shown significant performance improvements in recent years, often approaching or even rivaling supervised learning in various tasks. However, its learning mechanism is fundamentally different from that of supervised learning. Previous works have shown that difficult-to-learn examples (well-recognized in supervised learning as examples around the decision boundary), which are essential in supervised learning, contribute minimally in unsupervised settings. In this paper, perhaps surprisingly, we find that the direct removal of difficult-to-learn examples, although reduces the sample size, can boost the downstream classification performance of contrastive learning. To uncover the reasons behind this, we develop a theoretical framework modeling the similarity between different pairs of samples. Guided by this theoretical framework, we conduct a thorough theoretical analysis revealing that the presence of difficult-to-learn examples negatively affects the generalization of contrastive learning. Furthermore, we demonstrate that the removal of these examples, and techniques such as margin tuning and temperature scaling can enhance its generalization bounds, thereby improving performance. Empirically, we propose a simple and efficient mechanism for selecting difficult-to-learn examples and validate the effectiveness of the aforementioned methods, which substantiates the reliability of our proposed theoretical framework.	 | 无监督对比学习近年来在各种任务中展现了显著的性能提升，有时甚至接近或与监督学习相媲美。然而，其学习机制与监督学习的基本机制截然不同。先前的研究表明，在监督学习中被视为关键（即在决策边界附近易于识别的样本），难以学习的样本在无监督设置中贡献甚微。在本文中，我们发现，尽管直接去除难以学习的样本会减少样本量，但这可以提升对比学习的下游分类性能。为了探究这一现象背后的原因，我们发展了一个理论框架，用以建模不同样本对之间的相似性。根据这一理论框架，我们进行了详尽的理论分析，揭示出难以学习的样本对对比学习的泛化能力产生负面影响。此外，我们证明了去除这些样本以及采用诸如边界调整和温度缩放的技术可以增强其泛化界，从而提高性能。  我们在实验中提出了一种简单而高效的机制来选择难以学习的样本，并验证了上述方法的有效性，这进一步证实了我们提出的理论框架的可靠性。
2501.01293	 | LEO-Split: A Semi-Supervised Split Learning Framework over LEO Satellite Networks	 | Zheng Lin,Yuxin Zhang,Zhe Chen,Zihan Fang,Cong Wu,Xianhao Chen,Yue Gao,Jun Luo	 | 本文提出了一种名为LEO-Split的半监督分割学习设计方案，专门针对低地轨道（LEO）卫星网络，以解决间歇性连接、标签稀缺性和数据不平衡等问题，从而提高分布式学习的效率和性能。该框架利用半监督学习和伪标签算法来处理标签数据稀缺，并设计了自适应激活插值方案以防止过拟合，实验结果表明其性能优于现有基准。	 | Recently, the increasing deployment of LEO satellite systems has enabled various space analytics (e.g., crop and climate monitoring), which heavily relies on the advancements in deep learning (DL). However, the intermittent connectivity between LEO satellites and ground station (GS) significantly hinders the timely transmission of raw data to GS for centralized learning, while the scaled-up DL models hamper distributed learning on resource-constrained LEO satellites. Though split learning (SL) can be a potential solution to these problems by partitioning a model and offloading primary training workload to GS, the labor-intensive labeling process remains an obstacle, with intermittent connectivity and data heterogeneity being other challenges. In this paper, we propose LEO-Split, a semi-supervised (SS) SL design tailored for satellite networks to combat these challenges. Leveraging SS learning to handle (labeled) data scarcity, we construct an auxiliary model to tackle the training failure of the satellite-GS non-contact time. Moreover, we propose a pseudo-labeling algorithm to rectify data imbalances across satellites. Lastly, an adaptive activation interpolation scheme is devised to prevent the overfitting of server-side sub-model training at GS. Extensive experiments with real-world LEO satellite traces (e.g., Starlink) demonstrate that our LEO-Split framework achieves superior performance compared to state-ofthe-art benchmarks.	 | 近年来，低地轨道（LEO）卫星系统的部署越来越多，这使得各种太空分析（例如作物和气候监测）成为可能，这些分析严重依赖于深度学习（DL）的进步。然而，LEO卫星与地面站（GS）之间的间歇性连接显著阻碍了原始数据及时传输到地面站进行集中学习，而大规模的DL模型又阻碍了资源受限的LEO卫星上的分布式学习。尽管分割学习（SL）可以解决这些问题的一种潜在方案，通过分区模型并将主要训练工作负载卸载到GS，但由于劳动密集型的标签过程仍然是一项障碍，而间歇性连接和数据异质性也是其他挑战。在本文中，我们提出了一种名为LEO-Split的半监督（SS）SL设计方案，专门针对卫星网络，以应对这些挑战。利用半监督学习来处理标签数据的稀缺性，我们构建了一个辅助模型以应对卫星与GS之间非接触时间的训练失败问题。此外，我们提出了一种伪标签算法来纠正卫星之间数据的不平衡。最后，我们设计了一种自适应激活插值方案，以防止服务器端子模型在GS处的过拟合。通过使用真实世界的LEO卫星轨迹（例如Starlink）进行的广泛实验表明，我们的LEO-Split框架在与最新基准相比时，实现了更好的性能。
2501.01287	 | Optimized Relay Lens Design For High-Resolution Image Transmission In Military Target Detection Systems	 | Burak Celik,Kivanc Dogan,Ezgi Taskin,Ayhan Akbal,Ahmet Orhan	 | 本文使用ZEMAX软件设计并优化了一种中继透镜系统，以实现军事光学系统中长距离清晰图像传输，并通过分析调制传递函数等关键光学特性，验证了其在高分辨率和低畸变目标检测与跟踪方面的潜力。	 | The design and performance analysis of relay lenses that provide high-performance image transmission for target acquisition and tracking in military optical systems. Relay lenses are critical components for clear and lossless image transmission over long distances. In this study, the optical performance of a relay lens system designed and optimized using ZEMAX software is investigated in detail. The analysis focuses on important optical properties such as modulation transfer function (MTF), spot diagrams, Seidel diagram, field curvature and distortion. The results show that the lens has significant potential in military applications for target detection and tracking with high resolution and low aberration.	 | 军事光学系统中目标获取与跟踪中高性能图像传输的中继透镜的设计与性能分析。中继透镜是实现长距离清晰无损图像传输的关键组件。本研究使用ZEMAX软件设计并优化了一种中继透镜系统，并对其光学性能进行了详细分析。分析重点关注调制传递函数（MTF）、光斑图、塞德尔图、视场弯曲和畸变等重要光学特性。结果表明，该透镜在军事应用中具有显著潜力，可用于高分辨率和低畸变的目标检测与跟踪。
2501.01248	 | Bayesian Active Learning By Distribution Disagreement	 | Thorben Werner,Lars Schmidt-Thieme	 | 本文提出了 BALSA（Bayesian Active Learning for Scoring Approximations），一种针对使用正常化流进行回归问题的主动学习方法，特别关注如何有效区分 aleatoric 不确定性和 epistemic 不确定性。实验结果表明，BALSA 在多种数据集和架构上取得了最佳性能。	 | Active Learning (AL) for regression has been systematically under-researched due to the increased difficulty of measuring uncertainty in regression models. Since normalizing flows offer a full predictive distribution instead of a point forecast, they facilitate direct usage of known heuristics for AL like Entropy or Least-Confident sampling. However, we show that most of these heuristics do not work well for normalizing flows in pool-based AL and we need more sophisticated algorithms to distinguish between aleatoric and epistemic uncertainty. In this work we propose BALSA, an adaptation of the BALD algorithm, tailored for regression with normalizing flows. With this work we extend current research on uncertainty quantification with normalizing flows \cite{berry2023normalizing, berry2023escaping} to real world data and pool-based AL with multiple acquisition functions and query sizes. We report SOTA results for BALSA across 4 different datasets and 2 different architectures.	 | 主动学习（AL）在回归中的研究相对不足，主要是因为衡量回归模型不确定性增加的难度。尽管正常化流提供的是完整的预测分布而不是点预测，这使得可以直接使用已知的主动学习启发式方法，如熵或最少自信抽样。然而，我们发现这些启发式方法在基于池的主动学习中对正常化流并不奏效，我们需要更先进的算法来区分 aleatoric 不确定性和 epistemic 不确定性。在本文中，我们提出了一种适应 BALD 算法的 BALSA（Bayesian Active Learning for Scoring Approximations），专门针对使用正常化流的回归问题。通过这项工作，我们将当前使用正常化流进行不确定性量化的研究 \cite{berry2023normalizing, berry2023escaping} 扩展到真实数据以及基于池的主动学习，涉及多种获取函数和查询大小。我们报告了 BALSA 在 4 个不同数据集和 2 种不同架构上的最佳结果。
2501.01239	 | High-Order Tensor Regression in Sparse Convolutional Neural Networks	 | Roberto Dias Algarte	 | 本文提出了一种处理高阶张量的通用卷积方法，该方法通过发展基于张量的神经网络回归理论，重新定义了反向传播算法，从而提供了一种简单且有效的稀疏卷积神经网络框架。	 | This article presents a generic approach to convolution that significantly differs from conventional methodologies in the current Machine Learning literature. The approach, in its mathematical aspects, proved to be simple and advantageous, particularly when high-order tensors are involved. In this context, a rational theory of regression in neural networks is developed, as a framework for a generic view of sparse convolutional neural networks, the primary focus of this study. As a direct outcome, the classic Backpropagation Algorithmic is redefined to align with this rational tensor-based approach and presented in its simplest, most generic form.	 | 本文提出了一种通用的卷积方法，该方法在当前机器学习文献中的传统方法方面具有显著差异。从数学角度来看，该方法证明是简单且有利的，特别是在处理高阶张量时。在这个背景下，本文发展了一种神经网络回归的合理理论，作为通用稀疏卷积神经网络框架的基础，这也是本文的主要研究重点。作为直接结果，经典的反向传播算法被重新定义，以符合这一基于张量的合理方法，并以最简单、最通用的形式呈现。
2501.01230	 | Modeling Multi-Task Model Merging as Adaptive Projective Gradient Descent	 | Yongxian Wei,Anke Tang,Li Shen,Feng Xiong,Chun Yuan,Xiaochun Cao	 | 该研究将多个专家模型合并以进行多任务学习，通过自适应投影梯度下降法解决模型合并中的任务冲突，并优化合并模型与个体模型之间的性能差距，从而在视觉和自然语言处理任务上实现了最先进的性能。	 | Merging multiple expert models offers a promising approach for performing multi-task learning without accessing their original data. Existing methods attempt to alleviate task conflicts by sparsifying task vectors or promoting orthogonality among them. However, they overlook the fundamental requirement of model merging: ensuring the merged model performs comparably to task-specific models on respective tasks. We find these methods inevitably discard task-specific information that, while causing conflicts, is crucial for performance. Based on our findings, we frame model merging as a constrained optimization problem ($\textit{i.e.}$, minimizing the gap between the merged model and individual models, subject to the constraint of retaining shared knowledge) and solve it via adaptive projective gradient descent. Specifically, we align the merged model with individual models by decomposing and reconstituting the loss function, alleviating conflicts through $\textit{data-free}$ optimization of task vectors. To retain shared knowledge, we optimize this objective by projecting gradients within a $\textit{shared subspace}$ spanning all tasks. Moreover, we view merging coefficients as adaptive learning rates and propose a task-aware, training-free strategy. Experiments show that our plug-and-play approach consistently outperforms previous methods, achieving state-of-the-art results across diverse architectures and tasks in both vision and NLP domains.	 | 将多个专家模型合并起来，是一种在不访问其原始数据的情况下进行多任务学习的有希望的方法。现有的方法试图通过稀疏化任务向量或促进它们之间的正交性来缓解任务冲突。然而，它们忽视了模型合并的基本要求：确保合并后的模型在各自的任务上与任务特定模型具有相当的性能。我们发现这些方法不可避免地会丢弃一些特定于任务的信息，这些信息虽然导致了冲突，但对于性能来说至关重要。基于我们的发现，我们将模型合并视为一个受约束的优化问题（即，最小化合并模型与个体模型之间的差距，同时保留共享知识的约束），并通过自适应投影梯度下降法来解决。具体来说，我们通过分解并重构损失函数，使合并模型与个体模型对齐，通过无数据优化任务向量来缓解冲突。为了保留共享知识，我们通过投射梯度到涵盖所有任务的共享子空间中，优化该目标。此外，我们将合并系数视为适应性学习率，并提出了一种任务感知的、无需训练的策略。实验表明，我们提出的即插即用方法在视觉和自然语言处理领域多种架构和任务上的一致地优于先前的方法，达到了最新的技术水平。
2501.01227	 | Comparative Analysis of Topic Modeling Techniques on ATSB Text Narratives Using Natural Language Processing	 | Aziida Nanyonga,Hassan Wasswa,Ugur Turhan,Keith Joiner,Graham Wild	 | 本文应用四种主题建模技术（pLSA、LSA、LDA和NMF）分析澳大利亚运输安全局数据集中的航空事件叙述，以提取有价值的 Insights，研究展示了这些方法在航空安全领域的潜力及其各自的优缺点。	 | Improvements in aviation safety analysis call for innovative techniques to extract valuable insights from the abundance of textual data available in accident reports. This paper explores the application of four prominent topic modelling techniques, namely Probabilistic Latent Semantic Analysis (pLSA), Latent Semantic Analysis (LSA), Latent Dirichlet Allocation (LDA), and Non-negative Matrix Factorization (NMF), to dissect aviation incident narratives using the Australian Transport Safety Bureau (ATSB) dataset. The study examines each technique's ability to unveil latent thematic structures within the data, providing safety professionals with a systematic approach to gain actionable insights. Through a comparative analysis, this research not only showcases the potential of these methods in aviation safety but also elucidates their distinct advantages and limitations.	 | 航空安全分析的进步需要创新的技术从事故报告中大量的文本数据中提取有价值的信息。本文探讨了应用四种主要的主题建模技术——概率潜在语义分析（pLSA）、潜在语义分析（LSA）、潜在狄利克雷分配（LDA）和非负矩阵分解（NMF）——来剖析澳大利亚运输安全局（ATSB）数据集中的航空事件叙述。研究考察了每种技术在揭示数据中潜在主题结构方面的能力，为安全专业人士提供了一种系统的方法，以获得可操作的见解。通过比较分析，这项研究不仅展示了这些方法在航空安全领域的潜力，还阐明了它们各自的优点和局限性。
2501.01222	 | Classification of Operational Records in Aviation Using Deep Learning Approaches	 | Aziida Nanyonga,Graham Wild	 | 本研究评估了四种深度学习模型（BLSTM、CNN、LSTM和sRNN）在多类别航空数据分类任务中的性能，结果显示BLSTM和LSTM在处理序列依赖性和复杂分类任务方面表现最佳，但所有模型在处理类别不平衡问题上仍存在挑战。	 | Ensuring safety in the aviation industry is critical, even minor anomalies can lead to severe consequences. This study evaluates the performance of four different models for DP (deep learning), including: Bidirectional Long Short-Term Memory (BLSTM), Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM), and Simple Recurrent Neural Networks (sRNN), on a multi-class classification task involving Commercial, Military, and Private categories using the Socrata aviation dataset of 4,864 records. The models were assessed using a classification report, confusion matrix analysis, accuracy metrics, validation loss and accuracy curves. Among the models, BLSTM achieved the highest overall accuracy of 72%, demonstrating superior performance in stability and balanced classification, while LSTM followed closely with 71%, excelling in recall for the Commercial class. CNN and sRNN exhibited lower accuracies of 67% and 69%, with significant misclassifications in the Private class. While the results highlight the strengths of BLSTM and LSTM in handling sequential dependencies and complex classification tasks, all models faced challenges with class imbalance, particularly in predicting the Military and Private categories. Addressing these limitations through data augmentation, advanced feature engineering, and ensemble learning techniques could enhance classification accuracy and robustness. This study underscores the importance of selecting appropriate architectures for domain specific tasks	 | 确保航空业的安全至关重要，即使是轻微的异常也可能导致严重的后果。本研究评估了四种不同的深度学习模型在涉及商用、军用和私人分类的多类别分类任务中的性能，使用了包含4,864条记录的Socrata航空数据集，这四种模型包括双向长短期记忆网络（BLSTM）、卷积神经网络（CNN）、长短期记忆网络（LSTM）和简单循环神经网络（sRNN）。模型通过分类报告、混淆矩阵分析、准确性指标、验证损失和准确率曲线进行了评估。在这四种模型中，BLSTM获得了最高的整体准确率72%，显示出在稳定性和平衡分类中的优越性能，而LSTM紧随其后，准确率为71%，在商用类别的召回率方面表现出色。CNN和sRNN的准确率分别为67%和69%，在私人类别上出现了显著的误分类。尽管结果显示BLSTM和LSTM在处理序列依赖性和复杂分类任务方面具有优势，但所有模型在处理类别不平衡问题上都遇到了挑战，特别是在预测军用和私人类别时。通过数据增强、高级特征工程和集成学习技术来解决这些局限性，可以提高分类准确性和鲁棒性。本研究强调了在特定领域任务中选择适当架构的重要性。
2501.01216	 | TabTreeFormer: Tree Augmented Tabular Data Generation using Transformers	 | Jiayu Li,Bingyin Zhao,Zilong Zhao,Kevin Yee,Uzair Javaid,Yingjie Lao,Biplab Sikdar	 | TabTreeFormer是一种混合变压器架构，结合了基于树的模型以保留表数据的特定特性，并通过双量化分词器提高数值分布的学习效果，从而在合成数据的保真度、实用性和效率方面优于其他生成模型；该模型在多个数据集上的实验结果表明，它在不牺牲模型能力的情况下显著减小了模型规模，实现了显著的性能提升。	 | Transformers have achieved remarkable success in tabular data generation. However, they lack domain-specific inductive biases which are critical to preserving the intrinsic characteristics of tabular data. Meanwhile, they suffer from poor scalability and efficiency due to quadratic computational complexity. In this paper, we propose TabTreeFormer, a hybrid transformer architecture that incorporates a tree-based model that retains tabular-specific inductive biases of non-smooth and potentially low-correlated patterns due to its discreteness and non-rotational invariance, and hence enhances the fidelity and utility of synthetic data. In addition, we devise a dual-quantization tokenizer to capture the multimodal continuous distribution and further facilitate the learning of numerical value distribution. Moreover, our proposed tokenizer reduces the vocabulary size and sequence length due to the limited dimension-wise semantic meaning and training set size of tabular data, rendering a significant model size shrink without sacrificing the capability of the transformer model. We evaluate TabTreeFormer on 10 datasets against multiple generative models on various metrics; our experimental results show that TabTreeFormer achieves superior fidelity, utility, privacy, and efficiency. Our best model yields a 40% utility improvement with 1/16 of the baseline model size.	 | Transformer模型在生成表数据方面取得了显著的成功。然而，它们缺乏专门针对表数据固有特性的归纳偏差，这对于保持表数据的内在特性至关重要。同时，由于其计算复杂度为二次方，它们在扩展性和效率方面表现不佳。在本文中，我们提出了一种名为TabTreeFormer的混合变压器架构，该架构结合了基于树的模型，该模型保留了由于其离散性和非旋转不变性所导致的非光滑和潜在低相关模式的表数据特定的归纳偏差，从而提高了合成数据的保真度和实用性。此外，我们设计了一种双量化分词器来捕捉多模态连续分布，并进一步促进数值值分布的学习。更重要的是，我们提出的分词器由于表数据在维度上语义意义有限和训练集大小有限，减少了词汇量和序列长度，从而在不牺牲transformer模型能力的情况下显著减小了模型规模。我们在10个数据集上对TabTreeFormer与多个生成模型进行了多种指标的评估；实验结果表明，TabTreeFormer在保真度、实用性、隐私性和效率方面都取得了显著优势。我们的最佳模型在保持基线模型1/16的大小情况下，实现了40%的实用性提升。
2501.01202	 | Empirical Analysis of Nature-Inspired Algorithms for Autism Spectrum Disorder Detection Using 3D Video Dataset	 | Aneesh Panchal,Kainat Khan,Rahul Katarya	 | 本文提出了一种结合监督机器学习分类算法和启发式优化算法的方法，通过分析三维行走视频数据来检测自闭症谱系障碍（ASD），该方法能够高效且准确地识别ASD患者，并显著减少计算时间，具有较高的分类准确性和泛化能力。	 | Autism Spectrum Disorder (ASD) is a chronic neurodevelopmental disorder symptoms of which includes repetitive behaviour and lack of social and communication skills. Even though these symptoms can be seen very clearly in social but a large number of individuals with ASD remain undiagnosed. In this paper, we worked on a methodology for the detection of ASD from a 3-dimensional walking video dataset, utilizing supervised machine learning (ML) classification algorithms and nature-inspired optimization algorithms for feature extraction from the dataset. The proposed methodology involves the classification of ASD using a supervised ML classification algorithm and extracting important and relevant features from the dataset using nature-inspired optimization algorithms. We also included the ranking coefficients to find the initial leading particle. This selection of particle significantly reduces the computation time and hence, improves the total efficiency and accuracy for ASD detection. To evaluate the efficiency of the proposed methodology, we deployed various combinationsalgorithms of classification algorithm and nature-inspired algorithms resulting in an outstanding classification accuracy of $100\%$ using the random forest classification algorithm and gravitational search algorithm for feature selection. The application of the proposed methodology with different datasets would enhance the robustness and generalizability of the proposed methodology. Due to high accuracy and less total computation time, the proposed methodology will offer a significant contribution to the medical and academic fields, providing a foundation for future research and advancements in ASD diagnosis.	 | 自闭症谱系障碍（ASD）是一种慢性神经发育障碍，其症状包括重复性行为和社交与沟通技能的缺乏。尽管这些症状在社交场合中非常明显，但仍有许多ASD患者未被诊断出来。本文中，我们提出了一种利用监督机器学习（ML）分类算法和启发式优化算法从三维行走视频数据集中检测ASD的方法。该方法包括使用监督机器学习分类算法进行ASD分类，并使用启发式优化算法从数据集中提取重要且相关的特征。我们还引入了排序系数来找到初始主导粒子。这种粒子的选择显著减少了计算时间，从而提高了ASD检测的总体效率和准确性。为了评估所提出方法的效率，我们使用随机森林分类算法和引力搜索算法进行特征选择，实现了高达100%的分类准确性。应用不同数据集的所提出方法将增强该方法的稳健性和泛化能力。由于高准确性和较低的总体计算时间，所提出的方法将为医学和学术领域提供重大贡献，并为未来的ASD诊断研究奠定基础。
2501.01183	 | Machine Learning-Based Prediction of ICU Readmissions in Intracerebral Hemorrhage Patients: Insights from the MIMIC Databases	 | Shuheng Chen,Junyi Fan,Armin Abdollahi,Negin Ashrafi,Kamiar Alaei,Greg Placencia,Maryam Pishgar	 | 该研究利用MIMIC-III和MIMIC-IV数据库中的数据，通过机器学习技术如人工神经网络、XGBoost和随机森林，预测颅内出血患者的ICU再入院风险，并确定了关键预测因素包括人口统计信息、临床参数和实验室测量结果，为临床决策提供了支持。	 | Intracerebral hemorrhage (ICH) is a life-risking condition characterized by bleeding within the brain parenchyma. ICU readmission in ICH patients is a critical outcome, reflecting both clinical severity and resource utilization. Accurate prediction of ICU readmission risk is crucial for guiding clinical decision-making and optimizing healthcare resources. This study utilized the Medical Information Mart for Intensive Care (MIMIC-III and MIMIC-IV) databases, which contain comprehensive clinical and demographic data on ICU patients. Patients with ICH were identified from both databases. Various clinical, laboratory, and demographic features were extracted for analysis based on both overview literature and experts' opinions. Preprocessing methods like imputing and sampling were applied to improve the performance of our models. Machine learning techniques, such as Artificial Neural Network (ANN), XGBoost, and Random Forest, were employed to develop predictive models for ICU readmission risk. Model performance was evaluated using metrics such as AUROC, accuracy, sensitivity, and specificity. The developed models demonstrated robust predictive accuracy for ICU readmission in ICH patients, with key predictors including demographic information, clinical parameters, and laboratory measurements. Our study provides a predictive framework for ICU readmission risk in ICH patients, which can aid in clinical decision-making and improve resource allocation in intensive care settings.	 | 颅内出血（ICH）是一种危及生命的状况，其特征是在脑实质内出血。ICH患者的ICU再入院是关键的临床结果，反映了临床严重程度和资源利用情况。对ICU再入院风险的准确预测对于指导临床决策和优化医疗资源至关重要。本研究利用了包含ICU患者全面临床和人口统计学数据的Medical Information Mart for Intensive Care (MIMIC-III 和 MIMIC-IV) 数据库，从中识别出ICH患者。基于综述文献和专家意见，提取了各种临床、实验室和人口统计学特征进行分析。应用了诸如插补和采样等预处理方法来提高模型性能。使用了诸如人工神经网络（ANN）、XGBoost和随机森林等机器学习技术来开发预测ICU再入院风险的模型。通过AUROC、准确率、敏感性和特异性的指标评估了模型性能。开发的模型在ICH患者ICU再入院风险预测方面表现出色，关键预测因素包括人口统计信息、临床参数和实验室测量结果。本研究为ICH患者的ICU再入院风险提供了预测框架，有助于临床决策并提高重症监护环境中的资源配置。
2501.01132	 | Missing Data as Augmentation in the Earth Observation Domain: A Multi-View Learning Approach	 | Francisco Mena,Diego Arenas,Andreas Dengel	 | 本文提出了一种针对多视角学习（MVL）的新方法，专门用于包含缺失视角的地球观测应用，通过模拟缺失视角的组合来增强模型的鲁棒性和预测性能。实验结果表明，该方法在中度缺失数据情况下能提高模型鲁棒性，并且在所有视角存在时还能进一步提升预测性能。	 | Multi-view learning (MVL) leverages multiple sources or views of data to enhance machine learning model performance and robustness. This approach has been successfully used in the Earth Observation (EO) domain, where views have a heterogeneous nature and can be affected by missing data. Despite the negative effect that missing data has on model predictions, the ML literature has used it as an augmentation technique to improve model generalization, like masking the input data. Inspired by this, we introduce novel methods for EO applications tailored to MVL with missing views. Our methods integrate the combination of a set to simulate all combinations of missing views as different training samples. Instead of replacing missing data with a numerical value, we use dynamic merge functions, like average, and more complex ones like Transformer. This allows the MVL model to entirely ignore the missing views, enhancing its predictive robustness. We experiment on four EO datasets with temporal and static views, including state-of-the-art methods from the EO domain. The results indicate that our methods improve model robustness under conditions of moderate missingness, and improve the predictive performance when all views are present. The proposed methods offer a single adaptive solution to operate effectively with any combination of available views.	 | 多视角学习（MVL）通过利用多种数据源或视角来提升机器学习模型的性能和鲁棒性。这种方法在地球观测（EO）领域已被成功应用，其中视角具有异质性，并且可能会受到缺失数据的影响。尽管缺失数据会对模型预测产生负面影响，但机器学习文献中已将其用作增强模型泛化能力的一种增强技术，例如通过遮掩输入数据。受此启发，我们提出了针对MVL的新型方法，专门适用于包含缺失视角的EO应用。我们的方法结合了一组视角的组合，以模拟所有视角缺失的组合情况，并将其视为不同的训练样本。我们不使用数值填补缺失数据，而是使用动态合并函数，如平均值，以及更复杂的函数如Transformer。这使得MVL模型可以完全忽略缺失视角，从而增强其预测鲁棒性。我们使用四个包含时序和静态视角的EO数据集进行实验，并包含了EO领域的先进方法。实验结果表明，在中度缺失数据的情况下，我们的方法可提高模型的鲁棒性，当所有视角都存在时，则可进一步提升预测性能。我们提出的这些方法提供了一个适应性解决方案，可以在任何视角组合的情况下有效运行。
2501.01130	 | An Inclusive Theoretical Framework of Robust Supervised Contrastive Loss against Label Noise	 | Jingyi Cui,Yi-Ge Zhang,Hengyu Liu,Yisen Wang	 | 本文提出了一种适用于成对对比范式的统一稳健损失理论框架，推导出任意对比损失的一般稳健条件，并据此开发了一种新的稳健版本的InfoNCE损失（称为SymNCE），实验结果显示SymNCE在面对标签噪声时表现出优越性。	 | Learning from noisy labels is a critical challenge in machine learning, with vast implications for numerous real-world scenarios. While supervised contrastive learning has recently emerged as a powerful tool for navigating label noise, many existing solutions remain heuristic, often devoid of a systematic theoretical foundation for crafting robust supervised contrastive losses. To address the gap, in this paper, we propose a unified theoretical framework for robust losses under the pairwise contrastive paradigm. In particular, we for the first time derive a general robust condition for arbitrary contrastive losses, which serves as a criterion to verify the theoretical robustness of a supervised contrastive loss against label noise. The theory indicates that the popular InfoNCE loss is in fact non-robust, and accordingly inspires us to develop a robust version of InfoNCE, termed Symmetric InfoNCE (SymNCE). Moreover, we highlight that our theory is an inclusive framework that provides explanations to prior robust techniques such as nearest-neighbor (NN) sample selection and robust contrastive loss. Validation experiments on benchmark datasets demonstrate the superiority of SymNCE against label noise.	 | 机器学习中从噪声标签学习是一个关键挑战，对众多实际应用场景有着广泛的影响。尽管监督对比学习最近已成为处理标签噪声的有效工具，但许多现有解决方案仍基于经验，缺乏系统性的理论基础来构建稳健的监督对比损失。为填补这一空白，本文提出了一种适用于成对对比范式的统一稳健损失理论框架。特别是，我们首次推导出任意对比损失的一般稳健条件，这作为验证监督对比损失在面对标签噪声时理论稳健性的标准。该理论表明，流行的InfoNCE损失实际上是不稳健的，因此激发我们开发了InfoNCE的稳健版本，称为对称InfoNCE（SymNCE）。此外，我们强调，我们的理论提供了一个包容性的框架，可以解释先前的稳健技术，如最近邻（NN）样本选择和稳健对比损失。在基准数据集上的验证实验表明，SymNCE在面对标签噪声时表现出优越性。
2501.01124	 | Graph2text or Graph2token: A Perspective of Large Language Models for Graph Learning	 | Shuo Yu,Yingbo Wang,Ruolin Li,Guchun Liu,Yanming Shen,Shaoxiong Ji,Bowen Li,Fengling Han,Xiuzhen Zhang,Feng Xia	 | 本文综述了将大型语言模型（LLMs）应用于图数据的方法，提出了一种新的分类，即Graph2text和Graph2token，并指出了转换过程中的四个挑战，为研究人员提供了模型和LLMs的选择指南，并提出了未来研究的方向。	 | Graphs are data structures used to represent irregular networks and are prevalent in numerous real-world applications. Previous methods directly model graph structures and achieve significant success. However, these methods encounter bottlenecks due to the inherent irregularity of graphs. An innovative solution is converting graphs into textual representations, thereby harnessing the powerful capabilities of Large Language Models (LLMs) to process and comprehend graphs. In this paper, we present a comprehensive review of methodologies for applying LLMs to graphs, termed LLM4graph. The core of LLM4graph lies in transforming graphs into texts for LLMs to understand and analyze. Thus, we propose a novel taxonomy of LLM4graph methods in the view of the transformation. Specifically, existing methods can be divided into two paradigms: Graph2text and Graph2token, which transform graphs into texts or tokens as the input of LLMs, respectively. We point out four challenges during the transformation to systematically present existing methods in a problem-oriented perspective. For practical concerns, we provide a guideline for researchers on selecting appropriate models and LLMs for different graphs and hardware constraints. We also identify five future research directions for LLM4graph.	 | 图是用于表示不规则网络的数据结构，并在许多实际应用中广泛使用。之前的方法直接建模图结构并取得了显著的成功。然而，这些方法由于图的固有不规则性而遇到了瓶颈。一种创新的解决方案是将图转换为文本表示，从而利用大型语言模型（LLMs）的强大能力来处理和理解图。在这篇论文中，我们对将LLMs应用于图的方法进行了全面的综述，称为LLM4graph。LLM4graph的核心在于将图转换为文本，使LLMs能够理解和分析。因此，我们从转换的角度提出了一种LLM4graph方法的新型分类。具体来说，现有的方法可以分为两类：Graph2text和Graph2token，分别将图转换为文本或标记作为LLMs的输入。我们指出了转换过程中存在的四个挑战，以系统地从问题导向的角度呈现现有方法。对于实际问题，我们为研究人员提供了在不同图和硬件约束下选择合适模型和LLMs的指南。我们还确定了LLM4graph未来研究的五个方向。
2501.01118	 | Pruning-based Data Selection and Network Fusion for Efficient Deep Learning	 | Humaira Kousar,Hasnain Irshad Bhatti,Jaekyun Moon	 | PruneFuse是一种结合剪枝和网络融合的数据选择方法，用于提高深度神经网络的训练效率并减少标注成本，实验结果表明它能显著降低计算成本、优于基线方法并加速训练过程。	 | Efficient data selection is essential for improving the training efficiency of deep neural networks and reducing the associated annotation costs. However, traditional methods tend to be computationally expensive, limiting their scalability and real-world applicability. We introduce PruneFuse, a novel method that combines pruning and network fusion to enhance data selection and accelerate network training. In PruneFuse, the original dense network is pruned to generate a smaller surrogate model that efficiently selects the most informative samples from the dataset. Once this iterative data selection selects sufficient samples, the insights learned from the pruned model are seamlessly integrated with the dense model through network fusion, providing an optimized initialization that accelerates training. Extensive experimentation on various datasets demonstrates that PruneFuse significantly reduces computational costs for data selection, achieves better performance than baselines, and accelerates the overall training process.	 | 高效的数据选择对于提高深度神经网络的训练效率并减少相关注释成本至关重要。然而，传统的数据选择方法通常计算成本较高，限制了其可扩展性和实际应用性。我们提出了一种名为PruneFuse的新方法，该方法结合了剪枝和网络融合，以增强数据选择并加速网络训练。在PruneFuse中，原始的密集网络被剪枝以生成一个较小的代理模型，该模型能有效地从数据集中选择最具信息价值的样本。一旦这种迭代的数据选择过程选择了足够的样本，剪枝模型学到的见解将通过网络融合无缝整合到密集模型中，从而提供一个优化初始化，加快训练过程。在各种数据集上的广泛实验表明，PruneFuse显著降低了数据选择的计算成本，性能优于基线方法，并加快了整体训练过程。
2501.01100	 | Long-range Brain Graph Transformer	 | Shuo Yu,Shan Jin,Ming Li,Tabinda Sarwar,Feng Xia	 | 本文提出了Adaptive Long-range aware TransformER (ALTER)，一种利用偏随机游走捕捉脑区之间长程依赖性的脑图变换器，能够自适应地整合短程和长程依赖性，从而实现对全脑多层次交流的综合理解，并在神经疾病诊断方面优于现有图学习方法。	 | Understanding communication and information processing among brain regions of interest (ROIs) is highly dependent on long-range connectivity, which plays a crucial role in facilitating diverse functional neural integration across the entire brain. However, previous studies generally focused on the short-range dependencies within brain networks while neglecting the long-range dependencies, limiting an integrated understanding of brain-wide communication. To address this limitation, we propose Adaptive Long-range aware TransformER (ALTER), a brain graph transformer to capture long-range dependencies between brain ROIs utilizing biased random walk. Specifically, we present a novel long-range aware strategy to explicitly capture long-range dependencies between brain ROIs. By guiding the walker towards the next hop with higher correlation value, our strategy simulates the real-world brain-wide communication. Furthermore, by employing the transformer framework, ALERT adaptively integrates both short- and long-range dependencies between brain ROIs, enabling an integrated understanding of multi-level communication across the entire brain. Extensive experiments on ABIDE and ADNI datasets demonstrate that ALTER consistently outperforms generalized state-of-the-art graph learning methods (including SAN, Graphormer, GraphTrans, and LRGNN) and other graph learning based brain network analysis methods (including FBNETGEN, BrainNetGNN, BrainGNN, and BrainNETTF) in neurological disease diagnosis. Cases of long-range dependencies are also presented to further illustrate the effectiveness of ALTER. The implementation is available at \url{https://github.com/yushuowiki/ALTER}.	 | 理解感兴趣脑区（ROIs）之间的交流和信息处理高度依赖于长程连接，后者在促进整个大脑的多种功能神经整合方面发挥着关键作用。然而，以往的研究通常侧重于大脑网络内的短程依赖性，而忽略了长程依赖性，这限制了对全脑交流的综合理解。为了解决这一限制，我们提出了Adaptive Long-range aware TransformER（ALTER），这是一种利用偏随机游走捕捉脑区之间长程依赖性的脑图变换器。具体而言，我们提出了一种新颖的长程依赖意识策略，以明确捕捉脑区之间的长程依赖性。通过引导游走者趋向相关性值更高的下一步，我们的策略模拟了真实世界中的全脑交流。此外，通过采用变换器框架，ALTER能够自适应地整合脑区之间的短程和长程依赖性，从而实现对整个大脑多层次交流的综合理解。在ABIDE和ADNI数据集上的广泛实验表明，ALTER在神经疾病诊断方面始终优于通用的图学习方法（包括SAN、Graphormer、GraphTrans和LRGNN），以及其他基于图学习的大脑网络分析方法（包括FBNETGEN、BrainNetGNN、BrainGNN和BrainNETTF）。还展示了长程依赖性的案例，以进一步说明ALTER的有效性。项目的实现可从 \url{https://github.com/yushuowiki/ALTER} 获取。
2501.01087	 | Bridging Simplicity and Sophistication using GLinear: A Novel Architecture for Enhanced Time Series Prediction	 | Syed Tahir Hussain Rizvi,Neel Kanwal,Muddasar Naeem,Alfredo Cuzzocrea,Antonio Coronato	 | 本文提出了一种新的数据高效架构GLinear，用于多变量时间序列预测，通过利用周期性模式提高预测准确性，并在多种数据集上优于现有最先进的线性预测器和基于Transformer的时间序列预测器。	 | Time Series Forecasting (TSF) is an important application across many fields. There is a debate about whether Transformers, despite being good at understanding long sequences, struggle with preserving temporal relationships in time series data. Recent research suggests that simpler linear models might outperform or at least provide competitive performance compared to complex Transformer-based models for TSF tasks. In this paper, we propose a novel data-efficient architecture, GLinear, for multivariate TSF that exploits periodic patterns to provide better accuracy. It also provides better prediction accuracy by using a smaller amount of historical data compared to other state-of-the-art linear predictors. Four different datasets (ETTh1, Electricity, Traffic, and Weather) are used to evaluate the performance of the proposed predictor. A performance comparison with state-of-the-art linear architectures (such as NLinear, DLinear, and RLinear) and transformer-based time series predictor (Autoformer) shows that the GLinear, despite being parametrically efficient, significantly outperforms the existing architectures in most cases of multivariate TSF. We hope that the proposed GLinear opens new fronts of research and development of simpler and more sophisticated architectures for data and computationally efficient time-series analysis. The source code is publicly available on GitHub.	 | 时间序列预测（TSF）是许多领域中的一个重要应用。关于Transformer是否能够有效地保留时间序列数据中的时间关系，尽管它们擅长处理长序列，存在一定的争议。最近的研究表明，简单的线性模型在TSF任务中可能表现出更优或至少提供具有竞争力的性能，而不需要复杂的Transformer模型。在本文中，我们提出了一种新的数据高效架构GLinear，用于多变量时间序列预测，它通过利用周期性模式来提供更好的准确性。与其它最先进的线性预测器相比，GLinear 使用较少的历史数据也能提供更好的预测准确性。我们使用了四种不同的数据集（ETTh1、Electricity、Traffic、Weather）来评估所提出的预测器的性能。与最先进的线性架构（如NLinear、DLinear、RLinear）和基于Transformer的时间序列预测器（Autoformer）进行性能比较表明，尽管GLinear 在参数效率方面表现出色，但在大多数多变量时间序列预测的情况下，它显著优于现有的架构。我们希望GLinear 的提出能够开启更简单且更复杂架构的研究和发展方向，以实现高效的数据和计算时间序列分析。源代码已在GitHub上公开。
2501.01085	 | Noise-Resilient Symbolic Regression with Dynamic Gating Reinforcement Learning	 | Chenglu Sun,Shuo Shen,Wenzhi Tao,Deyi Xue,Zixia Zhou	 | 本文提出了一种新的噪声鲁棒符号回归（NRSR）方法，该方法通过结合强化学习和噪声鲁棒门控模块，在高噪声数据中表现出强大的表达式恢复能力。实验结果表明，NRSR不仅在高噪声数据上优于多种基线方法，还在干净数据上达到了最先进的性能。	 | Symbolic regression (SR) has emerged as a pivotal technique for uncovering the intrinsic information within data and enhancing the interpretability of AI models. However, current state-of-the-art (sota) SR methods struggle to perform correct recovery of symbolic expressions from high-noise data. To address this issue, we introduce a novel noise-resilient SR (NRSR) method capable of recovering expressions from high-noise data. Our method leverages a novel reinforcement learning (RL) approach in conjunction with a designed noise-resilient gating module (NGM) to learn symbolic selection policies. The gating module can dynamically filter the meaningless information from high-noise data, thereby demonstrating a high noise-resilient capability for the SR process. And we also design a mixed path entropy (MPE) bonus term in the RL process to increase the exploration capabilities of the policy. Experimental results demonstrate that our method significantly outperforms several popular baselines on benchmarks with high-noise data. Furthermore, our method also can achieve sota performance on benchmarks with clean data, showcasing its robustness and efficacy in SR tasks.	 | 符号回归（Symbolic Regression, SR）已成为揭示数据内在信息和增强AI模型可解释性的关键技术。然而，当前最先进的（state-of-the-art, sota）SR方法在处理高噪声数据时难以正确恢复符号表达式。为解决这一问题，我们提出了一种新的噪声鲁棒SR（Noise-Resilient Symbolic Regression, NRSR）方法，能够从高噪声数据中恢复表达式。该方法结合了一种新颖的强化学习（Reinforcement Learning, RL）方法和一个设计的噪声鲁棒门控模块（Noise-Resilient Gating Module, NGM），以学习符号选择策略。门控模块能够动态过滤高噪声数据中的无意义信息，从而在SR过程中表现出强大的噪声鲁棒性。此外，我们在RL过程中设计了一种混合路径熵（Mixed Path Entropy, MPE）奖励项，以增加策略的探索能力。实验结果证明，我们的方法在高噪声数据的基准测试中显著优于几种流行的基线方法。此外，我们的方法在干净数据的基准测试中也能达到最先进的性能，展示了其在SR任务中的稳健性和有效性。
2501.01073	 | Graph Generative Pre-trained Transformer	 | Xiaohui Chen,Yinkai Wang,Jiaxing He,Yuanqi Du,Soha Hassoun,Xiaolin Xu,Li-Ping Liu	 | 该研究提出了一种新的图生成方法，将图表示为节点集和边集的序列，并基于此提出了图生成预训练变换器（G2PT），该模型在多种下游任务中表现出色，尤其是在通用图和分子数据集上的生成性能优于现有模型。	 | Graph generation is a critical task in numerous domains, including molecular design and social network analysis, due to its ability to model complex relationships and structured data. While most modern graph generative models utilize adjacency matrix representations, this work revisits an alternative approach that represents graphs as sequences of node set and edge set. We advocate for this approach due to its efficient encoding of graphs and propose a novel representation. Based on this representation, we introduce the Graph Generative Pre-trained Transformer (G2PT), an auto-regressive model that learns graph structures via next-token prediction. To further exploit G2PT's capabilities as a general-purpose foundation model, we explore fine-tuning strategies for two downstream applications: goal-oriented generation and graph property prediction. We conduct extensive experiments across multiple datasets. Results indicate that G2PT achieves superior generative performance on both generic graph and molecule datasets. Furthermore, G2PT exhibits strong adaptability and versatility in downstream tasks from molecular design to property prediction.	 | 图生成是一项在许多领域中至关重要的任务，包括分子设计和社会网络分析，这是因为图能够建模复杂的相互关系和结构化数据。虽然大多数现代图生成模型使用邻接矩阵表示，但本工作重新探讨了一种替代方法，该方法将图表示为节点集和边集的序列。我们提倡这种表示方法，因为它可以高效地编码图，并提出了一个新颖的表示方法。基于这种表示方法，我们引入了图生成预训练变换器（G2PT），这是一种自回归模型，通过下一个图元预测来学习图结构。为了进一步利用G2PT作为通用基础模型的能力，我们探索了两种下游应用的微调策略：目标导向生成和图属性预测。我们在多个数据集上进行了广泛的实验。结果表明，G2PT在通用图和分子数据集上的生成性能优于其他模型。此外，G2PT在从分子设计到属性预测的各种下游任务中都表现出较强的适应性和多功能性。
2501.01067	 | Enhancing Precision of Automated Teller Machines Network Quality Assessment: Machine Learning and Multi Classifier Fusion Approaches	 | Alireza Safarzadeh,Mohammad Reza Jamali,Behzad Moshiri	 | 本研究提出了一种利用多分类器融合技术，特别是堆叠分类器，并结合合成少数类过采样技术（SMOTE），以提高ATM网络可靠性的方法，显著降低了误报率并提升了整体准确率。这不仅优化了ATM状态检测，还为金融机构提供了实用的解决方案，以提升服务质量和客户满意度。	 | Ensuring reliable ATM services is essential for modern banking, directly impacting customer satisfaction and the operational efficiency of financial institutions. This study introduces a data fusion approach that utilizes multi-classifier fusion techniques, with a special focus on the Stacking Classifier, to enhance the reliability of ATM networks. To address class imbalance, the Synthetic Minority Over-sampling Technique (SMOTE) was applied, enabling balanced learning for both frequent and rare events. The proposed framework integrates diverse classification models - Random Forest, LightGBM, and CatBoost - within a Stacking Classifier, achieving a dramatic reduction in false alarms from 3.56 percent to just 0.71 percent, along with an outstanding overall accuracy of 99.29 percent. This multi-classifier fusion method synthesizes the strengths of individual models, leading to significant cost savings and improved operational decision-making. By demonstrating the power of machine learning and data fusion in optimizing ATM status detection, this research provides practical and scalable solutions for financial institutions aiming to enhance their ATM network performance and customer satisfaction.	 | 确保现代银行的自动取款机（ATM）服务可靠是至关重要的，直接影响客户满意度和金融机构的操作效率。本研究引入了一种数据融合方法，利用多分类器融合技术，特别关注堆叠分类器（Stacking Classifier），以增强ATM网络的可靠性。为了解决类别不平衡问题，我们应用了合成少数类过采样技术（SMOTE），从而使频繁事件和稀有事件都能得到平衡学习。所提出的框架将不同的分类模型——随机森林（Random Forest）、LightGBM 和 CatBoost ——整合到堆叠分类器中，将误报率从3.56%显著降低到0.71%，同时整体准确率达到了99.29%。这种多分类器融合方法综合了各个模型的优点，从而实现显著的成本节约和改进的操作决策。通过展示机器学习和数据融合在优化ATM状态检测方面的强大能力，本研究为金融机构提供了实用且可扩展的解决方案，用于提升其ATM网络性能和客户满意度。
2501.01029	 | State-of-the-art AI-based Learning Approaches for Deepfake Generation and Detection, Analyzing Opportunities, Threading through Pros, Cons, and Future Prospects	 | Harshika Goyal,Mohammad Saif Wajid,Mohd Anas Wajid,Akib Mohi Ud Din Khanday,Mehdi Neshat,Amir Gandomi	 | 随着深度伪造技术的迅猛发展，生成对抗网络、变分自编码器等先进方法使其效果达到前所未有的水平，同时检测技术也在不断进步以应对滥用问题，本文综述了深度伪造生成与检测的最新进展，并深入分析了相关挑战和解决方案。	 | The rapid advancement of deepfake technologies, specifically designed to create incredibly lifelike facial imagery and video content, has ignited a remarkable level of interest and curiosity across many fields, including forensic analysis, cybersecurity and the innovative creation of digital characters. By harnessing the latest breakthroughs in deep learning methods, such as Generative Adversarial Networks, Variational Autoencoders, Few-Shot Learning Strategies, and Transformers, the outcomes achieved in generating deepfakes have been nothing short of astounding and transformative. Also, the ongoing evolution of detection technologies is being developed to counteract the potential for misuse associated with deepfakes, effectively addressing critical concerns that range from political manipulation to the dissemination of fake news and the ever-growing issue of cyberbullying. This comprehensive review paper meticulously investigates the most recent developments in deepfake generation and detection, including around 400 publications, providing an in-depth analysis of the cutting-edge innovations shaping this rapidly evolving landscape. Starting with a thorough examination of systematic literature review methodologies, we embark on a journey that delves into the complex technical intricacies inherent in the various techniques used for deepfake generation, comprehensively addressing the challenges faced, potential solutions available, and the nuanced details surrounding manipulation formulations. Subsequently, the paper is dedicated to accurately benchmarking leading approaches against prominent datasets, offering thorough assessments of the contributions that have significantly impacted these vital domains. Ultimately, we engage in a thoughtful discussion of the existing challenges, paving the way for continuous advancements in this critical and ever-dynamic study area.	 | 随着深度伪造技术的迅猛发展，这些技术专为创造极其逼真的人脸图像和视频内容而设计，引发了各领域，包括法医分析、网络安全以及数字角色创新创作等方面的极大关注和好奇心。通过利用最新的深度学习方法，如生成对抗网络（GAN）、变分自编码器（VAE）、少样本学习策略和变换器（Transformer），生成深度伪造的效果堪称惊人且具有革命性。此外，检测技术也在不断进化，旨在应对深度伪造可能带来的滥用问题，从而有效解决从政治操纵到假新闻传播以及日益增长的网络欺凌等关键问题。  本文综述了深度伪造生成与检测的最新进展，仔细研究了约400篇相关文献，深入分析了塑造这一快速变化领域的前沿创新。首先，我们详细介绍了系统的文献综述方法，随后深入探讨了各种深度伪造生成技术背后的复杂技术细节，全面分析了面临的挑战、可用的解决方案以及操纵形式的微妙之处。接下来，本文致力于准确评估领先方法在重要数据集上的表现，提供对这些关键领域贡献的全面评估。最后，我们进行了一场有意义的讨论，指出了现有挑战，为这一关键且不断发展的研究领域持续进步铺平了道路。
2501.01025	 | Towards Adversarially Robust Deep Metric Learning	 | Xiaopeng Ke	 | 本文指出深度度量学习（DML）模型在基于聚类的推理场景中存在鲁棒性问题，并提出了一种新的防御方法——集成对抗训练（EAT），该方法通过利用集成学习和对抗训练，提高了DML模型的鲁棒性。	 | Deep Metric Learning (DML) has shown remarkable successes in many domains by taking advantage of powerful deep neural networks. Deep neural networks are prone to adversarial attacks and could be easily fooled by adversarial examples. The current progress on this robustness issue is mainly about deep classification models but pays little attention to DML models. Existing works fail to thoroughly inspect the robustness of DML and neglect an important DML scenario, the clustering-based inference. In this work, we first point out the robustness issue of DML models in clustering-based inference scenarios. We find that, for the clustering-based inference, existing defenses designed DML are unable to be reused and the adaptions of defenses designed for deep classification models cannot achieve satisfactory robustness performance. To alleviate the hazard of adversarial examples, we propose a new defense, the Ensemble Adversarial Training (EAT), which exploits ensemble learning and adversarial training. EAT promotes the diversity of the ensemble, encouraging each model in the ensemble to have different robustness features, and employs a self-transferring mechanism to make full use of the robustness statistics of the whole ensemble in the update of every single model. We evaluate the EAT method on three widely-used datasets with two popular model architectures. The results show that the proposed EAT method greatly outperforms the adaptions of defenses designed for deep classification models.	 | 深度度量学习（DML）通过利用强大的深度神经网络已在许多领域取得了显著的成功。然而，深度神经网络容易受到对抗攻击的影响，并且容易被对抗样本欺骗。目前在鲁棒性问题上的进展主要集中在深度分类模型上，而对DML模型关注较少。现有的研究未能彻底检验DML的鲁棒性，并忽视了一个重要的DML场景：基于聚类的推理。在本文中，我们首先指出了DML模型在基于聚类的推理场景中的鲁棒性问题。我们发现，在基于聚类的推理中，现有的设计用于DML的防御措施无法重用，而针对深度分类模型设计的防御措施的适应性也无法达到满意的鲁棒性表现。为了缓解对抗样本的危害，我们提出了一种新的防御方法——集成对抗训练（EAT），它利用了集成学习和对抗训练。EAT促进了集成的多样性，鼓励集成中的每个模型具有不同的鲁棒性特征，同时使用自我转移机制确保每个模型在更新时充分利用整个集成的鲁棒性统计信息。我们在两个流行模型架构和三种广泛使用的数据集上评估了EAT方法。结果表明，所提出的EAT方法显著优于针对深度分类模型设计的防御措施的适应性。
2501.01011	 | Prediction of Geoeffective CMEs Using SOHO Images and Deep Learning	 | Khalid A. Alobaid,Jason T. L. Wang,Haimin Wang,Ju Jing,Yasser Abduallah,Zhenduo Wang,Hameedullah Farooki,Huseyin Cavus,Vasyl Yurchyshyn	 | 一种名为GeoCME的深度学习框架被开发用于预测日冕物质抛射事件是否会导致地磁风暴，该框架利用SOHO卫星的数据表现出良好的预测性能，马修相关系数和真技能统计分分别为0.807和0.714。	 | The application of machine learning to the study of coronal mass ejections (CMEs) and their impacts on Earth has seen significant growth recently. Understanding and forecasting CME geoeffectiveness is crucial for protecting infrastructure in space and ensuring the resilience of technological systems on Earth. Here we present GeoCME, a deep-learning framework designed to predict, deterministically or probabilistically, whether a CME event that arrives at Earth will cause a geomagnetic storm. A geomagnetic storm is defined as a disturbance of the Earth's magnetosphere during which the minimum Dst index value is less than -50 nT. GeoCME is trained on observations from the instruments including LASCO C2, EIT and MDI on board the Solar and Heliospheric Observatory (SOHO), focusing on a dataset that includes 136 halo/partial halo CMEs in Solar Cycle 23. Using ensemble and transfer learning techniques, GeoCME is capable of extracting features hidden in the SOHO observations and making predictions based on the learned features. Our experimental results demonstrate the good performance of GeoCME, achieving a Matthew's correlation coefficient of 0.807 and a true skill statistics score of 0.714 when the tool is used as a deterministic prediction model. When the tool is used as a probabilistic forecasting model, it achieves a Brier score of 0.094 and a Brier skill score of 0.493. These results are promising, showing that the proposed GeoCME can help enhance our understanding of CME-triggered solar-terrestrial interactions.	 | 将机器学习应用于日冕物质抛射（CME）及其对地球的影响研究取得了显著增长。了解并预测CME的地磁效应对于保护太空中的基础设施以及确保地球上的技术系统的韧性至关重要。在此，我们介绍了一种名为GeoCME的深度学习框架，用于预测CME事件是否会在到达地球后引发地磁风暴。地磁风暴被定义为地球磁层受到扰动期间，最小Dst指数值小于-50 nT。GeoCME是基于太阳和日地观测卫星（SOHO）上的LASCO C2、EIT和MDI等仪器观测数据，重点训练了包括136个日冕/部分日冕CME事件的数据集。通过集成学习和迁移学习技术，GeoCME能够从SOHO观测数据中提取隐藏的特征，并基于学习到的特征进行预测。我们的实验结果表明，GeoCME表现出良好的性能，当该工具作为确定性预测模型使用时，其马修相关系数为0.807，真技能统计分为0.714。当该工具作为概率预测模型使用时，其布里尔得分（Brier score）为0.094，布里尔技能分为0.493。这些结果很有前景，表明提出的GeoCME有助于增强我们对CME引发的太阳-地球相互作用的理解。
2501.01010	 | CryptoMamba: Leveraging State Space Models for Accurate Bitcoin Price Prediction	 | Mohammad Shahab Sepehri,Asal Mehradfar,Mahdi Soltanolkotabi,Salman Avestimehr	 | CryptoMamba是一种新型的状态空间模型架构，用于预测比特币价格，能够在捕捉长范围依赖性和制度转换方面优于现有模型，提供更准确的预测和增强的泛化能力。结合交易算法，CryptoMamba展示了在实际应用场景中的实用价值。	 | Predicting Bitcoin price remains a challenging problem due to the high volatility and complex non-linear dynamics of cryptocurrency markets. Traditional time-series models, such as ARIMA and GARCH, and recurrent neural networks, like LSTMs, have been widely applied to this task but struggle to capture the regime shifts and long-range dependencies inherent in the data. In this work, we propose CryptoMamba, a novel Mamba-based State Space Model (SSM) architecture designed to effectively capture long-range dependencies in financial time-series data. Our experiments show that CryptoMamba not only provides more accurate predictions but also offers enhanced generalizability across different market conditions, surpassing the limitations of previous models. Coupled with trading algorithms for real-world scenarios, CryptoMamba demonstrates its practical utility by translating accurate forecasts into financial outcomes. Our findings signal a huge advantage for SSMs in stock and cryptocurrency price forecasting tasks.	 | 预测比特币价格仍然是一个具有挑战性的问题，因为加密货币市场的高波动性和复杂的非线性动态。传统的时序模型，如ARIMA和GARCH，以及循环神经网络，如LSTMs，已经被广泛应用于这一任务，但它们在捕捉数据中的制度转换和长范围依赖性方面存在困难。在这项工作中，我们提出了一种名为CryptoMamba的新型Mamba基状态空间模型（SSM）架构，旨在有效捕捉金融时序数据中的长范围依赖性。我们的实验表明，CryptoMamba不仅提供了更准确的预测，而且在不同市场条件下的泛化能力也得到了增强，超越了先前模型的限制。结合用于实际场景的交易算法，CryptoMamba通过将准确的预测转化为金融成果，展示了其实用价值。我们的研究成果表明，状态空间模型在股票和加密货币价格预测任务中具有巨大优势。
2501.01002	 | Multi-Objective Optimization-Based Anonymization of Structured Data for Machine Learning	 | Yusi Wei,Hande Y. Benson,Joseph K. Agor,Muge Capan	 | 研究识别了现有隐私保护优化模型的局限性，并提出了一种新的多目标优化模型，该模型在最小化信息损失的同时增强对抗攻击的保护，实验结果显示其在某些情况下优于现有方法，实现了更好的隐私保护和机器学习性能的平衡。	 | Data is essential for secondary use, but ensuring its privacy while allowing such use is a critical challenge. Various techniques have been proposed to address privacy concerns in data sharing and publishing. However, these methods often degrade data utility, impacting the performance of machine learning (ML) models. Our research identifies key limitations in existing optimization models for privacy preservation, particularly in handling categorical variables, assessing data utility, and evaluating effectiveness across diverse datasets. We propose a novel multi-objective optimization model that simultaneously minimizes information loss and maximizes protection against attacks. This model is empirically validated using diverse datasets and compared with two existing algorithms. We assess information loss, the number of individuals subject to linkage or homogeneity attacks, and ML performance after anonymization. The results indicate that our model achieves lower information loss and more effectively mitigates the risk of attacks, reducing the number of individuals susceptible to these attacks compared to alternative algorithms in some cases. Additionally, our model maintains comparative ML performance relative to the original data or data anonymized by other methods. Our findings highlight significant improvements in privacy protection and ML model performance, offering a comprehensive framework for balancing privacy and utility in data sharing.	 | 数据对于二次使用至关重要，但在确保其隐私的同时允许这种使用则是一个关键挑战。为了应对数据共享和发布中的隐私问题，已经提出了一系列技术方法。然而，这些方法往往会导致数据效用下降，影响机器学习（ML）模型的性能。我们的研究识别了现有隐私保护优化模型的关键局限性，特别是在处理分类变量、评估数据效用以及在不同数据集上评估有效性方面。我们提出了一个新颖的多目标优化模型，该模型同时最小化信息损失并最大化对抗攻击的保护。我们使用多样化的数据集对该模型进行了实证验证，并将其与两个现有算法进行了比较。我们评估了信息损失、受到链接或同质性攻击的个体数量以及数据匿名化后的机器学习性能。结果表明，我们的模型在某些情况下实现了更低的信息损失，并更有效地缓解了攻击风险，减少了受这些攻击影响的个体数量，同时在其他方面保持了与原始数据或通过其他方法匿名化数据的类似机器学习性能。我们的研究结果强调了在数据共享中实现显著的隐私保护和机器学习模型性能提升，提供了一个综合框架来平衡隐私和效用。
2501.01000	 | Physics-informed Gaussian Processes for Safe Envelope Expansion	 | D. Isaiah Harp,Joshua Ott,Dylan M. Asmar,John Alora,Mykel J. Kochenderfer	 | 该研究提出了一种使用高斯过程结合物理信息均值函数的方法，从任意飞行试验数据中估计气动量，以减少大规模实验活动的需求，并展示了该方法在预测T-38飞机在不同飞行条件下的短期振荡频率和阻尼方面的有效性。	 | Flight test analysis often requires predefined test points with arbitrarily tight tolerances, leading to extensive and resource-intensive experimental campaigns. To address this challenge, we propose a novel approach to flight test analysis using Gaussian processes (GPs) with physics-informed mean functions to estimate aerodynamic quantities from arbitrary flight test data, validated using real T-38 aircraft data collected in collaboration with the United States Air Force Test Pilot School. We demonstrate our method by estimating the pitching moment coefficient without requiring predefined or repeated flight test points, significantly reducing the need for extensive experimental campaigns. Our approach incorporates aerodynamic models as priors within the GP framework, enhancing predictive accuracy across diverse flight conditions and providing robust uncertainty quantification. Key contributions include the integration of physics-based priors in a probabilistic model, which allows for precise computation from arbitrary flight test maneuvers, and the demonstration of our method capturing relevant dynamic characteristics such as short-period mode behavior. The proposed framework offers a scalable and generalizable solution for efficient data-driven flight test analysis and is able to accurately predict the short period frequency and damping for the T-38 across several Mach and dynamic pressure profiles.	 | 飞行试验分析通常需要预先定义的测试点，并且这些测试点的要求非常严格，从而导致了大量且资源密集型的实验活动。为了解决这一挑战，我们提出了一种使用高斯过程（GPs）结合物理信息均值函数的新方法，以从任意飞行试验数据中估计气动量，该方法通过与美国空军试飞学校合作收集的实际T-38飞机数据进行了验证。我们通过在不需要预先定义或重复的飞行试验点的情况下估计滚转力矩系数来演示我们的方法，从而大大减少了大规模实验活动的需求。我们的方法在高斯过程框架中将气动模型作为先验知识使用，从而提高了在各种飞行条件下预测准确性，并提供了稳健的不确定量化。主要贡献包括在概率模型中整合基于物理的先验知识，这允许从任意飞行试验机动中精确计算，并展示了我们的方法能够捕捉到诸如短期振荡模态行为等重要动态特征。提出的框架提供了一种可扩展且通用的解决方案，用于高效的数据驱动飞行试验分析，并能够准确预测T-38在不同马赫数和动态压力剖面下的短期振荡频率和阻尼。
2501.00995	 | Is It Still Fair? Investigating Gender Fairness in Cross-Corpus Speech Emotion Recognition	 | Shreya G. Upadhyay,Woan-Shiuan Chien,Chi-Chun Lee	 | 该研究关注跨语料库语音情感识别（SER）模型中性别公平性的泛化能力，提出了一种结合公平性调整机制的方法，以增强性别公平性，并提供了跨语料库SER系统中性别公平性的首批见解。	 | Speech emotion recognition (SER) is a vital component in various everyday applications. Cross-corpus SER models are increasingly recognized for their ability to generalize performance. However, concerns arise regarding fairness across demographics in diverse corpora. Existing fairness research often focuses solely on corpus-specific fairness, neglecting its generalizability in cross-corpus scenarios. Our study focuses on this underexplored area, examining the gender fairness generalizability in cross-corpus SER scenarios. We emphasize that the performance of cross-corpus SER models and their fairness are two distinct considerations. Moreover, we propose the approach of a combined fairness adaptation mechanism to enhance gender fairness in the SER transfer learning tasks by addressing both source and target genders. Our findings bring one of the first insights into the generalizability of gender fairness in cross-corpus SER systems.	 | 语音情感识别（SER）是各种日常应用中的一个重要组成部分。跨语料库的SER模型因其能够泛化性能而受到越来越多的关注。然而，在多样化的语料库中，不同人群之间的公平性问题引起了人们的担忧。现有的公平性研究往往仅关注语料库特定的公平性，而忽略了其在跨语料库场景中的泛化能力。我们的研究集中在这一未被充分探索的领域，探讨了跨语料库SER场景中性别公平性的泛化能力。我们强调，跨语料库SER模型的性能和其公平性是两个不同的考虑因素。此外，我们提出了一种结合公平性调整机制的方法，通过同时解决源性别和目标性别问题，来增强SER迁移学习任务中的性别公平性。我们的研究结果为跨语料库SER系统中性别公平性的泛化能力带来了首批见解。
2501.00989	 | Bootstrapped Reward Shaping	 | Jacob Adamczyk,Volodymyr Makarenko,Stas Tiomkin,Rahul V. Kulkarni	 | 本文提出了一种名为BSRS（Bootstraped Reward Shaping）的新奖励塑形方法，通过代理当前状态价值函数的估计来充当潜在函数，从而在稀疏奖励环境中提高训练效率，实验证明该方法在Atari游戏中的训练速度有所提升。	 | In reinforcement learning, especially in sparse-reward domains, many environment steps are required to observe reward information. In order to increase the frequency of such observations, "potential-based reward shaping" (PBRS) has been proposed as a method of providing a more dense reward signal while leaving the optimal policy invariant. However, the required "potential function" must be carefully designed with task-dependent knowledge to not deter training performance. In this work, we propose a "bootstrapped" method of reward shaping, termed BSRS, in which the agent's current estimate of the state-value function acts as the potential function for PBRS. We provide convergence proofs for the tabular setting, give insights into training dynamics for deep RL, and show that the proposed method improves training speed in the Atari suite.	 | 在强化学习中，特别是在稀疏奖励环境中，需要大量的环境步骤才能观察到奖励信息。为了增加这种观察的频率，“基于潜在函数的奖励塑形”（PBRS，Potential-Based Reward Shaping）方法被提出，以提供更密集的奖励信号，同时保持最优策略不变。然而，所需的“潜在函数”必须根据任务相关知识谨慎设计，以不阻碍训练性能。在本工作中，我们提出了一种“递推”奖励塑形方法，称为BSRS（Bootstraped Reward Shaping）。在这种方法中，代理当前对状态值函数的估计充当PBRS中的潜在函数。我们为表征环境提供了收敛性证明，提供了深层强化学习训练动态的见解，并展示了所提出的方法在Atari套件中的训练速度有所提高。
2501.00988	 | Optimizing Noise Schedules of Generative Models in High Dimensionss	 | Santiago Aranguri,Giulio Biroli,Marc Mezard,Eric Vanden-Eijnden	 | 研究发现，扩散模型在生成样本时会经历相变，需要通过不同的噪声调度方法来解决，如方差保持（VP）和方差爆炸（VE），它们分别在恢复低级和高级特征方面存在局限性，但通过定制噪声调度方法可以同时恢复这两种特征，从而提高生成模型的质量和效率。	 | Recent works have shown that diffusion models can undergo phase transitions, the resolution of which is needed for accurately generating samples. This has motivated the use of different noise schedules, the two most common choices being referred to as variance preserving (VP) and variance exploding (VE). Here we revisit these schedules within the framework of stochastic interpolants. Using the Gaussian Mixture (GM) and Curie-Weiss (CW) data distributions as test case models, we first investigate the effect of the variance of the initial noise distribution and show that VP recovers the low-level feature (the distribution of each mode) but misses the high-level feature (the asymmetry between modes), whereas VE performs oppositely. We also show that this dichotomy, which happens when denoising by a constant amount in each step, can be avoided by using noise schedules specific to VP and VE that allow for the recovery of both high- and low-level features. Finally we show that these schedules yield generative models for the GM and CW model whose probability flow ODE can be discretized using $Θ_d(1)$ steps in dimension $d$ instead of the $Θ_d(\sqrt{d})$ steps required by constant denoising.	 | 近期的研究表明，扩散模型可以经历相变，准确生成样本需要解决这些相变问题。这促使人们使用不同的噪声调度方法，其中最常见的两种选择分别被称为方差保持（VP）和方差爆炸（VE）。在这里，我们重新审视这些调度方法，将其置于随机插值的框架内。使用高斯混合（GM）和库尔利-魏斯（CW）数据分布作为测试模型，我们首先研究初始噪声分布的方差效应，发现VP能够恢复低级特征（每种模式的分布），但忽略了高级特征（模式之间的不对称性），而VE则相反。我们还展示了当每次去噪量固定时会发生这种二分现象，可以通过为VP和VE定制特定的噪声调度方法来避免，这些调度方法可以同时恢复高级和低级特征。最后，我们展示了这些调度方法能够为GM和CW模型生成生成模型，其概率流ODE在维度$d$中的离散化只需$Θ_d(1)$步，而不是恒定去噪所需的$Θ_d(\sqrt{d})$步。
2501.00961	 | The Silent Majority: Demystifying Memorization Effect in the Presence of Spurious Correlations	 | Chenyu You,Haocheng Dai,Yifei Min,Jasjeet S. Sekhon,Sarang Joshi,James S. Duncan	 | 本文通过记忆视角探讨了机器学习模型在处理少数群体数据时的性能失衡问题，发现一小部分神经元中存在“嘈杂”的虚假记忆，这些记忆导致了模型对少数群体的识别能力下降，并通过实验验证了这一假设。	 | Machine learning models often rely on simple spurious features -- patterns in training data that correlate with targets but are not causally related to them, like image backgrounds in foreground classification. This reliance typically leads to imbalanced test performance across minority and majority groups. In this work, we take a closer look at the fundamental cause of such imbalanced performance through the lens of memorization, which refers to the ability to predict accurately on \textit{atypical} examples (minority groups) in the training set but failing in achieving the same accuracy in the testing set. This paper systematically shows the ubiquitous existence of spurious features in a small set of neurons within the network, providing the first-ever evidence that memorization may contribute to imbalanced group performance. Through three experimental sources of converging empirical evidence, we find the property of a small subset of neurons or channels in memorizing minority group information. Inspired by these findings, we articulate the hypothesis: the imbalanced group performance is a byproduct of ``noisy'' spurious memorization confined to a small set of neurons. To further substantiate this hypothesis, we show that eliminating these unnecessary spurious memorization patterns via a novel framework during training can significantly affect the model performance on minority groups. Our experimental results across various architectures and benchmarks offer new insights on how neural networks encode core and spurious knowledge, laying the groundwork for future research in demystifying robustness to spurious correlation.	 | 机器学习模型通常依赖于简单的虚假特征——训练数据中的模式与目标相关但与其没有因果关系，例如前景分类中的图像背景。这种依赖通常会导致测试性能在少数群体和多数群体之间失衡。在本文中，我们通过记忆的视角——指模型能够在训练集中的非典型示例（少数群体）上准确预测，但在测试集上却无法保持同样准确性的能力——更深入地探讨了这种失衡性能的根本原因。本文系统地展示了在神经网络中一小部分神经元中普遍存在虚假特征的现象，这是首次证据表明记忆可能对少数群体的失衡性能有所贡献。通过三种实验来源的综合实证证据，我们发现一小部分神经元或通道具有存储少数群体信息的特性。受这些发现的启发，我们提出了假设：少数群体的失衡性能是由于一小部分神经元中“嘈杂”的虚假记忆所导致的结果。为进一步验证这一假设，我们通过在训练过程中使用一个新颖框架消除这些不必要的虚假记忆模式，展示了其对少数群体模型性能的重大影响。我们针对各种架构和基准的实验结果为神经网络如何编码核心和虚假知识提供了新的见解，为未来研究揭示对虚假相关性的鲁棒性奠定了基础。
2501.00942	 | Efficient Unsupervised Shortcut Learning Detection and Mitigation in Transformers	 | Lukas Kuhn,Sari Sadiya,Jorg Schlotterer,Christin Seifert,Gemma Roig	 | 该研究提出了一种无监督框架，旨在检测并减轻变换器模型中的捷径学习现象，从而提高模型在敏感决策任务上的公平性和准确性；实验结果表明，该框架能够显著提升最差群体的准确率、减少标注工作量，并且具有良好的计算效率。	 | Shortcut learning, i.e., a model's reliance on undesired features not directly relevant to the task, is a major challenge that severely limits the applications of machine learning algorithms, particularly when deploying them to assist in making sensitive decisions, such as in medical diagnostics. In this work, we leverage recent advancements in machine learning to create an unsupervised framework that is capable of both detecting and mitigating shortcut learning in transformers. We validate our method on multiple datasets. Results demonstrate that our framework significantly improves both worst-group accuracy (samples misclassified due to shortcuts) and average accuracy, while minimizing human annotation effort. Moreover, we demonstrate that the detected shortcuts are meaningful and informative to human experts, and that our framework is computationally efficient, allowing it to be run on consumer hardware.	 | 捷径学习，即模型对与任务无关的非直接特征的依赖，是机器学习算法应用的重大挑战，尤其是在将其部署用于辅助作出敏感决策（如医疗诊断）时。在本项工作中，我们利用机器学习的最新进展，创建了一个无监督框架，该框架能够同时检测并减轻变换器中的捷径学习现象。我们在多个数据集上验证了该方法。结果表明，我们的框架显著提高了最差群体准确率（由于捷径而导致的错误分类样本）和平均准确率，同时减少了人工标注的工作量。此外，我们证明了检测到的捷径对人类专家是有意义和有信息价值的，而且我们的框架在计算效率方面表现出色，可以运行在消费级硬件上。
2501.00941	 | A Novel Diffusion Model for Pairwise Geoscience Data Generation with Unbalanced Training Dataset	 | Junhuan Yang,Yuzhou Zhang,Yi Sheng,Youzuo Lin,Lei Yang	 | 近年来，生成型AI技术虽在日常生活中产生了显著影响，但在科学计算中的应用仍处于早期阶段。本文介绍了一种名为“UB-Diff”的新扩散模型，用于生成多模态配对科学数据，特别是在地震成像领域，该模型在性能上显著优于现有技术，证明了多模态配对数据生成的有效性。	 | Recently, the advent of generative AI technologies has made transformational impacts on our daily lives, yet its application in scientific applications remains in its early stages. Data scarcity is a major, well-known barrier in data-driven scientific computing, so physics-guided generative AI holds significant promise. In scientific computing, most tasks study the conversion of multiple data modalities to describe physical phenomena, for example, spatial and waveform in seismic imaging, time and frequency in signal processing, and temporal and spectral in climate modeling; as such, multi-modal pairwise data generation is highly required instead of single-modal data generation, which is usually used in natural images (e.g., faces, scenery). Moreover, in real-world applications, the unbalance of available data in terms of modalities commonly exists; for example, the spatial data (i.e., velocity maps) in seismic imaging can be easily simulated, but real-world seismic waveform is largely lacking. While the most recent efforts enable the powerful diffusion model to generate multi-modal data, how to leverage the unbalanced available data is still unclear. In this work, we use seismic imaging in subsurface geophysics as a vehicle to present ``UB-Diff'', a novel diffusion model for multi-modal paired scientific data generation. One major innovation is a one-in-two-out encoder-decoder network structure, which can ensure pairwise data is obtained from a co-latent representation. Then, the co-latent representation will be used by the diffusion process for pairwise data generation. Experimental results on the OpenFWI dataset show that UB-Diff significantly outperforms existing techniques in terms of Fréchet Inception Distance (FID) score and pairwise evaluation, indicating the generation of reliable and useful multi-modal pairwise data.	 | 近年来，生成型AI技术的诞生在我们的日常生活中产生了颠覆性的影响，但在科学应用中的应用仍处于早期阶段。数据驱动的科学计算中，数据稀缺是一个众所周知的障碍，因此，基于物理指导的生成型AI具有巨大的潜力。在科学计算中，大多数任务涉及将多种数据模态转换以描述物理现象，例如地震成像中的空间和波形、信号处理中的时间和频率、气候建模中的时间和频谱；因此，生成多模态配对数据比生成单一模态数据更为重要，后者通常用于自然图像（例如，人脸、风景）。此外，在实际应用中，模态间数据的不平衡性在数据可用性方面非常常见；例如，在地震成像中，空间数据（即速度图）可以容易地模拟，但实际的地震波形却严重缺乏。尽管最新努力使强大的扩散模型能够生成多模态数据，但如何利用不平衡的数据仍然是不清楚的。在本文中，我们以地下地质物理学中的地震成像为例，介绍了“UB-Diff”这种新颖的扩散模型，用于生成多模态配对科学数据。一个主要的创新是采用一进二出的编码器-解码器网络结构，这可以确保从共享潜在表示中获得配对数据。然后，共享潜在表示将用于扩散过程中的配对数据生成。在OpenFWI数据集上的实验结果表明，UB-Diff在弗雷彻入射距离（FID）得分和配对评估方面显著优于现有技术，表明多模态配对数据的生成是可靠且有用的。
2501.00924	 | On the Low-Complexity of Fair Learning for Combinatorial Multi-Armed Bandit	 | Xiaoyi Wu,Bo Ji,Bin Li	 | 该研究提出了一个低复杂度的公平学习算法，该算法通过“挑选并比较”方法减少计算复杂度，同时在公平性和遗憾性能方面仅做轻微妥协。该算法适用于组合多臂赌博机问题中的公平性约束，并特别适用于无线网络中的干扰约束。	 | Combinatorial Multi-Armed Bandit with fairness constraints is a framework where multiple arms form a super arm and can be pulled in each round under uncertainty to maximize cumulative rewards while ensuring the minimum average reward required by each arm. The existing pessimistic-optimistic algorithm linearly combines virtual queue-lengths (tracking the fairness violations) and Upper Confidence Bound estimates as a weight for each arm and selects a super arm with the maximum total weight. The number of super arms could be exponential to the number of arms in many scenarios. In wireless networks, interference constraints can cause the number of super arms to grow exponentially with the number of arms. Evaluating all the feasible super arms to find the one with the maximum total weight can incur extremely high computational complexity in the pessimistic-optimistic algorithm. To avoid this, we develop a low-complexity fair learning algorithm based on the so-called pick-and-compare approach that involves randomly picking $M$ feasible super arms to evaluate. By setting $M$ to a constant, the number of comparison steps in the pessimistic-optimistic algorithm can be reduced to a constant, thereby significantly reducing the computational complexity. Our theoretical proof shows this low-complexity design incurs only a slight sacrifice in fairness and regret performance. Finally, we validate the theoretical result by extensive simulations.	 | 组合多臂赌博机（Combinatorial Multi-Armed Bandit, CMAB）在具有公平性约束的框架中，多个臂组成一个超级臂，并在不确定性下每轮可以被拉动，以最大化累积奖励同时确保每个臂的最小平均奖励要求。现有的悲观-乐观算法通过线性组合虚拟队列长度（追踪公平性违规情况）和上界置信区间估计值作为每个臂的权重，并选择总权重最大的超级臂。在许多情况下，超级臂的数量可能指数级地依赖于臂的数量。在无线网络中，干扰约束可能导致超级臂的数量随着臂的数量指数级增长。在悲观-乐观算法中，评估所有可行的超级臂以找到总权重最大的一个可能带来极高的计算复杂度。为避免这种情况，我们基于所谓的“挑选并比较”方法开发了一种低复杂度的公平学习算法，该方法包括随机挑选 $M$ 个可行的超级臂进行评估。通过将 $M$ 设定为常数，悲观-乐观算法中的比较步骤数量可以减少到常数，从而显著降低计算复杂度。我们的理论证明显示，这种低复杂度的设计仅在公平性和遗憾性能方面略有牺牲。最后，我们通过广泛的仿真验证了理论结果。
2501.00919	 | Exploring Geometric Representational Alignment through Ollivier-Ricci Curvature and Ricci Flow	 | Nahid Torbati,Michael Gaebler,Simon M. Hofmann,Nico Scherf	 | 该研究探讨了神经系统的输入数据在高维空间中的编码方式，并采用了奥利维耶-里奇曲率和里奇流工具来比较人工神经网络和大脑的表征，特别是在局部几何特性方面。通过这一离散几何框架，研究发现并分析了VGG-Face网络、人类对齐的VGG-Face版本以及来自大规模在线研究的人脸刺激之间的表征差异和相似性。	 | Representational analysis explores how input data of a neural system are encoded in high dimensional spaces of its distributed neural activations, and how we can compare different systems, for instance, artificial neural networks and brains, on those grounds. While existing methods offer important insights, they typically do not account for local intrinsic geometrical properties within the high-dimensional representation spaces. To go beyond these limitations, we explore Ollivier-Ricci curvature and Ricci flow as tools to study the alignment of representations between humans and artificial neural systems on a geometric level. As a proof-of-principle study, we compared the representations of face stimuli between VGG-Face, a human-aligned version of VGG-Face, and corresponding human similarity judgments from a large online study. Using this discrete geometric framework, we were able to identify local structural similarities and differences by examining the distributions of node and edge curvature and higher-level properties by detecting and comparing community structure in the representational graphs.	 | 表征分析探讨了神经系统的输入数据是如何在分布式神经激活的高维空间中编码的，以及我们如何在这些基础上比较不同的系统，比如人工神经网络和大脑。虽然现有的方法提供了重要的见解，但它们通常不考虑高维表示空间内的局部内在几何特性和性质。为了超越这些限制，我们探索了奥利维耶-里奇曲率和里奇流作为工具，以几何水平研究人类与人工神经系统的表征对齐。作为原理性研究，我们将VGG-Face、人类对齐的VGG-Face版本以及来自大规模在线研究的人脸刺激的相应人类相似性判断之间的表征进行了比较。使用这个离散几何框架，我们通过检查节点和边的曲率分布以及通过检测和比较表征图中的社区结构来识别局部结构的相似性和差异性，从而探讨更高层次的属性。
2501.00913	 | $\beta$-DQN: Improving Deep Q-Learning By Evolving the Behavior	 | Hongming Zhang,Fengshuo Bai,Chenjun Xiao,Chao Gao,Bo Xu,Martin Müller	 | 研究人员倾向于使用简单且计算成本较低的方法，如ε-贪婪策略，而非复杂的探索方法。为此，本文提出了一种称为β-DQN的方法，通过在标准DQN中引入一个行为函数β，生成多样化的策略以平衡状态-动作覆盖和过度估算偏差修正，并在实验中展示了其在多种任务上的优越性能。	 | While many sophisticated exploration methods have been proposed, their lack of generality and high computational cost often lead researchers to favor simpler methods like $ε$-greedy. Motivated by this, we introduce $β$-DQN, a simple and efficient exploration method that augments the standard DQN with a behavior function $β$. This function estimates the probability that each action has been taken at each state. By leveraging $β$, we generate a population of diverse policies that balance exploration between state-action coverage and overestimation bias correction. An adaptive meta-controller is designed to select an effective policy for each episode, enabling flexible and explainable exploration. $β$-DQN is straightforward to implement and adds minimal computational overhead to the standard DQN. Experiments on both simple and challenging exploration domains show that $β$-DQN outperforms existing baseline methods across a wide range of tasks, providing an effective solution for improving exploration in deep reinforcement learning.	 | 尽管提出了许多复杂的探索方法，但由于它们的通用性较差和高计算成本，研究人员往往更偏好简单的方法，如$\varepsilon$-贪婪策略。为此，我们引入了一种简单且高效的探索方法$β$-DQN，该方法通过在标准DQN中添加一个行为函数$β$来增强DQN。该函数估计每个状态下每个动作被采取的概率。通过利用$β$，我们生成了一族多样化的策略，这些策略在状态-动作覆盖和过度估算偏差修正之间实现了平衡。我们设计了一个自适应元控制器，以根据每个episode选择有效的策略，从而实现灵活且可解释的探索。$β$-DQN 实现简单，并且仅在标准DQN上增加了少量的计算开销。在简单和复杂的探索域上的实验表明，$β$-DQN 在广泛的任务中均优于现有的基线方法，为提高深度强化学习中的探索提供了一个有效的解决方案。
2501.00911	 | Aligning LLMs with Domain Invariant Reward Models	 | David Wu,Sanjiban Choudhury	 | 该研究提出了一种名为\method的框架，通过利用从简单源领域收集的反馈来学习跨领域的一致性奖励模型，从而提高大型语言模型在缺乏偏好数据的目标领域的对齐效果。该框架在多种设定下展示了显著提升，包括语言迁移、数据质量迁移、示例数量迁移和任务复杂度迁移。	 | Aligning large language models (LLMs) to human preferences is challenging in domains where preference data is unavailable. We address the problem of learning reward models for such target domains by leveraging feedback collected from simpler source domains, where human preferences are easier to obtain. Our key insight is that, while domains may differ significantly, human preferences convey \emph{domain-agnostic} concepts that can be effectively captured by a reward model. We propose \method, a framework that trains domain-invariant reward models by optimizing a dual loss: a domain loss that minimizes the divergence between source and target distribution, and a source loss that optimizes preferences on the source domain. We show \method is a general approach that we evaluate and analyze across 4 distinct settings: (1) Cross-lingual transfer (accuracy: $0.621 \rightarrow 0.661$), (2) Clean-to-noisy (accuracy: $0.671 \rightarrow 0.703$), (3) Few-shot-to-full transfer (accuracy: $0.845 \rightarrow 0.920$), and (4) Simple-to-complex tasks transfer (correlation: $0.508 \rightarrow 0.556$). Our code, models and data are available at \url{https://github.com/portal-cornell/dial}.	 | 在缺乏偏好数据的领域中对大型语言模型（LLMs）进行对齐具有挑战性。我们通过利用从较简单的源领域收集的反馈来解决这种目标领域的奖励模型学习问题，其中人类的偏好更容易获得。我们的核心洞察是，尽管各个领域可能存在显著差异，但人类的偏好传达了跨领域的概念，这些概念可以通过奖励模型有效捕捉。我们提出了\method框架，通过优化双重损失函数来训练领域不变的奖励模型：一个领域损失，旨在最小化源领域和目标领域的分布差异；一个源领域损失，优化源领域中的偏好。我们展示了\method是一个通用的方法，并通过四种不同的设置进行了评估和分析：（1）跨语言迁移（准确率：从0.621提升到0.661），（2）干净数据到噪声数据（准确率：从0.671提升到0.703），（3）少量示例到全面迁移（准确率：从0.845提升到0.920），（4）简单任务到复杂任务的迁移（相关性：从0.508提升到0.556）。我们的代码、模型和数据可在\url{https://github.com/portal-cornell/dial}获取。
2501.00910	 | Population Aware Diffusion for Time Series Generation	 | Yang Li,Han Meng,Zhenyu Bi,Ingolv T. Urnes,Haipeng Chen	 | 扩散模型在生成时间序列数据方面表现出色，但通常忽视了保留整个数据集的群体级别属性。为此，研究提出了一种新的面向群体的扩散模型（PaD-TS），该模型能更好地保留时间序列的群体级别属性，实验结果显示其在保持数据真实性的同时，显著提高了互相关性分布的准确性。	 | Diffusion models have shown promising ability in generating high-quality time series (TS) data. Despite the initial success, existing works mostly focus on the authenticity of data at the individual level, but pay less attention to preserving the population-level properties on the entire dataset. Such population-level properties include value distributions for each dimension and distributions of certain functional dependencies (e.g., cross-correlation, CC) between different dimensions. For instance, when generating house energy consumption TS data, the value distributions of the outside temperature and the kitchen temperature should be preserved, as well as the distribution of CC between them. Preserving such TS population-level properties is critical in maintaining the statistical insights of the datasets, mitigating model bias, and augmenting downstream tasks like TS prediction. Yet, it is often overlooked by existing models. Hence, data generated by existing models often bear distribution shifts from the original data. We propose Population-aware Diffusion for Time Series (PaD-TS), a new TS generation model that better preserves the population-level properties. The key novelties of PaD-TS include 1) a new training method explicitly incorporating TS population-level property preservation, and 2) a new dual-channel encoder model architecture that better captures the TS data structure. Empirical results in major benchmark datasets show that PaD-TS can improve the average CC distribution shift score between real and synthetic data by 5.9x while maintaining a performance comparable to state-of-the-art models on individual-level authenticity.	 | 扩散模型在生成高质量时间序列（TS）数据方面显示出令人鼓舞的能力。尽管早期取得了成功，但现有工作主要关注单个数据点的真实性，而在整个数据集上保持群体级别的属性方面则关注较少。这些群体级别的属性包括每个维度上的值分布以及不同维度之间的某些函数依赖性（例如，不同维度之间的互相关性，CC）。例如，在生成家庭能源消耗的TS数据时，外部温度和厨房温度的值分布应该被保留，同时还需要保持它们之间的互相关性的分布。保持这些TS群体级别的属性对于维护数据集的统计洞察、减轻模型偏差以及增强TS预测等下游任务至关重要。然而，现有的模型往往忽略了这一点，因此由现有模型生成的数据常常与原始数据存在分布上的差异。我们提出了面向群体的扩散模型（PaD-TS），这是一种新的TS生成模型，能够更好地保留群体级别的属性。PaD-TS的关键创新点包括：1）一种新的训练方法，明确将TS群体级别的属性保留纳入其中；2）一种新的双通道编码器模型架构，能够更好地捕捉TS数据结构。在主要基准数据集上的实验证明，PaD-TS可以将真实数据和合成数据之间的平均互相关性分布偏移分数提高5.9倍，同时保持与最新模型相当的单个数据点的真实性性能。
2501.00891	 | Demystifying Online Clustering of Bandits: Enhanced Exploration Under Stochastic and Smoothed Adversarial Contexts	 | Zhuohua Li,Maoli Liu,Xiangxiang Dai,John C.S. Lui	 | 本文提出了两种新的在线聚类多臂博弈算法UniCLUB和PhaseUniCLUB，通过引入增强的探索机制有效识别用户簇，同时在较弱的假设条件下仍能达到与现有方法相当的性能。此外，受平滑分析框架启发，本文还提出了一种无需独立同分布上下文生成假设的实用设置，进一步提高了算法性能，并在多种数据集上验证了其优越性。	 | The contextual multi-armed bandit (MAB) problem is crucial in sequential decision-making. A line of research, known as online clustering of bandits, extends contextual MAB by grouping similar users into clusters, utilizing shared features to improve learning efficiency. However, existing algorithms, which rely on the upper confidence bound (UCB) strategy, struggle to gather adequate statistical information to accurately identify unknown user clusters. As a result, their theoretical analyses require several strong assumptions about the "diversity" of contexts generated by the environment, leading to impractical settings, complicated analyses, and poor practical performance. Removing these assumptions has been a long-standing open problem in the clustering of bandits literature. In this paper, we provide two solutions to this open problem. First, following the i.i.d. context generation setting in existing studies, we propose two novel algorithms, UniCLUB and PhaseUniCLUB, which incorporate enhanced exploration mechanisms to accelerate cluster identification. Remarkably, our algorithms require substantially weaker assumptions while achieving regret bounds comparable to prior work. Second, inspired by the smoothed analysis framework, we propose a more practical setting that eliminates the requirement for i.i.d. context generation used in previous studies, thus enhancing the performance of existing algorithms for online clustering of bandits. Our technique can be applied to both graph-based and set-based clustering of bandits frameworks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our proposed algorithms consistently outperform existing approaches.	 | 连续上下文多臂博弈（Contextual Multi-Armed Bandit, MAB）问题在序列决策中至关重要。一项研究方向，即在线聚类多臂博弈，通过将相似的用户分组到簇中，并利用共享特征来提高学习效率，扩展了上下文多臂博弈。然而，现有的算法依赖于上置信界（Upper Confidence Bound, UCB）策略，难以收集足够的统计信息以准确识别未知用户簇。因此，它们的理论分析需要对环境生成的“多样性”条件做出若干强假设，导致不切实际的设置、复杂的分析和较差的实用性能。消除这些假设一直是在多臂博弈聚类文献中的一个长期未解问题。在本文中，我们提供了两个解决方案来解决这个问题。首先，遵循现有研究中的独立同分布（i.i.d.）上下文生成设置，我们提出了两种新的算法——UniCLUB和PhaseUniCLUB，这些算法引入了增强的探索机制以加速簇的识别。令人惊讶的是，我们的算法需要更弱的假设，但仍能达到与先前工作相当的后悔边界。其次，受平滑分析框架的启发，我们提出了一个更实用的设置，消除了先前研究中使用的独立同分布上下文生成的要求，从而提高了现有算法在在线多臂博弈聚类中的性能。我们的技术可以应用于基于图的和基于集合的多臂博弈聚类框架。在合成数据集和真实世界数据集上的广泛评估表明，我们提出的算法在所有方面都优于现有方法。
2501.00889	 | Evaluating Time Series Foundation Models on Noisy Periodic Time Series	 | Syamantak Datta Gupta	 | 通过在两个合成数据集上评估几种领先的时序基础模型（TSFM）的零样本、长期预测能力，研究发现TSFM在有界周期和高采样率时间序列上可与统计方法匹敌甚至更优，但在周期较长、噪声水平高、采样率低及时间序列形状复杂的情况下表现较差。	 | While recent advancements in foundation models have significantly impacted machine learning, rigorous tests on the performance of time series foundation models (TSFMs) remain largely underexplored. This paper presents an empirical study evaluating the zero-shot, long-horizon forecasting abilities of several leading TSFMs over two synthetic datasets constituting noisy periodic time series. We assess model efficacy across different noise levels, underlying frequencies, and sampling rates. As benchmarks for comparison, we choose two statistical techniques: a Fourier transform (FFT)-based approach and a linear autoregressive (AR) model. Our findings demonstrate that while for time series with bounded periods and higher sampling rates, TSFMs can match or outperform the statistical approaches, their forecasting abilities deteriorate with longer periods, higher noise levels, lower sampling rates and more complex shapes of the time series.	 | 尽管近期基础模型在机器学习领域取得了显著进展，但对时间序列基础模型（TSFM）性能的严格测试仍然很大程度上未被探索。本文通过在两个合成数据集上评估几种领先TSFM的零样本、长期预测能力，进行了实证研究。这两个数据集由带有噪声的时间序列组成。我们评估了模型在不同噪声水平、基本频率和采样率下的有效性。作为比较基准，我们选择了两种统计技术：傅里叶变换（FFT）方法和线性自回归（AR）模型。研究结果表明，对于有界周期和高采样率的时间序列，TSFM可以与统计方法匹敌甚至优于统计方法，但当时间序列周期较长、噪声水平较高、采样率较低以及时间序列形状更为复杂时，其预测能力会下降。
2501.00884	 | Diversity Optimization for Travelling Salesman Problem via Deep Reinforcement Learning	 | Qi Li,Zhiguang Cao,Yining Ma,Yaoxin Wu,Yue-Jiao Gong	 | 本文提出了一种基于深度强化学习的神经求解器，采用编码器-解码器结构并通过相对化滤波器和多注意自适应主动搜索策略，以发现旅行商问题（TSP）的多元高质量解；实验结果表明该方法在基准实例上优于现有神经基线模型，并且在某些情况下比最先进的传统启发式算法快1.3到15倍。	 | Existing neural methods for the Travelling Salesman Problem (TSP) mostly aim at finding a single optimal solution. To discover diverse yet high-quality solutions for Multi-Solution TSP (MSTSP), we propose a novel deep reinforcement learning based neural solver, which is primarily featured by an encoder-decoder structured policy. Concretely, on the one hand, a Relativization Filter (RF) is designed to enhance the robustness of the encoder to affine transformations of the instances, so as to potentially improve the quality of the found solutions. On the other hand, a Multi-Attentive Adaptive Active Search (MA3S) is tailored to allow the decoders to strike a balance between the optimality and diversity. Experimental evaluations on benchmark instances demonstrate the superiority of our method over recent neural baselines across different metrics, and its competitive performance against state-of-the-art traditional heuristics with significantly reduced computational time, ranging from $1.3\times$ to $15\times$ faster. Furthermore, we demonstrate that our method can also be applied to the Capacitated Vehicle Routing Problem (CVRP).	 | 现有的神经网络方法主要针对旅行商问题（TSP）寻找单一最优解。为了发现多元且高质量的解，我们提出了一种新颖的基于深度强化学习的神经求解器，其主要特点是采用了编码器-解码器结构策略。具体来说，一方面，我们设计了一个相对化滤波器（RF），以增强编码器对实例仿射变换的鲁棒性，从而提高找到的解的质量。另一方面，我们定制了一种多注意自适应主动搜索（MA3S），以允许解码器在最优性和多样性之间取得平衡。在基准实例上的实验评估表明，我们的方法在不同性能指标上优于最近的神经基线模型，并且在计算时间显著减少的情况下，其性能与最先进的传统启发式算法相当，甚至在某些情况下快至1.3倍到15倍。此外，我们还展示了我们的方法可以应用于车辆路线问题中的容量受限版本（CVRP）。
2501.00852	 | Hybridising Reinforcement Learning and Heuristics for Hierarchical Directed Arc Routing Problems	 | Van Quang Nguyen,Quoc Chuong Nguyen,Thu Huong Dang,Truong-Son Hy	 | 本文提出了一种新的混合启发式算法（HRDA），结合了强化学习和传统启发式方法，以有效解决层级导向容量受限弧路由问题（HDCARP）。该算法通过实时反馈动态优化路由策略，显著提高了大规模实例的求解速度而不牺牲解的质量。	 | The Hierarchical Directed Capacitated Arc Routing Problem (HDCARP) is an extension of the Capacitated Arc Routing Problem (CARP), where the arcs of a graph are divided into classes based on their priority. The traversal of these classes is determined by either precedence constraints or a hierarchical objective, resulting in two distinct HDCARP variants. To the best of our knowledge, only one matheuristic has been proposed for these variants, but it performs relatively slowly, particularly for large-scale instances (Ha et al., 2024). In this paper, we propose a fast heuristic to efficiently address the computational challenges of HDCARP. Furthermore, we incorporate Reinforcement Learning (RL) into our heuristic to effectively guide the selection of local search operators, resulting in a hybrid algorithm. We name this hybrid algorithm as the Hybrid Reinforcement Learning and Heuristic Algorithm for Directed Arc Routing (HRDA). The hybrid algorithm adapts to changes in the problem dynamically, using real-time feedback to improve routing strategies and solution's quality by integrating heuristic methods. Extensive computational experiments on artificial instances demonstrate that this hybrid approach significantly improves the speed of the heuristic without deteriorating the solution quality. Our source code is publicly available at: https://github.com/HySonLab/ArcRoute	 | 层级导向容量受限弧路由问题 (Hierarchical Directed Capacitated Arc Routing Problem, HDCARP) 是容量受限弧路由问题 (Capacitated Arc Routing Problem, CARP) 的扩展，其中图中的边被分为基于优先级的类别。这些类别的遍历由优先级约束或层级目标决定，从而产生了两种不同的 HDCARP 变体。据我们所知，迄今为止仅提出了一种混合启发式算法（matheuristic）来解决这些变体的问题，但对于大规模实例而言，其运行速度相对较慢（Ha et al., 2024）。在本文中，我们提出了一种快速启发式算法，以有效地应对 HDCARP 的计算挑战。此外，我们将强化学习（Reinforcement Learning, RL）融入到我们的启发式算法中，以有效指导局部搜索操作的选择，从而形成一种混合算法。我们将这种混合算法命名为“导向强化学习和启发式算法用于定向弧路由（Hybrid Reinforcement Learning and Heuristic Algorithm for Directed Arc Routing, HRDA）”。该混合算法能够动态适应问题的变化，通过实时反馈来改进路由策略和解决方案的质量，并结合启发式方法。在人工实例的广泛计算实验中表明，该混合方法在不降低解的质量的情况下显著提高了启发式的运行速度。我们的源代码已公开发布在：https://github.com/HySonLab/ArcRoute
2501.00823	 | Decoupling Knowledge and Reasoning in Transformers: A Modular Architecture with Generalized Cross-Attention	 | Zhenyu Guo,Wenguang Chen	 | 本文提出了一种新型模块化变压器架构，通过通用跨注意力机制将知识和推理显式地解耦到共享的知识库中，特别适用于有效的知识检索，并提供了一种严格的数学推导，证明标准变压器中的前馈网络是通用跨注意力的一个特殊形式，为未来的研究探索增强了可解释性、适应性和可扩展性奠定了基础。	 | Transformers have achieved remarkable success across diverse domains, but their monolithic architecture presents challenges in interpretability, adaptability, and scalability. This paper introduces a novel modular Transformer architecture that explicitly decouples knowledge and reasoning through a generalized cross-attention mechanism to a shared knowledge base, specifically designed for effective knowledge retrieval. Critically, we provide a rigorous mathematical derivation demonstrating that the Feed-Forward Network (FFN) in a standard Transformer is a specialized case (a closure) of this generalized cross-attention, revealing its role in implicit knowledge retrieval and validating our design. This theoretical framework provides a new lens for understanding FFNs and lays the foundation for future research exploring enhanced interpretability, adaptability, and scalability, enabling richer interplay with external knowledge bases and other systems.	 | 变压器在多个领域取得了显著的成功，但其单一的架构在可解释性、适应性和可扩展性方面带来了挑战。本文介绍了一种新型的模块化变压器架构，通过通用的跨注意力机制将知识和推理显式地解耦到共享的知识库中，特别适用于有效的知识检索。关键地，我们提供了一种严格的数学推导，证明标准变压器中的前馈网络（FFN）是这种通用跨注意力的一个特殊形式（闭包），揭示了其在隐式知识检索中的作用，并验证了我们的设计。这种理论框架提供了一种新的视角来理解FFNs，并为未来的研究探索增强的可解释性、适应性和可扩展性奠定了基础，使得与外部知识库和其他系统的互动更加丰富。
2501.00817	 | Hardness of Learning Fixed Parities with Neural Networks	 | Itamar Shoshani,Ohad Shamir	 | 该研究解决了对称函数学习中的一个开放问题，通过证明使用某种最小尺寸的固定对称函数进行训练的一层ReLU网络难以产生有意义的结果。为此，研究还证明了一个关于线性阈值函数的傅里叶系数衰减的新结果，该结果可能具有独立的价值。	 | Learning parity functions is a canonical problem in learning theory, which although computationally tractable, is not amenable to standard learning algorithms such as gradient-based methods. This hardness is usually explained via statistical query lower bounds [Kearns, 1998]. However, these bounds only imply that for any given algorithm, there is some worst-case parity function that will be hard to learn. Thus, they do not explain why fixed parities - say, the full parity function over all coordinates - are difficult to learn in practice, at least with standard predictors and gradient-based methods [Abbe and Boix-Adsera, 2022]. In this paper, we address this open problem, by showing that for any fixed parity of some minimal size, using it as a target function to train one-hidden-layer ReLU networks with perturbed gradient descent will fail to produce anything meaningful. To establish this, we prove a new result about the decay of the Fourier coefficients of linear threshold (or weighted majority) functions, which may be of independent interest.	 | 学习对称函数是学习理论中的一个典型问题，尽管计算上是可行的，但无法使用标准的学习算法，如梯度基方法进行处理。这种困难通常通过统计查询下界[Kearns, 1998]来解释。然而，这些下界只表明对于任何给定的算法，存在一个最坏情况的对称函数使其难以学习。因此，它们并不能解释为什么固定对称函数（例如，所有坐标上的全对称函数）在实践中用标准预测器和梯度基方法[Abbe 和 Boix-Adsera, 2022]难以学习。在这篇论文中，我们通过证明使用某种最小尺寸的固定对称函数作为目标函数来训练具有扰动梯度下降的一层 ReLU 网络将无法产生有意义的结果，来解决这一开放问题。为此，我们证明了一个关于线性阈值（或加权多数）函数的傅里叶系数衰减的新结果，该结果可能具有独立的价值。
2501.00799	 | Follow The Sparse Approximate Leader for No-Regret Online Sparse Linear Approximation	 | Samrat Mukhopadhyay,Debasmita Mukherjee	 | 该研究提出了一个高效的在线元策略Follow-The-Approximate-Sparse-Leader (FASL)，用于解决在线稀疏线性逼近问题，并通过理论分析和数值模拟证明了其在某些条件下的性能上界，验证了该策略的有效性。	 | We consider the problem of \textit{online sparse linear approximation}, where one predicts the best sparse approximation of a sequence of measurements in terms of linear combination of columns of a given measurement matrix. Such online prediction problems are ubiquitous, ranging from medical trials to web caching to resource allocation. The inherent difficulty of offline recovery also makes the online problem challenging. In this letter, we propose Follow-The-Approximate-Sparse-Leader, an efficient online meta-policy to address this online problem. Through a detailed theoretical analysis, we prove that under certain assumptions on the measurement sequence, the proposed policy enjoys a data-dependent sublinear upper bound on the static regret, which can range from logarithmic to square-root. Numerical simulations are performed to corroborate the theoretical findings and demonstrate the efficacy of the proposed online policy.	 | 我们考虑了在线稀疏线性逼近问题，即在给定测量矩阵的列的线性组合中，预测序列测量的最佳稀疏逼近。这样的在线预测问题在医学试验、网络缓存和资源分配等多个领域都很常见。由于离线恢复的固有难度，使得在线问题也变得具有挑战性。在这封信中，我们提出了跟随近似稀疏领导者（Follow-The-Approximate-Sparse-Leader, FASL）这一高效的在线元策略来解决这一在线问题。通过详细的理论分析，我们在某些测量序列假设下证明了所提出策略的数据依赖性次线性静态遗憾上界，遗憾上界可以从对数级别到平方根级别变化。我们进行了数值模拟来验证理论发现，并展示所提出在线策略的有效性。
2501.00773	 | Revisiting Graph Neural Networks on Graph-level Tasks: Comprehensive Experiments, Analysis, and Improvements	 | Haoyang Li,Yuming Xu,Chen Jason Zhang,Alexander Zhou,Lei Chen,Qing Li	 | 本文提出了一种统一的图形神经网络（GNN）评估框架，旨在跨多种数据集和任务评估GNNs，并提出了一种增强表达能力和泛化能力的新GNN模型，该模型通过$k$路径根子图方法和统一的图形对比学习算法实现，实验表明其在27个图形数据集上优于现有基准模型。	 | Graphs are essential data structures for modeling complex interactions in domains such as social networks, molecular structures, and biological systems. Graph-level tasks, which predict properties or classes for the entire graph, are critical for applications, such as molecular property prediction and subgraph counting. Graph Neural Networks (GNNs) have shown promise in these tasks, but their evaluations are often limited to narrow datasets, tasks, and inconsistent experimental setups, restricting their generalizability. To address these limitations, we propose a unified evaluation framework for graph-level GNNs. This framework provides a standardized setting to evaluate GNNs across diverse datasets, various graph tasks (e.g., graph classification and regression), and challenging scenarios, including noisy, imbalanced, and few-shot graphs. Additionally, we propose a novel GNN model with enhanced expressivity and generalization capabilities. Specifically, we enhance the expressivity of GNNs through a $k$-path rooted subgraph approach, enabling the model to effectively count subgraphs (e.g., paths and cycles). Moreover, we introduce a unified graph contrastive learning algorithm for graphs across diverse domains, which adaptively removes unimportant edges to augment graphs, thereby significantly improving generalization performance. Extensive experiments demonstrate that our model achieves superior performance against fourteen effective baselines across twenty-seven graph datasets, establishing it as a robust and generalizable model for graph-level tasks.	 | 图形是用于建模社交网络、分子结构和生物系统等领域的复杂相互作用的重要数据结构。图形级别任务旨在预测整个图的属性或类别，对于分子属性预测和子图计数等应用至关重要。图形神经网络（GNNs）在这些任务中显示出潜力，但其评估往往局限于狭窄的数据集、任务和不一致的实验设置，限制了其普适性。为解决这些限制，我们提出了一种统一的GNN评估框架，该框架提供了一个标准化的环境，用于跨多种数据集、各种图形任务（例如图分类和回归）以及具有挑战性的场景（包括噪声、不平衡和少样本图形）评估GNNs。此外，我们还提出了一种具有增强表达能力和泛化能力的新颖GNN模型。具体来说，我们通过$k$路径根子图方法增强了GNNs的表达能力，使模型能够有效地计数子图（例如路径和环）。此外，我们还提出了一个统一的图形对比学习算法，适用于跨不同领域的图形，该算法自适应地去除不重要的边以增强图形，从而显著提高泛化性能。广泛的实验表明，我们的模型在27个图形数据集上的性能优于14个有效基准模型，从而确立了其在图形级别任务中的稳健性和普适性。
2501.00762	 | Avoiding Oversmoothing in Deep Graph Neural Networks: A Multiplicative Ergodic Analysis	 | Ziang Chen,Zhengjiang Lin,Shi Chen,Yury Polyanskiy,Philippe Rigollet	 | 本文分析了深层图神经网络（GNNs）中的过平滑问题，通过推导归一化顶点相似度的收敛率，并证明添加残差连接可以有效缓解这一问题，实验结果也验证了理论发现。	 | Graph neural networks (GNNs) have achieved remarkable empirical success in processing and representing graph-structured data across various domains. However, a significant challenge known as "oversmoothing" persists, where vertex features become nearly indistinguishable in deep GNNs, severely restricting their expressive power and practical utility. In this work, we analyze the asymptotic oversmoothing rates of deep GNNs with and without residual connections by deriving explicit convergence rates for a normalized vertex similarity measure. Our analytical framework is grounded in the multiplicative ergodic theorem. Furthermore, we demonstrate that adding residual connections effectively mitigates or prevents oversmoothing across several broad families of parameter distributions. The theoretical findings are strongly supported by numerical experiments.	 | 图神经网络（GNNs）在处理和表示各种领域中的图结构数据方面取得了显著的实际成功。然而，一个名为“过平滑”的重大挑战仍然存在，在这种情况下，图中顶点的特征在深层GNN中变得几乎无法区分，严重限制了它们的表达能力和实际应用价值。在本文中，我们通过推导归一化顶点相似度测度的显式收敛率，分析了具有和不具有残差连接的深层GNN的渐近过平滑率。我们的分析框架基于乘法遍历定理。此外，我们证明了添加残差连接可以有效减轻或防止多种参数分布族中的过平滑现象。理论发现得到了数值实验的强烈支持。
2501.00756	 | FasterSTS: A Faster Spatio-Temporal Synchronous Graph Convolutional Networks for Traffic flow Forecasting	 | Ben-Ao Dai,Nengchao Lyu,Yongchao Miao	 | 本研究提出了一种新的快速且高效的空间时间和空间时间同步交通流量预测模型，旨在解决现有方法难以捕捉复杂的空间时间异质性并增加模型复杂性的问题。	 | Accurate traffic flow prediction heavily relies on the spatio-temporal correlation of traffic flow data. Most current studies separately capture correlations in spatial and temporal dimensions, making it difficult to capture complex spatio-temporal heterogeneity, and often at the expense of increasing model complexity to improve prediction accuracy. Although there have been groundbreaking attempts in the field of spatio-temporal synchronous modeling, significant limitations remain in terms of performance and complexity control.This study proposes a quicker and more effective spatio-temporal synchronous traffic flow forecast model to address these issues.	 | 准确的交通流量预测高度依赖于交通流量数据的空间时间和空间时间相关性。目前大多数研究分别在空间维度和时间维度上捕捉相关性，这使得难以捕捉复杂的空间时间异质性，且通常会增加模型复杂性以提高预测准确性。尽管在空间时间和时间同步建模领域已经有所突破，但在性能和复杂性控制方面仍然存在显著限制。本研究提出了一种更快且更有效的空间时间和空间时间同步交通流量预测模型，以解决这些问题。
2501.00743	 | AttriReBoost: A Gradient-Free Propagation Optimization Method for Cold Start Mitigation in Attribute Missing Graphs	 | Mengran Li,Chaojun Ding,Junzhou Chen,Wenbin Xing,Cong Ye,Ronghui Zhang,Songlin Zhuang,Jia Hu,Tony Z. Qiu,Huijun Gao	 | AttriReBoost（ARB）是一种新颖的图学习方法，通过重新定义初始边界条件和战略性地集成虚拟边来解决属性缺失图中的冷启动问题，提高节点连通性并确保更稳定的收敛，实验结果显示ARB在多个基准数据集上平均准确率提高了5.11%，且计算效率高，能在单个GPU上处理大规模图数据。	 | Missing attribute issues are prevalent in the graph learning, leading to biased outcomes in Graph Neural Networks (GNNs). Existing methods that rely on feature propagation are prone to cold start problem, particularly when dealing with attribute resetting and low-degree nodes, which hinder effective propagation and convergence. To address these challenges, we propose AttriReBoost (ARB), a novel method that incorporates propagation-based method to mitigate cold start problems in attribute-missing graphs. ARB enhances global feature propagation by redefining initial boundary conditions and strategically integrating virtual edges, thereby improving node connectivity and ensuring more stable and efficient convergence. This method facilitates gradient-free attribute reconstruction with lower computational overhead. The proposed method is theoretically grounded, with its convergence rigorously established. Extensive experiments on several real-world benchmark datasets demonstrate the effectiveness of ARB, achieving an average accuracy improvement of 5.11% over state-of-the-art methods. Additionally, ARB exhibits remarkable computational efficiency, processing a large-scale graph with 2.49 million nodes in just 16 seconds on a single GPU. Our code is available at https://github.com/limengran98/ARB.	 | 在图学习中，缺失属性问题是普遍存在的，这会导致图神经网络（GNNs）产生有偏的结果。现有依赖于特征传播的方法容易遇到冷启动问题，尤其是在处理属性重置和低度节点时，这阻碍了有效的传播和收敛。为了解决这些问题，我们提出了AttriReBoost（ARB），一种结合了传播方法的新颖方法，以缓解属性缺失图中的冷启动问题。ARB通过重新定义初始边界条件和战略性地集成虚拟边来增强全局特征传播，从而提高节点连通性并确保更加稳定和高效的收敛。该方法能够实现无梯度的属性重构，并具有较低的计算开销。该方法具有坚实的理论基础，其收敛性得到了严格的证明。在多个真实世界基准数据集上的广泛实验表明，ARB的有效性高于现有方法，平均准确率提高了5.11%。此外，ARB在计算效率方面也表现出色，能够在单个GPU上仅用16秒处理包含249万节点的大型图。我们的代码可在https://github.com/limengran98/ARB获取。
2501.00742	 | Experimental Demonstration of an Optical Neural PDE Solver via On-Chip PINN Training	 | Yequan Zhao,Xian Xiao,Antoine Descos,Yuan Yuan,Xinling Yu,Geza Kurczveil,Marco Fiorentino,Zheng Zhang,Raymond G. Beausoleil	 | 本文利用无反向传播的光子芯片上物理感知神经网络，实现实验条件下光学神经PDE求解器的训练和运行。	 | Partial differential equation (PDE) is an important math tool in science and engineering. This paper experimentally demonstrates an optical neural PDE solver by leveraging the back-propagation-free on-photonic-chip training of physics-informed neural networks.	 | 偏微分方程（PDE）是科学和工程中重要的数学工具。本文通过利用无反向传播的光子芯片上物理感知神经网络的训练，实验展示了光学神经PDE求解器。
2501.00725	 | Automatic Construction of Pattern Classifiers Capable of Continuous Incremental Learning and Unlearning Tasks Based on Compact-Sized Probabilistic Neural Network	 | Tetsuya Hoya,Shunpei Morita	 | 本文提出了一种基于紧凑型概率神经网络的新型模式分类方法，该网络能够进行连续的增量学习和去学习任务，且无需超参数调整，仿真结果表明该方法在标准分类任务中的性能接近多层感知器神经网络，并在连续的类别增量学习和去学习任务中表现出色。	 | This paper proposes a novel approach to pattern classification using a probabilistic neural network model. The strategy is based on a compact-sized probabilistic neural network capable of continuous incremental learning and unlearning tasks. The network is constructed/reconstructed using a simple, one-pass network-growing algorithm with no hyperparameter tuning. Then, given the training dataset, its structure and parameters are automatically determined and can be dynamically varied in continual incremental and decremental learning situations. The algorithm proposed in this work involves no iterative or arduous matrix-based parameter approximations but a simple data-driven updating scheme. Simulation results using nine publicly available databases demonstrate the effectiveness of this approach, showing that compact-sized probabilistic neural networks constructed have a much smaller number of hidden units compared to the original probabilistic neural network model and yet can achieve a similar classification performance to that of multilayer perceptron neural networks in standard classification tasks, while also exhibiting sufficient capability in continuous class incremental learning and unlearning tasks.	 | 本文提出了一种使用概率神经网络模型进行模式分类的新颖方法。该方法基于一个紧凑型的概率神经网络，该网络能够进行连续的增量学习和去学习任务。该网络通过一个简单的、一次过的信息增长算法构建/重建，无需进行超参数调整。然后，在给定训练数据集的情况下，其结构和参数会自动确定，并且可以在连续的增量和去学习情况下动态变化。本文提出的方法中的算法不涉及迭代或繁琐的矩阵参数近似，而是一种简单的数据驱动更新方案。使用九个公开可用的数据集进行的仿真结果显示了该方法的有效性，表明构建的紧凑型概率神经网络中的隐藏单元数量远少于原始概率神经网络模型，但在标准的分类任务中能达到类似多层感知器神经网络的分类性能，同时在连续的类别增量学习和去学习任务中也表现出足够的能力。
2501.00709	 | KAN KAN Buff Signed Graph Neural Networks?	 | Muhieddine Shebaro,Jelena Tešić	 | 本文将柯尔莫哥洛夫-阿诺尔德神经网络（KANs）集成到有符号图卷积网络（SGCNs）中，形成KASGCN，并通过有符号图上的下游任务评估其性能，结果显示KASGCN在某些上下文中与有符号SGCN表现相当或更优。	 | Graph Representation Learning aims to create embeddings for nodes and edges, capturing their features and interconnections. Graph Neural Networks (GNNs) have excelled in this task, leveraging neural networks to model complex graph relationships. Recently, the Kolmogorov-Arnold Neural Network (KAN) emerged as an alternative to Multi-Layer Perceptron (MLP), showing improved accuracy and interpretability with fewer parameters. While KANs have been integrated into unsigned GNNs, their application in signed GNNs remains unexplored. This paper integrates KAN into Signed Graph Convolutional Networks (SGCNs) to evaluate its performance on signed graphs where edges have positive or negative signs. We empirically assess KAN-enhanced SGCNs (KASGCN) on downstream tasks such as signed community detection and link sign prediction to enhance the embedding quality in signed networks. Considering the variability in the results indicated by the relatively large standard deviation, KASGCN demonstrates competitive performance with, or similar to, the vanilla SGCN in the evaluated downstream tasks, and its effectiveness is context-dependent (signed graph and parameters...etc.).	 | 图表示学习旨在为节点和边创建嵌入，捕捉它们的特征和相互连接。图神经网络（GNNs）在这方面表现出色，通过神经网络来建模复杂的图关系。最近，柯尔莫哥洛夫-阿诺尔德神经网络（KAN）作为一种多层感知器（MLP）的替代方案出现，显示了更高的准确性和可解释性，并且参数更少。虽然KANs已经在无符号GNNs中得到集成，但它们在有符号GNNs中的应用尚未被探索。本文将KAN集成到有符号图卷积网络（SGCNs）中，以评估其在有符号图上的性能，这些图中的边具有正或负符号。我们通过下游任务如有符号社区检测和链接符号预测，实证评估KAN增强的SGCNs（KASGCN）以提高有符号网络中的嵌入质量。考虑到结果的变异性，由相对较大的标准差表明，KASGCN在评估的下游任务中表现出与有符号常规SGCN相当或相似的性能，其有效性取决于上下文（有符号图和参数...等）。
2501.00704	 | Kolmogorov GAM Networks are all you need!	 | Sarah Polson,Vadim Sokolov	 | K-GAM网络是一种高效的训练和推理架构，基于Kolmogorov超positions定理提供了一种具有 fewer 参数的加性模型表示方法，适用于机器学习和AI应用，并且可以通过并行化计算提高效率。	 | Kolmogorov GAM (K-GAM) networks are shown to be an efficient architecture for training and inference. They are an additive model with an embedding that is independent of the function of interest. They provide an alternative to the transformer architecture. They are the machine learning version of Kolmogorov's Superposition Theorem (KST) which provides an efficient representations of a multivariate function. Such representations have use in machine learning for encoding dictionaries (a.k.a. "look-up" tables). KST theory also provides a representation based on translates of the Köppen function. The goal of our paper is to interpret this representation in a machine learning context for applications in Artificial Intelligence (AI). Our architecture is equivalent to a topological embedding which is independent of the function together with an additive layer that uses a Generalized Additive Model (GAM). This provides a class of learning procedures with far fewer parameters than current deep learning algorithms. Implementation can be parallelizable which makes our algorithms computationally attractive. To illustrate our methodology, we use the Iris data from statistical learning. We also show that our additive model with non-linear embedding provides an alternative to transformer architectures which from a statistical viewpoint are kernel smoothers. Additive KAN models therefore provide a natural alternative to transformers. Finally, we conclude with directions for future research.	 | Kolmogorov广义可加模型（K-GAM）网络被证明是一种高效的训练和推理架构。它们是一种加性模型，其中嵌入与所关注的功能无关。它们为变压器架构提供了一个替代方案。它们是Kolmogorov超positions定理（KST）的机器学习版本，KST提供了一种多变量函数的有效表示。这种表示在机器学习中可用于编码字典（即“查找”表）。KST理论还提供了一种基于柯帕恩函数移位的表示方法。我们论文的目标是在人工智能（AI）应用的机器学习背景下解释这种表示方法。我们的架构等同于一个独立于函数的拓扑嵌入，以及一个使用广义可加模型（GAM）的加性层。这提供了一类具有远 fewer参数的学习过程，比当前的深度学习算法要少得多。实现可以并行化，这使得我们的算法在计算上具有吸引力。为了说明我们的方法论，我们使用了统计学习中的鸢尾花数据。我们还展示了我们的加性模型结合非线性嵌入为统计视点下的核平滑提供了替代方案。因此，加性KAN模型提供了一种自然替代变压器的方法。最后，我们提出了未来研究的方向。
2501.00701	 | NN-ResDMD: Learning Koopman Representations for Complex Dynamics with Spectral Residuals	 | Yuanchao Xu,Kaidi Shao,Nikos Logothetis,Zhongwei Shen	 | 该研究提出了神经网络-残差动态模式分解（NN-ResDMD）方法，通过直接最小化频谱残差来估计Koopman谱成分，从而克服了传统方法在大规模系统中的稳健性不足问题，提高了分析复杂动力系统的准确性和可扩展性。	 | Analyzing long-term behaviors in high-dimensional nonlinear dynamical systems remains a significant challenge. The Koopman operator framework has emerged as a powerful tool to address this issue by providing a globally linear perspective on nonlinear dynamics. However, existing methods for approximating the Koopman operator and its spectral components, particularly in large-scale systems, often lack robust theoretical guarantees. Residual Dynamic Mode Decomposition (ResDMD) introduces a spectral residual measure to assess the convergence of the estimated Koopman spectrum, which helps filter out spurious spectral components. Nevertheless, it depends on pre-computed spectra, thereby inheriting their inaccuracies. To overcome its limitations, we introduce the Neural Network-ResDMD (NN-ResDMD), a method that directly estimates Koopman spectral components by minimizing the spectral residual. By leveraging neural networks, NN-ResDMD automatically identifies the optimal basis functions of the Koopman invariant subspace, eliminating the need for manual selection and improving the reliability of the analysis. Experiments on physical and biological systems demonstrate that NN-ResDMD significantly improves both accuracy and scalability, making it an effective tool for analyzing complex dynamical systems.	 | 分析高维非线性动力系统中的长期行为仍然是一个显著的挑战。Koopman算子框架已经成为了应对这一问题的强大工具，因为它提供了对非线性动力学的全局线性视角。然而，现有方法在近似Koopman算子及其频谱成分，尤其是大规模系统中，通常缺乏稳健的理论保证。残差动态模式分解（ResDMD）引入了一种频谱残差度量来评估估计的Koopman谱的收敛性，这有助于滤除虚假的频谱成分。然而，它依赖于预计算的频谱，因此继承了其不准确性。为了克服这些限制，我们引入了神经网络-残差动态模式分解（NN-ResDMD）方法，该方法直接通过最小化频谱残差来估计Koopman谱成分。通过利用神经网络，NN-ResDMD 自动识别Koopman不变子空间的最优基函数，从而消除了人工选择的需要，并提高了分析的可靠性。在物理和生物系统中的实验表明，NN-ResDMD 显著提高了准确性和可扩展性，使其成为分析复杂动力系统的有效工具。
2501.00696	 | Cost and Reward Infused Metric Elicitation	 | Chethan Bhateja,Joseph O'Brien,Afnaan Hashmi,Eva Prakash	 | 本文扩展了现有的多类度量选择框架，引入了对角线线性性能度量选择（DLPME）算法以考虑额外的有界成本和奖励，从而更好地反映模型在实际应用中的性能。实验结果表明，该方法能够快速收敛到最优度量。	 | In machine learning, metric elicitation refers to the selection of performance metrics that best reflect an individual's implicit preferences for a given application. Currently, metric elicitation methods only consider metrics that depend on the accuracy values encoded within a given model's confusion matrix. However, focusing solely on confusion matrices does not account for other model feasibility considerations such as varied monetary costs or latencies. In our work, we build upon the multiclass metric elicitation framework of Hiranandani et al., extrapolating their proposed Diagonal Linear Performance Metric Elicitation (DLPME) algorithm to account for additional bounded costs and rewards. Our experimental results with synthetic data demonstrate our approach's ability to quickly converge to the true metric.	 | 在机器学习中，度量选择指的是选择能够最好地反映给定应用中个体隐含偏好的性能度量。目前，度量选择方法仅考虑依赖于模型混淆矩阵中编码的准确性值的度量。然而，仅关注混淆矩阵无法考虑到其他模型的可行性因素，例如不同的货币成本或延迟。在我们的工作中，我们基于Hiranandani等人提出的多类度量选择框架，将他们提出的对角线线性性能度量选择（DLPME）算法扩展到考虑额外的有界成本和奖励。我们的实验结果表明，我们的方法能够快速收敛到真正的度量。
2501.00692	 | Adjoint sharding for very long context training of state space models	 | Xingzi Xu,Amir Tavanaei,Kavosh Asadi,Karim Bouyarmane	 | 尽管取得了快速进步，训练大规模语言模型在非常长的上下文中仍然面临挑战，主要受限于GPU内存和训练时间。为此，研究提出了一种名为反向分片的新技术，通过在训练过程中分片梯度计算来大幅减少内存需求，使在超长上下文中训练大规模语言模型成为可能。	 | Despite very fast progress, efficiently training large language models (LLMs) in very long contexts remains challenging. Existing methods fall back to training LLMs with short contexts (a maximum of a few thousands tokens in training) and use inference time techniques when evaluating on long contexts (above 1M tokens context window at inference). As opposed to long-context-inference, training on very long context input prompts is quickly limited by GPU memory availability and by the prohibitively long training times it requires on state-of-the-art hardware. Meanwhile, many real-life applications require not only inference but also training/fine-tuning with long context on specific tasks. Such applications include, for example, augmenting the context with various sources of raw reference information for fact extraction, fact summarization, or fact reconciliation tasks. We propose adjoint sharding, a novel technique that comprises sharding gradient calculation during training to reduce memory requirements by orders of magnitude, making training on very long context computationally tractable. Adjoint sharding is based on the adjoint method and computes equivalent gradients to backpropagation. We also propose truncated adjoint sharding to speed up the algorithm while maintaining performance. We provide a distributed version, and a paralleled version of adjoint sharding to further speed up training. Empirical results show the proposed adjoint sharding algorithm reduces memory usage by up to 3X with a 1.27B parameter large language model on 1M context length training. This allows to increase the maximum context length during training or fine-tuning of a 1.27B parameter model from 35K tokens to above 100K tokens on a training infrastructure composed of five AWS P4 instances.	 | 尽管取得了非常快速的进步，但在非常长的上下文中高效训练大规模语言模型（LLM）仍然具有挑战性。现有的方法通常通过短上下文（训练时最多几千个词元）来训练LLM，并且在评估长上下文（推理时上下文窗口超过100万词元）时使用推理时间的技术。与长上下文推理不同，训练非常长上下文输入提示很快受到GPU内存可用性和所需训练时间过长的限制。同时，许多实际应用场景不仅需要推理，还需要在特定任务中进行长上下文的训练/微调。例如，通过各种来源的原始参考信息来扩充上下文，用于事实提取、事实总结或事实校对等任务。我们提出了一种名为反向分片（Adjoint Sharding）的新技术，该技术在训练过程中将梯度计算进行分片，从而大幅减少内存需求，使在非常长的上下文中进行训练变得计算上可行。反向分片基于反向方法，计算等效于反向传播的梯度。我们还提出了一种截断反向分片，以加快算法速度同时保持性能。此外，我们提供了分布式版本和并行版本的反向分片，以进一步加快训练速度。实验结果表明，在100万词元长度的训练中，提出的反向分片算法将一个127亿参数的大型语言模型的内存使用量降低至最多3倍。这使得在训练基础设施由五个AWS P4实例组成的系统中，训练或微调127亿参数模型的最长上下文长度从3.5万词元增加到超过10万词元变得可能。
2501.00684	 | IGC: Integrating a Gated Calculator into an LLM to Solve Arithmetic Tasks Reliably and Efficiently	 | Florian Dietz,Dietrich Klakow	 | 一种名为集成门控计算器（IGC）的新模块使大型语言模型（LLMs）能够在GPU上执行算术运算，显著提升了Llama模型在BigBench Arithmetic基准测试中的表现，优于现有最佳水平，甚至包括更大规模的模型。	 | Solving arithmetic tasks is a simple and fundamental skill, yet modern Large Language Models (LLMs) have great difficulty with them. We introduce the Integrated Gated Calculator (IGC), a module that enables LLMs to perform arithmetic by emulating a calculator on the GPU. We finetune a Llama model with our module and test it on the BigBench Arithmetic benchmark, where it beats the State of the Art, outperforming all models on the benchmark, including models almost two orders of magnitude larger. Our approach takes only a single iteration to run and requires no external tools. It performs arithmetic operations entirely inside the LLM without the need to produce intermediate tokens. It is computationally efficient, interpretable, and avoids side-effects on tasks that do not require arithmetic operations. It reliably achieves 98\% to 99\% accuracy across multiple training runs and for all subtasks, including the substantially harder subtask of multiplication, which was previously unsolved.	 | 解决算术任务是一项简单而基础的技能，但现代大型语言模型（LLMs）在处理这些任务时却有很大困难。我们引入了一种集成门控计算器（IGC）模块，该模块使LLMs能够在GPU上模拟计算器来执行算术运算。我们对Llama模型进行了微调，并在BigBench Arithmetic基准测试中测试了该模型，其性能超越了现有最佳水平，甚至在基准测试中优于所有模型，包括几乎是其规模大两个数量级的模型。我们的方法仅需一次迭代就可以运行，并不需要外部工具。它完全在LLM内部执行算术运算，无需生成中间令牌。它计算效率高、可解释，并且在不需要算术运算的任务上不会产生副作用。它在多个训练运行中和所有子任务中都能可靠地实现98%到99%的准确率，包括以前无法解决的乘法子任务。
2501.00677	 | Deeply Learned Robust Matrix Completion for Large-scale Low-rank Data Recovery	 | HanQin Cai,Chandra Kundu,Jialin Liu,Wotao Yin	 | 本文提出了一种新型的学习鲁棒矩阵完成（LRMC）方法，通过引入可学习的非凸优化框架，解决了大规模RMC问题，并在多个实际应用中展示了优于现有方法的性能。	 | Robust matrix completion (RMC) is a widely used machine learning tool that simultaneously tackles two critical issues in low-rank data analysis: missing data entries and extreme outliers. This paper proposes a novel scalable and learnable non-convex approach, coined Learned Robust Matrix Completion (LRMC), for large-scale RMC problems. LRMC enjoys low computational complexity with linear convergence. Motivated by the proposed theorem, the free parameters of LRMC can be effectively learned via deep unfolding to achieve optimum performance. Furthermore, this paper proposes a flexible feedforward-recurrent-mixed neural network framework that extends deep unfolding from fix-number iterations to infinite iterations. The superior empirical performance of LRMC is verified with extensive experiments against state-of-the-art on synthetic datasets and real applications, including video background subtraction, ultrasound imaging, face modeling, and cloud removal from satellite imagery.	 | 鲁棒矩阵完成（RMC）是一种广泛使用的机器学习工具，同时解决了低秩数据分析中的两个关键问题：缺失数据项和极端异常值。本文提出了一种新颖的可扩展和可学习的非凸方法，称为学习鲁棒矩阵完成（LRMC），用于大规模RMC问题。LRMC具有较低的计算复杂度和线性收敛性。受提出的定理启发，LRMC的自由参数可以通过深度展开学习来有效学习，以实现最佳性能。此外，本文提出了一种灵活的前馈-循环-混合神经网络框架，将深度展开从固定迭代次数扩展到无限迭代次数。通过在合成数据集和实际应用（包括视频背景减法、超声成像、面部建模和卫星图像中的云去除）上进行广泛实验，验证了LRMC的优越的实证性能，优于最先进的方法。
2501.00673	 | Controlled Causal Hallucinations Can Estimate Phantom Nodes in Multiexpert Mixtures of Fuzzy Cognitive Maps	 | Akash Kumar Panda,Bart Kosko	 | 一种自适应的多专家反馈因果模型混合方法能够逼近大规模因果模型中的缺失或虚假节点，通过近似样本动态系统的主极限环实现这一目标；每个专家首先构建包含模糊认知图，通过监督学习估计虚假节点，并通过混合生成新的因果反馈模型来近似动态系统的未来轨迹。	 | An adaptive multiexpert mixture of feedback causal models can approximate missing or phantom nodes in large-scale causal models. The result gives a scalable form of \emph{big knowledge}. The mixed model approximates a sampled dynamical system by approximating its main limit-cycle equilibria. Each expert first draws a fuzzy cognitive map (FCM) with at least one missing causal node or variable. FCMs are directed signed partial-causality cyclic graphs. They mix naturally through convex combination to produce a new causal feedback FCM. Supervised learning helps each expert FCM estimate its phantom node by comparing the FCM's partial equilibrium with the complete multi-node equilibrium. Such phantom-node estimation allows partial control over these causal hallucinations and helps approximate the future trajectory of the dynamical system. But the approximation can be computationally heavy. Mixing the tuned expert FCMs gives a practical way to find several phantom nodes and thereby better approximate the feedback system's true equilibrium behavior.	 | 一种自适应多专家反馈因果模型混合可以逼近大规模因果模型中的缺失节点或虚假节点。该结果提供了一种可扩展形式的“大规模知识”。混合模型通过近似样本动态系统的主极限环来近似该系统。每个专家首先绘制至少包含一个缺失因果节点或变量的模糊认知图（FCM）。FCM 是有向带符号部分因果循环图，它们自然地通过凸组合混合来生成一个新的因果反馈 FCM。监督学习帮助每个专家 FCM 通过比较 FCM 的部分均衡与完整多节点均衡，来估计其虚假节点。这种虚假节点估计允许对这些因果幻象进行部分控制，并有助于近似动态系统的未来轨迹。但近似计算可能会非常繁重。调优后的专家 FCM 混合提供了一种实用的方法来找到多个虚假节点，从而更好地逼近反馈系统的真实均衡行为。
2501.00669	 | Leaf diseases detection using deep learning methods	 | El Houcine El Fatimi	 | 本研究开发了一种新的深度学习方法，利用叶片图像数据集进行植物叶片疾病识别和检测，以克服现有方法的挑战，并提出了一种基于CNN的新模型，该模型在识别和检测植物叶片疾病方面表现出色，且评估结果显示其有效性优于一些最先进的预训练架构。	 | This study, our main topic is to devlop a new deep-learning approachs for plant leaf disease identification and detection using leaf image datasets. We also discussed the challenges facing current methods of leaf disease detection and how deep learning may be used to overcome these challenges and enhance the accuracy of disease detection. Therefore, we have proposed a novel method for the detection of various leaf diseases in crops, along with the identification and description of an efficient network architecture that encompasses hyperparameters and optimization methods. The effectiveness of different architectures was compared and evaluated to see the best architecture configuration and to create an effective model that can quickly detect leaf disease. In addition to the work done on pre-trained models, we proposed a new model based on CNN, which provides an efficient method for identifying and detecting plant leaf disease. Furthermore, we evaluated the efficacy of our model and compared the results to those of some pre-trained state-of-the-art architectures.	 | 本研究的主要主题是开发一种新的深度学习方法，用于利用叶片图像数据集进行植物叶片疾病识别和检测。我们还讨论了当前叶片疾病检测方法所面临的挑战，并探讨了深度学习如何克服这些挑战，以提高疾病检测的准确性。因此，我们提出了一种新的方法来检测农作物中的各种叶片疾病，同时还提出了一种有效的网络架构，该架构涵盖了超参数和优化方法，并对不同架构进行了比较和评估，以确定最佳的架构配置，并创建一个能够快速检测叶片疾病的高效模型。除了对预训练模型的研究之外，我们还提出了一种基于CNN的新模型，该模型提供了一种有效的方法来识别和检测植物叶片疾病。此外，我们评估了我们模型的有效性，并将结果与一些最先进的预训练架构的结果进行了比较。
2501.00663	 | Titans: Learning to Memorize at Test Time	 | Ali Behrouz,Peilin Zhong,Vahab Mirrokni	 | 研究提出了一个新的神经长时记忆模块，结合递归模型和注意力机制，有效处理长上下文，并应用到新的架构家族Titans中，显著提升了语言建模、常识推理、基因组学和时间序列任务的表现，尤其在大上下文窗口和细粒度任务上展现出色性能。	 | Over more than a decade there has been an extensive research effort on how to effectively utilize recurrent models and attention. While recurrent models aim to compress the data into a fixed-size memory (called hidden state), attention allows attending to the entire context window, capturing the direct dependencies of all tokens. This more accurate modeling of dependencies, however, comes with a quadratic cost, limiting the model to a fixed-length context. We present a new neural long-term memory module that learns to memorize historical context and helps attention to attend to the current context while utilizing long past information. We show that this neural memory has the advantage of fast parallelizable training while maintaining a fast inference. From a memory perspective, we argue that attention due to its limited context but accurate dependency modeling performs as a short-term memory, while neural memory due to its ability to memorize the data, acts as a long-term, more persistent, memory. Based on these two modules, we introduce a new family of architectures, called Titans, and present three variants to address how one can effectively incorporate memory into this architecture. Our experimental results on language modeling, common-sense reasoning, genomics, and time series tasks show that Titans are more effective than Transformers and recent modern linear recurrent models. They further can effectively scale to larger than 2M context window size with higher accuracy in needle-in-haystack tasks compared to baselines.	 | 在超过十年的时间里，人们一直在研究如何有效地利用递归模型和注意力机制。递归模型的目标是将数据压缩到一个固定大小的记忆（称为隐藏状态），而注意力机制则允许模型在整个上下文中进行关注，捕捉所有词项之间的直接依赖关系。然而，这种更准确的依赖建模带来了二次成本，限制了模型只能处理固定长度的上下文。我们提出了一种新的神经长时记忆模块，该模块学习记忆历史上下文，并在利用过去信息的同时帮助注意力机制关注当前上下文。我们展示了这种神经记忆模块具有快速并行训练的优点，同时还能保持快速推理。从记忆的角度来看，我们论断：由于其有限的上下文和准确的依赖建模，注意力机制相当于短期记忆；而由于其能够记忆数据的能力，神经记忆则相当于长期、更持久的记忆。基于这两个模块，我们引入了一种新的架构家族，称为Titans，并提出了三种变体，以探讨如何有效地将记忆融入该架构中。我们在语言建模、常识推理、基因组学和时间序列任务上的实验结果表明，Titans相对于Transformer和最近的现代线性递归模型更有效。此外，它们还能有效扩展到超过2M的上下文窗口大小，并在细粒度任务（如“针扎麦堆”任务）中具有更高的准确性。
2501.00659	 | Why Are Positional Encodings Nonessential for Deep Autoregressive Transformers? Revisiting a Petroglyph	 | Kazuki Irie	 | 自回归Transformer语言模型在多层结构下无需显式位置编码即可区分乱序词汇序列，这一特性早已有之但并未广泛传播；本文旨在回顾并重新确认这一原理，强调显式位置编码在多层模型中的非必要性。	 | Do autoregressive Transformer language models require explicit positional encodings (PEs)? The answer is "no" as long as they have more than one layer -- they can distinguish sequences with permuted tokens without requiring explicit PEs. This property has been known since early efforts (those contemporary with GPT-2) adopting the Transformer for language modeling. However, this result does not appear to have been well disseminated and was even rediscovered recently. This may be partially due to a sudden growth of the language modeling community after the advent of GPT-2, but perhaps also due to the lack of a clear explanation in prior publications, despite being commonly understood by practitioners in the past. Here we review this long-forgotten explanation why explicit PEs are nonessential for multi-layer autoregressive Transformers (in contrast, one-layer models require PEs to discern order information of their input tokens). We also review the origin of this result, and hope to re-establish it as a common knowledge.	 | 自回归Transformer语言模型是否需要显式的位置编码（PE）？答案是否定的，只要它们有超过一层——它们可以在不依赖显式位置编码的情况下区分具有乱序词汇的序列。这一特性自早期采用Transformer进行语言建模的努力（与GPT-2同期）以来就已经为人所知。然而，这一结果似乎并没有广泛传播，甚至在最近又被重新发现。这可能部分归因于GPT-2问世后语言模型社区的突然增长，但也可能是因为早期出版物中缺乏清晰的解释，尽管在过去，这一原理普遍被从业者所理解。在这里，我们回顾了这一被遗忘的解释，即显式位置编码对多层自回归Transformer是不必要的（相比之下，单层模型需要位置编码来区分输入词汇的顺序信息）。我们还回顾了这一结果的起源，希望重新将其确立为常识。
2501.00658	 | Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing	 | Peihao Wang,Ruisi Cai,Yuehao Wang,Jiajun Zhu,Pragya Srivastava,Zhangyang Wang,Pan Li	 | 结构化状态空间模型（SSMs）虽然在长序列依赖方面表现良好，但由于其固有的近期偏见和过度平滑化问题限制了其深度和性能，本文提出了一种极化技术以解决这些问题，从而提高了SSMs在捕捉长距离依赖关系方面的效果。	 | Structured State Space Models (SSMs) have emerged as alternatives to transformers. While SSMs are often regarded as effective in capturing long-sequence dependencies, we rigorously demonstrate that they are inherently limited by strong recency bias. Our empirical studies also reveal that this bias impairs the models' ability to recall distant information and introduces robustness issues. Our scaling experiments then discovered that deeper structures in SSMs can facilitate the learning of long contexts. However, subsequent theoretical analysis reveals that as SSMs increase in depth, they exhibit another inevitable tendency toward over-smoothing, e.g., token representations becoming increasingly indistinguishable. This fundamental dilemma between recency and over-smoothing hinders the scalability of existing SSMs. Inspired by our theoretical findings, we propose to polarize two channels of the state transition matrices in SSMs, setting them to zero and one, respectively, simultaneously addressing recency bias and over-smoothing. Experiments demonstrate that our polarization technique consistently enhances the associative recall accuracy of long-range tokens and unlocks SSMs to benefit further from deeper architectures. All source codes are released at https://github.com/VITA-Group/SSM-Bottleneck.	 | 结构化状态空间模型（SSMs）已成为Transformer的替代选择。尽管SSMs通常被认为在捕捉长序列依赖方面非常有效，但我们严格证明了它们在根本上受到强烈近期偏见的限制。我们的实证研究还表明，这种偏见会削弱模型回忆远距离信息的能力，并引入了稳健性问题。随后的扩展实验发现，SSMs中的更深结构有助于学习长上下文。然而，后续的理论分析揭示，随着SSMs的深度增加，它们不可避免地向过度平滑化趋势发展，例如，token表示变得越来越难以区分。这种基于近期偏见和过度平滑化的基本困境阻碍了现有SSMs的扩展性。受我们理论发现的启发，我们建议在SSMs的状态转换矩阵中极化两个通道，分别将其设置为零和一，从而同时解决近期偏见和过度平滑化问题。实验结果表明，我们的极化技术能一致地提高长距离token的关联回忆准确性，并使SSMs能够从中受益于更深层次的架构。所有源代码均发布在https://github.com/VITA-Group/SSM-Bottleneck。
2501.00636	 | Applying Graph Explanation to Operator Fusion	 | Keith G. Mills,Muhammad Fetrat Qharabagh,Weichen Qiu,Fred X. Han,Mohammad Salameh,Wei Lu,Shangling Jui,Di Niu	 | 本文提出了一种结合图解释技术（GET）的可解释人工智能方法，用于优化深度神经网络中的层融合，以减少DRAM访问次数，从而提高推理效率。该方法在多种卷积神经网络上实验验证有效，尤其是在EfficientNet-B3上实现了超过20%的DRAM访问减少。	 | Layer fusion techniques are critical to improving the inference efficiency of deep neural networks (DNN) for deployment. Fusion aims to lower inference costs by reducing data transactions between an accelerator's on-chip buffer and DRAM. This is accomplished by grouped execution of multiple operations like convolution and activations together into single execution units - fusion groups. However, on-chip buffer capacity limits fusion group size and optimizing fusion on whole DNNs requires partitioning into multiple fusion groups. Finding the optimal groups is a complex problem where the presence of invalid solutions hampers traditional search algorithms and demands robust approaches. In this paper we incorporate Explainable AI, specifically Graph Explanation Techniques (GET), into layer fusion. Given an invalid fusion group, we identify the operations most responsible for group invalidity, then use this knowledge to recursively split the original fusion group via a greedy tree-based algorithm to minimize DRAM access. We pair our scheme with common algorithms and optimize DNNs on two types of layer fusion: Line-Buffer Depth First (LBDF) and Branch Requirement Reduction (BRR). Experiments demonstrate the efficacy of our scheme on several popular and classical convolutional neural networks like ResNets and MobileNets. Our scheme achieves over 20% DRAM Access reduction on EfficientNet-B3.	 | 层融合技术对提高深度神经网络（DNN）部署时的推理效率至关重要。融合的目标是通过减少加速器片上缓存与DRAM之间的数据传输来降低推理成本。这是通过将多个操作（如卷积和激活）按组合并执行到单个执行单元（融合组）来实现的。然而，片上缓冲区容量限制了融合组的大小，因此在优化整个DNN时需要将DNN分割成多个融合组。发现最优组是一个复杂的问题，其中无效的解决方案会阻碍传统搜索算法，需要强大的方法。在这篇论文中，我们将可解释的人工智能（Explainable AI），特别是图解释技术（GET），融入到层融合中。给定一个无效的融合组，我们确定最负责导致该组无效的操作，然后利用这些知识通过贪婪树基算法递归地将原始融合组拆分，以最小化DRAM访问次数。我们将该方案与常见算法结合，并在两种类型的层融合（线缓冲深度优先（LBDF）和分支需求减少（BRR））上优化DNN。实验表明，我们的方案在多个流行的卷积神经网络（如ResNets和MobileNets）中有效。我们的方案在EfficientNet-B3上实现了超过20%的DRAM访问减少。
2501.00628	 | Matrix factorization and prediction for high dimensional co-occurrence count data via shared parameter alternating zero inflated Gamma model	 | Taejoon Kim,Haiyan Wang	 | 本文提出了一种利用高维稀疏矩阵数据（如加权词词共现计数和电子商务共现数据）来预测项目或用户之间的相关性的方法，通过假设项目或用户可以用未知的密集向量表示，并使用零膨胀Γ分布和余弦相似度来建模共现计数，同时提出了一种交替零膨胀Gamma回归模型来估计未知参数。	 | High-dimensional sparse matrix data frequently arise in various applications. A notable example is the weighted word-word co-occurrence count data, which summarizes the weighted frequency of word pairs appearing within the same context window. This type of data typically contains highly skewed non-negative values with an abundance of zeros. Another example is the co-occurrence of item-item or user-item pairs in e-commerce, which also generates high-dimensional data. The objective is to utilize this data to predict the relevance between items or users. In this paper, we assume that items or users can be represented by unknown dense vectors. The model treats the co-occurrence counts as arising from zero-inflated Gamma random variables and employs cosine similarity between the unknown vectors to summarize item-item relevance. The unknown values are estimated using the shared parameter alternating zero-inflated Gamma regression models (SA-ZIG). Both canonical link and log link models are considered. Two parameter updating schemes are proposed, along with an algorithm to estimate the unknown parameters. Convergence analysis is presented analytically. Numerical studies demonstrate that the SA-ZIG using Fisher scoring without learning rate adjustment may fail to fi nd the maximum likelihood estimate. However, the SA-ZIG with learning rate adjustment performs satisfactorily in our simulation studies.	 | 高维稀疏矩阵数据在各种应用中频繁出现。一个典型例子是加权词词共现计数数据，它总结了在相同上下文中出现的词对的加权频率。这种数据通常包含大量偏斜的非负值，并且零值极为丰富。另一个例子是电子商务中项目-项目或用户-项目对的共现，这也产生了高维数据。目标是利用此类数据来预测项目或用户之间的相关性。在本文中，我们假设项目或用户可以用未知的密集向量表示。模型将共现计数视为零膨胀Γ分布随机变量的结果，并使用未知向量之间的余弦相似度来总结项目项目相关性。未知值通过共享参数交替零膨胀Gamma回归模型（SA-ZIG）进行估计。考虑了正则连接和对数连接模型。提出了两种参数更新方案，并设计了一种算法来估计未知参数。进行了收敛性分析。数值研究显示，不进行学习率调整的Fisher评分SA-ZIG可能无法找到最大似然估计。然而，在我们的模拟研究中，带有学习率调整的SA-ZIG表现良好。
2501.00623	 | Global dense vector representations for words or items using shared parameter alternating Tweedie model	 | Taejoon Kim,Haiyan Wang	 | 本文提出了一种基于共享参数交替特威德（SA-Tweedie）模型来分析高维共现计数数据的方法，该模型通过定义共现窗口并采用加权计数来处理大数据问题，并通过学习率调整和Fisher评分法改进了参数估计过程。	 | In this article, we present a model for analyzing the cooccurrence count data derived from practical fields such as user-item or item-item data from online shopping platform, cooccurring word-word pairs in sequences of texts. Such data contain important information for developing recommender systems or studying relevance of items or words from non-numerical sources. Different from traditional regression models, there are no observations for covariates. Additionally, the cooccurrence matrix is typically of so high dimension that it does not fit into a computer's memory for modeling. We extract numerical data by defining windows of cooccurrence using weighted count on the continuous scale. Positive probability mass is allowed for zero observations. We present Shared parameter Alternating Tweedie (SA-Tweedie) model and an algorithm to estimate the parameters. We introduce a learning rate adjustment used along with the Fisher scoring method in the inner loop to help the algorithm stay on track of optimizing direction. Gradient descent with Adam update was also considered as an alternative method for the estimation. Simulation studies and an application showed that our algorithm with Fisher scoring and learning rate adjustment outperforms the other two methods. Pseudo-likelihood approach with alternating parameter update was also studied. Numerical studies showed that the pseudo-likelihood approach is not suitable in our shared parameter alternating regression models with unobserved covariates.	 | 在本文中，我们提出了一种分析来自实际领域的共现计数数据的模型，这些数据来源于在线购物平台的用户项或项项数据，文本序列中的共现词对等。这类数据包含用于开发推荐系统或研究非数值来源中项或词的相关性的重要信息。与传统的回归模型不同，这类数据中没有协变量的观测值。此外，共现矩阵通常具有非常高的维度，以至于无法适应计算机内存进行建模。我们通过在连续尺度上定义共现窗口，并采用加权计数提取数值数据。允许零观察值具有正概率质量。我们提出了一种共享参数交替特威德（SA-Tweedie）模型及其参数估计算法。我们引入了一种学习率调整方法，与Fisher评分法结合使用，以帮助算法保持优化方向。梯度下降法和Adam更新也被考虑作为替代的估计方法。仿真研究和应用表明，使用Fisher评分法和学习率调整的算法优于其他两种方法。还研究了伪似然方法，并交替更新参数。数值研究显示，在包含未观察协变量的共享参数交替回归模型中，伪似然方法并不适用。
2501.00615	 | Predicting Barge Presence and Quantity on Inland Waterways using Vessel Tracking Data: A Machine Learning Approach	 | Geoffery Agorkua,Sarah Hernandez,Maria Falquez,Subhadipto Poddar,Shihao Pang	 | 本研究利用AIS数据和交通摄像头观测数据，通过机器学习方法预测内河航道上的驳船数量，并通过AdaBoost和随机森林模型取得了较好的预测效果，为交通规划者提供了宝贵的数据支持。	 | This study presents a machine learning approach to predict the number of barges transported by vessels on inland waterways using tracking data from the Automatic Identification System (AIS). While AIS tracks the location of tug and tow vessels, it does not monitor the presence or number of barges transported by those vessels. Understanding the number and types of barges conveyed along river segments, between ports, and at ports is crucial for estimating the quantities of freight transported on the nation's waterways. This insight is also valuable for waterway management and infrastructure operations impacting areas such as targeted dredging operations, and data-driven resource allocation. Labeled sample data was generated using observations from traffic cameras located along key river segments and matched to AIS data records. A sample of 164 vessels representing up to 42 barge convoys per vessel was used for model development. The methodology involved first predicting barge presence and then predicting barge quantity. Features derived from the AIS data included speed measures, vessel characteristics, turning measures, and interaction terms. For predicting barge presence, the AdaBoost model achieved an F1 score of 0.932. For predicting barge quantity, the Random Forest combined with an AdaBoost ensemble model achieved an F1 score of 0.886. Bayesian optimization was used for hyperparameter tuning. By advancing predictive modeling for inland waterways, this study offers valuable insights for transportation planners and organizations, which require detailed knowledge of traffic volumes, including the flow of commodities, their destinations, and the tonnage moving in and out of ports.	 | 本研究提出了一种机器学习方法，利用自动识别系统（AIS）的跟踪数据预测内河航道上船舶运输的驳船数量。虽然AIS可以跟踪拖船和驳船组合的位置，但它并不监测那些船舶所运输的驳船的存在或数量。了解内河河段、港口之间以及港口内的驳船数量和类型对于估算美国水道上运输的货物量至关重要。这一洞察对于航道管理及影响诸如定向疏浚操作和数据驱动资源配置等领域的基础设施运营也具有价值。通过在关键河段安装的交通摄像头观测数据生成带标签的样本数据，并将其与AIS数据记录匹配。用于模型开发的样本包括164艘船舶，每艘船舶最多代表42个驳船编队。方法论包括首先预测驳船的存在，然后预测驳船的数量。从AIS数据中派生的特征包括速度指标、船舶特征、转向指标以及交互项。在预测驳船存在时，AdaBoost模型实现了F1分数0.932。在预测驳船数量时，结合AdaBoost集成模型的随机森林实现了F1分数0.886。使用了贝叶斯优化进行超参数调整。通过推进内河航道预测建模，本研究为交通规划者和组织提供了宝贵的见解，这些组织需要了解详细的交通流量，包括商品的流量、目的地以及进出港口的吨位。
2501.00606	 | Time-Varying Graph Learning for Data with Heavy-Tailed Distribution	 | Amirhossein Javaheri,Jiaxi Ying,Daniel P. Palomar,Farokh Marvasti	 | 本文研究了如何学习能够高效表示重尾数据的时变图模型，通过引入具有特定谱性质的图结构来增强模型的鲁棒性，并采用非负向量自回归模型和学生t分布来捕捉图的变化和表示信号，从而有效处理数据中的噪声和缺失值。	 | Graph models provide efficient tools to capture the underlying structure of data defined over networks. Many real-world network topologies are subject to change over time. Learning to model the dynamic interactions between entities in such networks is known as time-varying graph learning. Current methodology for learning such models often lacks robustness to outliers in the data and fails to handle heavy-tailed distributions, a common feature in many real-world datasets (e.g., financial data). This paper addresses the problem of learning time-varying graph models capable of efficiently representing heavy-tailed data. Unlike traditional approaches, we incorporate graph structures with specific spectral properties to enhance data clustering in our model. Our proposed method, which can also deal with noise and missing values in the data, is based on a stochastic approach, where a non-negative vector auto-regressive (VAR) model captures the variations in the graph and a Student-t distribution models the signal originating from this underlying time-varying graph. We propose an iterative method to learn time-varying graph topologies within a semi-online framework where only a mini-batch of data is used to update the graph. Simulations with both synthetic and real datasets demonstrate the efficacy of our model in analyzing heavy-tailed data, particularly those found in financial markets.	 | 图模型提供了捕捉网络上数据潜在结构的有效工具。许多现实世界的网络拓扑随着时间变化而改变。学习在这种网络中实体之间动态交互的模型被称为时变图学习。当前用于学习此类模型的方法往往缺乏对数据中的离群值的鲁棒性，并且无法处理重尾分布，这是许多现实世界数据集（例如金融数据）中的一个常见特征。本文针对学习能够高效表示重尾数据的时变图模型的问题进行了研究。与传统方法不同，我们通过引入具有特定谱性质的图结构来增强我们的模型中的数据聚类。我们的方法不仅可以处理数据中的噪声和缺失值，还是基于一种随机方法，其中非负向量自回归（VAR）模型捕捉图的变化，而学生t分布则表示来自这种潜在时变图的信号。我们提出了一种迭代方法，在半在线框架中学习时变图拓扑，仅使用一小批数据来更新图。使用合成数据集和真实数据集的模拟表明，我们的模型在分析重尾数据（特别是在金融市场中发现的数据）方面是有效的。
2501.00595	 | Unbiased GNN Learning via Fairness-Aware Subgraph Diffusion	 | Abdullah Alchihabi,Yuhong Guo	 | 图神经网络在图相关任务中表现出色，但由于消息传递机制，它们容易产生有偏见的预测，尤其是在敏感属性如年龄和性别方面。为此，本文提出了一种新型的生成型公平感知子图扩散（FASD）方法，该方法通过策略性地采样小子图并结合随机微分方程进行生成型公平感知图扩散，有效降低了偏见，进而提高预测的公平性。	 | Graph Neural Networks (GNNs) have demonstrated remarkable efficacy in tackling a wide array of graph-related tasks across diverse domains. However, a significant challenge lies in their propensity to generate biased predictions, particularly with respect to sensitive node attributes such as age and gender. These biases, inherent in many machine learning models, are amplified in GNNs due to the message-passing mechanism, which allows nodes to influence each other, rendering the task of making fair predictions notably challenging. This issue is particularly pertinent in critical domains where model fairness holds paramount importance. In this paper, we propose a novel generative Fairness-Aware Subgraph Diffusion (FASD) method for unbiased GNN learning. The method initiates by strategically sampling small subgraphs from the original large input graph, and then proceeds to conduct subgraph debiasing via generative fairness-aware graph diffusion processes based on stochastic differential equations (SDEs). To effectively diffuse unfairness in the input data, we introduce additional adversary bias perturbations to the subgraphs during the forward diffusion process, and train score-based models to predict these applied perturbations, enabling them to learn the underlying dynamics of the biases present in the data. Subsequently, the trained score-based models are utilized to further debias the original subgraph samples through the reverse diffusion process. Finally, FASD induces fair node predictions on the input graph by performing standard GNN learning on the debiased subgraphs. Experimental results demonstrate the superior performance of the proposed method over state-of-the-art Fair GNN baselines across multiple benchmark datasets.	 | 图神经网络（GNNs）在解决各种图相关任务方面展现了显著的效果，覆盖了多个不同的领域。然而，在生成预测时，GNNs 存在一个显著的挑战，即它们倾向于产生有偏见的预测，特别是在敏感节点属性如年龄和性别方面。这些偏见存在于许多机器学习模型中，而在 GNNs 中，由于消息传递机制的存在，节点之间可以互相影响，使得实现公平预测的任务变得更加困难。这一问题在模型公平性至关重要的关键领域尤为突出。在本文中，我们提出了一种新型的生成型公平感知子图扩散（FASD）方法，以实现无偏的 GNN 学习。该方法首先通过策略性地从原始的大输入图中采样小的子图，然后基于随机微分方程（SDEs）进行生成型公平感知图扩散过程来进行子图去偏。为了有效扩散输入数据中的不公平性，在正向扩散过程中，我们引入了额外的对抗性偏见扰动到子图中，并通过训练基于得分的模型来预测这些扰动，使其能够学习数据中存在的偏见动态。随后，训练好的基于得分的模型用于通过逆向扩散过程进一步去偏原来的子图样本。最后，FASD 通过在去偏后的子图上进行标准的 GNN 学习，在输入图上生成公平的节点预测。实验结果表明，该方法在多个基准数据集上相比最先进的公平 GNN 基线方法具有更好的性能。
2501.00555	 | Monty Hall and Optimized Conformal Prediction to Improve Decision-Making with LLMs	 | Harit Vishwakarma,Alan Mishler,Thomas Cook,Niccolò Dalmasso,Natraj Raman,Sumitra Ganesh	 | 该研究提出了CP-OPT和CROQ方法，以优化大型语言模型（LLMs）在高风险环境中的决策支持，通过减少预测集的大小并提高准确性，从而提高安全性和有效性。CP-OPT学习高质量的评分函数以最小化集合大小并保持覆盖率，而CROQ通过修订问题来进一步提高LLM回答问题的准确性。	 | Large language models (LLMs) are empowering decision-making in several applications, including tool or API usage and answering multiple-choice questions (MCQs). However, they often make overconfident, incorrect predictions, which can be risky in high-stakes settings like healthcare and finance. To mitigate these risks, recent works have used conformal prediction (CP), a model-agnostic framework for distribution-free uncertainty quantification. CP transforms a \emph{score function} into prediction sets that contain the true answer with high probability. While CP provides this coverage guarantee for arbitrary scores, the score quality significantly impacts prediction set sizes. Prior works have relied on LLM logits or other heuristic scores, lacking quality guarantees. We address this limitation by introducing CP-OPT, an optimization framework to learn scores that minimize set sizes while maintaining coverage. Furthermore, inspired by the Monty Hall problem, we extend CP's utility beyond uncertainty quantification to improve accuracy. We propose \emph{conformal revision of questions} (CROQ) to revise the problem by narrowing down the available choices to those in the prediction set. The coverage guarantee of CP ensures that the correct choice is in the revised question prompt with high probability, while the smaller number of choices increases the LLM's chances of answering it correctly. Experiments on MMLU, ToolAlpaca, and TruthfulQA datasets with Gemma-2, Llama-3 and Phi-3 models show that CP-OPT significantly reduces set sizes while maintaining coverage, and CROQ improves accuracy over the standard inference, especially when paired with CP-OPT scores. Together, CP-OPT and CROQ offer a robust framework for improving both the safety and accuracy of LLM-driven decision-making.	 | 大型语言模型（LLMs）正在多个应用中赋能决策，包括工具或API使用和回答多项选择题（MCQs）。然而，在医疗保健和金融等高风险环境下，它们往往会产生过于自信且错误的预测，这存在风险。为了缓解这些风险，最近的研究使用了符合性预测（Conformal Prediction, CP），一种无模型框架，用于不确定性量化。CP 将一个 *评分函数* 转化为包含正确答案的预测集。尽管 CP 可以为任意评分提供这种覆盖率保证，评分的质量显著影响预测集的大小。先前的工作依赖于LLM的输出概率或其它启发式评分，缺乏质量保证。为此，我们引入了 CP-OPT，这是一种优化框架，用于学习既能最小化集合大小又能保持覆盖率的评分。此外，受到蒙提霍尔问题的启发，我们扩展了 CP 的功能，不仅仅是用于不确定性量化，还能提高准确性。我们提出了 *符合性问题修订*（Conformal Revision of Questions, CROQ）来通过缩小可用选项范围到预测集中，从而修订问题。CP 的覆盖率保证确保修订后的提问中包含正确选项的概率很高，而选项数量较少则能增加LLM正确回答问题的机会。在MMLU、ToolAlpaca和TruthfulQA数据集上使用Gemma-2、Llama-3和Phi-3模型进行的实验表明，CP-OPT 有效减少了集合大小并保持了覆盖率，而CROQ在标准推理之上提升了准确性，尤其是在与CP-OPT评分结合使用时效果更佳。CP-OPT和CROQ共同提供了一个强大的框架，旨在提高LLM驱动决策的安全性和准确性。
2501.00538	 | Adaptive Tabu Dropout for Regularization of Deep Neural Network	 | Md. Tarek Hasan,Arifa Akter,Mohammad Nazmush Shamael,Md Al Emran Hossain,H. M. Mutasim Billah,Sumayra Islam,Swakkhar Shatabda	 | 本文改进了Tabu Dropout机制，通过引入Tabu任期和自适应Tabu算法来提高深度神经网络的训练效果，并在多个基准数据集上实验验证了其在多样性和性能上的优越性。	 | Dropout is an effective strategy for the regularization of deep neural networks. Applying tabu to the units that have been dropped in the recent epoch and retaining them for training ensures diversification in dropout. In this paper, we improve the Tabu Dropout mechanism for training deep neural networks in two ways. Firstly, we propose to use tabu tenure, or the number of epochs a particular unit will not be dropped. Different tabu tenures provide diversification to boost the training of deep neural networks based on the search landscape. Secondly, we propose an adaptive tabu algorithm that automatically selects the tabu tenure based on the training performances through epochs. On several standard benchmark datasets, the experimental results show that the adaptive tabu dropout and tabu tenure dropout diversify and perform significantly better compared to the standard dropout and basic tabu dropout mechanisms.	 |  Dropout是一种有效的深度神经网络正则化策略。在最近的epoch中对已drop掉的单元应用tabu，并保留它们进行训练，确保了dropout的多样化。在本文中，我们通过两种方式改进了tabu dropout机制，以提高深度神经网络的训练效果。首先，我们提出使用tabu任期，即特定单元将在多长时间内不被drop。不同的tabu任期根据搜索景观提供了多样化的训练增强。其次，我们提出了一种自适应tabu算法，该算法可以根据每轮次的训练表现自动选择tabu任期。在多个标准基准数据集上，实验结果表明，自适应tabu dropout和tabu任期dropout相比标准dropout和基本tabu dropout机制在多样性和性能上表现更优。
2501.00533	 | Rapid Learning in Constrained Minimax Games with Negative Momentum	 | Zijian Fang,Zongkai Liu,Chao Yu,Chaohao Hu	 | 本文提出了一种新的动量缓冲更新框架，将负动量技术应用于受限极大极小游戏，并为带有熵正则化的动量增强算法提供了理论保证，实验结果表明该技术能够在正常形式博弈和广义形式博弈中显著提升算法性能。	 | In this paper, we delve into the utilization of the negative momentum technique in constrained minimax games. From an intuitive mechanical standpoint, we introduce a novel framework for momentum buffer updating, which extends the findings of negative momentum from the unconstrained setting to the constrained setting and provides a universal enhancement to the classic game-solver algorithms. Additionally, we provide theoretical guarantee of convergence for our momentum-augmented algorithms with entropy regularizer. We then extend these algorithms to their extensive-form counterparts. Experimental results on both Normal Form Games (NFGs) and Extensive Form Games (EFGs) demonstrate that our momentum techniques can significantly improve algorithm performance, surpassing both their original versions and the SOTA baselines by a large margin.	 | 在本文中，我们探讨了在受限极大极小游戏中利用负动量技术的应用。从直观的机械角度出发，我们提出了一种新的动量缓冲更新框架，将负动量在无约束设置下的研究成果扩展到受限设置，并为经典的博弈求解算法提供了普遍的增强。此外，我们为带有熵正则化的动量增强算法提供了收敛的理论保证。然后，我们将这些算法扩展为其广义形式的对应算法。在正常形式博弈（NFGs）和广义形式博弈（EFGs）上的实验结果表明，我们的动量技术可以显著提高算法性能，并在很大程度上超越了它们的原始版本和当前最先进的基准。
2501.00528	 | PyMilo: A Python Library for ML I/O	 | AmirHosein Rostami,Sepand Haghighi,Sadra Sabouri,Alireza Zolanvari	 | PyMilo 是一个开源 Python 包，它通过透明且安全的方法解决现有机器学习模型存储格式的局限性，提供了一种端到端的解决方案来导出和部署预训练模型。	 | PyMilo is an open-source Python package that addresses the limitations of existing Machine Learning (ML) model storage formats by providing a transparent, reliable, and safe method for exporting and deploying trained models. Current formats, such as pickle and other binary formats, have significant problems, such as reliability, safety, and transparency issues. In contrast, PyMilo serializes ML models in a transparent non-executable format, enabling straightforward and safe model exchange, while also facilitating the deserialization and deployment of exported models in production environments. This package aims to provide a seamless, end-to-end solution for the exportation and importation of pre-trained ML models, which simplifies the model development and deployment pipeline.	 | PyMilo 是一个开源的 Python 包，它通过提供一种透明、可靠且安全的方法来导出和部署训练好的模型，来解决现有机器学习（ML）模型存储格式的局限性。当前的格式，如 pickle 和其他二进制格式，存在显著的问题，例如可靠性和透明性方面的问题。相比之下，PyMilo 以一种透明的非执行格式序列化机器学习模型，这使得模型的交换既简单又安全，同时也有助于在生产环境中对导出的模型进行反序列化和部署。该包旨在提供一个无缝的端到端解决方案，用于导出和导入预训练的机器学习模型，从而简化模型开发和部署的流程。
2501.00511	 | Stochastic Extragradient with Flip-Flop Shuffling & Anchoring: Provable Improvements	 | Jiseok Chae,Chulhee Yun,Donghwan Kim	 | 基于洗牌的方法在极小极大优化问题中表现出色，尤其是在凸-凹问题中。通过引入“锚定”技巧，提出了一种新的翻转洗牌锚定的随机外梯度方法(SEG-FFA)，该方法在凸-凹问题中成功收敛，并展示了相较于其他基于洗牌的方法更快的收敛速度。	 | In minimax optimization, the extragradient (EG) method has been extensively studied because it outperforms the gradient descent-ascent method in convex-concave (C-C) problems. Yet, stochastic EG (SEG) has seen limited success in C-C problems, especially for unconstrained cases. Motivated by the recent progress of shuffling-based stochastic methods, we investigate the convergence of shuffling-based SEG in unconstrained finite-sum minimax problems, in search of convergent shuffling-based SEG. Our analysis reveals that both random reshuffling and the recently proposed flip-flop shuffling alone can suffer divergence in C-C problems. However, with an additional simple trick called anchoring, we develop the SEG with flip-flop anchoring (SEG-FFA) method which successfully converges in C-C problems. We also show upper and lower bounds in the strongly-convex-strongly-concave setting, demonstrating that SEG-FFA has a provably faster convergence rate compared to other shuffling-based methods.	 | 在最优化中的极小极大优化问题中，外梯度（Extragradient, EG）方法因其在凸-凹（C-C）问题上优于梯度下降-上升方法而被广泛研究。然而，随机外梯度（Stochastic Extragradient, SEG）在C-C问题中，特别是在无约束情况下，表现出了有限的成功。受到基于洗牌的随机方法近期进展的启发，我们探讨了基于洗牌的SEG在有限和无约束极小极大问题中的收敛性，以寻找收敛的基于洗牌的SEG方法。我们的分析表明，随机重洗牌和最近提出的翻转洗牌单独使用时，在C-C问题中都可能发散。然而，通过一个额外的简单技巧称为“锚定”，我们开发了翻转洗牌锚定的随机外梯度（SEG with Flip-Flop Anchoring, SEG-FFA）方法，该方法在C-C问题中成功收敛。我们还在强凸-强凹设置下给出了上界和下界，证明了SEG-FFA相较于其他基于洗牌的方法具有可证明更快的收敛速度。
2501.00508	 | Active Learning of General Halfspaces: Label Queries vs Membership Queries	 | Ilias Diakonikolas,Daniel M. Kane,Mingchen Ma	 | 在具有查询访问的情况下，研究了在高斯分布下的 $R^d$ 上学习一般半空间的问题，建立了主动学习的下界，表明除非样本池大小达到指数级，否则无法显著优于被动学习；而通过成员查询，提出了一种查询复杂度较低的学习算法，展现出了主动学习与成员查询模型之间的差异。	 | We study the problem of learning general (i.e., not necessarily homogeneous) halfspaces under the Gaussian distribution on $R^d$ in the presence of some form of query access. In the classical pool-based active learning model, where the algorithm is allowed to make adaptive label queries to previously sampled points, we establish a strong information-theoretic lower bound ruling out non-trivial improvements over the passive setting. Specifically, we show that any active learner requires label complexity of $\tildeΩ(d/(\log(m)ε))$, where $m$ is the number of unlabeled examples. Specifically, to beat the passive label complexity of $\tilde{O} (d/ε)$, an active learner requires a pool of $2^{poly(d)}$ unlabeled samples. On the positive side, we show that this lower bound can be circumvented with membership query access, even in the agnostic model. Specifically, we give a computationally efficient learner with query complexity of $\tilde{O}(\min\{1/p, 1/ε\} + d\cdot polylog(1/ε))$ achieving error guarantee of $O(opt)+ε$. Here $p \in [0, 1/2]$ is the bias and $opt$ is the 0-1 loss of the optimal halfspace. As a corollary, we obtain a strong separation between the active and membership query models. Taken together, our results characterize the complexity of learning general halfspaces under Gaussian marginals in these models.	 | 在具有某种形式查询访问的条件下，我们研究了在 $R^d$ 上的高斯分布下学习一般（即不一定同质）半空间的问题。在经典的基于池的主动学习模型中，算法被允许对先前采样的点进行自适应标签查询，我们建立了强有力的信息论下界，排除了在被动设置之外的任何实质性改进。具体来说，我们证明任何主动学习器需要标签复杂度为 $\tildeΩ(d/(\log(m)ε))$，其中 $m$ 是未标记示例的数量。特别是，要击败被动的标签复杂度 $\tilde{O}(d/ε)$，主动学习器需要一个包含 $2^{poly(d)}$ 个未标记样本的池。在积极的一面，我们展示了即使在不可知模型中，通过成员查询访问也可以规避这一下界。具体来说，我们给出了一种计算高效的学习器，其查询复杂度为 $\tilde{O}(\min\{1/p, 1/ε\} + d\cdot polylog(1/ε))$，达到误差保证 $O(opt)+ε$。其中 $p \in [0, 1/2]$ 是偏差，$opt$ 是最优半空间的 0-1 损失。作为推论，我们获得了一个主动学习模型和成员查询模型之间的强烈区分。综合来看，我们的结果刻画了在这些模型下，在高斯边缘分布下的学习一般半空间的复杂性。
2501.00502	 | Exploring Physics-Informed Neural Networks for Crop Yield Loss Forecasting	 | Miro Miranda,Marcela Charfuelan,Andreas Dengel	 | 该研究提出了一种结合机器学习和作物模拟模型优点的新方法，通过像素级别估计水利用和作物对缺水的敏感性，提高了在极端天气条件下作物产量预测的准确性和物理一致性，R²值可达0.77，优于其他先进模型，并为适应极端天气条件提供了支持。	 | In response to climate change, assessing crop productivity under extreme weather conditions is essential to enhance food security. Crop simulation models, which align with physical processes, offer explainability but often perform poorly. Conversely, machine learning (ML) models for crop modeling are powerful and scalable yet operate as black boxes and lack adherence to crop growths physical principles. To bridge this gap, we propose a novel method that combines the strengths of both approaches by estimating the water use and the crop sensitivity to water scarcity at the pixel level. This approach enables yield loss estimation grounded in physical principles by sequentially solving the equation for crop yield response to water scarcity, using an enhanced loss function. Leveraging Sentinel-2 satellite imagery, climate data, simulated water use data, and pixel-level yield data, our model demonstrates high accuracy, achieving an R2 of up to 0.77, matching or surpassing state-of-the-art models like RNNs and Transformers. Additionally, it provides interpretable and physical consistent outputs, supporting industry, policymakers, and farmers in adapting to extreme weather conditions.	 | 为了应对气候变化，评估在极端天气条件下的作物生产力对于提高食物安全至关重要。作物模拟模型能够与物理过程相一致，但往往表现不佳。相比之下，用于作物建模的机器学习（ML）模型虽然强大且具有扩展性，但作为黑箱模型，缺乏对作物生长物理原理的遵从性。为了解决这一问题，我们提出了一种结合两种方法优点的新方法，通过在像素级别估计水利用和作物对缺水的敏感性。这种方法通过一系列解决作物产量对缺水响应的方程，利用增强的损失函数，使得产量损失估计基于物理原理成为可能。利用Sentinel-2卫星图像、气候数据、模拟水利用数据和像素级别产量数据，我们的模型表现出高度的准确性，R²值可达0.77，超越或匹配如RNN和Transformer等最先进的模型。此外，该模型还提供了可解释且物理一致的输出，支持行业、政策制定者和农民适应极端天气条件。
2501.00467	 | Score-Based Metropolis-Hastings Algorithms	 | Ahmed Aloui,Ali Hasan,Juncheng Dong,Zihao Wu,Vahid Tarokh	 | 本文提出了一种将评分模型与Metropolis-Hastings算法集成的新方法，通过引入基于详细平衡条件的损失函数，解决了传统评分扩散模型缺乏能量函数的问题，从而能够在采样过程中应用Metropolis调整Langevin算法。	 | In this paper, we introduce a new approach for integrating score-based models with the Metropolis-Hastings algorithm. While traditional score-based diffusion models excel in accurately learning the score function from data points, they lack an energy function, making the Metropolis-Hastings adjustment step inaccessible. Consequently, the unadjusted Langevin algorithm is often used for sampling using estimated score functions. The lack of an energy function then prevents the application of the Metropolis-adjusted Langevin algorithm and other Metropolis-Hastings methods, limiting the wealth of other algorithms developed that use acceptance functions. We address this limitation by introducing a new loss function based on the \emph{detailed balance condition}, allowing the estimation of the Metropolis-Hastings acceptance probabilities given a learned score function. We demonstrate the effectiveness of the proposed method for various scenarios, including sampling from heavy-tail distributions.	 | 在本文中，我们提出了一种将评分模型与Metropolis-Hastings算法集成的新方法。虽然传统的评分扩散模型在从数据点学习评分函数方面表现出色，但它们缺乏能量函数，使得Metropolis-Hastings调整步骤无法实现。因此，通常使用未调整的拉angevin算法通过估计的评分函数进行采样。缺乏能量函数则阻止了Metropolis调整拉angevin算法和其他Metropolis-Hastings方法的应用，限制了其他使用接受函数开发的算法的应用范围。我们通过引入基于\emph{详细平衡条件}的新损失函数来解决这一限制，该函数允许根据学习到的评分函数估计Metropolis-Hastings接受概率。我们通过各种场景展示了所提出方法的有效性，包括从重尾分布采样。
2501.00465	 | Dementia Detection using Multi-modal Methods on Audio Data	 | Saugat Kannojia,Anirudh Praveen,Danish Vasdev,Saket Nandedkar,Divyansh Mittal,Sarthak Kalankar,Shaurya Johari,Vipul Arora	 | 该研究开发了一种基于自动语音识别(ASR)和RoBERTa回归模型的预测模型，通过分析痴呆患者音频记录来生成简易精神状态检查(MMSE)评分，从而评估患者的认知能力，模型的RMSE得分为2.6911，在基准线基础上有所改进。	 | Dementia is a neurodegenerative disease that causes gradual cognitive impairment, which is very common in the world and undergoes a lot of research every year to prevent and cure it. It severely impacts the patient's ability to remember events and communicate clearly, where most variations of it have no known cure, but early detection can help alleviate symptoms before they become worse. One of the main symptoms of dementia is difficulty in expressing ideas through speech. This paper attempts to talk about a model developed to predict the onset of the disease using audio recordings from patients. An ASR-based model was developed that generates transcripts from the audio files using Whisper model and then applies RoBERTa regression model to generate an MMSE score for the patient. This score can be used to predict the extent to which the cognitive ability of a patient has been affected. We use the PROCESS_V1 dataset for this task, which is introduced through the PROCESS Grand Challenge 2025. The model achieved an RMSE score of 2.6911 which is around 10 percent lower than the described baseline.	 | 痴呆是一种神经退行性疾病，会导致认知功能逐渐下降，这种疾病在全世界很常见，并且每年都会进行大量的研究以预防和治疗。它严重影响了患者的记忆和清晰沟通的能力，大多数变体目前尚无治愈方法，但早期诊断可以帮助减轻症状，防止病情恶化。痴呆的一个主要症状是通过言语表达思想的困难。本文旨在讨论一种使用患者音频记录开发的预测模型。我们基于ASR（自动语音识别）开发了一种模型，使用Whisper模型生成音频文件的文本转录，然后应用RoBERTa回归模型为患者生成MMSE评分。该评分可以用来预测患者的认知能力受到了多大程度的影响。我们使用了PROCESS_V1数据集来进行这项任务，该数据集是通过2025年PROCESS全球挑战赛引入的。模型的RMSE得分为2.6911，比描述的基线低约10%。
2501.00464	 | Addressing Challenges in Data Quality and Model Generalization for Malaria Detection	 | Kiswendsida Kisito Kabore,Desire Guel	 | 该研究分析了深度学习在疟疾检测中面临的数据质量差和模型泛化能力不足等问题，并提出了解决方案，如基于GAN的数据增强和领域自适应技术，以提高检测准确性。通过这些方法，研究旨在促进AI在资源有限地区的疟疾诊断应用，并提高诊断工具的可靠性。	 | Malaria remains a significant global health burden, particularly in resource-limited regions where timely and accurate diagnosis is critical to effective treatment and control. Deep Learning (DL) has emerged as a transformative tool for automating malaria detection and it offers high accuracy and scalability. However, the effectiveness of these models is constrained by challenges in data quality and model generalization including imbalanced datasets, limited diversity and annotation variability. These issues reduce diagnostic reliability and hinder real-world applicability.   This article provides a comprehensive analysis of these challenges and their implications for malaria detection performance. Key findings highlight the impact of data imbalances which can lead to a 20\% drop in F1-score and regional biases which significantly hinder model generalization. Proposed solutions, such as GAN-based augmentation, improved accuracy by 15-20\% by generating synthetic data to balance classes and enhance dataset diversity. Domain adaptation techniques, including transfer learning, further improved cross-domain robustness by up to 25\% in sensitivity.   Additionally, the development of diverse global datasets and collaborative data-sharing frameworks is emphasized as a cornerstone for equitable and reliable malaria diagnostics. The role of explainable AI techniques in improving clinical adoption and trustworthiness is also underscored. By addressing these challenges, this work advances the field of AI-driven malaria detection and provides actionable insights for researchers and practitioners. The proposed solutions aim to support the development of accessible and accurate diagnostic tools, particularly for resource-constrained populations.	 | 疟疾仍然是全球公共卫生的重要负担，特别是在资源有限的地区，及时和准确的诊断对于有效治疗和控制至关重要。深度学习（DL）已成为自动化疟疾检测的变革性工具，它提供了高准确性和可扩展性。然而，这些模型的效果受到数据质量差和模型泛化能力有限的限制，包括数据集不平衡、多样性有限和标注变异等问题。这些问题降低了诊断的可靠性，阻碍了其在现实世界中的应用。本文全面分析了这些挑战及其对疟疾检测性能的影响。关键发现强调了数据不平衡的影响，可能导致F1分数下降20%，区域偏差显著阻碍了模型的泛化能力。提出的解决方案，如基于GAN的数据增强，通过生成合成数据平衡类别和增强数据集多样性，提高了15-20%的准确性。领域自适应技术，包括迁移学习，进一步提高了敏感性，最多可以提高25%的跨域鲁棒性。此外，开发多样化的全球数据集和协作数据共享框架被视为实现公平和可靠疟疾诊断的基础。解释性AI技术在提高临床应用和可信度方面的作用也得到了强调。通过解决这些挑战，这项工作促进了AI驱动疟疾检测领域的进步，并为研究人员和从业人员提供了可操作的见解。提出的解决方案旨在支持开发可访问和准确的诊断工具，特别是对于资源受限的人群。
2501.00463	 | SAT-LDM: Provably Generalizable Image Watermarking for Latent Diffusion Models with Self-Augmented Training	 | Lu Zhang,Liang Zeng	 | 本文提出了一种名为SAT-LDM的方法，通过自增训练的隐空间扩散模型来增强图像水印技术的泛化能力，从而有效保护AI生成图像的知识产权并减少伪影；实验结果表明，该方法不仅提高了水印的稳健性，还显著提升了带有水印的图像质量。	 | The proliferation of AI-generated images necessitates effective watermarking to protect intellectual property and identify fake content. While existing training-based watermarking methods show promise, they often struggle with generalization across diverse prompts and tend to produce noticeable artifacts. To this end, we introduce a provably generalizable image watermarking method for Latent Diffusion Models with Self-Augmented Training (SAT-LDM), which aligns the training and testing phases by a free generation distribution to bolster the watermarking module's generalization capabilities. We theoretically consolidate our method by proving that the free generation distribution contributes to its tight generalization bound without the need to collect new data. Extensive experimental results show that SAT-LDM achieves robust watermarking while significantly improving the quality of watermarked images across diverse prompts. Furthermore, we conduct experimental analyses to demonstrate the strong generalization abilities of SAT-LDM. We hope our method offers a practical and convenient solution for securing high-fidelity AI-generated content.	 | 随着AI生成图像的增多，有效的水印技术变得尤为重要，以保护知识产权并识别虚假内容。虽然现有的基于训练的水印方法显示出潜力，但它们往往难以在多样化的提示下进行泛化，并且往往会生成明显的伪影。为了解决这一问题，我们提出了一种名为自增训练的隐空间扩散模型图像水印方法（Self-Augmented Training for Latent Diffusion Models, SAT-LDM），通过自由生成分布来对齐训练和测试阶段，从而增强水印模块的泛化能力。我们通过理论证明，自由生成分布能够确保在无需收集新数据的情况下实现紧密的泛化边界。大量实验结果表明，SAT-LDM不仅能够实现稳健的水印，还能显著提高带有水印的图像质量，适用于多样化的提示。此外，我们还进行了实验分析，展示了SAT-LDM的强大泛化能力。我们希望这种方法能为保护高质量AI生成内容提供一个实用且便捷的解决方案。
2501.00457	 | Differentiable Prompt Learning for Vision Language Models	 | Zhenhan Huang,Tejaswini Pedapati,Pin-Yu Chen,Jianxi Gao	 | 该研究提出了一种名为可微提示学习（DPL）的方法，旨在自动设计深层连续提示以优化预训练模型的下游任务性能。实验结果表明，DPL方法能够在仅使用有限数据的情况下显著提升模型表现，并且其设计简单灵活，适用于多种模型架构。	 | Prompt learning is an effective way to exploit the potential of large-scale pre-trained foundational models. Continuous prompts parameterize context tokens in prompts by turning them into differentiable vectors. Deep continuous prompts insert prompts not only in the input but also in the intermediate hidden representations. Manually designed deep continuous prompts exhibit a remarkable improvement compared to the zero-shot pre-trained model on downstream tasks. How to automate the continuous prompt design is an underexplored area, and a fundamental question arises, is manually designed deep prompt strategy optimal? To answer this question, we propose a method dubbed differentiable prompt learning (DPL). The DPL method is formulated as an optimization problem to automatically determine the optimal context length of the prompt to be added to each layer, where the objective is to maximize the performance. We test the DPL method on the pre-trained CLIP. We empirically find that by using only limited data, our DPL method can find deep continuous prompt configuration with high confidence. The performance on the downstream tasks exhibits the superiority of the automatic design: our method boosts the average test accuracy by 2.60% on 11 datasets compared to baseline methods. Besides, our method focuses only on the prompt configuration (i.e. context length for each layer), which means that our method is compatible with the baseline methods that have sophisticated designs to boost the performance. The DPL method can be deployed to large language models or computer vision models at no cost.	 | 大规模预训练基础模型的潜在能力可以通过提示学习有效利用。连续提示通过将提示中的上下文令牌转换为可微分向量来参数化上下文令牌，从而实现连续提示。深层连续提示不仅在输入中插入提示，还在中间隐藏表示中插入提示。与零样本预训练模型相比，人工设计的深层连续提示在下游任务上表现出显著改进。如何自动化连续提示设计是一个未充分探索的领域，一个基本的问题随之而来：人工设计的深层提示策略是否是最优的？为了回答这一问题，我们提出了一种名为可微提示学习（DPL）的方法。DPL方法被形式化为一个优化问题，以自动确定应添加到每个层的提示的最优上下文长度，目标是最大化性能。我们在预训练的CLIP上测试了DPL方法。实验证明，仅使用有限的数据，我们的DPL方法可以找到具有高置信度的深层连续提示配置。在下游任务上的性能展示了自动设计的优越性：我们的方法在11个数据集上将平均测试准确率提高了2.60%，相比基线方法。此外，我们的方法仅关注提示配置（即每个层的上下文长度），这意味着我们的方法与那些有复杂设计以提高性能的基线方法兼容。DPL方法可以无成本地部署到大型语言模型或计算机视觉模型上。
2501.00436	 | Intuitive Analysis of the Quantization-based Optimization: From Stochastic and Quantum Mechanical Perspective	 | Jinwuk Seok,Changsik Cho	 | 本文提出了基于目标函数量化的一种直观优化技术，通过减少包含多个鞍点和局部极小值的水平集的度量，并在极限水平集中找到最优点。通过推导过阻尼朗之万动力学模型和使用随机微分方程中的威滕拉普拉斯算子，本文将基于量化优化与热力学和量子力学优化方法联系起来，并通过基准函数的模拟结果展示了其有效性。	 | In this paper, we present an intuitive analysis of the optimization technique based on the quantization of an objective function. Quantization of an objective function is an effective optimization methodology that decreases the measure of a level set containing several saddle points and local minima and finds the optimal point at the limit level set. To investigate the dynamics of quantization-based optimization, we derive an overdamped Langevin dynamics model from an intuitive analysis to minimize the level set by iterative quantization. We claim that quantization-based optimization involves the quantities of thermodynamical and quantum mechanical optimization as the core methodologies of global optimization. Furthermore, on the basis of the proposed SDE, we provide thermodynamic and quantum mechanical analysis with Witten-Laplacian. The simulation results with the benchmark functions, which compare the performance of the nonlinear optimization, demonstrate the validity of the quantization-based optimization.	 | 在本文中，我们基于目标函数的量化提出了直观的优化技术分析。目标函数的量化是一种有效的优化方法，它通过减少包含多个鞍点和局部极小值的水平集的度量，并在极限水平集中找到最优点。为了研究基于量化优化的动力学，我们从直观分析出发推导出一个过阻尼朗之万动力学模型，通过迭代量化来最小化水平集。我们主张，基于量化优化包括作为全局优化核心方法的热力学和量子力学优化。此外，基于提出的随机微分方程，我们使用威滕拉普拉斯算子提供了热力学和量子力学分析。使用基准函数的模拟结果展示了基于量化优化的有效性，这些基准函数比较了非线性优化的性能。
2501.00420	 | KAE: Kolmogorov-Arnold Auto-Encoder for Representation Learning	 | Fangchen Yu,Ruilizhen Hu,Yidong Lin,Yuqi Ma,Zhenghao Huang,Wenye Li	 | 本文提出了一种Kolmogorov-Arnold 自编码器（KAE），将Kolmogorov-Arnold 网络（KAN）与自编码器结合，以增强表示学习，实验表明KAE 在表示质量和下游任务性能上优于标准自编码器和其他KAN 变体。	 | The Kolmogorov-Arnold Network (KAN) has recently gained attention as an alternative to traditional multi-layer perceptrons (MLPs), offering improved accuracy and interpretability by employing learnable activation functions on edges. In this paper, we introduce the Kolmogorov-Arnold Auto-Encoder (KAE), which integrates KAN with autoencoders (AEs) to enhance representation learning for retrieval, classification, and denoising tasks. Leveraging the flexible polynomial functions in KAN layers, KAE captures complex data patterns and non-linear relationships. Experiments on benchmark datasets demonstrate that KAE improves latent representation quality, reduces reconstruction errors, and achieves superior performance in downstream tasks such as retrieval, classification, and denoising, compared to standard autoencoders and other KAN variants. These results suggest KAE's potential as a useful tool for representation learning. Our code is available at \url{https://github.com/SciYu/KAE/}.	 | 最近，Kolmogorov-Arnold 网络（KAN）作为一种替代传统多层感知机（MLPs）的选择引起了广泛关注，通过在边上传递可学习的激活函数，KAN 提供了更好的准确性和可解释性。本文中，我们引入了Kolmogorov-Arnold 自编码器（KAE），将KAN 与自编码器（AE）结合，以增强表示学习，适用于检索、分类和去噪任务。借助KAN 层中的灵活多项式函数，KAE 能捕捉复杂的数据模式和非线性关系。基准数据集上的实验表明，与标准自编码器和其他KAN 变体相比，KAE 在表示质量、降低重构误差以及在检索、分类和去噪等下游任务中的性能方面均表现出优越性。这些结果表明KAE 可能是一个有价值的工具，用于表示学习。我们的代码可在 \url{https://github.com/SciYu/KAE/} 获取。
2501.00418	 | Generalizing Trust: Weak-to-Strong Trustworthiness in Language Models	 | Martin Pawelczyk,Lillian Sun,Zhenting Qi,Aounon Kumar,Himabindu Lakkaraju	 | 本文探讨了在微调弱模型输出时强模型是否能继承关键可信性属性（如稳健性、公平性和隐私），并通过引入两种训练策略——弱可信性微调和弱和弱到强可信性微调——在实际数据集上验证了部分可信性属性可以“弱到强”泛化，但隐私属性则不然。	 | The rapid proliferation of generative AI, especially large language models, has led to their integration into a variety of applications. A key phenomenon known as weak-to-strong generalization - where a strong model trained on a weak model's outputs surpasses the weak model in task performance - has gained significant attention. Yet, whether critical trustworthiness properties such as robustness, fairness, and privacy can generalize similarly remains an open question. In this work, we study this question by examining if a stronger model can inherit trustworthiness properties when fine-tuned on a weaker model's outputs, a process we term weak-to-strong trustworthiness generalization. To address this, we introduce two foundational training strategies: 1) Weak Trustworthiness Finetuning (Weak TFT), which leverages trustworthiness regularization during the fine-tuning of the weak model, and 2) Weak and Weak-to-Strong Trustworthiness Finetuning (Weak+WTS TFT), which extends regularization to both weak and strong models. Our experimental evaluation on real-world datasets reveals that while some trustworthiness properties, such as fairness, adversarial, and OOD robustness, show significant improvement in transfer when both models were regularized, others like privacy do not exhibit signs of weak-to-strong trustworthiness. As the first study to explore trustworthiness generalization via weak-to-strong generalization, our work provides valuable insights into the potential and limitations of weak-to-strong generalization.	 | 生成式人工智能，尤其是大型语言模型的迅速发展，使其被集成到各种应用程序中。一个被称为“弱到强泛化”的关键现象——即在弱模型输出上训练的强模型在任务性能上超越了弱模型——引起了广泛关注。然而，诸如稳健性、公平性和隐私等关键可信性属性是否也能以类似的方式泛化仍然是一个悬而未决的问题。在本文中，我们通过研究强模型在微调弱模型输出时是否能够继承这些可信性属性来探讨这一问题，这一过程我们称之为“弱到强可信性泛化”。为了解决这一问题，我们引入了两种基本的训练策略：1）弱可信性微调（Weak TFT），该策略在弱模型微调过程中利用可信性正则化；2）弱和弱到强可信性微调（Weak+WTS TFT），该策略将正则化扩展到弱和强模型。我们在实际数据集上的实验结果显示，虽然一些可信性属性，如公平性、对抗性和OOD稳健性，在两个模型都进行正则化时在转移学习中得到了显著提升，但其他属性如隐私并未显示出“弱到强”可信性泛化的迹象。作为首次通过“弱到强”泛化研究可信性泛化的研究，我们的工作提供了关于“弱到强”泛化潜力和限制的重要见解。
2501.00397	 | Efficient Relational Context Perception for Knowledge Graph Completion	 | Wenkai Tu,Guojia Wan,Zhengchun Shang,Bo Du	 | 本文提出了一种名为Triple Receptance Perception (TRP)的新架构，该架构通过建模序列信息来动态学习实体和关系的上下文，从而在链接预测和三元组分类任务中优于现有模型。TRP利用张量分解计算三元组分数，提供更丰富的表示能力和稳健的关系解码能力。	 | Knowledge Graphs (KGs) provide a structured representation of knowledge but often suffer from challenges of incompleteness. To address this, link prediction or knowledge graph completion (KGC) aims to infer missing new facts based on existing facts in KGs. Previous knowledge graph embedding models are limited in their ability to capture expressive features, especially when compared to deeper, multi-layer models. These approaches also assign a single static embedding to each entity and relation, disregarding the fact that entities and relations can exhibit different behaviors in varying graph contexts. Due to complex context over a fact triple of a KG, existing methods have to leverage complex non-linear context encoder, like transformer, to project entity and relation into low dimensional representations, resulting in high computation cost. To overcome these limitations, we propose Triple Receptance Perception (TRP) architecture to model sequential information, enabling the learning of dynamic context of entities and relations. Then we use tensor decomposition to calculate triple scores, providing robust relational decoding capabilities. This integration allows for more expressive representations. Experiments on benchmark datasets such as YAGO3-10, UMLS, FB15k, and FB13 in link prediction and triple classification tasks demonstrate that our method performs better than several state-of-the-art models, proving the effectiveness of the integration.	 | 知识图谱（KGs）提供了知识的结构化表示，但往往面临着不完整性的挑战。为了解决这个问题，链接预测或知识图谱完成（KGC）旨在基于KG中已有的事实推断缺失的新事实。以往的知识图嵌入模型在捕捉表达性特征方面存在局限，尤其是在与更深层、多层模型相比时。这些方法还将每个实体和关系分配为单一的静态嵌入，忽略了实体和关系在不同图上下文中的行为可能有所不同这一事实。由于KG中的三元组事实具有复杂的上下文，现有方法不得不利用复杂的非线性上下文编码器（如transformer）将实体和关系投影到低维表示中，从而导致计算成本高。为克服这些局限，我们提出Triple Receptance Perception（TRP）架构来建模序列信息，使实体和关系的动态上下文学习成为可能。然后我们使用张量分解来计算三元组分数，提供稳健的.relational解码能力。这种集成允许更丰富的表示。在链路预测和三元组分类任务中的基准数据集如YAGO3-10、UMLS、FB15k和FB13上的实验表明，我们的方法在多个最先进的模型中表现更优，证明了这种集成的有效性。
2501.00381	 | Toward Information Theoretic Active Inverse Reinforcement Learning	 | Ondrej Bajgar,Sid William Gould,Rohan Narayan Langford Mitta,Jonathon Liu,Oliver Newcombe,Jack Golden	 | 该研究探讨了如何通过逆强化学习从有限的人类演示中推断出奖励函数，特别是在难以手动编写奖励函数的情况下，并提出了一种可以收集更长轨迹的积极IRL方法，通过信息论获取函数和高效算法在格子世界实验中展示了其有效性。	 | As AI systems become increasingly autonomous, aligning their decision-making to human preferences is essential. In domains like autonomous driving or robotics, it is impossible to write down the reward function representing these preferences by hand. Inverse reinforcement learning (IRL) offers a promising approach to infer the unknown reward from demonstrations. However, obtaining human demonstrations can be costly. Active IRL addresses this challenge by strategically selecting the most informative scenarios for human demonstration, reducing the amount of required human effort. Where most prior work allowed querying the human for an action at one state at a time, we motivate and analyse scenarios where we collect longer trajectories. We provide an information-theoretic acquisition function, propose an efficient approximation scheme, and illustrate its performance through a set of gridworld experiments as groundwork for future work expanding to more general settings.	 | 随着人工智能系统变得越来越自主，使其决策与人类偏好保持一致变得至关重要。在自动驾驶或机器人等领域，很难手动编写代表这些偏好的奖励函数。逆强化学习（IRL）提供了一种有潜力的方法来从演示中推断出未知的奖励。然而，获得人类演示可能成本很高。积极的IRL通过战略性地选择最具有信息量的场景来进行人类演示，从而减少所需的劳动量。与大多数之前的工作一次只允许查询人类在一个状态下采取的行动不同，我们提出了并分析了可以收集更长轨迹的场景。我们提供了一种信息论获取函数，提出了一个高效的近似方案，并通过一系列格子世界实验展示了其性能，为将来的工作扩展到更广泛的情境奠定了基础。
2501.00379	 | Federated Dropout: Convergence Analysis and Resource Allocation	 | Sijing Xie,Dingzhu Wen,Xiaonan Liu,Changsheng You,Tharmalingam Ratnarajah,Kaibin Huang	 | 联邦丢弃是一种有效减少联邦学习中通信和计算瓶颈的技术，通过更新并传输由典型丢弃方法生成的子模型来减少延迟，但其理论收敛分析尚不充分，尤其是对于量化丢弃率对收敛的影响。本文通过泰勒展开方法证明了梯度方差与丢弃率的关系，并提出了一个低复杂度算法来优化每轮次的丢弃率和带宽分配，以最小化整体损失函数。	 | Federated Dropout is an efficient technique to overcome both communication and computation bottlenecks for deploying federated learning at the network edge. In each training round, an edge device only needs to update and transmit a sub-model, which is generated by the typical method of dropout in deep learning, and thus effectively reduces the per-round latency. \textcolor{blue}{However, the theoretical convergence analysis for Federated Dropout is still lacking in the literature, particularly regarding the quantitative influence of dropout rate on convergence}. To address this issue, by using the Taylor expansion method, we mathematically show that the gradient variance increases with a scaling factor of $γ/(1-γ)$, with $γ\in [0, θ)$ denoting the dropout rate and $θ$ being the maximum dropout rate ensuring the loss function reduction. Based on the above approximation, we provide the convergence analysis for Federated Dropout. Specifically, it is shown that a larger dropout rate of each device leads to a slower convergence rate. This provides a theoretical foundation for reducing the convergence latency by making a tradeoff between the per-round latency and the overall rounds till convergence. Moreover, a low-complexity algorithm is proposed to jointly optimize the dropout rate and the bandwidth allocation for minimizing the loss function in all rounds under a given per-round latency and limited network resources. Finally, numerical results are provided to verify the effectiveness of the proposed algorithm.	 | 联邦丢弃（Federated Dropout）是一种有效的技术，能够克服在边缘部署联邦学习中遇到的通信和计算瓶颈。在每次训练轮次中，边缘设备只需要更新并传输由深度学习中典型的丢弃方法生成的子模型，从而有效减少了每轮次的延迟。\textcolor{blue}{然而，关于联邦丢弃的理论收敛分析在文献中仍缺乏，尤其是在量化丢弃率对收敛的影响方面}。为了解决这一问题，通过使用泰勒展开方法，我们从数学上证明了梯度方差随着缩放因子 $\frac{\gamma}{1-\gamma}$ 增加，其中 $\gamma \in [0, \theta)$ 表示丢弃率，而 $\theta$ 是确保损失函数降低的最大丢弃率。基于上述近似，我们对联邦丢弃进行了收敛分析。具体来说，我们证明了每个设备的更大丢弃率会导致收敛速度变慢。这为通过在每轮次延迟与直至收敛的整体轮次之间做出权衡来减少收敛延迟提供了理论基础。此外，我们提出了一种低复杂度算法，以联合优化每轮次的丢弃率和带宽分配，从而在给定每轮次延迟和有限网络资源的情况下，最小化所有轮次中的损失函数。最后，我们提供了一些数值结果来验证所提算法的有效性。
2501.00365	 | Low-Rank Adaptation for Foundation Models: A Comprehensive Review	 | Menglin Yang,Jialin Chen,Yifei Zhang,Jiahong Liu,Jiasheng Zhang,Qiyao Ma,Harshit Verma,Qianru Zhang,Min Zhou,Irwin King,Rex Ying	 | LoRA技术通过提供一种参数高效机制，有效缓解了大规模神经网络在适应特定下游任务时所面临的巨大挑战；本文综述了LoRA在不同基础模型和领域的最新进展，并探讨了其理论理解、扩展性和鲁棒性方面的关键挑战与未来研究方向。	 | The rapid advancement of foundation modelslarge-scale neural networks trained on diverse, extensive datasetshas revolutionized artificial intelligence, enabling unprecedented advancements across domains such as natural language processing, computer vision, and scientific discovery. However, the substantial parameter count of these models, often reaching billions or trillions, poses significant challenges in adapting them to specific downstream tasks. Low-Rank Adaptation (LoRA) has emerged as a highly promising approach for mitigating these challenges, offering a parameter-efficient mechanism to fine-tune foundation models with minimal computational overhead. This survey provides the first comprehensive review of LoRA techniques beyond large Language Models to general foundation models, including recent techniques foundations, emerging frontiers and applications of low-rank adaptation across multiple domains. Finally, this survey discusses key challenges and future research directions in theoretical understanding, scalability, and robustness. This survey serves as a valuable resource for researchers and practitioners working with efficient foundation model adaptation.	 | 大规模神经网络（训练于多样且庞大的数据集）的快速进步已经彻底改变了人工智能领域，使其在自然语言处理、计算机视觉和科学研究等多个领域取得了前所未有的进展。然而，这些模型的参数量往往达到数十亿甚至数万亿，给它们适应特定下游任务带来了巨大挑战。低秩适应（LoRA）作为一种极具前景的方法，能够有效缓解这些问题，提供了一种在轻量化计算开销下对基础模型进行微调的参数高效机制。本文综述了LoRA技术的最新进展，不仅限于大型语言模型，还涵盖了对一般基础模型的低秩适应方法、最新技术前沿、以及在多个领域中的应用。最后，本文讨论了理论理解、扩展性和鲁棒性方面的关键挑战与未来研究方向。本文为研究者和从业者提供了一个宝贵的资源，以应对高效基础模型适应的挑战。
2501.00356	 | A New Dataset and Methodology for Malicious URL Classification	 | Ilan Schvartzman,Roei Sarussi,Maor Ashkenazi,Ido kringel,Yaniv Tocker,Tal Furman Shohet	 | 本文介绍了一个新的多类数据集DeepURLBench，用于改善恶意URL分类，并提出了一系列改进基于字符串的URL分类器的方法，特别是整合DNS衍生特征，从而提升了模型性能并保持了实时处理能力。	 | Malicious URL (Uniform Resource Locator) classification is a pivotal aspect of Cybersecurity, offering defense against web-based threats. Despite deep learning's promise in this area, its advancement is hindered by two main challenges: the scarcity of comprehensive, open-source datasets and the limitations of existing models, which either lack real-time capabilities or exhibit suboptimal performance. In order to address these gaps, we introduce a novel, multi-class dataset for malicious URL classification, distinguishing between benign, phishing and malicious URLs, named DeepURLBench. The data has been rigorously cleansed and structured, providing a superior alternative to existing datasets. Notably, the multi-class approach enhances the performance of deep learning models, as compared to a standard binary classification approach. Additionally, we propose improvements to string-based URL classifiers, applying these enhancements to URLNet. Key among these is the integration of DNS-derived features, which enrich the model's capabilities and lead to notable performance gains while preserving real-time runtime efficiency-achieving an effective balance for cybersecurity applications.	 | 恶意URL（统一资源定位符）分类是网络安全的关键组成部分，有助于防御基于网络的威胁。尽管深度学习在这个领域具有巨大潜力，但其发展受到了两个主要挑战的阻碍：缺乏全面的开源数据集和现有模型的局限性，这些模型要么缺乏实时能力，要么表现欠佳。为了应对这些差距，我们引入了一个新的多类数据集，用于恶意URL分类，该数据集区分良性、钓鱼和恶意URL，名为DeepURLBench。数据经过严格清洗和结构化，提供了现有数据集的优越替代品。值得注意的是，多类方法在与标准二分类方法相比时，增强了深度学习模型的性能。此外，我们还提出了一些针对基于字符串的URL分类器的改进，并将这些改进应用于URLNet。这些改进的关键之一是整合DNS衍生特征，这不仅增强了模型的能力，还实现了显著的性能提升，同时保持了实时运行效率——这对于网络安全应用而言是一种有效的平衡。
2501.00307	 | Fast and Interpretable Mixed-Integer Linear Program Solving by Learning Model Reduction	 | Yixuan Li,Can Chen,Jiajun Li,Jiahui Duan,Xiongwei Han,Tao Zhong,Vincent Chau,Weiwei Wu,Wanyuan Wang	 | 本文提出了一种基于偏好模型简化学习的方法，通过利用混合整数线性规划（MILP）实例中简化模型的相对性能作为偏好信息，以提高大规模MILP问题的求解效率和准确性，并取得了比现有商业求解器更高的解准确性和两个到四个数量级的速度提升。	 | By exploiting the correlation between the structure and the solution of Mixed-Integer Linear Programming (MILP), Machine Learning (ML) has become a promising method for solving large-scale MILP problems. Existing ML-based MILP solvers mainly focus on end-to-end solution learning, which suffers from the scalability issue due to the high dimensionality of the solution space. Instead of directly learning the optimal solution, this paper aims to learn a reduced and equivalent model of the original MILP as an intermediate step. The reduced model often corresponds to interpretable operations and is much simpler, enabling us to solve large-scale MILP problems much faster than existing commercial solvers. However, current approaches rely only on the optimal reduced model, overlooking the significant preference information of all reduced models. To address this issue, this paper proposes a preference-based model reduction learning method, which considers the relative performance (i.e., objective cost and constraint feasibility) of all reduced models on each MILP instance as preferences. We also introduce an attention mechanism to capture and represent preference information, which helps improve the performance of model reduction learning tasks. Moreover, we propose a SetCover based pruning method to control the number of reduced models (i.e., labels), thereby simplifying the learning process. Evaluation on real-world MILP problems shows that 1) compared to the state-of-the-art model reduction ML methods, our method obtains nearly 20% improvement on solution accuracy, and 2) compared to the commercial solver Gurobi, two to four orders of magnitude speedups are achieved.	 | 通过利用混合整数线性规划（MILP）结构与其解之间的相关性，机器学习（ML）已成为解决大规模MILP问题的一种有前途的方法。现有的基于ML的MILP求解器主要集中在端到端的解学习上，但由于解空间的高维性，这面临着可扩展性的问题。本文的目标不是直接学习最优解，而是学习原始MILP的简化且等效模型作为中间步骤。简化模型通常对应于可解释的操作，并且远比现有的商业求解器简单，从而使我们能够更快地解决大规模MILP问题。然而，当前的方法仅依赖于最优简化模型，忽略了所有简化模型的重要偏好信息。为了解决这一问题，本文提出了一种基于偏好模型简化学习方法，该方法将每个MILP实例上所有简化模型的相对性能（即目标成本和约束可行性）视为偏好信息。我们还引入了一个注意力机制来捕捉并表示偏好信息，这有助于提高模型简化学习任务的性能。此外，我们提出了一个基于SetCover的剪枝方法来控制简化模型的数量（即标签数量），从而简化学习过程。在实际MILP问题上的评估显示：1）与最新的模型简化ML方法相比，我们的方法在解的准确性上提高了近20%；2）与商业求解器Gurobi相比，实现了两个到四个数量级的速度提升。
2501.00305	 | diffIRM: A Diffusion-Augmented Invariant Risk Minimization Framework for Spatiotemporal Prediction over Graphs	 | Zhaobin Mo,Haotian Xiang,Xuan Di	 | 本文针对时空图上的空间-时间预测（STPG）面临的分布外（OOD）泛化问题，提出了一种新的扩散增强不变风险最小化（diffIRM）框架，通过结合因果增强和环境增强来学习不变特征，从而提高预测模型的泛化能力。实验结果表明，diffIRM在三个真实世界的人类移动数据集上表现优于基线方法。	 | Spatiotemporal prediction over graphs (STPG) is challenging, because real-world data suffers from the Out-of-Distribution (OOD) generalization problem, where test data follow different distributions from training ones. To address this issue, Invariant Risk Minimization (IRM) has emerged as a promising approach for learning invariant representations across different environments. However, IRM and its variants are originally designed for Euclidean data like images, and may not generalize well to graph-structure data such as spatiotemporal graphs due to spatial correlations in graphs. To overcome the challenge posed by graph-structure data, the existing graph OOD methods adhere to the principles of invariance existence, or environment diversity. However, there is little research that combines both principles in the STPG problem. A combination of the two is crucial for efficiently distinguishing between invariant features and spurious ones. In this study, we fill in this research gap and propose a diffusion-augmented invariant risk minimization (diffIRM) framework that combines these two principles for the STPG problem. Our diffIRM contains two processes: i) data augmentation and ii) invariant learning. In the data augmentation process, a causal mask generator identifies causal features and a graph-based diffusion model acts as an environment augmentor to generate augmented spatiotemporal graph data. In the invariant learning process, an invariance penalty is designed using the augmented data, and then serves as a regularizer for training the spatiotemporal prediction model. The real-world experiment uses three human mobility datasets, i.e. SafeGraph, PeMS04, and PeMS08. Our proposed diffIRM outperforms baselines.	 | 时空图上的空间-temporal预测（STPG）面临着挑战，因为现实世界的数据遭受着分布外（Out-of-Distribution, OOD）泛化问题，即测试数据遵循与训练数据不同的分布。为了解决这一问题，不变风险最小化（Invariant Risk Minimization, IRM）作为一种方法已经崭露头角，旨在学习在不同环境下的不变表示。然而，IRM及其变体最初是为像图像这样的欧几里得数据设计的，可能并不适用于具有空间相关性的图结构数据，如时空图。为克服图结构数据带来的挑战，现有的图OOD方法遵循不变性存在或环境多样性原则。然而，很少有研究将这两个原则结合应用于STPG问题。结合这两个原则对于高效地区分不变特征和虚假特征是至关重要的。在本研究中，我们填补了这一研究空白，并提出了一种扩散增强不变风险最小化（diffIRM）框架，用于STPG问题，将这两个原则结合起来。我们的diffIRM包括两个过程：i) 数据增强和ii) 不变性学习。在数据增强过程中，因果掩码生成器识别因果特征，而图基的扩散模型作为环境增强器生成增强的时空图数据。在不变性学习过程中，使用增强的数据设计不变性惩罚，并将其作为时空预测模型训练的正则化项。现实世界实验使用了三个真实世界的人类移动数据集，即SafeGraph、PeMS04和PeMS08。我们提出的diffIRM在基线方法上表现更优。
2501.00282	 | ReFormer: Generating Radio Fakes for Data Augmentation	 | Yagna Kaasaragadda,Silvija Kokalj-Filipovic	 | ReFormer 是一种生成型人工智能模型，能够高效生成与训练数据统计特性相似的合成射频数据，适用于信道估计和建模等应用，通过基于变压器的自回归生成技术实现；不同变压器架构和其他设计选择对生成RF数据的质量有显著影响。	 | We present ReFormer, a generative AI (GAI) model that can efficiently generate synthetic radio-frequency (RF) data, or RF fakes, statistically similar to the data it was trained on, or with modified statistics, in order to augment datasets collected in real-world experiments. For applications like this, adaptability and scalability are important issues. This is why ReFormer leverages transformer-based autoregressive generation, trained on learned discrete representations of RF signals. By using prompts, such GAI can be made to generate the data which complies with specific constraints or conditions, particularly useful for training channel estimation and modeling. It may also leverage the data from a source system to generate training data for a target system. We show how different transformer architectures and other design choices affect the quality of generated RF fakes, evaluated using metrics such as precision and recall, classification accuracy and signal constellation diagrams.	 | 我们介绍了ReFormer，这是一种生成型人工智能（GAI）模型，能够高效生成与训练数据统计特性相似的合成射频（RF）数据，或具有修改统计特性的RF数据，以增强在实际实验中收集的数据集。对于这类应用，适应性和可扩展性是非常重要的问题。因此，ReFormer利用基于变压器的自回归生成技术，并在学习到的RF信号离散表示上进行训练。通过使用提示，这种GAI可以生成符合特定约束或条件的数据，特别适用于训练信道估计和建模。它还可以利用源系统数据来生成目标系统的训练数据。我们展示了不同变压器架构和其他设计选择如何影响生成的RF数据的质量，这些质量通过精度和召回率、分类准确率和信号星座图等指标进行评估。
2501.00265	 | Outlier-Robust Training of Machine Learning Models	 | Rajat Talak,Charis Georgiou,Jingnan Shi,Luca Carlone	 | 该研究探讨了在机器学习中处理离群值的稳健训练方法，通过修改Black-Rangarajan对偶，提出了一个统一的稳健损失核，并基于此开发了一种自适应交替算法（AAA），该算法能够在包含离群值的情况下有效训练模型，同时无需复杂参数调优。	 | Robust training of machine learning models in the presence of outliers has garnered attention across various domains. The use of robust losses is a popular approach and is known to mitigate the impact of outliers. We bring to light two literatures that have diverged in their ways of designing robust losses: one using M-estimation, which is popular in robotics and computer vision, and another using a risk-minimization framework, which is popular in deep learning. We first show that a simple modification of the Black-Rangarajan duality provides a unifying view. The modified duality brings out a definition of a robust loss kernel $σ$ that is satisfied by robust losses in both the literatures. Secondly, using the modified duality, we propose an Adaptive Alternation Algorithm (AAA) for training machine learning models with outliers. The algorithm iteratively trains the model by using a weighted version of the non-robust loss, while updating the weights at each iteration. The algorithm is augmented with a novel parameter update rule by interpreting the weights as inlier probabilities, and obviates the need for complex parameter tuning. Thirdly, we investigate convergence of the adaptive alternation algorithm to outlier-free optima. Considering arbitrary outliers (i.e., with no distributional assumption on the outliers), we show that the use of robust loss kernels σ increases the region of convergence. We experimentally show the efficacy of our algorithm on regression, classification, and neural scene reconstruction problems. We release our implementation code: https://github.com/MIT-SPARK/ORT.	 | 在面对离群值时机器学习模型的稳健训练引起了各个领域的广泛关注。使用稳健损失是常见的方法，这种损失能够减轻离群值的影响。我们揭示了两个在设计稳健损失方面分道扬镳的文献：一个是采用M-估计的方法，这种方法在机器人学和计算机视觉领域非常流行；另一个是风险最小化框架的方法，这种方法在深度学习领域非常流行。我们首先展示了对Black-Rangarajan 对偶进行简单修改后提供了统一的观点。该修改后的对偶揭示了一种稳健损失核 $σ$ 的定义，这种核同时适用于这两个文献中的稳健损失。其次，通过修改后的对偶，我们提出了一种自适应交替算法（AAA）来训练具有离群值的机器学习模型。该算法通过迭代使用非稳健损失的加权版本来训练模型，同时在每次迭代中更新权重。算法通过将权重解释为内点概率，增加了新的参数更新规则，从而消除了复杂参数调优的需求。第三，我们研究了自适应交替算法收敛到无离群值最优解的情况。考虑到任意离群值（即不假设离群值的分布），我们证明使用稳健损失核 $σ$ 可以增加收敛区域。我们在回归、分类和神经场景重建问题上实验性地展示了我们算法的有效性。我们的实现代码已发布：https://github.com/MIT-SPARK/ORT。
2501.00252	 | Towards Pattern-aware Data Augmentation for Temporal Knowledge Graph Completion	 | Jiasheng Zhang,Deqiang Ouyang,Shuang Liang,Jie Shao	 | 该研究针对时间知识图谱完成任务中的数据分布不平衡和模型偏好问题，提出了一种名为Booster的数据增强策略，通过生成符合TKGs语义和时间模式的新样本，并结合两阶段训练方法来优化模型学习，从而显著提升了TKGC的性能。	 | Predicting missing facts for temporal knowledge graphs (TKGs) is a fundamental task, called temporal knowledge graph completion (TKGC). One key challenge in this task is the imbalance in data distribution, where facts are unevenly spread across entities and timestamps. This imbalance can lead to poor completion performance or long-tail entities and timestamps, and unstable training due to the introduction of false negative samples. Unfortunately, few previous studies have investigated how to mitigate these effects. Moreover, for the first time, we found that existing methods suffer from model preferences, revealing that entities with specific properties (e.g., recently active) are favored by different models. Such preferences will lead to error accumulation and further exacerbate the effects of imbalanced data distribution, but are overlooked by previous studies. To alleviate the impacts of imbalanced data and model preferences, we introduce Booster, the first data augmentation strategy for TKGs. The unique requirements here lie in generating new samples that fit the complex semantic and temporal patterns within TKGs, and identifying hard-learning samples specific to models. Therefore, we propose a hierarchical scoring algorithm based on triadic closures within TKGs. By incorporating both global semantic patterns and local time-aware structures, the algorithm enables pattern-aware validation for new samples. Meanwhile, we propose a two-stage training approach to identify samples that deviate from the model's preferred patterns. With a well-designed frequency-based filtering strategy, this approach also helps to avoid the misleading of false negatives. Experiments justify that Booster can seamlessly adapt to existing TKGC models and achieve up to an 8.7% performance improvement.	 | 预测时间知识图谱（Temporal Knowledge Graphs, TKGs）中缺失的事实是基础任务，称为时间知识图谱完成（Temporal Knowledge Graph Completion, TKGC）。在这个任务中，一个主要的挑战在于数据分布的不平衡，即事实在实体和时间戳之间分布不均。这种不平衡会导致完成性能不佳或长尾实体和时间戳，以及因引入假阴性样本而导致的训练不稳定。不幸的是，很少有先前的研究探讨如何缓解这些影响。此外，首次发现现有方法存在模型偏好问题，揭示出具有特定属性（例如，最近活跃）的实体被不同模型偏好。这种偏好会导致误差积累并进一步加剧数据分布不平衡的影响，但这些影响在先前的研究中被忽视。为了缓解数据不平衡和模型偏好带来的影响，我们引入了Booster，这是第一个针对TKGs的数据增强策略。这里的独特要求在于生成符合TKGs中复杂语义和时间模式的新样本，并识别模型特有的难以学习的样本。因此，我们提出了一种基于TKGs中的三角闭包的分层评分算法。通过结合全局语义模式和局部时间感知结构，该算法能够为新样本提供模式感知验证。同时，我们提出了两阶段训练方法，以识别偏离模型偏好模式的样本。通过精心设计的频率过滤策略，这种方法也有助于避免假阴性的误导。实验结果证明，Booster可以无缝适应现有的TKGC模型，并可实现最高8.7%的性能提升。
2501.00230	 | Federated Deep Subspace Clustering	 | Yupei Zhang,Ruojia Feng,Yifei Wang,Xuequn Shang	 | 本文提出了一种基于联邦学习架构的私有保护子空间聚类方法FDSC，该方法通过在客户端中使用深度子空间聚类网络并对编码网络进行服务器端通信来实现，并保留局部邻域关系，从而提高聚类性能；实验结果表明FDSC在公共数据集上优于其他聚类方法。	 | This paper introduces FDSC, a private-protected subspace clustering (SC) approach with federated learning (FC) schema. In each client, there is a deep subspace clustering network accounting for grouping the isolated data, composed of a encode network, a self-expressive layer, and a decode network. FDSC is achieved by uploading the encode network to communicate with other clients in the server. Besides, FDSC is also enhanced by preserving the local neighborhood relationship in each client. With the effects of federated learning and locality preservation, the learned data features from the encoder are boosted so as to enhance the self-expressiveness learning and result in better clustering performance. Experiments test FDSC on public datasets and compare with other clustering methods, demonstrating the effectiveness of FDSC.	 | 本文介绍了基于联邦学习（FL）架构的私有保护子空间聚类（SC）方法FDSC。每个客户端中包含一个深度子空间聚类网络，用于对孤立数据进行分组，该网络由编码网络、自表达层和解码网络组成。FDSC通过将编码网络上传到服务器，与其他客户端进行通信来实现。此外，FDSC还在每个客户端中保留了局部邻域关系。通过联邦学习和局部保留的效果，编码器学习到的数据特征得到了增强，从而增强了自我表达学习并提高了聚类性能。实验在公共数据集上测试了FDSC，并与其他聚类方法进行比较，证明了FDSC的有效性。
2501.00200	 | Scalable Neural Network Verification with Branch-and-bound Inferred Cutting Planes	 | Duo Zhou,Christopher Brix,Grani A Hanasusanto,Huan Zhang	 | 本文提出了一种新颖的分支定界推断切割面与约束加强（BICCOS）方法，利用神经网络验证问题的结构生成特定且高效的剪枝平面，显著提高了大型神经网络的验证性能，并在VNN-COMP 2024中获得冠军。	 | Recently, cutting-plane methods such as GCP-CROWN have been explored to enhance neural network verifiers and made significant advances. However, GCP-CROWN currently relies on generic cutting planes (cuts) generated from external mixed integer programming (MIP) solvers. Due to the poor scalability of MIP solvers, large neural networks cannot benefit from these cutting planes. In this paper, we exploit the structure of the neural network verification problem to generate efficient and scalable cutting planes specific for this problem setting. We propose a novel approach, Branch-and-bound Inferred Cuts with COnstraint Strengthening (BICCOS), which leverages the logical relationships of neurons within verified subproblems in the branch-and-bound search tree, and we introduce cuts that preclude these relationships in other subproblems. We develop a mechanism that assigns influence scores to neurons in each path to allow the strengthening of these cuts. Furthermore, we design a multi-tree search technique to identify more cuts, effectively narrowing the search space and accelerating the BaB algorithm. Our results demonstrate that BICCOS can generate hundreds of useful cuts during the branch-and-bound process and consistently increase the number of verifiable instances compared to other state-of-the-art neural network verifiers on a wide range of benchmarks, including large networks that previous cutting plane methods could not scale to. BICCOS is part of the $α,β$-CROWN verifier, the VNN-COMP 2024 winner. The code is available at http://github.com/Lemutisme/BICCOS .	 | 近年来，剪枝平面方法（如GCP-CROWN）已被探索用于增强神经网络验证器，并取得了显著进展。然而，GCP-CROWN 目前依赖于从外部混合整数规划（MIP）求解器生成的通用剪枝平面（切割面）。由于MIP求解器的糟糕可扩展性，大型神经网络无法从这些剪枝平面中受益。本文中，我们利用神经网络验证问题的结构来生成特定于该问题设置的高效且可扩展的剪枝平面。我们提出了一种新颖的方法——分支定界推断切割面与约束加强（BICCOS），该方法利用了分支定界搜索树中验证子问题中神经元的逻辑关系，并引入切割面以阻止其他子问题中的这些关系。我们开发了一种机制，为每条路径中的神经元分配影响评分，以便加强这些切割面。此外，我们设计了一种多树搜索技术来识别更多的切割面，有效地缩小搜索空间并加速分支定界算法。我们的结果显示，BICCOS 在分支定界过程中可以生成数百个有用的切割面，并且在广泛基准测试中，与现有的其他神经网络验证器相比，能够一致地增加可验证实例的数量，包括以往剪枝平面方法无法扩展的大规模网络。BICCOS 是α,β-CROWN 验证器的一部分，该验证器在VNN-COMP 2024 中获得冠军。代码可在 http://github.com/Lemutisme/BICCOS 获取。
2501.00195	 | Towards Unraveling and Improving Generalization in World Models	 | Qiaoyi Fang,Weiyu Du,Hang Wang,Junshan Zhang	 | 本文研究了世界模型在强化学习中的鲁棒性和泛化能力，通过将世界模型学习视为随机动力系统，并开发了一种随机微分方程形式化描述，发现适度的潜空间表示错误可以提高鲁棒性，并提出了一种雅可比正则化方案来减轻累积误差，从而提高训练稳定性和长期预测准确性。	 | World models have recently emerged as a promising approach to reinforcement learning (RL), achieving state-of-the-art performance across a wide range of visual control tasks. This work aims to obtain a deep understanding of the robustness and generalization capabilities of world models. Thus motivated, we develop a stochastic differential equation formulation by treating the world model learning as a stochastic dynamical system, and characterize the impact of latent representation errors on robustness and generalization, for both cases with zero-drift representation errors and with non-zero-drift representation errors. Our somewhat surprising findings, based on both theoretic and experimental studies, reveal that for the case with zero drift, modest latent representation errors can in fact function as implicit regularization and hence result in improved robustness. We further propose a Jacobian regularization scheme to mitigate the compounding error propagation effects of non-zero drift, thereby enhancing training stability and robustness. Our experimental studies corroborate that this regularization approach not only stabilizes training but also accelerates convergence and improves accuracy of long-horizon prediction.	 | 世界模型近年来已成为强化学习（RL）的一种有前途的方法，在多种视觉控制任务中取得了最先进的性能。本工作旨在深入理解世界模型的鲁棒性和泛化能力。为实现这一目标，我们通过将世界模型学习视为随机动力系统，开发了一种随机微分方程形式化描述，并对潜空间表示错误对鲁棒性和泛化能力的影响进行了表征，无论是零漂移表示错误的情况还是非零漂移表示错误的情况。基于理论和实验研究的意外发现显示，在零漂移的情况下，适度的潜空间表示错误实际上可以起到隐式正则化的作用，从而提高鲁棒性。此外，我们提出了一种雅可比正则化方案，以减轻非零漂移导致的累积误差传播效应，从而提高训练稳定性和鲁棒性。我们的实验研究证实，这种正则化方法不仅稳定了训练，还加速了收敛并提高了长期预测的准确性。
2501.00190	 | SepsisCalc: Integrating Clinical Calculators into Early Sepsis Prediction via Dynamic Temporal Graph Construction	 | Changchang Yin,Shihan Fu,Bingsheng Yao,Thai-Hoang Pham,Weidan Cao,Dakuo Wang,Jeffrey Caterino,Ping Zhang	 | SepsisCalc是一种新的框架，将临床计算工具整合到人工智能败血症预测模型中，以提高模型的透明度和准确性，并帮助医生进行早期干预。该框架通过将电子健康记录表示为时间图，动态添加准确估计的计算工具来处理变量缺失的问题，并在实际数据集上展示了优于现有方法的性能。	 | Sepsis is an organ dysfunction caused by a deregulated immune response to an infection. Early sepsis prediction and identification allow for timely intervention, leading to improved clinical outcomes. Clinical calculators (e.g., the six-organ dysfunction assessment of SOFA) play a vital role in sepsis identification within clinicians' workflow, providing evidence-based risk assessments essential for sepsis diagnosis. However, artificial intelligence (AI) sepsis prediction models typically generate a single sepsis risk score without incorporating clinical calculators for assessing organ dysfunctions, making the models less convincing and transparent to clinicians. To bridge the gap, we propose to mimic clinicians' workflow with a novel framework SepsisCalc to integrate clinical calculators into the predictive model, yielding a clinically transparent and precise model for utilization in clinical settings. Practically, clinical calculators usually combine information from multiple component variables in Electronic Health Records (EHR), and might not be applicable when the variables are (partially) missing. We mitigate this issue by representing EHRs as temporal graphs and integrating a learning module to dynamically add the accurately estimated calculator to the graphs. Experimental results on real-world datasets show that the proposed model outperforms state-of-the-art methods on sepsis prediction tasks. Moreover, we developed a system to identify organ dysfunctions and potential sepsis risks, providing a human-AI interaction tool for deployment, which can help clinicians understand the prediction outputs and prepare timely interventions for the corresponding dysfunctions, paving the way for actionable clinical decision-making support for early intervention.	 | 以下是对中文翻译：  败血症是由感染引发的免疫反应失调所导致的一种器官功能障碍。早期败血症预测和识别能够实现及时干预，从而改善临床结果。临床计算工具（例如SOFA的六器官功能障碍评估）在医生的工作流程中起到了关键作用，提供了基于证据的风险评估，这对于败血症的诊断至关重要。然而，人工智能（AI）败血症预测模型通常会生成单一的败血症风险评分，而不考虑临床计算工具来评估器官功能障碍，这使得模型在医生面前缺乏说服力和透明度。为了解决这一问题，我们提出了一种新的框架SepsisCalc，该框架模仿了医生的工作流程，将临床计算工具整合到预测模型中，从而产生一个在临床环境中既透明又精确的预测模型。实践中，临床计算工具通常会结合电子健康记录（EHR）中的多个组成部分变量的信息，但当这些变量部分缺失时，这种方法可能不适用。我们通过将EHR表示为时间图，并集成一个学习模块以动态添加准确估计的计算工具来解决这个问题。在实际数据集上的实验结果显示，所提出的模型在败血症预测任务上优于最先进的方法。此外，我们还开发了一个系统来识别器官功能障碍和潜在的败血症风险，并提供了一个人机交互工具以促进部署，帮助医生理解预测输出并为相应的功能障碍准备及时的干预措施，从而为早期干预提供可操作的临床决策支持。
2501.00184	 | TrajLearn: Trajectory Prediction Learning using Deep Generative Models	 | Amirhossein Nadiri,Jing Li,Ali Faraji,Ghadeer Abuoda,Manos Papagelis	 | TrajLearn是一种新型轨迹预测模型，通过六边形空间表示生成高级次移动流，并结合定制的束搜索方法来预测未来的路径，显著提升了性能；该模型还提出了一种混合分辨率的地图生成算法，以优化数据存储和计算效率。	 | Trajectory prediction aims to estimate an entity's future path using its current position and historical movement data, benefiting fields like autonomous navigation, robotics, and human movement analytics. Deep learning approaches have become key in this area, utilizing large-scale trajectory datasets to model movement patterns, but face challenges in managing complex spatial dependencies and adapting to dynamic environments. To address these challenges, we introduce TrajLearn, a novel model for trajectory prediction that leverages generative modeling of higher-order mobility flows based on hexagonal spatial representation. TrajLearn predicts the next $k$ steps by integrating a customized beam search for exploring multiple potential paths while maintaining spatial continuity. We conducted a rigorous evaluation of TrajLearn, benchmarking it against leading state-of-the-art approaches and meaningful baselines. The results indicate that TrajLearn achieves significant performance gains, with improvements of up to ~40% across multiple real-world trajectory datasets. In addition, we evaluated different prediction horizons (i.e., various values of $k$), conducted resolution sensitivity analysis, and performed ablation studies to assess the impact of key model components. Furthermore, we developed a novel algorithm to generate mixed-resolution maps by hierarchically subdividing hexagonal regions into finer segments within a specified observation area. This approach supports selective detailing, applying finer resolution to areas of interest or high activity (e.g., urban centers) while using coarser resolution for less significant regions (e.g., rural areas), effectively reducing data storage requirements and computational overhead. We promote reproducibility and adaptability by offering complete code, data, and detailed documentation with flexible configuration options for various applications.	 | 轨迹预测旨在根据实体的当前位置和历史运动数据来估计其未来的路径，这在自主导航、机器人技术和人类运动分析等领域具有重要意义。深度学习方法已成为这一领域的关键工具，利用大规模轨迹数据集来建模运动模式，但在处理复杂的空间依赖性和适应动态环境方面面临挑战。为解决这些问题，我们提出了一种名为TrajLearn的新型轨迹预测模型，该模型基于六边形空间表示生成高级次移动流。TrajLearn通过结合定制的束搜索方法来预测接下来的k步，以探索多种潜在路径并保持空间连续性。  我们对TrajLearn进行了严格的评估，并将其与最先进的基准方法进行了对比。结果表明，TrajLearn在多个实际轨迹数据集上取得了显著的性能提升，改进幅度最高可达约40%。此外，我们还评估了不同的预测范围（即不同的k值），进行了分辨率敏感性分析，并进行了消融研究以评估关键模型组件的影响。进一步地，我们开发了一种新的算法，通过逐层细分六边形区域，生成混合分辨率的地图。这种方法在特定观察区域内部对感兴趣区域或高活动区域（如城市中心）使用更高分辨率，而在不那么重要的区域（如农村地区）使用较低分辨率，从而有效减少了数据存储需求和计算开销。  为了促进可重复性和适应性，我们提供了完整的代码、数据和详细的文档，并提供了灵活的配置选项以适应各种应用。
2501.00170	 | Federated Learning with Workload Reduction through Partial Training of Client Models and Entropy-Based Data Selection	 | Hongrui Shi,Valentin Radu,Po Yang	 | 本文提出了一种名为FedFT-EDS的方法，该方法结合了客户端模型微调和基于熵的数据选择，以减少边缘设备上的训练工作量，从而提高了联邦学习的效率和性能。实验结果表明，FedFT-EDS能够在使用更少用户数据的情况下，提高学习效率并优于现有基准方法。	 | With the rapid expansion of edge devices, such as IoT devices, where crucial data needed for machine learning applications is generated, it becomes essential to promote their participation in privacy-preserving Federated Learning (FL) systems. The best way to achieve this desiderate is by reducing their training workload to match their constrained computational resources. While prior FL research has address the workload constrains by introducing lightweight models on the edge, limited attention has been given to optimizing on-device training efficiency through reducing the amount of data need during training. In this work, we propose FedFT-EDS, a novel approach that combines Fine-Tuning of partial client models with Entropy-based Data Selection to reduce training workloads on edge devices. By actively selecting the most informative local instances for learning, FedFT-EDS reduces training data significantly in FL and demonstrates that not all user data is equally beneficial for FL on all rounds. Our experiments on CIFAR-10 and CIFAR-100 show that FedFT-EDS uses only 50% user data while improving the global model performance compared to baseline methods, FedAvg and FedProx. Importantly, FedFT-EDS improves client learning efficiency by up to 3 times, using one third of training time on clients to achieve an equivalent performance to the baselines. This work highlights the importance of data selection in FL and presents a promising pathway to scalable and efficient Federate Learning.	 | 随着边缘设备的迅速扩展，如物联网设备，这些设备生成了机器学习应用所需的关键数据，使其参与隐私保护的联邦学习（FL）系统变得至关重要。实现这一目标的最佳方式是减少它们的训练工作量，以匹配其受限的计算资源。虽然先前的FL研究通过引入轻量级模型来缓解工作量限制，但对通过减少训练所需数据量来优化边缘设备上的训练效率的关注相对较少。在本工作中，我们提出了FedFT-EDS这一创新方法，该方法结合了对客户端模型的微调与基于熵的数据选择，以减少边缘设备上的训练工作量。通过主动选择最具信息量的本地实例进行学习，FedFT-EDS在FL中显著减少了训练数据，并展示了并非所有用户数据在所有轮次中都对FL有益。我们的实验表明，在CIFAR-10和CIFAR-100数据集上，FedFT-EDS仅使用50%的用户数据，其全球模型性能优于基准方法FedAvg和FedProx。更重要的是，FedFT-EDS将客户端学习效率提高了3倍，使用三分之一的训练时间就能达到基准方法的同等性能。这项工作强调了在FL中进行数据选择的重要性，并展示了实现可扩展且高效的联邦学习的有希望途径。
2501.00162	 | Class-based Subset Selection for Transfer Learning under Extreme Label Shift	 | Akul Goyal,Carl Edwards	 | 本文提出了一种新的迁移学习方法，通过选择和加权源域中的类来最小化不同域之间的距离，从而在目标域上实现更好的泛化性能。实验结果表明，该方法在多个数据集上优于现有方法，尤其是在标签空间不相交的极端情况下也表现出色。	 | Existing work within transfer learning often follows a two-step process -- pre-training over a large-scale source domain and then finetuning over limited samples from the target domain. Yet, despite its popularity, this methodology has been shown to suffer in the presence of distributional shift -- specifically when the output spaces diverge. Previous work has focused on increasing model performance within this setting by identifying and classifying only the shared output classes between distributions. However, these methods are inherently limited as they ignore classes outside the shared class set, disregarding potential information relevant to the model transfer. This paper proposes a new process for few-shot transfer learning that selects and weighs classes from the source domain to optimize the transfer between domains. More concretely, we use Wasserstein distance to choose a set of source classes and their weights that minimize the distance between the source and target domain. To justify our proposed algorithm, we provide a generalization analysis of the performance of the learned classifier over the target domain and show that our method corresponds to a bound minimization algorithm. We empirically demonstrate the effectiveness of our approach (WaSS) by experimenting on several different datasets and presenting superior performance within various label shift settings, including the extreme case where the label spaces are disjoint.	 | 现有的迁移学习工作通常遵循两步过程——在大规模源域进行预训练，然后在目标域的有限样本上进行微调。尽管这种方法非常流行，但已有研究表明，在出现分布偏移的情况下，这种方法会受到影响，特别是当输出空间存在差异时。以往的工作主要集中在通过识别和分类不同分布之间的共享输出类来提高模型在这一环境中的性能。然而，这些方法本身存在局限性，因为它们忽略了共享类集之外的类，从而忽略了可能对模型传输有用的潜在信息。本文提出了一种新的少量样本迁移学习过程，该过程从源域中选择并加权类，以优化不同域之间的传输。具体来说，我们使用 Wasserstein 距离来选择一个源类集及其权重，以最小化源域和目标域之间的距离。为了证明我们提出的算法的有效性，我们提供了所学分类器在目标域上的泛化分析，并证明我们的方法对应于一种边界最小化算法。通过在多个不同数据集上进行实验并展示在各种标签偏移设置下优于其他方法的效果，我们实证展示了我们方法（WaSS）的有效性，包括极端情况，即标签空间是不相交的。
2501.00158	 | Urban Water Consumption Forecasting Using Deep Learning and Correlated District Metered Areas	 | Kleanthis Malialis,Nefeli Mavri,Stelios G. Vrachimis,Marios S. Kyriakou,Demetrios G. Eliades,Marios M. Polycarpou	 | 本文提出了一种利用深度学习进行区计量区域（DMAs）短期用水量预测的新方法，通过结合相关DMAs的历史用水量模式和局部数据来提高预测准确性，研究结果表明该方法优于传统统计模型，并能有效应对数据缺失和传感器故障等问题。	 | Accurate water consumption forecasting is a crucial tool for water utilities and policymakers, as it helps ensure a reliable supply, optimize operations, and support infrastructure planning. Urban Water Distribution Networks (WDNs) are divided into District Metered Areas (DMAs), where water flow is monitored to efficiently manage resources. This work focuses on short-term forecasting of DMA consumption using deep learning and aims to address two key challenging issues. First, forecasting based solely on a DMA's historical data may lack broader context and provide limited insights. Second, DMAs may experience sensor malfunctions providing incorrect data, or some DMAs may not be monitored at all due to computational costs, complicating accurate forecasting. We propose a novel method that first identifies DMAs with correlated consumption patterns and then uses these patterns, along with the DMA's local data, as input to a deep learning model for forecasting. In a real-world study with data from five DMAs, we show that: i) the deep learning model outperforms a classical statistical model; ii) accurate forecasting can be carried out using only correlated DMAs' consumption patterns; and iii) even when a DMA's local data is available, including correlated DMAs' data improves accuracy.	 | 准确的用水量预测是水资源管理部门和政策制定者的重要工具，它有助于确保可靠的供水、优化运营并支持基础设施规划。城市供水网络（WDNs）被划分为区计量区域（DMAs），通过监测水流量来高效管理资源。本文的重点是利用深度学习进行DMAs短期用水量预测，并旨在解决两个关键挑战。首先，仅基于DMAs的历史数据进行预测可能缺乏更广泛的情境，提供的洞见有限。其次，DMAs可能存在传感器故障提供错误数据的情况，或者由于计算成本问题，并非所有DMAs都会被监测，这会增加准确预测的复杂性。我们提出了一种新颖的方法，即首先识别具有相关用水量模式的DMAs，然后将这些模式与DMAs的本地数据作为输入，送入深度学习模型进行预测。在使用来自五个DMAs的实际数据的研究中，我们展示了以下结果：i) 深度学习模型优于传统的统计模型；ii) 仅使用相关DMAs的用水量模式即可进行准确预测；iii) 即使DMAs的本地数据可用，包括相关DMAs的数据也能提高预测精度。
2501.00107	 | An Unsupervised Anomaly Detection in Electricity Consumption Using Reinforcement Learning and Time Series Forest Based Framework	 | Jihan Ghanim,Mariette Awad	 | 该研究提出了一种结合时间序列森林和强化学习的新型异常检测模型选择方法，能够在无需昂贵的地面真值标签的情况下有效检测时间序列中的异常，并在多个数据集上表现出色，特别是在F1分数上优于其他方法。	 | Anomaly detection (AD) plays a crucial role in time series applications, primarily because time series data is employed across real-world scenarios. Detecting anomalies poses significant challenges since anomalies take diverse forms making them hard to pinpoint accurately. Previous research has explored different AD models, making specific assumptions with varying sensitivity toward particular anomaly types. To address this issue, we propose a novel model selection for unsupervised AD using a combination of time series forest (TSF) and reinforcement learning (RL) approaches that dynamically chooses an AD technique. Our approach allows for effective AD without explicitly depending on ground truth labels that are often scarce and expensive to obtain. Results from the real-time series dataset demonstrate that the proposed model selection approach outperforms all other AD models in terms of the F1 score metric. For the synthetic dataset, our proposed model surpasses all other AD models except for KNN, with an impressive F1 score of 0.989. The proposed model selection framework also exceeded the performance of GPT-4 when prompted to act as an anomaly detector on the synthetic dataset. Exploring different reward functions revealed that the original reward function in our proposed AD model selection approach yielded the best overall scores. We evaluated the performance of the six AD models on an additional three datasets, having global, local, and clustered anomalies respectively, showing that each AD model exhibited distinct performance depending on the type of anomalies. This emphasizes the significance of our proposed AD model selection framework, maintaining high performance across all datasets, and showcasing superior performance across different anomaly types.	 | 异常检测（AD）在时间序列应用中起着至关重要的作用，主要是因为时间序列数据在现实世界场景中被广泛使用。检测异常具有重大挑战性，因为异常的表现形式多样，难以准确识别。之前的研究所探索了不同的AD模型，这些模型针对特定类型的异常具有不同的敏感性。为了解决这一问题，我们提出了一种利用时间序列森林（TSF）和强化学习（RL）相结合的新型模型选择方法，动态选择合适的AD技术。该方法允许在无需明确依赖稀缺且昂贵的地面真值标签的情况下进行有效的异常检测。来自实时序列数据集的结果表明，所提出的方法在F1分数指标上优于所有其他AD模型。对于合成数据集，所提出的方法除了KNN外，所有其他AD模型均被超越，F1分数为0.989。所提出的方法选择框架在合成数据集上的表现也超过了GPT-4。探索不同的奖励函数显示，我们提出的AD模型选择方法中的原始奖励函数给出了最佳的整体得分。我们在另外三个分别具有全局、局部和聚类异常的数据集上评估了六种AD模型的表现，显示每种AD模型在不同类型的异常下的表现各不相同。这强调了所提出AD模型选择框架的重要性，它在所有数据集上保持了高表现，并在不同类型的异常上展示了优异的表现。
2501.00085	 | Machine Learning-Based Security Policy Analysis	 | Krish Jain,Joann Sum,Pranav Kapoor,Amir Eaman	 | 本研究探讨了使用图技术结合机器学习方法自动分析SELinux策略以检测异常，并发现MLP神经网络在不同数据集上表现最优，达到95%的准确率。这种方法为复杂SELinux策略的分析提供了更高效、自动化的途径。	 | Security-Enhanced Linux (SELinux) is a robust security mechanism that enforces mandatory access controls (MAC), but its policy language's complexity creates challenges for policy analysis and management. This research investigates the automation of SELinux policy analysis using graph-based techniques combined with machine learning approaches to detect policy anomalies. The study addresses two key questions: Can SELinux policy analysis be automated through graph analysis, and how do different anomaly detection models compare in analyzing SELinux policies? We will be comparing different machine learning models by evaluating their effectiveness in detecting policy violations and anomalies. Our approach utilizes Neo4j for graph representation of policies, with Node2vec transforming these graph structures into meaningful vector embeddings that can be processed by our machine learning models. In our results, the MLP Neural Network consistently demonstrated superior performance across different dataset sizes, achieving 95% accuracy with balanced precision and recall metrics, while both Random Forest and SVM models showed competitive but slightly lower performance in detecting policy violations. This combination of graph-based modeling and machine learning provides a more sophisticated and automated approach to understanding and analyzing complex SELinux policies compared to traditional manual analysis methods.	 | 安全增强型Linux（SELinux）是一种强大的安全机制，它实现了强制访问控制（MAC），但其策略语言的复杂性给策略分析和管理带来了挑战。本研究探讨了使用基于图的技术结合机器学习方法自动进行SELinux策略分析，以检测策略异常。该研究主要解决两个关键问题：SELinux策略分析能否通过图分析实现自动化，以及不同异常检测模型在分析SELinux策略方面的表现如何？我们将通过评估不同机器学习模型在检测策略违规和异常方面的有效性来比较不同的机器学习模型。我们的方法利用Neo4j表示策略的图形结构，并使用Node2vec将这些图形结构转换为有意义的向量嵌入，以便我们的机器学习模型进行处理。在我们的结果中，MLP神经网络在不同数据集大小上始终表现出更优越的性能，实现了95%的准确率，同时平衡的精确度和召回率指标，而随机森林和SVM模型在检测策略违规方面表现竞争但略低。这种基于图的建模与机器学习的结合为理解并分析复杂的SELinux策略提供了一种更高级和自动化的途径，相比传统的手动分析方法更为复杂和高效。
2501.00076	 | A Novel Framework for Learning Stochastic Representations for Sequence Generation and Recognition	 | Jungsik Hwang,Ahmadreza Ahmadi	 | 该研究提出了一种参数偏置的随机递归神经网络（RNNPB），通过引入潜在空间中的随机性，提高了生成和识别序列数据的能力，特别是在机器人运动数据集上表现优于传统确定性模型，增强了系统的鲁棒性和适应性。	 | The ability to generate and recognize sequential data is fundamental for autonomous systems operating in dynamic environments. Inspired by the key principles of the brain-predictive coding and the Bayesian brain-we propose a novel stochastic Recurrent Neural Network with Parametric Biases (RNNPB). The proposed model incorporates stochasticity into the latent space using the reparameterization trick used in variational autoencoders. This approach enables the model to learn probabilistic representations of multidimensional sequences, capturing uncertainty and enhancing robustness against overfitting. We tested the proposed model on a robotic motion dataset to assess its performance in generating and recognizing temporal patterns. The experimental results showed that the stochastic RNNPB model outperformed its deterministic counterpart in generating and recognizing motion sequences. The results highlighted the proposed model's capability to quantify and adjust uncertainty during both learning and inference. The stochasticity resulted in a continuous latent space representation, facilitating stable motion generation and enhanced generalization when recognizing novel sequences. Our approach provides a biologically inspired framework for modeling temporal patterns and advances the development of robust and adaptable systems in artificial intelligence and robotics.	 | 生成和识别序列数据的能力对于在动态环境中运行的自主系统至关重要。受脑预测编码和贝叶斯大脑的关键原理启发，我们提出了一种参数偏置的随机递归神经网络（RNNPB）。该模型利用变分自编码器中的重参数化技巧将随机性引入到潜在空间中，使模型能够学习多维序列的概率表示，捕获不确定性并增强对过拟合的鲁棒性。我们通过一个机器人运动数据集测试了该模型，以评估其在生成和识别时间模式方面的性能。实验结果表明，随机RNNPB模型在生成和识别运动序列方面优于其确定性版本。结果突显了所提模型在学习和推断过程中量化和调整不确定性的能力。随机性导致了连续的潜在空间表示，促进了稳定的运动生成，并在识别新序列时增强了泛化能力。我们的方法为建模时间模式提供了生物启发的框架，并推动了人工智能和机器人领域中鲁棒和适应性系统的开发。
2501.00072	 | Open-Book Neural Algorithmic Reasoning	 | Hefei Li,Chao Peng,Chenyang Xu,Zhengfeng Yang	 | 该研究提出了一种开书学习框架，允许神经网络在处理特定任务时访问整个训练数据集，以提升其在复杂算法任务上的推理能力。通过引入注意力机制，该框架不仅提高了神经网络的性能，还揭示了不同算法任务之间的内在联系，并提供了可解释的多任务训练工具。	 | Neural algorithmic reasoning is an emerging area of machine learning that focuses on building neural networks capable of solving complex algorithmic tasks. Recent advancements predominantly follow the standard supervised learning paradigm -- feeding an individual problem instance into the network each time and training it to approximate the execution steps of a classical algorithm. We challenge this mode and propose a novel open-book learning framework. In this framework, whether during training or testing, the network can access and utilize all instances in the training dataset when reasoning for a given instance.   Empirical evaluation is conducted on the challenging CLRS Algorithmic Reasoning Benchmark, which consists of 30 diverse algorithmic tasks. Our open-book learning framework exhibits a significant enhancement in neural reasoning capabilities. Further, we notice that there is recent literature suggesting that multi-task training on CLRS can improve the reasoning accuracy of certain tasks, implying intrinsic connections between different algorithmic tasks. We delve into this direction via the open-book framework. When the network reasons for a specific task, we enable it to aggregate information from training instances of other tasks in an attention-based manner. We show that this open-book attention mechanism offers insights into the inherent relationships among various tasks in the benchmark and provides a robust tool for interpretable multi-task training.	 | 神经算法推理是机器学习的一个新兴领域，专注于构建能够解决复杂算法任务的神经网络。近期的进步大多遵循传统的监督学习范式——每次将单个问题实例输入网络，并训练它来近似经典算法的执行步骤。我们对这种模式提出了挑战，并提出了一种新型的开书学习框架。在该框架中，无论是训练还是测试期间，网络在处理给定实例时可以访问并利用训练数据集中的所有实例来进行推理。在CLRS算法推理基准测试上进行了实证评估，该基准测试包含30个不同的算法任务。我们的开书学习框架在神经推理能力上表现出显著的提升。此外，我们注意到最近有文献表明，在CLRS上进行多任务训练可以提高某些任务的推理准确性，暗示了不同算法任务之间的内在联系。通过开书框架，我们探索了这一方向。当网络处理特定任务时，我们可以使其以注意力机制的方式聚合来自其他任务的训练实例信息。我们展示了这种开书注意力机制为理解基准测试中各种任务之间的固有联系提供了见解，并提供了一种用于可解释多任务训练的稳健工具。
2501.00065	 | Predicting Preschoolers' Externalizing Problems with Mother-Child Interaction Dynamics and Deep Learning	 | Xi Chen,Yu Ji,Cong Xia,Wen Wu	 | 该研究通过分析母亲与儿童在完成挑战性任务时的互动动态，利用剩余动态结构方程模型和深度学习技术预测儿童未来的行为外化问题，并发现母亲的自主支持反应对儿童行为外化问题有重要影响，而深度学习模型能进一步提高预测准确性。	 | Objective: Predicting children's future levels of externalizing problems helps to identify children at risk and guide targeted prevention. Existing studies have shown that mothers providing support in response to children's dysregulation was associated with children's lower levels of externalizing problems. The current study aims to evaluate and improve the accuracy of predicting children's externalizing problems with mother-child interaction dynamics. Method: This study used mother-child interaction dynamics during a challenging puzzle task to predict children's externalizing problems six months later (N=101, 46 boys, Mage=57.41 months, SD=6.58). Performance of the Residual Dynamic Structural Equation Model (RDSEM) was compared with the Attention-based Sequential Behavior Interaction Modeling (ASBIM) model, developed using the deep learning techniques. Results: The RDSEM revealed that children whose mothers provided more autonomy support after increases of child defeat had lower levels of externalizing problems. Five-fold cross-validation showed that the RDSEM had good prediction accuracy. The ASBIM model further improved prediction accuracy, especially after including child inhibitory control as a personalized individual feature. Conclusions: The dynamic process of mother-child interaction provides important information for predicting children's externalizing problems, especially maternal autonomy supportive response to child defeat. The deep learning model is a useful tool to further improve prediction accuracy.	 | 目标：预测儿童未来的行为外化问题有助于识别高风险儿童并指导针对性的预防措施。现有研究显示，母亲在孩子出现调节不良时给予支持与儿童行为外化问题较低有关。本研究旨在通过母亲与儿童互动动态来评估和提高预测儿童行为外化问题的准确性。方法：本研究使用母亲与儿童在完成具有挑战性的拼图任务时的互动动态来预测六个月后儿童的行为外化问题（N=101，46名男孩，平均年龄=57.41个月，标准差=6.58）。比较了剩余动态结构方程模型（RDSEM）和使用深度学习技术开发的注意力基于顺序行为互动建模（ASBIM）模型的预测性能。结果：RDSEM显示，在孩子遭遇失败后提供更高自主支持的母亲所养育的孩子行为外化问题较低。五折交叉验证显示，RDSEM具有良好的预测准确性。ASBIM模型进一步提高了预测准确性，特别是在包括儿童抑制控制作为个性化个体特征后。结论：母亲与儿童互动的动态过程为预测儿童行为外化问题提供了重要信息，特别是母亲对孩子失败的自主支持反应。深度学习模型是进一步提高预测准确性的有用工具。
2501.00063	 | "Generative Models for Financial Time Series Data: Enhancing Signal-to-Noise Ratio and Addressing Data Scarcity in A-Share Market	 | Guangming Che	 | 本文提出了两种创新的生成模型方法，专门用于中国A股市场的股票数据合成，以解决数据稀缺和低信噪比的问题，实验结果显示这些方法能够有效提高预测模型的性能和价格交易策略中单个股票信号的信噪比。	 | The financial industry is increasingly seeking robust methods to address the challenges posed by data scarcity and low signal-to-noise ratios, which limit the application of deep learning techniques in stock market analysis. This paper presents two innovative generative model-based approaches to synthesize stock data, specifically tailored for different scenarios within the A-share market in China. The first method, a sector-based synthesis approach, enhances the signal-to-noise ratio of stock data by classifying the characteristics of stocks from various sectors in China's A-share market. This method employs an Approximate Non-Local Total Variation algorithm to smooth the generated data, a bandpass filtering method based on Fourier Transform to eliminate noise, and Denoising Diffusion Implicit Models to accelerate sampling speed. The second method, a recursive stock data synthesis approach based on pattern recognition, is designed to synthesize data for stocks with short listing periods and limited comparable companies. It leverages pattern recognition techniques and Markov models to learn and generate variable-length stock sequences, while introducing a sub-time-level data augmentation method to alleviate data scarcity issues.We validate the effectiveness of these methods through extensive experiments on various datasets, including those from the main board, STAR Market, Growth Enterprise Market Board, Beijing Stock Exchange, NASDAQ, NYSE, and AMEX. The results demonstrate that our synthesized data not only improve the performance of predictive models but also enhance the signal-to-noise ratio of individual stock signals in price trading strategies. Furthermore, the introduction of sub-time-level data significantly improves the quality of synthesized data.	 | 金融行业正越来越多地寻求稳健的方法来应对数据稀缺和低信噪比带来的挑战，这限制了深度学习技术在股市分析中的应用。本文介绍了两种创新的生成模型方法，专门用于中国A股市场的不同场景，以合成股票数据。第一种方法是基于行业的合成方法，通过对中国A股市场各类股票的特点进行分类，提高股票数据的信噪比。该方法采用近似非局部总变差算法平滑生成的数据，基于傅里叶变换的带通滤波方法来消除噪声，并使用去噪扩散隐式模型来加速采样速度。第二种方法是一种基于模式识别的递归股票数据合成方法，旨在合成上市时间短且可比公司数量有限的股票的数据。该方法利用模式识别技术和马尔可夫模型来学习并生成不同长度的股票序列，并引入子时级数据增强方法来缓解数据稀缺问题。我们通过在各种数据集上进行广泛的实验，包括主板、科创板、创业板、北交所、纳斯达克、纽约证券交易所和美洲证券交易所的数据集，验证了这些方法的有效性。实验结果表明，我们合成的数据不仅提高了预测模型的性能，还提高了价格交易策略中单个股票信号的信噪比。此外，引入子时级数据显著提高了合成数据的质量。
2501.00061	 | Training-free Heterogeneous Model Merging	 | Zhengqi Xu,Han Zheng,Jie Song,Li Sun,Mingli Song	 | 该研究提出了一种新的异质模型融合框架，通过层对齐策略解决深度异质性，以及弹性神经元缝合算法解决宽度异质性，以实现不同架构模型的有效融合，实验表明该方法在视觉和自然语言处理任务中能达到与同质模型融合相当的性能。	 | Model merging has attracted significant attention as a powerful paradigm for model reuse, facilitating the integration of task-specific models into a singular, versatile framework endowed with multifarious capabilities. Previous studies, predominantly utilizing methods such as Weight Average (WA), have shown that model merging can effectively leverage pretrained models without the need for laborious retraining. However, the inherent heterogeneity among models poses a substantial constraint on its applicability, particularly when confronted with discrepancies in model architectures. To overcome this challenge, we propose an innovative model merging framework designed for heterogeneous models, encompassing both depth and width heterogeneity. To address depth heterogeneity, we introduce a layer alignment strategy that harmonizes model layers by segmenting deeper models, treating consecutive layers with similar representations as a cohesive segment, thus enabling the seamless merging of models with differing layer depths. For width heterogeneity, we propose a novel elastic neuron zipping algorithm that projects the weights from models of varying widths onto a common dimensional space, eliminating the need for identical widths. Extensive experiments validate the efficacy of these proposed methods, demonstrating that the merging of structurally heterogeneous models can achieve performance levels comparable to those of homogeneous merging, across both vision and NLP tasks. Our code is publicly available at https://github.com/zju-vipa/training_free_heterogeneous_model_merging.	 | 模型融合已因其作为模型重用强大力量的范式而备受关注，能够将特定任务模型集成到一个多功能、灵活的单一框架中。以往的研究大多使用加权平均（WA）等方法，表明模型融合可以在无需大量重新训练的情况下有效利用预训练模型。然而，模型之间的固有异质性在面对模型架构差异时构成了重大限制。为克服这一挑战，我们提出了一种新的异质模型融合框架，该框架涵盖了深度和宽度的异质性。为解决深度异质性，我们引入了一种层对齐策略，将较深的模型分割，将具有相似表示的连续层视为一个整体段，从而实现层深度不同的模型的无缝融合。对于宽度异质性，我们提出了一种新颖的弹性神经元缝合算法，将不同宽度模型的权重投影到共同维度空间中，从而消除需要相同宽度的需求。广泛实验验证了这些方法的有效性，表明结构异质模型的融合可以达到与同质模型融合相当的性能水平，涵盖视觉和自然语言处理任务。我们的代码已在 https://github.com/zju-vipa/training_free_heterogeneous_model_merging 公开。
2501.00057	 | VisTabNet: Adapting Vision Transformers for Tabular Data	 | Witold Wydmański,Ulvi Movsum-zada,Jacek Tabor,Marek Śmieja	 | VisTabNet 是一种跨模态迁移学习方法，利用预训练的视觉变换器处理表格数据，显著提升了小规模表格数据下游任务的性能，超越了传统方法和最近的深度学习模型。	 | Although deep learning models have had great success in natural language processing and computer vision, we do not observe comparable improvements in the case of tabular data, which is still the most common data type used in biological, industrial and financial applications. In particular, it is challenging to transfer large-scale pre-trained models to downstream tasks defined on small tabular datasets. To address this, we propose VisTabNet -- a cross-modal transfer learning method, which allows for adapting Vision Transformer (ViT) with pre-trained weights to process tabular data. By projecting tabular inputs to patch embeddings acceptable by ViT, we can directly apply a pre-trained Transformer Encoder to tabular inputs. This approach eliminates the conceptual cost of designing a suitable architecture for processing tabular data, while reducing the computational cost of training the model from scratch. Experimental results on multiple small tabular datasets (less than 1k samples) demonstrate VisTabNet's superiority, outperforming both traditional ensemble methods and recent deep learning models. The proposed method goes beyond conventional transfer learning practice and shows that pre-trained image models can be transferred to solve tabular problems, extending the boundaries of transfer learning.	 | 尽管深度学习模型在自然语言处理和计算机视觉领域取得了巨大成功，但在表格数据领域，我们并未观察到类似的改进，而表格数据仍然是生物、工业和金融应用中最常见的数据类型。特别是，将大规模预训练模型迁移应用于小规模定义的表格数据下游任务仍然极具挑战性。为了解决这个问题，我们提出了VisTabNet——一种跨模态迁移学习方法，它允许使用预训练权重的视觉变换器（ViT）来处理表格数据。通过将表格输入投影到ViT可接受的补丁嵌入中，可以直接将预训练的变换器编码器应用于表格输入。这种方法消除了设计适合处理表格数据的架构的观念成本，并降低了从零开始训练模型的计算成本。在多个小型表格数据集（少于1000个样本）上的实验结果表明，VisTabNet具有明显的优势，超过了传统的集成方法和最近的深度学习模型。所提出的方法超越了传统的迁移学习实践，表明预训练的图像模型可以被转移到解决表格问题，扩展了迁移学习的边界。
2501.00056	 | Transforming CCTV cameras into NO$_2$ sensors at city scale for adaptive policymaking	 | Mohamed R. Ibrahim,Terry Lyons	 | 该研究利用城市监控摄像头和预测图深度模型生成NO₂预测，揭示了特定交通模式对NO₂水平的影响，提出通过现有摄像头基础设施低成本监测和减轻空气污染。	 | Air pollution in cities, especially NO\textsubscript{2}, is linked to numerous health problems, ranging from mortality to mental health challenges and attention deficits in children. While cities globally have initiated policies to curtail emissions, real-time monitoring remains challenging due to limited environmental sensors and their inconsistent distribution. This gap hinders the creation of adaptive urban policies that respond to the sequence of events and daily activities affecting pollution in cities. Here, we demonstrate how city CCTV cameras can act as a pseudo-NO\textsubscript{2} sensors. Using a predictive graph deep model, we utilised traffic flow from London's cameras in addition to environmental and spatial factors, generating NO\textsubscript{2} predictions from over 133 million frames. Our analysis of London's mobility patterns unveiled critical spatiotemporal connections, showing how specific traffic patterns affect NO\textsubscript{2} levels, sometimes with temporal lags of up to 6 hours. For instance, if trucks only drive at night, their effects on NO\textsubscript{2} levels are most likely to be seen in the morning when people commute. These findings cast doubt on the efficacy of some of the urban policies currently being implemented to reduce pollution. By leveraging existing camera infrastructure and our introduced methods, city planners and policymakers could cost-effectively monitor and mitigate the impact of NO\textsubscript{2} and other pollutants.	 | 城市中的空气污染，尤其是NO\(_2\)，与多种健康问题相关，从死亡率到心理健康问题和儿童注意力缺陷等。尽管全球城市已经开始实施政策来减少排放，但由于环境传感器数量有限且分布不均，实时监测仍然面临挑战。这一差距阻碍了适应性强的城市政策的制定，这些政策能够响应影响城市污染的一系列事件和日常活动。在此，我们展示了城市监控摄像头如何作为伪NO\(_2\)传感器的作用。利用预测图深度模型，我们利用伦敦摄像头中的交通流量以及环境和空间因素，从超过1.33亿帧中生成了NO\(_2\)的预测。我们对伦敦出行模式的分析揭示了关键的空间和时间联系，展示了特定交通模式如何影响NO\(_2\)水平，有时甚至存在长达6小时的时间滞后。例如，如果卡车只在夜间行驶，他们在NO\(_2\)水平上的影响最可能在早晨通勤时显现。这些发现对当前旨在减少污染的一些城市政策的有效性提出了质疑。通过利用现有的摄像头基础设施和我们提出的方法，城市规划者和政策制定者可以以成本效益的方式监测和减轻NO\(_2\)和其他污染物的影响。
2501.00054	 | AdvAnchor: Enhancing Diffusion Model Unlearning with Adversarial Anchors	 | Mengnan Zhao,Lihe Zhang,Xingyi Yang,Tianhang Zheng,Baocai Yin	 | 本文分析了不同文本锚点对遗忘性能的影响，并提出了一种名为AdvAnchor的新方法，通过生成与不良概念高度相似的对抗性锚点来缓解性能权衡问题，从而有效消除不适当的概念同时保持其他概念的性能。	 | Security concerns surrounding text-to-image diffusion models have driven researchers to unlearn inappropriate concepts through fine-tuning. Recent fine-tuning methods typically align the prediction distributions of unsafe prompts with those of predefined text anchors. However, these techniques exhibit a considerable performance trade-off between eliminating undesirable concepts and preserving other concepts. In this paper, we systematically analyze the impact of diverse text anchors on unlearning performance. Guided by this analysis, we propose AdvAnchor, a novel approach that generates adversarial anchors to alleviate the trade-off issue. These adversarial anchors are crafted to closely resemble the embeddings of undesirable concepts to maintain overall model performance, while selectively excluding defining attributes of these concepts for effective erasure. Extensive experiments demonstrate that AdvAnchor outperforms state-of-the-art methods. Our code is publicly available at https://anonymous.4open.science/r/AdvAnchor.	 | 文本到图像扩散模型的安全问题促使研究人员通过微调来消除不适当的概念。最近的微调方法通常通过与预定义文本锚点的预测分布对齐来消除不安全提示。然而，这些技术在消除不良概念和保留其他概念之间表现出显著的性能权衡。在本文中，我们系统地分析了不同文本锚点对遗忘性能的影响。根据这一分析，我们提出了一种名为AdvAnchor的新方法，该方法生成对抗性锚点以缓解这种权衡问题。这些对抗性锚点被设计成与不良概念的嵌入高度相似，以保持整体模型性能，同时有选择地排除这些概念的定义属性，以实现有效的消除。大量实验表明，AdvAnchor优于最先进的方法。我们的代码可以在 https://anonymous.4open.science/r/AdvAnchor 公开获取。
2501.00052	 | Efficient and Scalable Deep Reinforcement Learning for Mean Field Control Games	 | Nianli Peng,Yilin Wang	 | 本文提出了一种基于深度强化学习的方法来近似均场控制博弈的均衡解，通过采用多种技术改进了基线算法的效率和可扩展性，并在实验中展示了显著的性能提升。这种方法为解决大规模实际问题，如自主交通系统和经济竞争等，提供了新的可能性。	 | Mean Field Control Games (MFCGs) provide a powerful theoretical framework for analyzing systems of infinitely many interacting agents, blending elements from Mean Field Games (MFGs) and Mean Field Control (MFC). However, solving the coupled Hamilton-Jacobi-Bellman and Fokker-Planck equations that characterize MFCG equilibria remains a significant computational challenge, particularly in high-dimensional or complex environments.   This paper presents a scalable deep Reinforcement Learning (RL) approach to approximate equilibrium solutions of MFCGs. Building on previous works, We reformulate the infinite-agent stochastic control problem as a Markov Decision Process, where each representative agent interacts with the evolving mean field distribution. We use the actor-critic based algorithm from a previous paper (Angiuli et.al., 2024) as the baseline and propose several versions of more scalable and efficient algorithms, utilizing techniques including parallel sample collection (batching); mini-batching; target network; proximal policy optimization (PPO); generalized advantage estimation (GAE); and entropy regularization. By leveraging these techniques, we effectively improved the efficiency, scalability, and training stability of the baseline algorithm.   We evaluate our method on a linear-quadratic benchmark problem, where an analytical solution to the MFCG equilibrium is available. Our results show that some versions of our proposed approach achieve faster convergence and closely approximate the theoretical optimum, outperforming the baseline algorithm by an order of magnitude in sample efficiency. Our work lays the foundation for adapting deep RL to solve more complicated MFCGs closely related to real life, such as large-scale autonomous transportation systems, multi-firm economic competition, and inter-bank borrowing problems.	 | 均场控制博弈（Mean Field Control Games, MFCGs）为分析无限多个相互作用代理系统的理论框架提供了强大的工具，结合了均场博弈（Mean Field Games, MFGs）和均场控制（Mean Field Control, MFC）的元素。然而，求解描述MFCG平衡解的耦合哈密尔顿-雅克比-贝尔曼方程和福克-Planck方程仍然是一个重大的计算挑战，尤其是在高维或复杂环境中。本文提出了一种可扩展的深度强化学习（Reinforcement Learning, RL）方法来近似MFCG的均衡解。基于先前的研究，我们将无限代理的随机控制问题重新表述为马尔可夫决策过程（Markov Decision Process, MDP），其中每个代表性代理与不断演变的均场分布相互作用。我们以Angiuli等人（2024）的一篇论文中的基于演员-评论家算法（actor-critic algorithm）为基础，并提出了几种更具可扩展性和效率的算法版本，采用了并行样本收集（批处理）、小批量处理、目标网络、近端策略优化（Proximal Policy Optimization, PPO）、广义优势估计（Generalized Advantage Estimation, GAE）和熵正则化等技术。通过利用这些技术，我们有效地提高了基线算法的效率、可扩展性和训练稳定性能。  我们使用线性-二次基准问题对我们的方法进行了评估，该问题的MFCG均衡解是可用的。实验结果表明，我们提出的某些版本方法实现了更快的收敛速度，并且能够接近理论最优解，在样本效率上比基线算法高出一个数量级。我们的研究为将深度强化学习（Deep RL）应用于解决与现实世界相关的更复杂的MFCG奠定了基础，例如大规模自主交通系统、多公司经济竞争和银行间借贷问题。
2501.00051	 | DDD-GenDT: Dynamic Data-driven Generative Digital Twin Framework	 | Yu-Zheng Lin,Qinxuan Shi,Zhanglong Yang,Banafsheh Saber Latibari,Sicong Shao,Soheil Salehi,Pratik Satam	 | 本文提出了一种名为DDD-GenDT的新方法，该方法利用大规模语言模型（LLM）与物理系统交互以生成动态数据驱动的数字孪生，克服了传统数字孪生在数据需求和适应性方面的限制，并在CNC加工过程中验证了其有效性和高效性。	 | Digital twin (DT) technology has emerged as a transformative approach to simulate, predict, and optimize the behavior of physical systems, with applications that span manufacturing, healthcare, climate science, and more. However, the development of DT models often faces challenges such as high data requirements, integration complexity, and limited adaptability to dynamic changes in physical systems. This paper presents a new method inspired by dynamic data-driven applications systems (DDDAS), called the dynamic data-driven generative of digital twins framework (DDD-GenDT), which combines the physical system with LLM, allowing LLM to act as DT to interact with the physical system operating status and generate the corresponding physical behaviors. We apply DDD-GenDT to the computer numerical control (CNC) machining process, and we use the spindle current measurement data in the NASA milling wear data set as an example to enable LLMs to forecast the physical behavior from historical data and interact with current observations. Experimental results show that in the zero-shot prediction setting, the LLM-based DT can adapt to the change in the system, and the average RMSE of the GPT-4 prediction is 0.479A, which is 4.79% of the maximum spindle motor current measurement of 10A, with little training data and instructions required. Furthermore, we analyze the performance of DDD-GenDT in this specific application and their potential to construct digital twins. We also discuss the limitations and challenges that may arise in practical implementations.	 | 数字孪生（DT）技术已成为模拟、预测和优化物理系统行为的一种变革性方法，适用于制造、医疗保健、气候科学等多个领域。然而，DT模型的开发常常面临高数据需求、集成复杂性和对物理系统动态变化的有限适应性等挑战。本文提出了一种受动态数据驱动应用系统（DDDAS）启发的新方法，称为动态数据驱动生成数字孪生框架（DDD-GenDT），该方法将物理系统与大规模语言模型（LLM）结合，使得LLM能够作为DT与物理系统运行状态进行交互，并生成相应的物理行为。我们将DDD-GenDT应用于计算机数控（CNC）加工过程，并使用NASA铣削磨损数据集中转子电流测量数据作为示例，使LLM能够从历史数据预测物理行为，并与当前观测进行交互。实验结果显示，在零样本预测设置下，基于LLM的DT能够适应系统变化，GPT-4预测的平均均方根误差（RMSE）为0.479A，即10A最大转子电机电流测量的4.79%，且所需的训练数据和指令很少。此外，我们在这一特定应用中分析了DDD-GenDT的性能及其构建数字孪生的潜力，并讨论了实际应用中可能出现的限制和挑战。
2501.00048	 | Stroke Prediction using Clinical and Social Features in Machine Learning	 | Aidan Chadha	 | 该研究分析了神经网络和逻辑回归模型在中风预测中的应用，旨在通过比较这两种方法的优缺点，开发出最有效的预测模型，以减少假阴性结果。背景介绍指出，中风是全球第二大致死和致残原因，每年在美国有80万人中风，每四分钟就有一个人因此死亡。	 | Every year in the United States, 800,000 individuals suffer a stroke - one person every 40 seconds, with a death occurring every four minutes. While individual factors vary, certain predictors are more prevalent in determining stroke risk. As strokes are the second leading cause of death and disability worldwide, predicting stroke likelihood based on lifestyle factors is crucial. Showing individuals their stroke risk could motivate lifestyle changes, and machine learning offers solutions to this prediction challenge. Neural networks excel at predicting outcomes based on training features like lifestyle factors, however, they're not the only option. Logistic regression models can also effectively compute the likelihood of binary outcomes based on independent variables, making them well-suited for stroke prediction. This analysis will compare both neural networks (dense and convolutional) and logistic regression models for stroke prediction, examining their pros, cons, and differences to develop the most effective predictor that minimizes false negatives.	 | 每年在美国，有80万人遭受中风——每40秒就有一个人中风，每四分钟就有一个人因此死亡。虽然个体因素有所不同，但某些预测因素在确定中风风险时更为普遍。由于中风是全球第二大致死和致残原因，基于生活方式因素预测中风的可能性至关重要。向个人展示其中风风险可能会激励他们做出生活方式改变，而机器学习则提供了应对这一预测挑战的解决方案。神经网络在基于训练特征（如生活方式因素）预测结果方面表现出色，但这并不是唯一的选项。逻辑回归模型也可以有效地根据独立变量计算二元结果的概率，使其非常适合用于中风预测。本分析将比较神经网络（密集型和卷积型）和逻辑回归模型在中风预测中的应用，评估它们各自的优缺点及其差异，以开发出最有效的预测模型，最大限度地减少假阴性结果。
2501.00046	 | Numerical solutions of fixed points in two-dimensional Kuramoto-Sivashinsky equation expedited by reinforcement learning	 | Juncheng Jiang,Dongdong Wan,Mengqi Zhang	 | 文章提出了一种结合深度强化学习和Jacobian-Free Newton-Krylov方法来增强识别二维Kuramoto-Sivashinsky方程固定点的有效性，并发现了新的固定点结果，同时利用并行强化学习技术优化了系统轨迹在已知固定点之间的导航。	 | This paper presents a combined approach to enhancing the effectiveness of Jacobian-Free Newton-Krylov (JFNK) method by deep reinforcement learning (DRL) in identifying fixed points within the 2D Kuramoto-Sivashinsky Equation (KSE). JFNK approach entails a good initial guess for improved convergence when searching for fixed points. With a properly defined reward function, we utilise DRL as a preliminary step to enhance the initial guess in the converging process. We report new results of fixed points in the 2D KSE which have not been reported in the literature. Additionally, we explored control optimization for the 2D KSE to navigate the system trajectories between known fixed points, based on parallel reinforcement learning techniques. This combined method underscores the improved JFNK approach to finding new fixed-point solutions within the context of 2D KSE, which may be instructive for other high-dimensional dynamical systems.	 | 本文提出了一种结合深度强化学习（DRL）的方法，以增强Jacobian-Free Newton-Krylov（JFNK）方法在识别二维Kuramoto-Sivashinsky方程（KSE）固定点中的有效性。JFNK方法通过提供一个良好的初始猜测来提高收敛性。通过定义一个合适的奖励函数，我们利用DRL作为预处理步骤来增强收敛过程中的初始猜测。我们报告了在文献中未被报道的2D KSE中的新的固定点结果。此外，我们基于并行强化学习技术探索了2D KSE的控制优化，以在已知固定点之间导航系统轨迹。这种结合方法强调了在2D KSE背景下寻找新的固定点解改进的JFNK方法，这可能对其他高维动力系统具有启发意义。
2501.00042	 | Resource-Efficient Transformer Architecture: Optimizing Memory and Execution Time for Real-Time Applications	 | Krisvarish V,Priyadarshini T,K P Abhishek Sri Saai,Vaidehi Vijayakumar	 | 本文提出了一种内存高效的变压器模型，通过减半嵌入尺寸、参数剪枝和量化等技术，实现了52%的内存使用减少和33%的执行时间减少，同时保持与原始模型相近的性能。实验结果显示，该模型在资源友好型深度学习架构领域中表现出色，尤其适用于实时和资源受限的应用场景。	 | This paper describes a memory-efficient transformer model designed to drive a reduction in memory usage and execution time by substantial orders of magnitude without impairing the model's performance near that of the original model. Recently, new architectures of transformers were presented, focused on parameter efficiency and computational optimization; however, such models usually require considerable resources in terms of hardware when deployed in real-world applications on edge devices. This approach addresses this concern by halving embedding size and applying targeted techniques such as parameter pruning and quantization to optimize the memory footprint with minimum sacrifices in terms of accuracy. Experimental results include a 52% reduction in memory usage and a 33% decrease in execution time, resulting in better efficiency than state-of-the-art models. This work compared our model with existing compelling architectures, such as MobileBERT and DistilBERT, and proved its feasibility in the domain of resource-friendly deep learning architectures, mainly for applications in real-time and in resource-constrained applications.	 | 本文描述了一种内存高效的变压器模型，该模型旨在通过大幅减少内存使用和执行时间来驱动性能改进，几乎不会影响模型的性能，使其接近原始模型的性能。最近，提出了一些新的变压器架构，侧重于参数效率和计算优化；然而，这些模型在实际应用中部署在边缘设备上时通常需要大量硬件资源。本文通过减半嵌入尺寸并应用参数剪枝和量化等针对性技术来优化内存占用，同时最大限度地减少准确性的损失来解决这一问题。实验结果表明，内存使用减少了52%，执行时间减少了33%，从而获得了优于现有最先进的模型的效率。本文将我们的模型与现有的具有竞争力的架构（如MobileBERT和DistilBERT）进行了比较，并证明了其在资源友好型深度学习架构领域中的可行性，特别是对于实时应用和资源受限的应用场景。
2501.00032	 | Highly Optimized Kernels and Fine-Grained Codebooks for LLM Inference on Arm CPUs	 | Dibakar Gope,David Mansell,Danny Loh,Ian Bratt	 | 本研究提出了一组优化内核，加速大规模语言模型推理，特别是在Arm CPU上，通过改进的数据布局、减少不必要的操作和优化的量化方法，实现了显著的性能提升，相较于现有解决方案，提示处理速度提升3-3.2倍，自回归解码速度提升2倍。	 | Large language models (LLMs) have transformed the way we think about language understanding and generation, enthralling both researchers and developers. However, deploying LLMs for inference has been a significant challenge due to their unprecedented size and resource requirements. While quantizing model weights to sub-byte precision has emerged as a promising solution to ease memory pressure, the group quantization formats commonly used for LLM quantization have significant compute overheads and a resource-intensive dequantization process. As a result, a higher proportion of compute instructions do not perform multiplies, i.e., real work, rendering them unsuitable for meeting the required latency requirements for LLMs deployed on commodity CPUs. In this work, we propose a set of highly optimized kernels to accelerate LLM inference and unleash the full potential of CPUs, particularly Arm CPUs. These kernels amortize the cost of loading the operands and the cost of weight unpacking across multiple output rows. This, along with the introduction of an optimized interleaved group data layout for weights and decompression path optimizations to reduce unnecessary operations and dequantization overhead while maximizing the use of vector and matrix multiply operations, significantly improves the efficiency of MAC operations. Furthermore, we present a groupwise non-uniform codebook-based quantization method for ultra-low-precision quantization of LLMs to better match non-uniform patterns in their weight distributions, demonstrating better throughput during token generation while ensuring better quality than the state-of-the-art. Applying these improvements to 4-bit LLMs results in a 3-3.2x improvement in prompt processing and a 2x improvement in autoregressive decoding on Arm CPUs, compared to LLaMA.cpp-based solution. The optimized kernels are available at https://github.com/ggerganov/llama.cpp.	 | 大规模语言模型（LLMs）已经改变了我们对语言理解和生成的看法，吸引了研究者和开发者的极大关注。然而，将LLMs部署用于推理仍然是一项重大挑战，因为它们的空前规模和资源需求。虽然将模型权重量化为次字节精度已成为减轻内存压力的有前途的解决方案，但常用的LLMs量化分组量化格式具有显著的计算开销和资源密集型的去量化过程。因此，在使用普通CPU部署LLMs时，更高比例的计算指令没有进行乘法操作，即没有执行实际工作，这使得它们无法满足所需的延迟要求。在本研究中，我们提出了一组高度优化的内核来加速LLMs推理，并充分发挥CPU的潜力，特别是Arm CPU。这些内核在多个输出行中分摊加载操作数和权重解码的成本。此外，通过引入优化的交织分组数据布局和解压缩路径优化，减少不必要的操作和去量化开销并最大化向量和矩阵乘法操作的使用，显著提高了MAC操作的效率。此外，我们提出了一个分组非均匀代码表量化方法，以更好地匹配LLMs权重分布中的非均匀模式，在生成标记时展示出更高的吞吐量，同时保持更好的质量，优于当前最先进的方法。将这些改进应用于4位LLMs，在Arm CPU上实现了3-3.2倍的提示处理速度提升和2倍的自回归解码速度提升，相比基于LLaMA.cpp的解决方案。优化后的内核可在https://github.com/ggerganov/llama.cpp获取。
2501.01426	 | Unifying Specialized Visual Encoders for Video Language Models	 | Jihoon Chung,Tyler Zhu,Max Gonzalez Saez-Diez,Juan Carlos Niebles,Honglu Zhou,Olga Russakovsky	 | MERV（Multi-Encoder Representation of Videos）通过使用多个冻结的视觉编码器来创建视频的统一表示，从而为视频理解模型提供全面的视觉知识，并在多种视频理解基准测试中表现出色，提升了准确性并降低了参数量和训练时间。	 | The recent advent of Large Language Models (LLMs) has ushered sophisticated reasoning capabilities into the realm of video through Video Large Language Models (VideoLLMs). However, VideoLLMs currently rely on a single vision encoder for all of their visual processing, which limits the amount and type of visual information that can be conveyed to the LLM. Our method, MERV, Multi-Encoder Representation of Videos, instead leverages multiple frozen visual encoders to create a unified representation of a video, providing the VideoLLM with a comprehensive set of specialized visual knowledge. Spatio-temporally aligning the features from each encoder allows us to tackle a wider range of open-ended and multiple-choice video understanding questions and outperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy than Video-LLaVA across the standard suite video understanding benchmarks, while also having a better Video-ChatGPT score. We also improve upon SeViLA, the previous best on zero-shot Perception Test accuracy, by 2.2%. MERV introduces minimal extra parameters and trains faster than equivalent single-encoder methods while parallelizing the visual processing. Finally, we provide qualitative evidence that MERV successfully captures domain knowledge from each of its encoders. Our results offer promising directions in utilizing multiple vision encoders for comprehensive video understanding.	 | 近年来，大型语言模型（LLMs）的应用使得视频语言模型（VideoLLMs）能够在视频领域实现复杂的推理能力。然而，当前的VideoLLMs主要依赖单一的视觉编码器进行所有的视觉处理，这限制了传递给LLM的视觉信息的种类和数量。我们的方法，即Multi-Encoder Representation of Videos (MERV)，与此不同，它利用多个冻结的视觉编码器来创建视频的统一表示，从而为VideoLLM提供了一套全面的专门视觉知识。我们通过时空对齐每个编码器的特征，能够解决更广泛的开放性和多项选择性视频理解问题，并超越了先前的最先进的工作。  在标准视频理解基准测试中，MERV在准确率方面比Video-LLaVA高出3.7%，同时在Video-ChatGPT评分上也有提高。我们在零样本感知测试中也比之前的最佳表现SeViLA提高了2.2%。MERV引入了极少的额外参数，训练速度也比等效的单一编码器方法更快，且能够并行化视觉处理。最后，我们提供了定性的证据，证明MERV成功地从其每个编码器中捕获了领域知识。我们的结果为利用多个视觉编码器实现全面的视频理解提供了有希望的方向。
2501.01423	 | Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models	 | Jingfeng Yao,Xinggang Wang	 | 尽管基于Transformer架构的潜在扩散模型在生成高保真图像方面表现出色，但增加视觉标记器的特征维度会带来计算成本的增加和收敛困难。为此，提出了一种VA-VAE方法，将潜在空间与预训练的视觉基础模型对齐，显著提高了扩散模型的重建和生成性能，使得DiT在ImageNet 256x256生成任务上达到了最先进的FID得分，并且收敛速度提升了21倍。	 | Latent diffusion models with Transformer architectures excel at generating high-fidelity images. However, recent studies reveal an optimization dilemma in this two-stage design: while increasing the per-token feature dimension in visual tokenizers improves reconstruction quality, it requires substantially larger diffusion models and more training iterations to achieve comparable generation performance. Consequently, existing systems often settle for sub-optimal solutions, either producing visual artifacts due to information loss within tokenizers or failing to converge fully due to expensive computation costs. We argue that this dilemma stems from the inherent difficulty in learning unconstrained high-dimensional latent spaces. To address this, we propose aligning the latent space with pre-trained vision foundation models when training the visual tokenizers. Our proposed VA-VAE (Vision foundation model Aligned Variational AutoEncoder) significantly expands the reconstruction-generation frontier of latent diffusion models, enabling faster convergence of Diffusion Transformers (DiT) in high-dimensional latent spaces. To exploit the full potential of VA-VAE, we build an enhanced DiT baseline with improved training strategies and architecture designs, termed LightningDiT. The integrated system achieves state-of-the-art (SOTA) performance on ImageNet 256x256 generation with an FID score of 1.35 while demonstrating remarkable training efficiency by reaching an FID score of 2.11 in just 64 epochs--representing an over 21 times convergence speedup compared to the original DiT. Models and codes are available at: https://github.com/hustvl/LightningDiT.	 | 基于Transformer架构的潜在扩散模型在生成高保真图像方面表现出色。然而，近期的研究揭示了这种两阶段设计中的优化困境：尽管增加视觉标记器中每个标记的特征维度可以提高重建质量，但这也需要更大的扩散模型和更多的训练迭代次数才能达到可比的生成性能。因此，现有系统常常采用次优解决方案，要么由于标记器内信息损失产生视觉伪影，要么由于昂贵的计算成本未能完全收敛。我们认为这种困境源于学习不受约束的高维潜在空间的固有难度。为了解决这一问题，我们提出在训练视觉标记器时将潜在空间与预训练的视觉基础模型对齐。我们提议的VA-VAE（视觉基础模型对齐的变分自编码器）显著扩展了潜在扩散模型的重建-生成边界，使扩散变换器（DiT）在高维潜在空间中能够更快地收敛。为了充分发挥VA-VAE的潜力，我们构建了一个增强的DiT基线，改进了训练策略和架构设计，称为LightningDiT。该集成系统在ImageNet 256x256生成任务上达到了最先进的（SOTA）性能，FID得分为1.35，并且在仅64个 epoch 后达到FID得分为2.11，显示出超过21倍的收敛速度提升，相较原始的DiT。模型和代码可在以下地址获取：https://github.com/hustvl/LightningDiT。
2501.01422	 | Multi-Modal Video Feature Extraction for Popularity Prediction	 | Haixu Liu,Wenning Wang,Haoxiang Zheng,Penghao Jiang,Qirui Wang,Ruiqing Yan,Qiuzhuang Sun	 | 该研究利用短视频及其特征预测其受欢迎程度，通过观看次数、点赞次数、评论次数和分享次数四个指标衡量，采用视频分类模型提取视频特征，并结合文本生成模型和BERT模型生成文本理解，最终训练神经网络和XGBoost模型进行预测。	 | This work aims to predict the popularity of short videos using the videos themselves and their related features. Popularity is measured by four key engagement metrics: view count, like count, comment count, and share count. This study employs video classification models with different architectures and training methods as backbone networks to extract video modality features. Meanwhile, the cleaned video captions are incorporated into a carefully designed prompt framework, along with the video, as input for video-to-text generation models, which generate detailed text-based video content understanding. These texts are then encoded into vectors using a pre-trained BERT model. Based on the six sets of vectors mentioned above, a neural network is trained for each of the four prediction metrics. Moreover, the study conducts data mining and feature engineering based on the video and tabular data, constructing practical features such as the total frequency of hashtag appearances, the total frequency of mention appearances, video duration, frame count, frame rate, and total time online. Multiple machine learning models are trained, and the most stable model, XGBoost, is selected. Finally, the predictions from the neural network and XGBoost models are averaged to obtain the final result.	 | 本研究旨在使用短视频及其相关特征来预测其受欢迎程度。受欢迎程度通过四个关键参与度指标进行衡量：观看次数、点赞次数、评论次数和分享次数。本研究采用具有不同架构和训练方法的视频分类模型作为主干网络，以提取视频模态特征。同时，经过清洗的视频字幕被整合到精心设计的提示框架中，与视频一起作为输入，供视频到文本生成模型使用，生成详细的基于文本的视频内容理解。这些文本随后使用预训练的BERT模型编码为向量。基于上述六组向量，分别针对四个预测指标训练了神经网络。此外，基于视频和表格数据进行了数据挖掘和特征工程，构建了实用特征，如Hashtag出现的总频率、提及出现的总频率、视频时长、帧数、帧率以及视频上线总时长。训练了多种机器学习模型，并选择了最稳定的XGBoost模型。最后，神经网络和XGBoost模型的预测结果被平均处理以获得最终结果。
2501.01420	 | A Multi-task Supervised Compression Model for Split Computing	 | Yoshitomo Matsubara,Matteo Mendula,Marco Levorato	 | 本文提出了Ladon，一种适用于多任务分拆计算的监督压缩模型，该模型在多个数据集上表现出色，同时显著降低了端到端延迟和移动设备的能耗。	 | Split computing ($\neq$ split learning) is a promising approach to deep learning models for resource-constrained edge computing systems, where weak sensor (mobile) devices are wirelessly connected to stronger edge servers through channels with limited communication capacity. State-of-theart work on split computing presents methods for single tasks such as image classification, object detection, or semantic segmentation. The application of existing methods to multitask problems degrades model accuracy and/or significantly increase runtime latency. In this study, we propose Ladon, the first multi-task-head supervised compression model for multi-task split computing. Experimental results show that the multi-task supervised compression model either outperformed or rivaled strong lightweight baseline models in terms of predictive performance for ILSVRC 2012, COCO 2017, and PASCAL VOC 2012 datasets while learning compressed representations at its early layers. Furthermore, our models reduced end-to-end latency (by up to 95.4%) and energy consumption of mobile devices (by up to 88.2%) in multi-task split computing scenarios.	 | 分拆计算（$\neq$ 分拆学习）是一种资源受限边缘计算系统中深度学习模型的有前途的方法，其中弱传感器（移动）设备通过具有有限通信能力的无线通道连接到更强的边缘服务器。现有分拆计算的工作主要针对单一任务，如图像分类、对象检测或语义分割。现有方法应用于多任务问题会降低模型精度并显著增加运行时延迟。在本研究中，我们提出了 Ladon，这是第一个适用于多任务分拆计算的监督压缩模型。实验结果表明，该多任务监督压缩模型在 ILSVRC 2012、COCO 2017 和 PASCAL VOC 2012 数据集上预测性能优于或可与强大的轻量级基线模型匹敌，同时在早期层中学习压缩表示。此外，我们的模型在多任务分拆计算场景中将端到端延迟减少了高达 95.4%，并将移动设备的能耗减少了高达 88.2%。
2501.01414	 | Deep Discrete Encoders: Identifiable Deep Generative Models for Rich Data with Discrete Latent Layers	 | Seunghyun Lee,Yuqi Gu	 | 本文提出了深度离散编码器（DDEs），一种可解释的深度生成建模框架，适用于丰富数据类型，通过逐层非线性谱初始化和带惩罚的随机近似EM算法进行高效参数估计，并在多个实际应用中展示了其可解释性和优越性能。	 | In the era of generative AI, deep generative models (DGMs) with latent representations have gained tremendous popularity. Despite their impressive empirical performance, the statistical properties of these models remain underexplored. DGMs are often overparametrized, non-identifiable, and uninterpretable black boxes, raising serious concerns when deploying them in high-stakes applications. Motivated by this, we propose an interpretable deep generative modeling framework for rich data types with discrete latent layers, called Deep Discrete Encoders (DDEs). A DDE is a directed graphical model with multiple binary latent layers. Theoretically, we propose transparent identifiability conditions for DDEs, which imply progressively smaller sizes of the latent layers as they go deeper. Identifiability ensures consistent parameter estimation and inspires an interpretable design of the deep architecture. Computationally, we propose a scalable estimation pipeline of a layerwise nonlinear spectral initialization followed by a penalized stochastic approximation EM algorithm. This procedure can efficiently estimate models with exponentially many latent components. Extensive simulation studies validate our theoretical results and demonstrate the proposed algorithms' excellent performance. We apply DDEs to three diverse real datasets for hierarchical topic modeling, image representation learning, response time modeling in educational testing, and obtain interpretable findings.	 | 在生成式AI的时代，具有潜在表示的深度生成模型（DGMs）已经获得了巨大的流行。尽管这些模型在实验性能上表现出色，但它们的统计特性仍然没有得到充分探索。DGMs通常参数过多、不可识别并且是不可解释的黑盒模型，这在高风险的应用场景中部署它们时引起了严重关切。鉴于此，我们提出了一种用于丰富数据类型的可解释深度生成建模框架，称为深度离散编码器（DDEs）。一个DDE是具有多个二元潜在层的定向图形模型。从理论上讲，我们提出了DDEs的透明可识别性条件，这表明随着模型的加深，潜在层的大小会逐渐减小。可识别性确保参数估计的一致性并启发深度架构的可解释设计。从计算上讲，我们提出了一种可扩展的估计管道，即逐层非线性谱初始化，随后是带惩罚的随机近似EM算法。此过程可以高效地估计具有指数潜在成分的模型。广泛的模拟研究验证了我们的理论结果，并展示了所提算法的出色性能。我们将DDEs应用于三个不同的真实数据集，用于层次主题建模、图像表示学习、教育测试中的反应时间建模，并获得了可解释的发现。
2501.01407	 | Nested Attention: Semantic-aware Attention Values for Concept Personalization	 | Or Patashnik,Rinon Gal,Daniil Ostashev,Sergey Tulyakov,Kfir Aberman,Daniel Cohen-Or	 | 本文提出了一种嵌套注意机制（Nested Attention），该机制能够在保持图像与输入文本一致性的同时，生成丰富且多风格的主题图像。该方法通过引入查询依赖的主题值，增强了模型的表达能力，并可在多种应用场景中训练和应用，支持结合不同领域的个性化主题。	 | Personalizing text-to-image models to generate images of specific subjects across diverse scenes and styles is a rapidly advancing field. Current approaches often face challenges in maintaining a balance between identity preservation and alignment with the input text prompt. Some methods rely on a single textual token to represent a subject, which limits expressiveness, while others employ richer representations but disrupt the model's prior, diminishing prompt alignment. In this work, we introduce Nested Attention, a novel mechanism that injects a rich and expressive image representation into the model's existing cross-attention layers. Our key idea is to generate query-dependent subject values, derived from nested attention layers that learn to select relevant subject features for each region in the generated image. We integrate these nested layers into an encoder-based personalization method, and show that they enable high identity preservation while adhering to input text prompts. Our approach is general and can be trained on various domains. Additionally, its prior preservation allows us to combine multiple personalized subjects from different domains in a single image.	 | 将特定文本转化为特定主题图像并生成跨多种场景和风格的图像是一项迅速发展的领域。当前的方法往往在保持身份保护与输入文本提示的一致性之间面临挑战。一些方法依赖单一的文本标记来表示主题，这限制了表达的丰富性，而其他方法则采用更丰富的表示形式，但会削弱模型的先验知识，减少提示的一致性。在本文中，我们引入了嵌套注意机制（Nested Attention），这是一种新颖的方法，可以在模型现有的交叉注意力层中注入丰富和表达性强的图像表示。我们的核心思想是生成查询依赖的主题值，这些值是从嵌套注意力层中学习到的选择生成图像中每个区域相关主题特征的。  我们将这些嵌套层整合到基于编码器的个性化方法中，并展示了它们能够在保持高身份保护的同时遵循输入文本提示。我们的方法具有通用性，可以在各种领域进行训练。此外，它保留先验知识的特点使我们能够在单张图像中结合来自不同领域的多个个性化主题。
2501.01367	 | Contrastive Learning from Exploratory Actions: Leveraging Natural Interactions for Preference Elicitation	 | Nathaniel Dennler,Stefanos Nikolaidis,Maja Matarić	 | 本文提出了一种新的探索性行为对比学习（CLEA）方法，通过利用用户在自定义机器人过程中的探索性行动数据来学习与用户偏好相一致的轨迹特征，并在用户研究中展示了CLEA特征在完整性、简洁性、最小性、解释性四个指标上的优越性。	 | People have a variety of preferences for how robots behave. To understand and reason about these preferences, robots aim to learn a reward function that describes how aligned robot behaviors are with a user's preferences. Good representations of a robot's behavior can significantly reduce the time and effort required for a user to teach the robot their preferences. Specifying these representations -- what "features" of the robot's behavior matter to users -- remains a difficult problem; Features learned from raw data lack semantic meaning and features learned from user data require users to engage in tedious labeling processes. Our key insight is that users tasked with customizing a robot are intrinsically motivated to produce labels through exploratory search; they explore behaviors that they find interesting and ignore behaviors that are irrelevant. To harness this novel data source of exploratory actions, we propose contrastive learning from exploratory actions (CLEA) to learn trajectory features that are aligned with features that users care about. We learned CLEA features from exploratory actions users performed in an open-ended signal design activity (N=25) with a Kuri robot, and evaluated CLEA features through a second user study with a different set of users (N=42). CLEA features outperformed self-supervised features when eliciting user preferences over four metrics: completeness, simplicity, minimality, and explainability.	 | 人们对于机器人行为有不同的偏好。为了理解和推理这些偏好，机器人旨在学习一个奖励函数，该函数描述了机器人行为与用户偏好的一致性程度。良好的机器人行为表示可以显著减少用户向机器人教授其偏好的时间和精力。明确这些表示——即用户认为哪些“特征”对机器人行为重要——仍然是一个难题；从原始数据中学习到的特征缺乏语义意义，而从用户数据中学习到的特征则需要用户进行繁冗的标注过程。我们的关键洞察是，负责自定义机器人的用户会出于内在动力通过探索性搜索生成标签；他们探索自己认为有趣的行为，并忽略与自身无关的行为。为了利用这种新的探索性行动数据源，我们提出了探索性行为对比学习（CLEA）来学习与用户关心的特征相一致的轨迹特征。我们从Kuri机器人的开放式信号设计活动中用户的探索性行动中学习到了CLEA特征（N=25），并通过另一组用户的第二次用户研究（N=42）评估了CLEA特征的表现。CLEA特征在四个指标（完整性、简洁性、最小性、解释性）上都优于自我监督学习特征，从而更好地引发了用户的偏好。
2501.01342	 | DeepFilter: An Instrumental Baseline for Accurate and Efficient Process Monitoring	 | Hao Wang,Zhichao Chen,Licheng Pan,Xiaoyu Jiang,Yichen Song,Qunshan He,Xinggao Liu	 | DeepFilter是一种基于变压器的框架，通过高效过滤层有效捕获长时间和周期性模式，解决了传统基于自注意力机制方法在准确性和效率上的局限性，实验结果表明其在准确性和效率方面优于现有模型。	 | Effective process monitoring is increasingly vital in industrial automation for ensuring operational safety, necessitating both high accuracy and efficiency. Although Transformers have demonstrated success in various fields, their canonical form based on the self-attention mechanism is inadequate for process monitoring due to two primary limitations: (1) the step-wise correlations captured by self-attention mechanism are difficult to capture discriminative patterns in monitoring logs due to the lacking semantics of each step, thus compromising accuracy; (2) the quadratic computational complexity of self-attention hampers efficiency. To address these issues, we propose DeepFilter, a Transformer-style framework for process monitoring. The core innovation is an efficient filtering layer that excel capturing long-term and periodic patterns with reduced complexity. Equipping with the global filtering layer, DeepFilter enhances both accuracy and efficiency, meeting the stringent demands of process monitoring. Experimental results on real-world process monitoring datasets validate DeepFilter's superiority in terms of accuracy and efficiency compared to existing state-of-the-art models.	 | 有效的过程监控在工业自动化中越来越重要，对于确保操作安全至关重要，这要求监控既具有高精度又具有高效性。尽管变压器在多个领域取得了成功，但基于自注意力机制的经典形式在过程监控中存在两个主要局限性：（1）自注意力机制捕捉的步骤间关联难以在监控日志中捕获具有区分性的模式，因为每个步骤缺乏语义信息，从而影响了准确性；（2）自注意力机制的二次计算复杂度降低了效率。为解决这些问题，我们提出了DeepFilter，这是一种基于变压器的框架，核心创新是高效过滤层，该层能够以较低复杂度捕获长期和周期性模式。通过配备全局过滤层，DeepFilter在提高准确性和效率方面表现出色，满足了过程监控的严格要求。在真实世界的过程监控数据集上的实验结果表明，与现有的最先进的模型相比，DeepFilter在准确性和效率方面具有优势。
2501.01335	 | CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for Benchmarking Large Language Models	 | Johan Wahréus,Ahmed Mohamed Hussain,Panos Papadimitratos	 | 该研究提出了CySecBench，一个针对网络安全领域设计的数据集，包含12662个特定提示用于评估破解大型语言模型的技术，并展示了基于提示混淆的方法成功诱导商业黑盒模型生成有害内容，相比之下现有方法效果较差。	 | Numerous studies have investigated methods for jailbreaking Large Language Models (LLMs) to generate harmful content. Typically, these methods are evaluated using datasets of malicious prompts designed to bypass security policies established by LLM providers. However, the generally broad scope and open-ended nature of existing datasets can complicate the assessment of jailbreaking effectiveness, particularly in specific domains, notably cybersecurity. To address this issue, we present and publicly release CySecBench, a comprehensive dataset containing 12662 prompts specifically designed to evaluate jailbreaking techniques in the cybersecurity domain. The dataset is organized into 10 distinct attack-type categories, featuring close-ended prompts to enable a more consistent and accurate assessment of jailbreaking attempts. Furthermore, we detail our methodology for dataset generation and filtration, which can be adapted to create similar datasets in other domains. To demonstrate the utility of CySecBench, we propose and evaluate a jailbreaking approach based on prompt obfuscation. Our experimental results show that this method successfully elicits harmful content from commercial black-box LLMs, achieving Success Rates (SRs) of 65% with ChatGPT and 88% with Gemini; in contrast, Claude demonstrated greater resilience with a jailbreaking SR of 17%. Compared to existing benchmark approaches, our method shows superior performance, highlighting the value of domain-specific evaluation datasets for assessing LLM security measures. Moreover, when evaluated using prompts from a widely used dataset (i.e., AdvBench), it achieved an SR of 78.5%, higher than the state-of-the-art methods.	 |  Numerous研究探讨了如何破解大型语言模型（LLMs）以生成有害内容的方法。这些方法通常使用旨在规避LLM提供商制定的安全政策的恶意提示数据集进行评估。然而，现有数据集范围广泛且具有开放性，这可能会在特定领域（特别是在网络安全领域）复杂化破解效果的评估。为解决这一问题，我们提出并公开发布了CySecBench，这是一个包含12662个特定设计用于评估网络安全领域破解技术的提示的全面数据集。该数据集按10种不同的攻击类型类别组织，包含封闭式提示，以实现更一致和准确的破解尝试评估。此外，我们详细描述了数据集生成和过滤的方法，这些方法可以适应其他领域的类似数据集的创建。为了展示CySecBench的实用性，我们提出了基于提示混淆的破解方法，并进行了评估。实验结果表明，这种方法成功诱导出商业黑盒LLMs生成有害内容，ChatGPT的成功率为65%，Gemini为88%；相比之下，Claude的破解成功率仅为17%。与现有基准方法相比，我们的方法显示出更好的性能，强调了针对LLM安全措施评估领域特定评估数据集的价值。此外，当使用广泛使用的数据集（即AdvBench）中的提示进行评估时，其成功率为78.5%，高于最先进的方法。
2501.01312	 | Learning Spectral Methods by Transformers	 | Yihan He,Yuan Cao,Hong-Yu Chen,Dennis Wu,Jianqing Fan,Han Liu	 | 研究发现，多层变压器在大规模无监督学习中表现出潜力，能够在预训练后执行统计估计任务，类似于人类通过经验学习技能的过程；实验验证了预训练变压器在主成分分析和聚类等任务上的强大能力。	 | Transformers demonstrate significant advantages as the building block of modern LLMs. In this work, we study the capacities of Transformers in performing unsupervised learning. We show that multi-layered Transformers, given a sufficiently large set of pre-training instances, are able to learn the algorithms themselves and perform statistical estimation tasks given new instances. This learning paradigm is distinct from the in-context learning setup and is similar to the learning procedure of human brains where skills are learned through past experience. Theoretically, we prove that pre-trained Transformers can learn the spectral methods and use the classification of bi-class Gaussian mixture model as an example. Our proof is constructive using algorithmic design techniques. Our results are built upon the similarities of multi-layered Transformer architecture with the iterative recovery algorithms used in practice. Empirically, we verify the strong capacity of the multi-layered (pre-trained) Transformer on unsupervised learning through the lens of both the PCA and the Clustering tasks performed on the synthetic and real-world datasets.	 | 变压器作为现代大规模语言模型的基本构建块，展现出显著的优势。在本工作中，我们研究了变压器在进行无监督学习方面的潜力。我们证明，在一个足够大的预训练实例集的基础上，多层变压器能够学习算法本身，并且能够在新实例中执行统计估计任务。这种学习范式不同于上下文学习设置，更像是人类大脑通过过往经验学习技能的过程。理论上，我们证明了预训练的变压器能够学习谱方法，并以二元高斯混合模型的分类为例进行说明。我们的证明使用了算法设计技术，具有建设性。我们的成果基于多层变压器架构与实践中使用的迭代恢复算法的相似性。实验上，我们通过在合成数据集和真实世界数据集上进行主成分分析（PCA）和聚类任务，验证了多层（预训练）变压器在无监督学习中的强大能力。
2501.01291	 | Change Detection-Based Procedures for Piecewise Stationary MABs: A Modular Approach	 | Yu-Han Huang,Argyrios Gerogiannis,Subhonmesh Bose,Venugopal V. Veeravalli	 | 该研究探讨了部分稳定多臂bandit（PS-MAB）环境下的算法设计，通过模块化分析变换检测器和稳定bandit算法，提出了最优阶的CDB过程，并通过仿真验证了其性能。	 | Conventional Multi-Armed Bandit (MAB) algorithms are designed for stationary environments, where the reward distributions associated with the arms do not change with time. In many applications, however, the environment is more accurately modeled as being nonstationary. In this work, piecewise stationary MAB (PS-MAB) environments are investigated, in which the reward distributions associated with a subset of the arms change at some change-points and remain stationary between change-points. Our focus is on the asymptotic analysis of PS-MABs, for which practical algorithms based on change detection (CD) have been previously proposed. Our goal is to modularize the design and analysis of such CD-based Bandit (CDB) procedures. To this end, we identify the requirements for stationary bandit algorithms and change detectors in a CDB procedure that are needed for the modularization. We assume that the rewards are sub-Gaussian. Under this assumption and a condition on the separation of the change-points, we show that the analysis of CDB procedures can indeed be modularized, so that regret bounds can be obtained in a unified manner for various combinations of change detectors and bandit algorithms. Through this analysis, we develop new modular CDB procedures that are order-optimal. We compare the performance of our modular CDB procedures with various other methods in simulations.	 | 传统多臂bandit（MAB）算法是为稳定环境设计的，其中与各臂相关的奖励分布不会随时间变化。然而，在许多应用中，环境更准确地被建模为非稳定性的。本工作中，我们研究了部分稳定（piecewise stationary）MAB（PS-MAB）环境，其中一部分臂的奖励分布会在某些变换点发生变化，而在变换点之间保持稳定。我们的重点是分析PS-MAB，对于这类环境，先前已经提出了基于变换检测（CD）的实际算法。我们的目标是模块化设计和分析此类基于变换检测的bandit（CDB）过程。为此，我们确定了在CDB过程中所需的稳定bandit算法和变换检测器的要求，以实现模块化。我们假设奖励服从次高斯分布。在这一假设和变换点之间分离的条件下，我们证明了可以以模块化的方式分析CDB过程，从而可以以统一的方式获得各种变换检测器和bandit算法组合的后悔界。通过这种分析，我们开发了新的最优阶模块化CDB过程，并通过仿真将我们模块化的CDB过程与其他方法的性能进行了比较。
2501.01276	 | Marketing Mix Modeling in Lemonade	 | Roy Ravid	 | 本文描述了为在线保险公司Lemonade构建的贝叶斯营销组合模型过程，该模型能够估计各营销渠道对总体绩效的贡献，并通过A/B测试验证其预测准确性，最终帮助Lemonade优化营销策略和预算分配。	 | Marketing mix modeling (MMM) is a widely used method to assess the effectiveness of marketing campaigns and optimize marketing strategies. Bayesian MMM is an advanced approach that allows for the incorporation of prior information, uncertainty quantification, and probabilistic predictions (1). In this paper, we describe the process of building a Bayesian MMM model for the online insurance company Lemonade. We first collected data on Lemonade's marketing activities, such as online advertising, social media, and brand marketing, as well as performance data. We then used a Bayesian framework to estimate the contribution of each marketing channel on total performance, while accounting for various factors such as seasonality, market trends, and macroeconomic indicators. To validate the model, we compared its predictions with the actual performance data from A/B-testing and sliding window holdout data (2). The results showed that the predicted contribution of each marketing channel is aligned with A/B test performance and is actionable. Furthermore, we conducted several scenario analyses using convex optimization to test the sensitivity of the model to different assumptions and to evaluate the impact of changes in the marketing mix on sales. The insights gained from the model allowed Lemonade to adjust their marketing strategy and allocate their budget more effectively. Our case study demonstrates the benefits of using Bayesian MMM for marketing attribution and optimization in a data-driven company like Lemonade. The approach is flexible, interpretable, and can provide valuable insights for decision-making.	 | 市场营销组合模型（MMM）是一种广泛使用的评估营销活动效果并优化营销策略的方法。贝叶斯营销组合模型是一种高级方法，允许纳入先验信息、不确定性量化和概率预测（1）。在本文中，我们描述了为在线保险公司Lemonade构建贝叶斯营销组合模型的过程。我们首先收集了Lemonade的营销活动数据，包括在线广告、社交媒体和品牌营销，以及绩效数据。然后，我们使用贝叶斯框架估计每个营销渠道对总体绩效的贡献，同时考虑到季节性、市场趋势和宏观经济指标等因素。为了验证模型，我们将模型的预测结果与A/B测试的实际绩效数据和滑动窗口保留数据进行了比较（2）。结果显示，每个营销渠道的预测贡献与A/B测试表现一致，并具有实际操作性。此外，我们使用凸优化进行了几种情景分析，以测试模型对不同假设的敏感性，并评估营销组合的变化对销售的影响。模型获得的洞察使Lemonade能够调整其营销策略并更有效地分配预算。我们的案例研究展示了在像Lemonade这样的数据驱动公司中使用贝叶斯营销组合模型进行营销归因和优化的好处。该方法灵活、可解释，并能为决策提供有价值的见解。
2501.01264	 | ProgCo: Program Helps Self-Correction of Large Language Models	 | Xiaoshuai Song,Yanan Wu,Weixun Wang,Jiaheng Liu,Wenbo Su,Bo Zheng	 | 本文提出了一种名为程序驱动的自我修正（ProgCo）的方法，该方法通过自我生成和执行验证伪程序（ProgVe）来实现复杂的验证逻辑，并通过双重反思和完善（ProgRe）来改善响应和验证程序，从而在大规模语言模型中实现有效的自我修正。实验结果表明，ProgCo 在指令遵循和数学任务中能够有效纠正错误，并且结合真实程序工具时性能进一步提升。	 | Self-Correction aims to enable large language models (LLMs) to self-verify and self-refine their initial responses without external feedback. However, LLMs often fail to effectively self-verify and generate correct feedback, further misleading refinement and leading to the failure of self-correction, especially in complex reasoning tasks. In this paper, we propose Program-driven Self-Correction (ProgCo). First, program-driven verification (ProgVe) achieves complex verification logic and extensive validation through self-generated, self-executing verification pseudo-programs. Then, program-driven refinement (ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement on both responses and verification programs to mitigate misleading of incorrect feedback in complex reasoning tasks. Experiments on three instruction-following and mathematical benchmarks indicate that ProgCo achieves effective self-correction, and can be further enhance performance when combined with real program tools.	 | 自我修正旨在使大规模语言模型（LLMs）能够在没有外部反馈的情况下自我验证和自我完善其初始响应。然而，LLMs 往往无法有效地自我验证，并生成正确的反馈，这进一步导致了错误的修正，特别是在复杂推理任务中。在本文中，我们提出了程序驱动的自我修正（ProgCo）。首先，程序驱动的验证（ProgVe）通过自我生成、自我执行的验证伪程序实现复杂的验证逻辑和广泛的验证。然后，程序驱动的完善（ProgRe）从ProgVe中获取反馈，并对响应和验证程序进行双重反思和完善，以减轻复杂推理任务中错误反馈的误导。在三个指令遵循和数学基准测试中的实验表明，ProgCo 能够实现有效的自我修正，并且在结合真实程序工具时可以进一步提升性能。
2501.01256	 | Digital Guardians: Can GPT-4, Perspective API, and Moderation API reliably detect hate speech in reader comments of German online newspapers?	 | Manuel Weber,Moritz Huber,Maximilian Auch,Alexander Döschl,Max-Emanuel Keller,Peter Mandl	 | 该研究比较了多个自动检测有毒内容和仇恨言论的解决方案，如GPT-4o、Perspective API和Moderation API，并发现GPT-4o在综合MCC和F2分数的指标下表现最佳，特别是在HOCON34k基准测试中高出约5个百分点。	 | In recent years, toxic content and hate speech have become widespread phenomena on the internet. Moderators of online newspapers and forums are now required, partly due to legal regulations, to carefully review and, if necessary, delete reader comments. This is a labor-intensive process. Some providers of large language models already offer solutions for automated hate speech detection or the identification of toxic content. These include GPT-4o from OpenAI, Jigsaw's (Google) Perspective API, and OpenAI's Moderation API. Based on the selected German test dataset HOCON34k, which was specifically created for developing tools to detect hate speech in reader comments of online newspapers, these solutions are compared with each other and against the HOCON34k baseline. The test dataset contains 1,592 annotated text samples. For GPT-4o, three different promptings are used, employing a Zero-Shot, One-Shot, and Few-Shot approach. The results of the experiments demonstrate that GPT-4o outperforms both the Perspective API and the Moderation API, and exceeds the HOCON34k baseline by approximately 5 percentage points, as measured by a combined metric of MCC and F2-score.	 | 近年来，有毒内容和仇恨言论在网络上变得越来越普遍。在线报纸和论坛的管理员现在由于法律法规的要求，需要仔细审查读者评论，并在必要时删除这些评论。这是一个劳动密集型的过程。一些提供大型语言模型的公司已经提供了自动检测仇恨言论或识别有毒内容的解决方案。这些解决方案包括OpenAI的GPT-4o、Google的Jigsaw的Perspective API以及OpenAI的Moderation API。基于专门为在线报纸读者评论中的仇恨言论检测工具开发而创建的HOCON34k德语测试数据集，这些解决方案被相互比较，并与HOCON34k基准进行比较。测试数据集包含1,592个标注的文本样本。对于GPT-4o，使用了三种不同的提示方法，包括零样本、单样本和少量样本方法。实验结果表明，GPT-4o 在综合MCC和F2分数的指标下，优于Perspective API和Moderation API，且在HOCON34k基准上的表现高出约5个百分点。
2501.01245	 | SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization	 | Yongle Huang,Haodong Chen,Zhenbang Xu,Zihan Jia,Haozhou Sun,Dian Shao	 | 本文提出了一种半监督学习框架SeFAR，用于细粒度动作识别任务，通过引入双重时间元素和自适应调节策略，显著提高了细粒度动作的识别性能，并在多个数据集上超过了现有方法。	 | Human action understanding is crucial for the advancement of multimodal systems. While recent developments, driven by powerful large language models (LLMs), aim to be general enough to cover a wide range of categories, they often overlook the need for more specific capabilities. In this work, we address the more challenging task of Fine-grained Action Recognition (FAR), which focuses on detailed semantic labels within shorter temporal duration (e.g., "salto backward tucked with 1 turn"). Given the high costs of annotating fine-grained labels and the substantial data needed for fine-tuning LLMs, we propose to adopt semi-supervised learning (SSL). Our framework, SeFAR, incorporates several innovative designs to tackle these challenges. Specifically, to capture sufficient visual details, we construct Dual-level temporal elements as more effective representations, based on which we design a new strong augmentation strategy for the Teacher-Student learning paradigm through involving moderate temporal perturbation. Furthermore, to handle the high uncertainty within the teacher model's predictions for FAR, we propose the Adaptive Regulation to stabilize the learning process. Experiments show that SeFAR achieves state-of-the-art performance on two FAR datasets, FineGym and FineDiving, across various data scopes. It also outperforms other semi-supervised methods on two classical coarse-grained datasets, UCF101 and HMDB51. Further analysis and ablation studies validate the effectiveness of our designs. Additionally, we show that the features extracted by our SeFAR could largely promote the ability of multimodal foundation models to understand fine-grained and domain-specific semantics.	 | 多模态系统的发展对人类行为理解至关重要。尽管近期的发展，受到强大大型语言模型（LLMs）的推动，旨在覆盖广泛的行为类别，但它们往往忽略了更加具体能力的需求。在本文中，我们关注更具挑战性的细粒度动作识别（Fine-grained Action Recognition, FAR）任务，该任务关注较短时间内详细语义标签（例如，“后空翻转体一周”）。由于细粒度标签的标注成本高昂且需要大量数据来微调LLMs，我们提出了半监督学习（Semi-supervised Learning, SSL）的方法。我们的框架SeFAR包含多项创新设计以应对这些挑战。具体而言，为了捕捉足够的视觉细节，我们构建了双重时间元素作为更有效的表示，并在此基础上设计了一种新的增强策略，通过引入适度的时间扰动应用于教师-学生学习范式。此外，为了应对教师模型在FAR预测中的高不确定性，我们提出了自适应调节以稳定学习过程。实验结果显示，SeFAR在两个FAR数据集FineGym和FineDiving上实现了最先进的性能，并在两个经典粗粒度数据集UCF101和HMDB51上超过了其他半监督方法。进一步的分析和消融研究验证了我们设计的有效性。此外，我们展示了SeFAR提取的特征显著增强了多模态基础模型理解细粒度和领域特定语义的能力。
2501.01238	 | EHCTNet: Enhanced Hybrid of CNN and Transformer Network for Remote Sensing Image Change Detection	 | Junjie Yang,Haibo Wan,Zhihai Shang	 | 本文提出了一种增强的卷积神经网络和变换器网络混合模型（EHCTNet），通过增强特征学习能力和整合频率成分信息，逐步提升召回率，有效解决了遥感变化检测中的漏检和断续问题。实验结果表明，EHCTNet在检测完整、连续的变化区域以及准确区分邻近区域方面优于现有模型。	 | Remote sensing (RS) change detection incurs a high cost because of false negatives, which are more costly than false positives. Existing frameworks, struggling to improve the Precision metric to reduce the cost of false positive, still have limitations in focusing on the change of interest, which leads to missed detections and discontinuity issues. This work tackles these issues by enhancing feature learning capabilities and integrating the frequency components of feature information, with a strategy to incrementally boost the Recall value. We propose an enhanced hybrid of CNN and Transformer network (EHCTNet) for effectively mining the change information of interest. Firstly, a dual branch feature extraction module is used to extract the multi scale features of RS images. Secondly, the frequency component of these features is exploited by a refined module I. Thirdly, an enhanced token mining module based on the Kolmogorov Arnold Network is utilized to derive semantic information. Finally, the semantic change information's frequency component, beneficial for final detection, is mined from the refined module II. Extensive experiments validate the effectiveness of EHCTNet in comprehending complex changes of interest. The visualization outcomes show that EHCTNet detects more intact and continuous changed areas and perceives more accurate neighboring distinction than state of the art models.	 | 遥感（RS）变化检测由于假阴性造成的成本较高，而假阴性的成本高于假阳性。现有的框架虽然努力通过提高精确度来减少假阳性造成的成本，但在关注目标变化方面仍然存在局限性，导致漏检和断续问题。本文通过增强特征学习能力和整合特征信息的频率成分，结合逐步提升召回率的策略来解决这些问题。我们提出了一种增强的卷积神经网络和变换器网络混合模型（EHCTNet），以有效地提取感兴趣的变化信息。首先，使用双分支特征提取模块来提取遥感图像的多尺度特征。其次，通过细化模块I利用这些特征的频率成分。第三，利用柯尔莫哥洛夫-阿诺德网络的增强标记挖掘模块来提取语义信息。最后，通过细化模块II从这些语义变化信息中提取频率成分，这些频率成分有助于最终检测。广泛的实验验证了EHCTNet在理解复杂变化方面的有效性。可视化结果表明，EHCTNet能够检测更完整和连续的变化区域，并且能够更准确地区分邻近区域，优于现有模型。
2501.01235	 | SVFR: A Unified Framework for Generalized Video Face Restoration	 | Zhiyao Wang,Xu Chen,Chengming Xu,Junwei Zhu,Xiaobin Hu,Jiangning Zhang,Chengjie Wang,Yuqi Liu,Yiyi Zhou,Rongrong Ji	 | 本文提出了一种新的稳定视频面部恢复（SVFR）框架，该框架结合了视频超分辨率、填充和着色任务，并通过统一框架和新型正则化策略提高了时间连贯性和恢复质量。该方法有效整合了各任务的互补优势，提升了视频面部恢复的效果，并为通用视频面部恢复设定了新标准。	 | Face Restoration (FR) is a crucial area within image and video processing, focusing on reconstructing high-quality portraits from degraded inputs. Despite advancements in image FR, video FR remains relatively under-explored, primarily due to challenges related to temporal consistency, motion artifacts, and the limited availability of high-quality video data. Moreover, traditional face restoration typically prioritizes enhancing resolution and may not give as much consideration to related tasks such as facial colorization and inpainting. In this paper, we propose a novel approach for the Generalized Video Face Restoration (GVFR) task, which integrates video BFR, inpainting, and colorization tasks that we empirically show to benefit each other. We present a unified framework, termed as stable video face restoration (SVFR), which leverages the generative and motion priors of Stable Video Diffusion (SVD) and incorporates task-specific information through a unified face restoration framework. A learnable task embedding is introduced to enhance task identification. Meanwhile, a novel Unified Latent Regularization (ULR) is employed to encourage the shared feature representation learning among different subtasks. To further enhance the restoration quality and temporal stability, we introduce the facial prior learning and the self-referred refinement as auxiliary strategies used for both training and inference. The proposed framework effectively combines the complementary strengths of these tasks, enhancing temporal coherence and achieving superior restoration quality. This work advances the state-of-the-art in video FR and establishes a new paradigm for generalized video face restoration.	 | 面部恢复（Face Restoration, FR）是图像和视频处理中的一个关键领域，专注于从降级输入中重建高质量的肖像。尽管在图像FR方面取得了进展，但视频FR仍然相对较少研究，主要原因是时间一致性、运动伪影以及高质量视频数据的有限可用性。此外，传统面部恢复通常侧重于提高分辨率，而可能不太关注诸如面部着色和填充等相关的任务。在本文中，我们提出了一种新的方法，用于通用视频面部恢复（Generalized Video Face Restoration, GVFR）任务，该方法将视频BFR、填充和着色任务结合在一起，我们实验性地证明这些任务可以相互受益。我们提出了一种统一框架，称为稳定视频面部恢复（Stable Video Face Restoration, SVFR），该框架利用了Stable Video Diffusion（SVD）的生成先验和运动先验，并通过统一面部恢复框架引入特定任务信息。引入了一个可学习的任务嵌入来增强任务识别。同时，我们引入了一种新的统一潜在正则化（Unified Latent Regularization, ULR），以鼓励不同子任务之间的共享特征表示学习。为了进一步提高恢复质量和时间稳定性，我们引入了面部先验学习和自我引用细化作为辅助策略，用于训练和推理。所提出的框架有效地结合了这些任务的互补优势，增强了时间连贯性，并实现了更高质量的恢复效果。这项工作在视频FR领域推进了现有技术水平，并为通用视频面部恢复确立了一个新的范式。
2501.01231	 | Exploiting Latent Properties to Optimize Neural Codecs	 | Muhammet Balcilar,Bharath Bhushan Damodaran,Karam Naser,Franck Galpin,Pierre Hellier	 | 本文提出了一种结合矢量量化和熵梯度的端到端编解码器优化方法，以提高现有神经编解码器的性能，实验结果显示这种方法在各种预训练方法下可节省1%到3%的比特率，同时保持相同质量，并且基于熵梯度的解决方案还显著提高了传统编解码器的性能。	 | End-to-end image and video codecs are becoming increasingly competitive, compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques, such as their straightforward adaptation to perceptual distortion metrics and high performance in specific fields thanks to their learning ability. However, current state-of-the-art neural codecs do not fully exploit the benefits of vector quantization and the existence of the entropy gradient in decoding devices. In this paper, we propose to leverage these two properties (vector quantization and entropy gradient) to improve the performance of off-the-shelf codecs. Firstly, we demonstrate that using non-uniform scalar quantization cannot improve performance over uniform quantization. We thus suggest using predefined optimal uniform vector quantization to improve performance. Secondly, we show that the entropy gradient, available at the decoder, is correlated with the reconstruction error gradient, which is not available at the decoder. We therefore use the former as a proxy to enhance compression performance. Our experimental results show that these approaches save between 1 to 3% of the rate for the same quality across various pretrained methods. In addition, the entropy gradient based solution improves traditional codec performance significantly as well.	 | 端到端的图像和视频编解码器正在变得越来越具有竞争力，相比之下，传统的压缩技术是通过数十年的手工工程开发出来的。这些可训练的编解码器有许多优势，例如它们可以直接适应感知失真度量，并且由于学习能力，它们在特定领域表现出色。然而，当前最先进的神经编解码器尚未充分利用矢量量化的优势以及解码设备中存在的熵梯度。在本文中，我们提出了利用这两种特性（矢量量化和熵梯度）来改善现成编解码器的性能。首先，我们证明使用非均匀标量化不能改善性能。因此，我们建议使用预定义的最佳均匀矢量化量化来提高性能。其次，我们展示了解码器中可用的熵梯度与重建误差梯度（解码器中不可用）相关。因此，我们使用前者作为增强压缩性能的代理。我们的实验结果表明，这些方法在各种预训练方法下节省了1%到3%的比特率，同时保持相同的质量。此外，基于熵梯度的解决方案也显著提高了传统编解码器的性能。
2501.01223	 | Conditional Consistency Guided Image Translation and Enhancement	 | A. V. Subramanyam,Amil Bhagat,Milind Jain	 | 本文提出了条件一致性模型（CCMs），通过引入特定任务的条件输入改进了一致性模型，以进行多域图像翻译，并在多个数据集上证明了其生成高质量翻译图像的能力。	 | Consistency models have emerged as a promising alternative to diffusion models, offering high-quality generative capabilities through single-step sample generation. However, their application to multi-domain image translation tasks, such as cross-modal translation and low-light image enhancement remains largely unexplored. In this paper, we introduce Conditional Consistency Models (CCMs) for multi-domain image translation by incorporating additional conditional inputs. We implement these modifications by introducing task-specific conditional inputs that guide the denoising process, ensuring that the generated outputs retain structural and contextual information from the corresponding input domain. We evaluate CCMs on 10 different datasets demonstrating their effectiveness in producing high-quality translated images across multiple domains. Code is available at https://github.com/amilbhagat/Conditional-Consistency-Models.	 | 一致性模型作为一种有前景的替代扩散模型，通过单步样本生成提供了高质量的生成能力。然而，它们在跨域图像翻译任务中的应用，如跨模态翻译和低光图像增强，仍然很大程度上未被探索。在本文中，我们通过引入附加条件输入，提出了条件一致性模型（CCMs）来进行多域图像翻译。我们通过引入特定任务的条件输入来实施这些修改，这些输入指导去噪过程，确保生成的输出保留来自相应输入域的结构和上下文信息。我们在10个不同的数据集上评估了CCMs，证明了它们在多个域中生成高质量翻译图像的有效性。相关代码可在https://github.com/amilbhagat/Conditional-Consistency-Models 获取。
2501.01209	 | A redescription mining framework for post-hoc explaining and relating deep learning models	 | Matej Mihelčić,Ivan Grubišić,Miha Keber	 | 该研究提出了一种新的框架，利用 redescriptions 对深度学习模型进行事后解释和关联，以提高其可解释性和理解性，尤其是在大规模的神经网络中。该框架不仅支持单个模型内层间的关联分析，还适用于多模型间的关联，并能处理复杂标签场景，提供不同于现有可解释AI方法的信息。	 | Deep learning models (DLMs) achieve increasingly high performance both on structured and unstructured data. They significantly extended applicability of machine learning to various domains. Their success in making predictions, detecting patterns and generating new data made significant impact on science and industry. Despite these accomplishments, DLMs are difficult to explain because of their enormous size. In this work, we propose a novel framework for post-hoc explaining and relating DLMs using redescriptions. The framework allows cohort analysis of arbitrary DLMs by identifying statistically significant redescriptions of neuron activations. It allows coupling neurons to a set of target labels or sets of descriptive attributes, relating layers within a single DLM or associating different DLMs. The proposed framework is independent of the artificial neural network architecture and can work with more complex target labels (e.g. multi-label or multi-target scenario). Additionally, it can emulate both pedagogical and decompositional approach to rule extraction. The aforementioned properties of the proposed framework can increase explainability and interpretability of arbitrary DLMs by providing different information compared to existing explainable-AI approaches.	 | 深度学习模型（DLMs）在结构化和非结构化数据上的性能不断提高，极大地扩展了机器学习在各个领域的应用范围。它们在预测、模式识别和生成新数据方面取得的成功对科学和工业产生了重大影响。尽管取得了这些成就，但DLMs由于其庞大的规模而难以解释。在本研究中，我们提出了一种新的框架，用于使用 redescriptions 对DLMs进行事后解释和关联。该框架通过识别神经元激活的统计显著 redescriptions，允许任意DLM的群体分析。它允许将神经元与目标标签集或描述性属性集进行关联，可以在单一DLM内部关联层或关联不同的DLM。所提出的框架独立于人工神经网络架构，并且可以处理更复杂的标签（例如，多标签或多目标场景）。此外，它还可以模拟规则提取的教学和分解方法。所提出框架的上述特性可以通过提供与现有可解释AI方法不同的信息，来增加任意DLM的可解释性和可理解性。
2501.01205	 | Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects	 | Abdullah Mushtaq,Muhammad Rafay Naeem,Ibrahim Ghaznavi,Muhammad Imran Taj,Imran Hashmi,Junaid Qadir	 | 多智能体大型语言模型（MALLMs）通过模拟多样化智能体的协作，能够有效支持工程学生在高级设计项目中解决多学科问题和冲突目标，如优化技术性能同时考虑伦理、社会和环境因素。该框架利用协调、合作和谈判机制，促进跨学科推理，并通过丰富的对话模拟真实工程团队的工作流程。	 | Multi-Agent Large Language Models (LLMs) are gaining significant attention for their ability to harness collective intelligence in complex problem-solving, decision-making, and planning tasks. This aligns with the concept of the wisdom of crowds, where diverse agents contribute collectively to generating effective solutions, making it particularly suitable for educational settings. Senior design projects, also known as capstone or final year projects, are pivotal in engineering education as they integrate theoretical knowledge with practical application, fostering critical thinking, teamwork, and real-world problem-solving skills. In this paper, we explore the use of Multi-Agent LLMs in supporting these senior design projects undertaken by engineering students, which often involve multidisciplinary considerations and conflicting objectives, such as optimizing technical performance while addressing ethical, social, and environmental concerns. We propose a framework where distinct LLM agents represent different expert perspectives, such as problem formulation agents, system complexity agents, societal and ethical agents, or project managers, thus facilitating a holistic problem-solving approach. This implementation leverages standard multi-agent system (MAS) concepts such as coordination, cooperation, and negotiation, incorporating prompt engineering to develop diverse personas for each agent. These agents engage in rich, collaborative dialogues to simulate human engineering teams, guided by principles from swarm AI to efficiently balance individual contributions towards a unified solution. We adapt these techniques to create a collaboration structure for LLM agents, encouraging interdisciplinary reasoning and negotiation similar to real-world senior design projects. To assess the efficacy of this framework, we collected six proposals of engineering and computer science of...	 | 多智能体大型语言模型（Multi-Agent Large Language Models, MALLMs）因其在复杂问题解决、决策和计划任务中能够利用集体智能而受到广泛关注。这与群体智慧的概念相契合，即多样化的智能体共同贡献，生成有效的解决方案，特别适用于教育环境。高级设计项目，也称为顶点项目或毕业设计，是工程教育中的关键组成部分，它们将理论知识与实际应用相结合，培养批判性思维、团队合作和解决实际问题的能力。在本文中，我们探讨了MALLMs在支持工程学生进行的高级设计项目中的应用，这些项目往往涉及多学科考量和冲突目标，例如在优化技术性能的同时解决伦理、社会和环境问题。我们提出了一种框架，其中不同的LLM智能体代表不同的专家视角，如问题定义智能体、系统复杂性智能体、社会和伦理智能体或项目经理，从而促进全面的问题解决方法。该实施利用了多智能体系统（MAS）的概念，如协调、合作和谈判，并结合提示工程技术为每个智能体开发多样化的身份。这些智能体通过丰富的协作对话模拟人类工程团队，由群人工智能原则指导，以高效平衡个体贡献，形成统一的解决方案。我们采用这些技术来为LLM智能体创建协作结构，鼓励跨学科推理和谈判，类似于真实的高级设计项目。为了评估该框架的有效性，我们收集了六个来自工程和计算机科学领域的项目提案……
2501.01182	 | RingFormer: A Neural Vocoder with Ring Attention and Convolution-Augmented Transformer	 | Seongho Hong,Yong-Hoon Choi	 | RingFormer 是一种将环注意力机制集成到卷积增强变压器中的新型神经 vocoder，能够在保持高时间分辨率的同时实现实时长音频生成，并在性能上与现有模型相当或更优。	 | While transformers demonstrate outstanding performance across various audio tasks, their application to neural vocoders remains challenging. Neural vocoders require the generation of long audio signals at the sample level, which demands high temporal resolution. This results in significant computational costs for attention map generation and limits their ability to efficiently process both global and local information. Additionally, the sequential nature of sample generation in neural vocoders poses difficulties for real-time processing, making the direct adoption of transformers impractical. To address these challenges, we propose RingFormer, a neural vocoder that incorporates the ring attention mechanism into a lightweight transformer variant, the convolution-augmented transformer (Conformer). Ring attention effectively captures local details while integrating global information, making it well-suited for processing long sequences and enabling real-time audio generation. RingFormer is trained using adversarial training with two discriminators. The proposed model is applied to the decoder of the text-to-speech model VITS and compared with state-of-the-art vocoders such as HiFi-GAN, iSTFT-Net, and BigVGAN under identical conditions using various objective and subjective metrics. Experimental results show that RingFormer achieves comparable or superior performance to existing models, particularly excelling in real-time audio generation. Our code and audio samples are available on GitHub.	 | 尽管变压器在各种音频任务中表现出色，但将其应用于神经 vocoder 仍然面临挑战。神经 vocoder 需要在样本级别生成长音频信号，这要求具有高时间分辨率。这导致了注意力图生成的显著计算成本，并限制了其高效处理全局和局部信息的能力。此外，神经 vocoder 中样本生成的顺序性质为实时处理带来了困难，使得直接采用变压器变得不切实际。为了解决这些挑战，我们提出了一种名为 RingFormer 的神经 vocoder，该 vocoder 将环注意力机制融入到轻量级的变压器变体——卷积增强变压器（Conformer）中。环注意力机制有效捕捉局部细节并整合全局信息，使其非常适合处理长序列，并能够实现实时音频生成。RingFormer 通过带有两个判别器的对抗训练进行训练。所提出模型应用于文本到语音模型 VITS 的解码器，并在相同的条件下与当前最先进的 vocoder（如 HiFi-GAN、iSTFT-Net 和 BigVGAN）进行了比较，使用了多种客观和主观指标。实验结果显示，RingFormer 的性能与现有模型相当或更优，特别是在实时音频生成方面表现突出。我们的代码和音频样本已发布在 GitHub 上。
2501.01157	 | Ultrasound Lung Aeration Map via Physics-Aware Neural Operators	 | Jiayun Wang,Oleksii Ostras,Masashi Sode,Bahareh Tolooshams,Zongyi Li,Kamyar Azizzadenesheli,Gianmarco Pinton,Anima Anandkumar	 | 肺超声是一种低成本、易获取的临床诊断工具，但由于声波在胸膜界面的复杂回波而解读困难，限制了其广泛应用。为此，研究提出了一种名为LUNA的AI模型，能够直接从RF数据重建肺充气图，提高了肺超声的可重复性和诊断精度。	 | Lung ultrasound is a growing modality in clinics for diagnosing and monitoring acute and chronic lung diseases due to its low cost and accessibility. Lung ultrasound works by emitting diagnostic pulses, receiving pressure waves and converting them into radio frequency (RF) data, which are then processed into B-mode images with beamformers for radiologists to interpret. However, unlike conventional ultrasound for soft tissue anatomical imaging, lung ultrasound interpretation is complicated by complex reverberations from the pleural interface caused by the inability of ultrasound to penetrate air. The indirect B-mode images make interpretation highly dependent on reader expertise, requiring years of training, which limits its widespread use despite its potential for high accuracy in skilled hands.   To address these challenges and democratize ultrasound lung imaging as a reliable diagnostic tool, we propose LUNA, an AI model that directly reconstructs lung aeration maps from RF data, bypassing the need for traditional beamformers and indirect interpretation of B-mode images. LUNA uses a Fourier neural operator, which processes RF data efficiently in Fourier space, enabling accurate reconstruction of lung aeration maps. LUNA offers a quantitative, reader-independent alternative to traditional semi-quantitative lung ultrasound scoring methods. The development of LUNA involves synthetic and real data: We simulate synthetic data with an experimentally validated approach and scan ex vivo swine lungs as real data. Trained on abundant simulated data and fine-tuned with a small amount of real-world data, LUNA achieves robust performance, demonstrated by an aeration estimation error of 9% in ex-vivo lung scans. We demonstrate the potential of reconstructing lung aeration maps from RF data, providing a foundation for improving lung ultrasound reproducibility and diagnostic utility.	 | 肺超声是一种在临床诊断和监测急性及慢性肺疾病中迅速增长的应用方式，因其低成本和易获取性。肺超声通过发射诊断脉冲，接收声压波，并将其转换成射频（RF）数据，然后利用束成型器将这些数据转换成B模式图像供放射科医生解读。然而，与用于软组织解剖成像的常规超声不同，肺超声的解读因其声波无法穿透空气而在胸膜界面引起的复杂回波而变得更加复杂。间接的B模式图像使得解读极为依赖读者的经验，这需要多年的培训，从而限制了其在更广泛范围内的应用，尽管在熟练的手中其高准确性具有巨大的潜力。 为了应对这些挑战，普及肺超声作为一种可靠的诊断工具，我们提出了一种名为LUNA的AI模型，可以直接从RF数据重建肺充气图，从而省去了传统束成型器和间接解读B模式图像的需要。LUNA使用傅里叶神经算子，在傅里叶空间中高效处理RF数据，从而实现肺充气图的准确重建。LUNA提供了一种定量、独立于读者的替代传统半定量肺超声评分方法的选择。LUNA的开发包括合成数据和真实数据：我们使用一个实验验证的方法模拟合成数据，并对离体猪肺进行真实数据扫描。LUNA在大量合成数据上进行训练，并用少量真实世界数据进行微调，表现出稳健的表现，实验证明离体肺扫描的充气估计误差为9%。我们展示了从RF数据重建肺充气图的潜力，为提高肺超声的可重复性和诊断用途奠定了基础。
2501.01156	 | TexAVi: Generating Stereoscopic VR Video Clips from Text Descriptions	 | Vriksha Srihari,R. Bhavya,Shruti Jayaraman,V. Mary Anita Rajam	 | 本文提出了一种结合文本到图像、Stable Diffusion和深度估计算法的方法，用于从文本生成立体虚拟现实视频，以解决将生成模型扩展至虚拟现实领域的挑战。该方法通过三个阶段实现：从文本生成初步图像、提升图像质量以生成具有真实感的帧、以及通过深度估计算法生成左右眼视图以创造沉浸式体验。	 | While generative models such as text-to-image, large language models and text-to-video have seen significant progress, the extension to text-to-virtual-reality remains largely unexplored, due to a deficit in training data and the complexity of achieving realistic depth and motion in virtual environments. This paper proposes an approach to coalesce existing generative systems to form a stereoscopic virtual reality video from text.   Carried out in three main stages, we start with a base text-to-image model that captures context from an input text. We then employ Stable Diffusion on the rudimentary image produced, to generate frames with enhanced realism and overall quality. These frames are processed with depth estimation algorithms to create left-eye and right-eye views, which are stitched side-by-side to create an immersive viewing experience. Such systems would be highly beneficial in virtual reality production, since filming and scene building often require extensive hours of work and post-production effort.   We utilize image evaluation techniques, specifically Fréchet Inception Distance and CLIP Score, to assess the visual quality of frames produced for the video. These quantitative measures establish the proficiency of the proposed method.   Our work highlights the exciting possibilities of using natural language-driven graphics in fields like virtual reality simulations.	 | 尽管文本到图像、大型语言模型和文本到视频生成模型已取得了显著进展，将这些模型扩展到文本到虚拟现实（VR）领域仍面临诸多挑战，主要原因在于缺乏训练数据以及在虚拟环境中实现逼真深度和运动的复杂性。本文提出了一种方法，将现有的生成系统结合起来，从文本生成立体虚拟现实视频。该方法分为三个主要阶段：首先，使用基础的文本到图像模型从输入文本中捕捉上下文信息。接着，使用Stable Diffusion对初步生成的图像进行处理，生成具有更高真实感和整体质量的帧。然后，使用深度估计算法生成左眼和右眼视图，并将这些视图并排缝合在一起，以创造沉浸式的观看体验。此类系统在虚拟现实制作中将非常有益，因为拍摄和场景构建通常需要大量的时间和后期制作努力。  我们利用图像评估技术，特别是Fréchet Inception Distance和CLIP Score，评估视频中生成帧的视觉质量。这些定量指标验证了所提出方法的有效性。我们的工作揭示了在虚拟现实模拟等领域使用自然语言驱动图形的激动人心的可能性。
2501.01144	 | BlockDialect: Block-wise Fine-grained Mixed Format for Energy-Efficient LLM Inference	 | Wonsuk Jang,Thierry Tambe	 | 作者提出了一种名为BlockDialect的技术，它通过选择最佳的数据表示格式和引入DialectFP4格式来提高大语言模型的效率，同时在LLaMA和LLaMA2模型上实现了显著的准确率提升和能效改进。	 | Large Language Models (LLMs) have achieved remarkable success, but their increasing size poses significant challenges in memory usage and computational costs. Quantizing both weights and activations can address these issues, with fine-grained block-wise quantization emerging as a promising hardware-supported solution to mitigate outliers. However, existing methods struggle to capture nuanced block data distributions. To address this, we propose BlockDialect, a block-wise fine-grained mixed format technique that assigns a per-block optimal number format from formatbook for better data representation. Additionally, we introduce DialectFP4, a formatbook of FP4 variants (akin to dialects) that adapt to diverse data distributions. Importantly, DialectFP4 ensures hardware efficiency by selecting representable values as scaled integers compatible with low-precision integer arithmetic. Furthermore, we propose a two-stage approach for online DialectFP4 activation quantization. BlockDialect achieves 11.40% (6.90%) accuracy gain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with a comparable bit usage per data, while being only 5.89% (3.31%) below full precision even when quantizing full-path matrix multiplication. Focusing on how to represent over how to scale, our work presents a promising path for energy-efficient LLM inference.	 | 大型语言模型（LLMs）已经取得了显著的成功，但它们不断增加的规模带来了内存使用和计算成本的重大挑战。通过量化权重和激活值可以解决这些问题，而细粒度的块量化作为一种由硬件支持的解决方案，正在成为缓解异常值的有效方法。然而，现有的方法难以捕捉到块数据分布的细微差别。为了解决这一问题，我们提出了一种名为BlockDialect的技术，这是一种块级别的细粒度混合格式方法，从formatbook中选择每个块的最佳格式以更好地表示数据。此外，我们还引入了DialectFP4格式，这是一种类似于方言的FP4变体集，能够适应多种数据分布。重要的是，DialectFP4通过选择与低精度整数算术兼容的缩放整数值来确保硬件效率。此外，我们提出了两阶段的方法来在线进行DialectFP4激活量化。BlockDialect在LLaMA3-8B（LLaMA2-7B）模型上相较于MXFP4格式实现了11.40%（6.90%）的准确率提升，同时在数据位使用量相近的情况下，即使在全路径矩阵乘法量化时，其精度也仅比全精度低5.89%（3.31%）。我们的工作关注的是如何表示而不是如何缩放，这为能效型大语言模型推理提供了一条有前景的道路。
2501.01136	 | Symmetries-enhanced Multi-Agent Reinforcement Learning	 | Nikolaos Bousias,Stefanos Pertigkiozoglou,Kostas Daniilidis,George Pappas	 | 本文提出了一种新的框架，通过群对称图former嵌入外部对称性于多智能体系统中，以提高多智能体强化学习的泛化能力和零样本可扩展性，并在四旋翼无人机集群实验中验证了其有效性和潜力。	 | Multi-agent reinforcement learning has emerged as a powerful framework for enabling agents to learn complex, coordinated behaviors but faces persistent challenges regarding its generalization, scalability and sample efficiency. Recent advancements have sought to alleviate those issues by embedding intrinsic symmetries of the systems in the policy. Yet, most dynamical systems exhibit little to no symmetries to exploit. This paper presents a novel framework for embedding extrinsic symmetries in multi-agent system dynamics that enables the use of symmetry-enhanced methods to address systems with insufficient intrinsic symmetries, expanding the scope of equivariant learning to a wide variety of MARL problems. Central to our framework is the Group Equivariant Graphormer, a group-modular architecture specifically designed for distributed swarming tasks. Extensive experiments on a swarm of symmetry-breaking quadrotors validate the effectiveness of our approach, showcasing its potential for improved generalization and zero-shot scalability. Our method achieves significant reductions in collision rates and enhances task success rates across a diverse range of scenarios and varying swarm sizes.	 | 多智能体强化学习作为一种使智能体学习复杂协调行为的强大框架已逐渐崭露头角，但仍然面临着泛化能力、可扩展性和样本效率等方面的挑战。最近的发展试图通过在策略中嵌入系统固有的对称性来缓解这些问题。然而，大多数动力系统几乎没有可用的对称性。本文提出了一种新的框架，用于在多智能体系统动力学中嵌入外部对称性，从而使增强对称性的方法能够应用于缺乏足够内在对称性的系统，将对称性学习的应用范围扩展到广泛多样的多智能体强化学习（MARL）问题。我们框架的核心是群对称图former（Group Equivariant Graphormer），这是一种专门针对分布式集群任务设计的群模块化架构。在打破对称性的四旋翼无人机集群的广泛实验中验证了我们方法的有效性，展示了其在提高泛化能力和零样本可扩展性方面的潜力。我们的方法在多种场景和不同规模的集群中显著减少了碰撞率，提高了任务成功率。
2501.01123	 | TED: Turn Emphasis with Dialogue Feature Attention for Emotion Recognition in Conversation	 | Junya Ono,Hiromi Wakaki	 | 该研究提出了一种名为对话增强注意力（TED）的方法，通过在注意力机制中加入对话特征来显式区分对话中的不同轮次，从而提高了对话中情绪识别的性能，并在多个基准数据集上取得了优异成果，特别是在IEMOCAP数据集上达到了最先进的性能。	 | Emotion recognition in conversation (ERC) has been attracting attention by methods for modeling multi-turn contexts. The multi-turn input to a pretraining model implicitly assumes that the current turn and other turns are distinguished during the training process by inserting special tokens into the input sequence. This paper proposes a priority-based attention method to distinguish each turn explicitly by adding dialogue features into the attention mechanism, called Turn Emphasis with Dialogue (TED). It has a priority for each turn according to turn position and speaker information as dialogue features. It takes multi-head self-attention between turn-based vectors for multi-turn input and adjusts attention scores with the dialogue features. We evaluate TED on four typical benchmarks. The experimental results demonstrate that TED has high overall performance in all datasets and achieves state-of-the-art performance on IEMOCAP with numerous turns.	 | 以下是对该摘要的中文翻译：  对话中的情绪识别（ERC）引起了通过建模多轮上下文方法的关注。一个多轮输入到预训练模型中隐含地假设在训练过程中通过在输入序列中插入特殊标记来区分当前轮次和其他轮次。本文提出了一种基于优先级的注意力方法，通过将对话特征添加到注意机制中来显式地区分每一轮次，称为对话增强注意力（TED）。每一轮次都根据轮次位置和说话人信息具有优先级。对于多轮输入，TED在轮次向量之间采取多头自注意力，并使用对话特征调整注意分数。我们在四个典型的基准上评估了TED。实验结果表明，TED在所有数据集中的整体性能都很高，并且在具有大量轮次的IEMOCAP数据集上达到了最先进的性能。
2501.01117	 | Robust COVID-19 Detection from Cough Sounds using Deep Neural Decision Tree and Forest: A Comprehensive Cross-Datasets Evaluation	 | Rofiqul Islam,Nihad Karim Chowdhury,Muhammad Ashad Kabir	 | 本文提出了一种利用深度神经决策树和深度神经决策森林对COVID-19咳嗽声音进行分类的方法，该方法在多个数据集上取得了优异的性能，AUC分数均显著优于现有方法。研究还揭示了不同数据集中与COVID-19相关的咳嗽声音差异，强调了数据集整合的重要性。	 | This research presents a robust approach to classifying COVID-19 cough sounds using cutting-edge machine-learning techniques. Leveraging deep neural decision trees and deep neural decision forests, our methodology demonstrates consistent performance across diverse cough sound datasets. We begin with a comprehensive extraction of features to capture a wide range of audio features from individuals, whether COVID-19 positive or negative. To determine the most important features, we use recursive feature elimination along with cross-validation. Bayesian optimization fine-tunes hyper-parameters of deep neural decision tree and deep neural decision forest models. Additionally, we integrate the SMOTE during training to ensure a balanced representation of positive and negative data. Model performance refinement is achieved through threshold optimization, maximizing the ROC-AUC score. Our approach undergoes a comprehensive evaluation in five datasets: Cambridge, Coswara, COUGHVID, Virufy, and the combined Virufy with the NoCoCoDa dataset. Consistently outperforming state-of-the-art methods, our proposed approach yields notable AUC scores of 0.97, 0.98, 0.92, 0.93, 0.99, and 0.99 across the respective datasets. Merging all datasets into a combined dataset, our method, using a deep neural decision forest classifier, achieves an AUC of 0.97. Also, our study includes a comprehensive cross-datasets analysis, revealing demographic and geographic differences in the cough sounds associated with COVID-19. These differences highlight the challenges in transferring learned features across diverse datasets and underscore the potential benefits of dataset integration, improving generalizability and enhancing COVID-19 detection from audio signals.	 | 本文介绍了一种利用先进机器学习技术对 COVID-19 咳嗽声音进行分类的稳健方法。该方法利用深度神经决策树和深度神经决策森林，展示了在多种咳嗽声音数据集上的稳定性能。我们首先进行全面的特征提取，以捕捉不同个体（无论是 COVID-19 阳性还是阴性）的广泛音频特征。为了确定最重要的特征，我们使用递归特征消除和交叉验证相结合的方法。贝叶斯优化进一步调整深度神经决策树和深度神经决策森林模型的超参数。此外，我们在训练过程中整合了 SMOTE，以确保阳性数据和阴性数据的均衡表示。通过阈值优化进行模型性能的进一步调整，从而最大化 ROC-AUC 分数。我们的方法在五个数据集（剑桥大学、Coswara、COUGHVID、Virufy 以及 NoCoCoDa 数据集合并数据集）上进行了全面评估。我们的方法在各个数据集上分别取得了 0.97、0.98、0.92、0.93、0.99 和 0.99 的显著 AUC 分数，均优于现有先进方法。将所有数据集合并成一个综合数据集后，使用深度神经决策森林分类器的方法实现了 0.97 的 AUC。此外，本研究还进行了跨数据集的全面分析，揭示了与 COVID-19 相关的咳嗽声音在不同人群和地理区域的差异。这些差异突显了在不同数据集中转移学习特征的挑战，并强调了数据集整合的潜在益处，从而提高泛化能力和通过音频信号检测 COVID-19 的能力。
2501.01111	 | Regularized Proportional Fairness Mechanism for Resource Allocation Without Money	 | Sihan Zeng,Sujay Bhatt,Alec Koppel,Sumitra Ganesh	 | 该研究探讨了在无支付设置下如何设计机制以最大化社会福利并确保激励相容，提出了一个名为正则化按比例公平网络（RPF-Net）的创新神经网络架构，以减少代理的错误报告动机，并通过实验展示了其优势。	 | Mechanism design in resource allocation studies dividing limited resources among self-interested agents whose satisfaction with the allocation depends on privately held utilities. We consider the problem in a payment-free setting, with the aim of maximizing social welfare while enforcing incentive compatibility (IC), i.e., agents cannot inflate allocations by misreporting their utilities. The well-known proportional fairness (PF) mechanism achieves the maximum possible social welfare but incurs an undesirably high exploitability (the maximum unilateral inflation in utility from misreport and a measure of deviation from IC). In fact, it is known that no mechanism can achieve the maximum social welfare and exact incentive compatibility (IC) simultaneously without the use of monetary incentives (Cole et al., 2013). Motivated by this fact, we propose learning an approximate mechanism that desirably trades off the competing objectives. Our main contribution is to design an innovative neural network architecture tailored to the resource allocation problem, which we name Regularized Proportional Fairness Network (RPF-Net). RPF-Net regularizes the output of the PF mechanism by a learned function approximator of the most exploitable allocation, with the aim of reducing the incentive for any agent to misreport. We derive generalization bounds that guarantee the mechanism performance when trained under finite and out-of-distribution samples and experimentally demonstrate the merits of the proposed mechanism compared to the state-of-the-art.	 | 资源分配中的机制设计研究了在自利代理之间分配有限资源的问题，这些代理对分配的满意度取决于其私有的效用。我们考虑在无支付设置下的问题，目标是在确保激励相容的前提下最大化社会福利，即代理不能通过错误报告其效用来增大分配。众所周知，流行的按比例公平（PF）机制能够实现最大的可能社会福利，但其可利用性极高（从错误报告中获得的最大单边效用增益，以及偏离激励相容的程度）。实际上，已知没有任何机制能够在不使用货币激励的情况下同时实现最大社会福利和精确的激励相容（Cole等，2013）。鉴于这一事实，我们提出学习一种近似机制，以合理地权衡竞争目标。我们的主要贡献是设计了一种针对资源分配问题的创新神经网络架构，我们将其命名为正则化按比例公平网络（RPF-Net）。RPF-Net通过一个学习函数逼近器，对PF机制的输出进行正则化，旨在减少任何代理错误报告的动机。我们推导出泛化界，保证在有限样本和分布外样本下机制的表现，并通过实验展示了与现有最佳机制相比的优势。
2501.01108	 | MuQ: Self-Supervised Music Representation Learning with Mel Residual Vector Quantization	 | Haina Zhu,Yizhi Zhou,Hangting Chen,Jianwei Yu,Ziyang Ma,Rongzhi Gu,Wei Tan,Xie Chen	 | 本文提出了一种名为MuQ的自我监督音乐表示学习模型，该模型通过预测由梅尔残差向量量化生成的标记来学习音乐表示，并在多种音乐理解任务中表现出色，特别是在使用少量预训练数据时优于现有模型。此外，MuQ还与对比学习结合形成了MuQ-MuLan，在零样本音乐标记任务中达到了最先进的性能。	 | Recent years have witnessed the success of foundation models pre-trained with self-supervised learning (SSL) in various music informatics understanding tasks, including music tagging, instrument classification, key detection, and more. In this paper, we propose a self-supervised music representation learning model for music understanding. Distinguished from previous studies adopting random projection or existing neural codec, the proposed model, named MuQ, is trained to predict tokens generated by Mel Residual Vector Quantization (Mel-RVQ). Our Mel-RVQ utilizes residual linear projection structure for Mel spectrum quantization to enhance the stability and efficiency of target extraction and lead to better performance. Experiments in a large variety of downstream tasks demonstrate that MuQ outperforms previous self-supervised music representation models with only 0.9K hours of open-source pre-training data. Scaling up the data to over 160K hours and adopting iterative training consistently improve the model performance. To further validate the strength of our model, we present MuQ-MuLan, a joint music-text embedding model based on contrastive learning, which achieves state-of-the-art performance in the zero-shot music tagging task on the MagnaTagATune dataset. Code and checkpoints are open source in https://github.com/tencent-ailab/MuQ.	 | 近年来，基于自我监督学习（SSL）预训练的通用模型在音乐信息理解任务中取得了成功，包括音乐标记、乐器分类、键识别等。在本文中，我们提出了一种用于音乐理解的自我监督音乐表示学习模型。与之前采用随机投影或现有神经编解码器的研究不同，我们提出的模型名为MuQ，它被训练以预测由梅尔残差向量量化（Mel-RVQ）生成的标记。我们的Mel-RVQ利用残差线性投影结构进行梅尔谱量化，以增强目标提取的稳定性和效率，并带来更好的性能。在大量下游任务中的实验表明，MuQ仅使用900小时的开源预训练数据就优于之前的自我监督音乐表示模型。将数据扩展到超过160,000小时并采用迭代训练一致提高了模型性能。为了进一步验证我们模型的性能，我们呈现了基于对比学习的联合音乐-文本嵌入模型MuQ-MuLan，在MagnaTagATune数据集的零样本音乐标记任务中取得了最先进的性能。代码和检查点可以在https://github.com/tencent-ailab/MuQ开源。
2501.01106	 | AIM: Additional Image Guided Generation of Transferable Adversarial Attacks	 | Teng Li,Xingjun Ma,Yu-Gang Jiang	 | 本文专注于提高生成对抗样本的目标可转移性，通过引入语义注入模块（SIM）将附加语义信息融入攻击中，并提出新的损失函数以增强可转移性，实验结果证明该方法在目标攻击和非目标攻击设置下均有效。	 | Transferable adversarial examples highlight the vulnerability of deep neural networks (DNNs) to imperceptible perturbations across various real-world applications. While there have been notable advancements in untargeted transferable attacks, targeted transferable attacks remain a significant challenge. In this work, we focus on generative approaches for targeted transferable attacks. Current generative attacks focus on reducing overfitting to surrogate models and the source data domain, but they often overlook the importance of enhancing transferability through additional semantics. To address this issue, we introduce a novel plug-and-play module into the general generator architecture to enhance adversarial transferability. Specifically, we propose a \emph{Semantic Injection Module} (SIM) that utilizes the semantics contained in an additional guiding image to improve transferability. The guiding image provides a simple yet effective method to incorporate target semantics from the target class to create targeted and highly transferable attacks. Additionally, we propose new loss formulations that can integrate the semantic injection module more effectively for both targeted and untargeted attacks. We conduct comprehensive experiments under both targeted and untargeted attack settings to demonstrate the efficacy of our proposed approach.	 | 可转移的对抗样本揭示了深度神经网络（DNNs）在各种实际应用中对不可见扰动的脆弱性。尽管在非目标可转移攻击方面取得了显著进展，但针对可转移攻击仍然是一个重大挑战。在本文中，我们重点关注生成对抗攻击中的目标可转移性方法。当前的生成攻击主要致力于减少对替代模型和源数据域的过拟合，但往往忽略了通过附加语义增强可转移性的关键性。为解决这一问题，我们引入了一个新的插件模块，以增强对抗样本的可转移性。具体来说，我们提出了一种称为“语义注入模块”（SIM，Semantic Injection Module）的方法，利用附加引导图像中包含的语义来改善可转移性。引导图像提供了一种简单而有效的方法，将目标类别的靶向语义融入到攻击中，从而创建出高可转移的靶向攻击。此外，我们还提出了新的损失函数形式，能够更有效地整合语义注入模块，适用于目标攻击和非目标攻击。我们分别在靶向攻击和非目标攻击设置下进行了全面的实验，以证明我们提出的方法的有效性。
2501.01069	 | BeliN: A Novel Corpus for Bengali Religious News Headline Generation using Contextual Feature Fusion	 | Md Osama,Ashim Dey,Kawsar Ahmed,Muhammad Ashad Kabir	 | 本研究引入了BeliN语料库和多输入上下文特征融合的标题生成方法MultiGen，以解决现有方法忽视情感、类别和方面等关键上下文特征的问题，实验结果表明MultiGen在孟加拉宗教新闻领域取得了显著的性能提升。	 | Automatic text summarization, particularly headline generation, remains a critical yet underexplored area for Bengali religious news. Existing approaches to headline generation typically rely solely on the article content, overlooking crucial contextual features such as sentiment, category, and aspect. This limitation significantly hinders their effectiveness and overall performance. This study addresses this limitation by introducing a novel corpus, BeliN (Bengali Religious News) - comprising religious news articles from prominent Bangladeshi online newspapers, and MultiGen - a contextual multi-input feature fusion headline generation approach. Leveraging transformer-based pre-trained language models such as BanglaT5, mBART, mT5, and mT0, MultiGen integrates additional contextual features - including category, aspect, and sentiment - with the news content. This fusion enables the model to capture critical contextual information often overlooked by traditional methods. Experimental results demonstrate the superiority of MultiGen over the baseline approach that uses only news content, achieving a BLEU score of 18.61 and ROUGE-L score of 24.19, compared to baseline approach scores of 16.08 and 23.08, respectively. These findings underscore the importance of incorporating contextual features in headline generation for low-resource languages. By bridging linguistic and cultural gaps, this research advances natural language processing for Bengali and other underrepresented languages. To promote reproducibility and further exploration, the dataset and implementation code are publicly accessible at https://github.com/akabircs/BeliN.	 | 自动文本摘要，尤其是标题生成，仍然是孟加拉宗教新闻领域一个关键但尚未充分探索的领域。现有标题生成方法通常仅依赖文章内容，忽视了诸如情感、类别和方面等关键上下文特征。这种限制极大阻碍了其有效性及整体性能。本研究通过引入一个新型语料库——BeliN（孟加拉宗教新闻）及多输入上下文特征融合标题生成方法——MultiGen，来解决这一限制。BeliN语料库包含来自著名孟加拉在线报纸的新闻文章，而MultiGen则利用转录器基预训练语言模型（如 BanglaT5、mBART、mT5 和 mT0），将类别、方面和情感等额外的上下文特征与新闻内容融合。这种融合使模型能够捕捉传统方法常忽略的关键上下文信息。实验结果表明，MultiGen 在仅使用新闻内容的基线方法上取得了显著优势，其 BLEU 分数为 18.61，ROUGE-L 分数为 24.19，而基线方法分别为 16.08 和 23.08。这些发现突显了在低资源语言的标题生成中整合上下文特征的重要性。通过弥合语言和文化差距，本研究推进了孟加拉语和其他少被代表语言的自然语言处理技术。为了促进可重复性和进一步探索，该数据集和实现代码已在 https://github.com/akabircs/BeliN 公开可供访问。
2501.01061	 | An Efficient Outlier Detection Algorithm for Data Streaming	 | Rui Hu, Luc (Zhilu)Chen,Yiwei Wang	 | 本文提出了一种新的高效增量LOF（EILOF）算法，该算法旨在提高局部异常因子（LOF）算法在实时异常检测中的效率，仅计算新数据点的LOF分数而不改变现有数据点的分数，从而在处理大量数据时显著降低计算成本并提高检测准确性。实验结果表明，EILOF在各种场景下的检测性能优于现有的增量LOF（ILOF）算法。	 | The nature of modern data is increasingly real-time, making outlier detection crucial in any data-related field, such as finance for fraud detection and healthcare for monitoring patient vitals. Traditional outlier detection methods, such as the Local Outlier Factor (LOF) algorithm, struggle with real-time data due to the need for extensive recalculations with each new data point, limiting their application in real-time environments. While the Incremental LOF (ILOF) algorithm has been developed to tackle the challenges of online anomaly detection, it remains computationally expensive when processing large streams of data points, and its detection performance may degrade after a certain threshold of points have streamed in. In this paper, we propose a novel approach to enhance the efficiency of LOF algorithms for online anomaly detection, named the Efficient Incremental LOF (EILOF) algorithm. The EILOF algorithm only computes the LOF scores of new points without altering the LOF scores of existing data points. Although exact LOF scores have not yet been computed for the existing points in the new algorithm, datasets often contain noise, and minor deviations in LOF score calculations do not necessarily degrade detection performance. In fact, such deviations can sometimes enhance outlier detection. We systematically tested this approach on both simulated and real-world datasets, demonstrating that EILOF outperforms ILOF as the volume of streaming data increases across various scenarios. The EILOF algorithm not only significantly reduces computational costs, but also systematically improves detection accuracy when the number of additional points increases compared to the ILOF algorithm.	 | 现代数据日益呈现出实时性，使得异常检测在各个相关领域变得尤为重要，如金融领域的欺诈检测和医疗保健领域的患者生命体征监测。传统的异常检测方法，如局部异常因子（LOF）算法，在处理实时数据时面临巨大挑战，因为每次新数据点到来都需要进行大量的重新计算，从而限制了其在实时环境中的应用。虽然增量LOF（ILOF）算法旨在解决在线异常检测的挑战，但在处理大量数据点时仍然计算成本高昂，且在数据点流经一定阈值后，检测性能可能会下降。本文提出了一种新的方法，以提高LOF算法在在线异常检测中的效率，称之为高效增量LOF（EILOF）算法。EILOF算法仅计算新数据点的LOF分数，而不改变现有数据点的LOF分数。尽管在新算法中尚未确切计算出现有点的LOF分数，但由于数据集通常包含噪声，LOF分数计算中的微小偏差并不一定会影响检测性能，有时甚至能增强异常检测。我们系统地在模拟数据集和真实数据集上测试了这种方法，结果显示，随着流式数据量的增加，EILOF在各种场景中的检测性能均优于ILOF算法。EILOF算法不仅显著降低了计算成本，而且随着新增点数量的增加，系统地提高了检测准确性，相比ILOF算法有所提升。
2501.01059	 | Dynamic Attention-Guided Context Decoding for Mitigating Context Faithfulness Hallucinations in Large Language Models	 | Yanwen Huang,Yong Zhang,Ning Cheng,Zhitao Li,Shaojun Wang,Jing Xiao	 | 该研究揭示了大型语言模型因上下文利用不足和高输出不确定性而导致的忠实性幻觉问题，并提出了一种名为DAGCD的轻量级框架，该框架通过整合注意力分布和不确定性信号来改善忠实度和稳健性。	 | Large language models (LLMs) often suffer from context faithfulness hallucinations, where outputs deviate from retrieved information due to insufficient context utilization and high output uncertainty. Our uncertainty evaluation experiments reveal a strong correlation between high uncertainty and hallucinations. We hypothesize that attention mechanisms encode signals indicative of contextual utilization, validated through probing analysis. Based on these insights, we propose Dynamic Attention-Guided Context Decoding (DAGCD), a lightweight framework that integrates attention distributions and uncertainty signals in a single-pass decoding process. Experiments across QA datasets demonstrate DAGCD's effectiveness, achieving significant improvements in faithfulness and robustness while maintaining computational efficiency.	 | 大型语言模型（LLMs）常常遭受上下文忠实性幻觉的问题，输出可能会偏离检索到的信息，主要是由于上下文利用不足和高输出不确定性。我们的不确定度评估实验揭示了高不确定度与幻觉之间存在很强的相关性。我们假设注意力机制编码了指示上下文利用的信号，这一点通过探针分析得到了验证。基于这些见解，我们提出了一种名为动态注意力引导上下文解码（DAGCD）的轻量级框架，该框架在单次解码过程中整合了注意力分布和不确定度信号。跨多个问答数据集的实验表明，DAGCD 在保持计算效率的同时，显著提升了忠实度和稳健性。
2501.01057	 | HPC Application Parameter Autotuning on Edge Devices: A Bandit Learning Approach	 | Abrar Hossain,Abdel-Hameed A. Badawy,Mohammad A. Islam,Tapasya Patki,Kishwar Ahmed	 | LASP（轻量级科学应用参数自调优）是一种利用多臂bandit技术的新型策略，能够在资源有限的边缘设备上有效优化高性能计算应用程序的参数，实现性能提升。	 | The growing necessity for enhanced processing capabilities in edge devices with limited resources has led us to develop effective methods for improving high-performance computing (HPC) applications. In this paper, we introduce LASP (Lightweight Autotuning of Scientific Application Parameters), a novel strategy designed to address the parameter search space challenge in edge devices. Our strategy employs a multi-armed bandit (MAB) technique focused on online exploration and exploitation. Notably, LASP takes a dynamic approach, adapting seamlessly to changing environments. We tested LASP with four HPC applications: Lulesh, Kripke, Clomp, and Hypre. Its lightweight nature makes it particularly well-suited for resource-constrained edge devices. By employing the MAB framework to efficiently navigate the search space, we achieved significant performance improvements while adhering to the stringent computational limits of edge devices. Our experimental results demonstrate the effectiveness of LASP in optimizing parameter search on edge devices.	 | 随着边缘设备资源有限但对增强处理能力的需求日益增长，我们开发了一种有效的方法来提升高性能计算（HPC）应用程序。本文介绍了LASP（轻量级科学应用参数自调优），这是一种针对边缘设备参数搜索空间挑战的新策略。我们的策略采用多臂 bandit (MAB) 技术，专注于在线探索和利用。值得注意的是，LASP 采取动态方法，能够无缝适应不断变化的环境。我们使用LASP 测试了四个HPC 应用程序：Lulesh、Kripke、Clomp 和 Hypre。其轻量级特性使其特别适合资源受限的边缘设备。通过使用 MAB 框架高效地导航搜索空间，我们在遵守边缘设备严格的计算限制的同时，实现了显著的性能提升。我们的实验结果表明，LASP在边缘设备上优化参数搜索方面的有效性。
2501.01045	 | ZeroFlow: Overcoming Catastrophic Forgetting is Easier than You Think	 | Tao Feng,Wei Li,DiDi Zhu,Hangjie Yuan,Wendi Zheng,Dan Zhang,Jie Tang	 | 该研究引入了ZeroFlow基准测试，评估无梯度优化算法在克服灾难性遗忘方面的效果，并发现仅通过前向传递即可有效减少遗忘现象，展示了前向传递在优化和管理任务冲突方面的潜力。	 | Backpropagation provides a generalized configuration for overcoming catastrophic forgetting. Like, SGD and Adam are commonly used for weight updates in continual learning and continual pre-training. In practice, permission to access gradient information is not always granted (the gradient ban), such as black-box APIs, hardware limitations, and non-differentiable systems. To bridge this gap, we introduce the first benchmark ZeroFlow to evaluate gradient-free optimization algorithms for overcoming forgetting. This benchmark examines a suite of forward pass methods across multiple methods, forgetting scenarios, and datasets. We find that forward passes alone are enough to overcome forgetting. Our findings reveal new optimization principles that highlight the potential of forward-pass in mitigating forgetting, managing task conflicts, and reducing memory demands, alongside novel enhancements that further mitigate forgetting with just one forward pass. This work provides essential insights and tools for advancing forward pass methods to overcome forgetting.	 | 反向传播提供了一种通用配置，用于克服灾难性遗忘。例如，SGD 和 Adam 常常被用于持续学习和持续预训练中的权重更新。在实践中，获取梯度信息的权限并不总是被授予（梯度禁令），这可能是因为黑盒 API、硬件限制或非可微系统。为了解决这一问题，我们引入了第一个基准测试 ZeroFlow，用于评估无梯度优化算法以克服遗忘的效果。该基准测试考察了多种前向传递方法在不同遗忘场景和数据集上的表现。我们发现，仅前向传递就足以克服遗忘。我们的研究结果揭示了新的优化原则，突显了前向传递在缓解遗忘、管理任务冲突和减少内存需求方面的潜力，并提出了一些新颖的增强方法，只需一次前向传递即可进一步减轻遗忘。这项工作为通过前向传递方法克服遗忘提供了重要的见解和工具。
2501.01042	 | Image-based Multimodal Models as Intruders: Transferable Multimodal Attacks on Video-based MLLMs	 | Linhao Huang,Xue Jiang,Zhiqiang Wang,Wentao Mo,Xi Xiao,Bo Han,Yongjie Yin,Feng Zheng	 | 本文研究了对抗视频样本在基于视频的多模态大型语言模型（V-MLLMs）之间的可转移性，并提出了一种名为Image-to-Video MLLM（I2V-MLLM）的新攻击方法，该方法通过利用图像为基础的多模态模型并在潜在空间中破坏视频表示来提高对抗样本的可转移性，从而在多种视频-文本任务中取得了显著的攻击成功率。	 | Video-based multimodal large language models (V-MLLMs) have shown vulnerability to adversarial examples in video-text multimodal tasks. However, the transferability of adversarial videos to unseen models--a common and practical real world scenario--remains unexplored. In this paper, we pioneer an investigation into the transferability of adversarial video samples across V-MLLMs. We find that existing adversarial attack methods face significant limitations when applied in black-box settings for V-MLLMs, which we attribute to the following shortcomings: (1) lacking generalization in perturbing video features, (2) focusing only on sparse key-frames, and (3) failing to integrate multimodal information. To address these limitations and deepen the understanding of V-MLLM vulnerabilities in black-box scenarios, we introduce the Image-to-Video MLLM (I2V-MLLM) attack. In I2V-MLLM, we utilize an image-based multimodal model (IMM) as a surrogate model to craft adversarial video samples. Multimodal interactions and temporal information are integrated to disrupt video representations within the latent space, improving adversarial transferability. In addition, a perturbation propagation technique is introduced to handle different unknown frame sampling strategies. Experimental results demonstrate that our method can generate adversarial examples that exhibit strong transferability across different V-MLLMs on multiple video-text multimodal tasks. Compared to white-box attacks on these models, our black-box attacks (using BLIP-2 as surrogate model) achieve competitive performance, with average attack success rates of 55.48% on MSVD-QA and 58.26% on MSRVTT-QA for VideoQA tasks, respectively. Our code will be released upon acceptance.	 | 基于视频的多模态大型语言模型（V-MLLMs）在视频-文本多模态任务中显示出对对抗样本的脆弱性。然而，对抗视频在未见过的模型上的可转移性——这是常见且实际的现实世界场景——尚未被探索。在本文中，我们开创性地研究了对抗视频样本在V-MLLMs之间可转移性的问题。我们发现，现有对抗攻击方法在应用于V-MLLMs的黑盒设置中存在显著的局限性，我们认为这些局限性来源于以下几点：（1）在扰动视频特征方面缺乏泛化能力，（2）仅关注稀疏的关键帧，以及（3）未能整合多模态信息。为了应对这些局限性并加深对V-MLLM在黑盒场景下脆弱性的理解，我们引入了Image-to-Video MLLM（I2V-MLLM）攻击。在I2V-MLLM中，我们利用图像为基础的多模态模型（IMM）作为代理模型来制作对抗视频样本。通过整合多模态交互和时间信息，破坏视频在潜在空间中的表示，从而提高对抗样本的可转移性。此外，我们还引入了一种扰动传播技术来处理不同的未知帧采样策略。实验结果表明，我们的方法可以在多种视频-文本多模态任务中，生成在不同V-MLLMs之间表现出强大可转移性的对抗样本。与这些模型的白盒攻击相比，我们的黑盒攻击（使用BLIP-2作为代理模型）在视频问答任务中分别在MSVD-QA和MSRVTT-QA上实现了55.48%和58.26%的平均攻击成功率。我们的代码将在接受后发布。
2501.01005	 | FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving	 | Zihao Ye,Lequn Chen,Ruihang Lai,Wuwei Lin,Yineng Zhang,Stephanie Wang,Tianqi Chen,Baris Kasikci,Vinod Grover,Arvind Krishnamurthy,Luis Ceze	 | FlashInfer是一种可定制且高效的大型语言模型推理引擎，通过优化内存访问、提供可定制的注意模板和动态负载均衡调度算法，显著提升了推理性能；与现有解决方案相比，FlashInfer在多种推理场景中实现了显著的性能提升，包括减少跨令牌延迟和长上下文推理延迟，以及加速并行生成的LLM服务。	 | Transformers, driven by attention mechanisms, form the foundation of large language models (LLMs). As these models scale up, efficient GPU attention kernels become essential for high-throughput and low-latency inference. Diverse LLM applications demand flexible and high-performance attention solutions. We present FlashInfer: a customizable and efficient attention engine for LLM serving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse format and composable formats to optimize memory access and reduce redundancy. It also offers a customizable attention template, enabling adaptation to various settings through Just-In-Time (JIT) compilation. Additionally, FlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user requests while maintaining compatibility with CUDAGraph which requires static configuration. FlashInfer have been integrated into leading LLM serving frameworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and end-to-end evaluations demonstrate FlashInfer's ability to significantly boost kernel performance across diverse inference scenarios: compared to state-of-the-art LLM serving solutions, FlashInfer achieve 29-69% inter-token-latency reduction compared to compiler backends for LLM serving benchmark, 28-30% latency reduction for long-context inference, and 13-17% speedup for LLM serving with parallel generation.	 | 基于注意机制的变压器构成了大规模语言模型（LLM）的基础。随着这些模型的规模扩大，高效的GPU注意内核对于高吞吐量和低延迟推理变得至关重要。不同的LLM应用需要灵活且高性能的注意解决方案。我们介绍了FlashInfer：一种可定制且高效的LLM推理引擎。FlashInfer通过使用块稀疏格式和可组合格式解决KV缓存存储异构性，从而优化内存访问并减少冗余。它还提供了一种可定制的注意模板，通过Just-In-Time（JIT）编译使适应各种设置成为可能。此外，FlashInfer的负载均衡调度算法能够适应用户请求的动态变化，同时保持与CUDAGraph的兼容性，后者需要静态配置。FlashInfer已整合到领先的LLM服务框架中，如SGLang、vLLM和MLC-Engine。全面的内核级和端到端评估表明，FlashInfer能够在多种推理场景中显著提升内核性能：与最先进的LLM推理解决方案相比，FlashInfer在LLM推理基准测试中的编译后端相比实现了29-69%的跨令牌延迟减少，长上下文推理延迟减少28-30%，并行生成的LLM服务加速13-17%。
2501.00975	 | CoordFlow: Coordinate Flow for Pixel-wise Neural Video Representation	 | Daniel Silver,Ron Kimmel	 | CoordFlow是一种基于像素的隐式神经表示方法，用于视频压缩，能够获得当前基于像素的INR中的最佳性能，同时与领先的基于帧的方法性能相当，通过分离视觉信息到不同层次并利用物体运动轨迹来补偿视觉时间冗余，实现了无监督的视频序列分割和多种视频处理能力。	 | In the field of video compression, the pursuit for better quality at lower bit rates remains a long-lasting goal. Recent developments have demonstrated the potential of Implicit Neural Representation (INR) as a promising alternative to traditional transform-based methodologies. Video INRs can be roughly divided into frame-wise and pixel-wise methods according to the structure the network outputs. While the pixel-based methods are better for upsampling and parallelization, frame-wise methods demonstrated better performance. We introduce CoordFlow, a novel pixel-wise INR for video compression. It yields state-of-the-art results compared to other pixel-wise INRs and on-par performance compared to leading frame-wise techniques. The method is based on the separation of the visual information into visually consistent layers, each represented by a dedicated network that compensates for the layer's motion. When integrated, a byproduct is an unsupervised segmentation of video sequence. Objects motion trajectories are implicitly utilized to compensate for visual-temporal redundancies. Additionally, the proposed method provides inherent video upsampling, stabilization, inpainting, and denoising capabilities.	 | 在视频压缩领域，追求在更低比特率下获得更好的质量一直是一个长期的目标。近期的发展显示，隐式神经表示（INR）作为一种替代传统变换方法的有前途的选择。视频INR可以大致分为基于帧的方法和基于像素的方法，根据网络输出的结构进行分类。虽然基于像素的方法更适用于上采样和并行化，但基于帧的方法表现出更好的性能。我们引入了CoordFlow，这是一种新颖的基于像素的INR方法，适用于视频压缩。相比其他基于像素的INR，它能获得最新的性能成果；并且与领先的基于帧的方法相比，其性能相当。该方法基于将视觉信息分离成视觉一致的层次结构，每个层次由一个专用网络表示，用于补偿该层次的运动。当这些层次联合起来时，会产生一个无监督的视频序列分割。物体的运动轨迹被隐式利用来补偿视觉时间冗余。此外，所提出的方法还具有内在的视频上采样、稳定、修复和去噪能力。
2501.00967	 | On the Implementation of a Bayesian Optimization Framework for Interconnected Systems	 | Leonardo D. González,Victor M. Zavala	 | 本文介绍了一种名为BOIS的灰盒贝叶斯优化方法，通过自适应线性化技术获得复合函数的统计矩，以利用结构知识优化昂贵的采样系统，并通过化学过程优化案例研究证明了其有效性和计算效率。	 | Bayesian optimization (BO) is an effective paradigm for the optimization of expensive-to-sample systems. Standard BO learns the performance of a system $f(x)$ by using a Gaussian Process (GP) model; this treats the system as a black-box and limits its ability to exploit available structural knowledge (e.g., physics and sparse interconnections in a complex system). Grey-box modeling, wherein the performance function is treated as a composition of known and unknown intermediate functions $f(x, y(x))$ (where $y(x)$ is a GP model) offers a solution to this limitation; however, generating an analytical probability density for $f$ from the Gaussian density of $y(x)$ is often an intractable problem (e.g., when $f$ is nonlinear). Previous work has handled this issue by using sampling techniques or by solving an auxiliary problem over an augmented space where the values of $y(x)$ are constrained by confidence intervals derived from the GP models; such solutions are computationally intensive. In this work, we provide a detailed implementation of a recently proposed grey-box BO paradigm, BOIS, that uses adaptive linearizations of $f$ to obtain analytical expressions for the statistical moments of the composite function. We show that the BOIS approach enables the exploitation of structural knowledge, such as that arising in interconnected systems as well as systems that embed multiple GP models and combinations of physics and GP models. We benchmark the effectiveness of BOIS against standard BO and existing grey-box BO algorithms using a pair of case studies focused on chemical process optimization and design. Our results indicate that BOIS performs as well as or better than existing grey-box methods, while also being less computationally intensive.	 | 贝叶斯优化（BO）是一种有效的范式，用于优化昂贵的采样系统。标准的BO通过使用高斯过程（GP）模型来学习系统$f(x)$的性能；这将系统视为黑盒，限制了其利用可用结构知识（例如，复杂系统中的物理知识和稀疏连接）的能力。灰盒建模，其中性能函数被视为已知和未知中间函数$f(x, y(x))$的组合（其中$y(x)$是GP模型），提供了解决这个问题的方法；然而，从GP模型的$y(x)$的高斯密度生成$f$的分析概率密度通常是不可解的问题（例如，当$f$是非线性时）。先前的工作通过使用采样技术或在一个附加的空间解决一个辅助问题来处理这个问题，其中$y(x)$的值受到从GP模型推导出的置信区间约束；这样的解决方案计算量很大。在本文中，我们详细介绍了最近提出的灰盒BO范式BOIS的实现方法，它使用自适应线性化来获得复合函数的统计矩的分析表达式。我们展示了BOIS方法能够利用结构知识，如互联系统中以及嵌入多个GP模型和物理知识与GP模型组合的系统的结构知识。我们通过两个案例研究对化学过程的优化和设计的BOIS方法的有效性进行了基准测试，这些案例研究旨在与标准BO方法和现有的灰盒BO算法进行比较。我们的结果显示，BOIS的表现与现有的灰盒方法相当甚至更好，同时计算成本也更低。
2501.00962	 | OASIS Uncovers: High-Quality T2I Models, Same Old Stereotypes	 | Sepehr Dehdashtian,Gautam Sreekumar,Vishnu Naresh Boddeti	 | 该研究提出了OASIS指标来量化文本到图像（T2I）模型生成数据集中存在的刻板印象，并分析了这些刻板印象的根源，发现较新的T2I模型如FLUX.1和SDv3对概念具有强烈的刻板印象，且刻板印象在足迹较低的国家中更为严重。	 | Images generated by text-to-image (T2I) models often exhibit visual biases and stereotypes of concepts such as culture and profession. Existing quantitative measures of stereotypes are based on statistical parity that does not align with the sociological definition of stereotypes and, therefore, incorrectly categorizes biases as stereotypes. Instead of oversimplifying stereotypes as biases, we propose a quantitative measure of stereotypes that aligns with its sociological definition. We then propose OASIS to measure the stereotypes in a generated dataset and understand their origins within the T2I model. OASIS includes two scores to measure stereotypes from a generated image dataset: (M1) Stereotype Score to measure the distributional violation of stereotypical attributes, and (M2) WALS to measure spectral variance in the images along a stereotypical attribute. OASIS also includes two methods to understand the origins of stereotypes in T2I models: (U1) StOP to discover attributes that the T2I model internally associates with a given concept, and (U2) SPI to quantify the emergence of stereotypical attributes in the latent space of the T2I model during image generation. Despite the considerable progress in image fidelity, using OASIS, we conclude that newer T2I models such as FLUX.1 and SDv3 contain strong stereotypical predispositions about concepts and still generate images with widespread stereotypical attributes. Additionally, the quantity of stereotypes worsens for nationalities with lower Internet footprints.	 | 文本到图像（T2I）模型生成的图像往往包含文化和职业概念上的视觉偏见和刻板印象。现有的刻板印象量化指标基于统计平等，这与社会学意义上的刻板印象定义不符，因此错误地将偏见归类为刻板印象。我们不希望通过简化刻板印象为偏见来处理刻板印象，而是提出了一种与社会学定义相一致的刻板印象量化指标。然后，我们提出了OASIS来衡量生成数据集中的刻板印象，并理解这些刻板印象在T2I模型中的根源。OASIS包括两个指标来衡量从生成图像数据集中提取的刻板印象：(M1)刻板印象分数，用于衡量刻板印象属性的分布违背程度；(M2)WALS，用于衡量沿刻板印象属性的图像光谱变化。OASIS还包括两种方法来理解T2I模型中刻板印象的根源：(U1)StOP，用于发现T2I模型内部关联的特征与给定概念之间的关联；(U2)SPI，用于量化在T2I模型的潜在空间中生成图像期间刻板印象属性的出现。尽管在图像保真度方面取得了显著进步，但使用OASIS，我们得出结论，如FLUX.1和SDv3等较新的T2I模型对概念具有强烈的先入为主的刻板印象，并且仍然生成带有广泛刻板印象属性的图像。此外，刻板印象的数量对于网络足迹较低的国家来说会更加严重。
2501.00958	 | 2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining	 | Wenqi Zhang,Hang Zhang,Xin Li,Jiashuo Sun,Yongliang Shen,Weiming Lu,Deli Zhao,Yueting Zhuang,Lidong Bing	 | 本文介绍了一个高质量的多模态教科书语料库，该语料库收集了超过2.5年教学视频中的视觉、音频和文本信息，并在知识和推理密集的任务中表现出色；该语料库能够帮助视觉语言模型更好地理解世界，并在交错上下文中利用视觉和文本提示解决任务。	 | Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality \textbf{multimodal textbook} corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving~\footnote{Our code are available at \url{https://github.com/DAMO-NLP-SG/multimodal_textbook}}.	 | 与图像-文本对数据相比，交错语料库使视觉语言模型（VLMs）能够像人类一样更自然地理解世界。然而，现有的这些数据集是从网页上抓取的，面临着知识密度低、图像-文本关系松散以及图像之间逻辑连贯性差等挑战。另一方面，互联网上拥有大量的教学视频（例如在线几何课程），这些视频广泛用于人类学习基础学科，但这些宝贵资源在VLM训练中尚未得到充分探索。本文中，我们介绍了一个高质量的多模态教科书语料库，用于增强VLM预训练中的基础知识。该语料库收集了超过2.5年的教学视频，共计22,000课时。我们首先使用一个LLM提出的分类体系，系统地收集教学视频。随后我们逐步从视频中提取并精炼出视觉（关键帧）、音频（ASR）和文本知识（OCR），并基于时间顺序组织成图像-文本交错的语料库。与同类数据集相比，我们的以视频为中心的教科书提供了更连贯的背景、更丰富的知识和更好的图像-文本对齐。实验表明，它在知识和推理密集的任务（如ScienceQA和MathVista）中表现出色。此外，使用我们教科书预训练的VLM在交错上下文中表现出色，能够利用视觉和文本提示解决任务~\footnote{我们的代码可以在\url{https://github.com/DAMO-NLP-SG/multimodal_textbook}获取}。
2501.00930	 | Tight Constraint Prediction of Six-Degree-of-Freedom Transformer-based Powered Descent Guidance	 | Julia Briden,Trey Gurga,Breanna Johnson,Abhishek Cauligi,Richard Linares	 | 本文介绍了基于Transformer的成功凸化（T-SCvx），这是一种扩展的基于Transformer的功率下降引导（T-PDG）方法，用于高效生成六自由度（6-DoF）燃料最优功率下降轨迹，并通过学习紧约束集来促进算法收敛，已在火星着陆应用中得到验证。	 | This work introduces Transformer-based Successive Convexification (T-SCvx), an extension of Transformer-based Powered Descent Guidance (T-PDG), generalizable for efficient six-degree-of-freedom (DoF) fuel-optimal powered descent trajectory generation. Our approach significantly enhances the sample efficiency and solution quality for nonconvex-powered descent guidance by employing a rotation invariant transformation of the sampled dataset. T-PDG was previously applied to the 3-DoF minimum fuel powered descent guidance problem, improving solution times by up to an order of magnitude compared to lossless convexification (LCvx). By learning to predict the set of tight or active constraints at the optimal control problem's solution, Transformer-based Successive Convexification (T-SCvx) creates the minimal reduced-size problem initialized with only the tight constraints, then uses the solution of this reduced problem to warm-start the direct optimization solver. 6-DoF powered descent guidance is known to be challenging to solve quickly and reliably due to the nonlinear and non-convex nature of the problem, the discretization scheme heavily influencing solution validity, and reference trajectory initialization determining algorithm convergence or divergence. Our contributions in this work address these challenges by extending T-PDG to learn the set of tight constraints for the successive convexification (SCvx) formulation of the 6-DoF powered descent guidance problem. In addition to reducing the problem size, feasible and locally optimal reference trajectories are also learned to facilitate convergence from the initial guess. T-SCvx enables onboard computation of real-time guidance trajectories, demonstrated by a 6-DoF Mars powered landing application problem.	 | 本文介绍了基于Transformer的成功凸化（T-SCvx），这是基于Transformer的功率下降引导（T-PDG）的一种扩展，适用于高效的六自由度（6-DoF）燃料最优功率下降轨迹生成。我们的方法通过将采样数据集进行旋转不变性变换，极大地提升了非凸功率下降引导的采样效率和解的质量。T-PDG曾应用于3自由度（3-DoF）燃料最优功率下降引导问题，相比无损凸化（LCvx），显著缩短了解决时间，最多缩短了一个数量级。通过学习预测最优控制问题解处的紧约束集，基于Transformer的成功凸化（T-SCvx）构建了一个仅包含紧约束的最小化缩减问题作为初始化，然后利用该缩减问题的解来预热直接优化求解器。由于6-DoF功率下降引导问题具有非线性和非凸性，离散方案对解决方案有效性有重大影响，初始轨迹初始化决定算法的收敛与否或发散，因此快速可靠地解决这一问题具有挑战性。本文的研究成果通过将T-PDG扩展到学习6-DoF功率下降引导问题的成功凸化（SCvx）形式下的紧约束集，解决了这些问题。除了减少问题规模，T-SCvx还学习了可行的局部最优参考轨迹，以促进从初始猜测开始的算法收敛。T-SCvx能够实现机载实时引导轨迹的计算，通过一个用于火星着陆的6-DoF功率着陆应用问题得到了验证。
2501.00915	 | Diffusion Policies for Generative Modeling of Spacecraft Trajectories	 | Julia Briden,Breanna Johnson,Richard Linares,Abhishek Cauligi	 | 这项工作利用合成扩散模型在少样本框架中高效地适应分布外数据和问题变化，以生成六自由度动力下降轨迹，这种方法能够表示包含各种轨迹设计规范和约束的解的概率密度函数，并通过动态可行且计算高效的轨迹生成提高资源受限航天器的在线轨迹优化能力。	 | Machine learning has demonstrated remarkable promise for solving the trajectory generation problem and in paving the way for online use of trajectory optimization for resource-constrained spacecraft. However, a key shortcoming in current machine learning-based methods for trajectory generation is that they require large datasets and even small changes to the original trajectory design requirements necessitate retraining new models to learn the parameter-to-solution mapping. In this work, we leverage compositional diffusion modeling to efficiently adapt out-of-distribution data and problem variations in a few-shot framework for 6 degree-of-freedom (DoF) powered descent trajectory generation. Unlike traditional deep learning methods that can only learn the underlying structure of one specific trajectory optimization problem, diffusion models are a powerful generative modeling framework that represents the solution as a probability density function (PDF) and this allows for the composition of PDFs encompassing a variety of trajectory design specifications and constraints. We demonstrate the capability of compositional diffusion models for inference-time 6 DoF minimum-fuel landing site selection and composable constraint representations. Using these samples as initial guesses for 6 DoF powered descent guidance enables dynamically feasible and computationally efficient trajectory generation.	 | 机器学习在解决轨迹生成问题以及为资源受限航天器开辟在线轨迹优化途径方面展现出了巨大潜力。然而，当前基于机器学习的轨迹生成方法的一个主要缺点是它们需要大量的数据集，而即使是原始轨迹设计要求的小变化也要求重新训练新的模型来学习参数到解的映射。在本工作中，我们利用合成扩散建模高效地在少样本框架中适应分布外数据和问题变化，用于6自由度（DoF）动力下降轨迹生成。与传统的仅能学习一个特定轨迹优化问题基础结构的深度学习方法不同，扩散模型是一种强大的生成性建模框架，能够将解表示为概率密度函数（PDF），这使得能够组合包含各种轨迹设计规范和约束的PDF。我们展示了合成扩散模型在推理时进行6 DoF最小燃料着陆点选择和可组合约束表示的能力。利用这些样本作为6 DoF动力下降制导的初始猜测，可以实现动态可行且计算高效的轨迹生成。
2501.00890	 | Spatial Temporal Attention based Target Vehicle Trajectory Prediction for Internet of Vehicles	 | Ouhan Huang,Huanle Rao,Xiaowen Cai,Tianyun Wang,Aolong Sun,Sizhe Xing,Yifan Sun,Gangyong Jia	 | 本文提出了一种基于时空注意力的目标车辆轨迹预测（STATVTPred）方法，通过结合图注意力网络和Transformer技术来提高车辆轨迹预测的准确性。实验结果表明，STATVTPred在多个数据集上的预测效果显著优于现有方法，尤其在平均匹配率方面取得了显著提升。	 | Forecasting vehicle behavior within complex traffic environments is pivotal within Intelligent Transportation Systems (ITS). Though this technology plays a significant role in alleviating the prevalent operational difficulties in logistics and transportation systems, the precise prediction of vehicle trajectories still poses a substantial challenge. To address this, our study introduces the Spatio Temporal Attention-based methodology for Target Vehicle Trajectory Prediction (STATVTPred). This approach integrates Global Positioning System(GPS) localization technology to track target movement and dynamically predict the vehicle's future path using comprehensive spatio-temporal trajectory data. We map the vehicle trajectory onto a directed graph, after which spatial attributes are extracted via a Graph Attention Networks(GATs). The Transformer technology is employed to yield temporal features from the sequence. These elements are then amalgamated with local road network structure maps to filter and deliver a smooth trajectory sequence, resulting in precise vehicle trajectory prediction.This study validates our proposed STATVTPred method on T-Drive and Chengdu taxi-trajectory datasets. The experimental results demonstrate that STATVTPred achieves 6.38% and 10.55% higher Average Match Rate (AMR) than the Transformer model on the Beijing and Chengdu datasets, respectively. Compared to the LSTM Encoder-Decoder model, STATVTPred boosts AMR by 37.45% and 36.06% on the same datasets. This is expected to establish STATVTPred as a new approach for handling trajectory prediction of targets in logistics and transportation scenarios, thereby enhancing prediction accuracy.	 | 在复杂的交通环境中预测车辆行为是智能交通系统（ITS）中的关键问题。尽管这项技术在缓解物流和交通系统的运营难题方面发挥着重要作用，准确预测车辆轨迹仍然是一项巨大的挑战。为了解决这个问题，我们的研究引入了一种基于时空注意力的方法，称为目标车辆轨迹预测（STATVTPred）。该方法结合了全球定位系统（GPS）定位技术来跟踪目标移动，并使用综合的时空轨迹数据动态预测车辆的未来路径。我们将车辆轨迹映射到有向图上，然后通过图注意力网络（GATs）提取空间属性。利用Transformer技术从序列中提取时间特征。随后，将这些元素与局部道路网络结构图结合，过滤并提供平滑的轨迹序列，从而实现精确的车辆轨迹预测。我们在T-Drive和成都出租车轨迹数据集上验证了我们提出的STATVTPred方法。实验结果表明，与北京和成都数据集上的Transformer模型相比，STATVTPred在平均匹配率（AMR）上分别提高了6.38%和10.55%。与LSTM编码器-解码器模型相比，STATVTPred在北京和成都数据集上的AMR分别提高了37.45%和36.06%。这有望将STATVTPred确立为物流和交通场景中处理目标轨迹预测的新方法，从而提高预测准确性。
2501.00885	 | Representation in large language models	 | Cameron C. Yetman	 | 这篇论文旨在解决大规模语言模型（LLMs）行为是由基于表示的信息处理还是由记忆和随机表查找过程驱动的问题，认为LLM的行为部分受基于表示的信息处理驱动，并提出了一系列技术来调查和解释这些表示，为未来的研究奠定基础。	 | The extraordinary success of recent Large Language Models (LLMs) on a diverse array of tasks has led to an explosion of scientific and philosophical theorizing aimed at explaining how they do what they do. Unfortunately, disagreement over fundamental theoretical issues has led to stalemate, with entrenched camps of LLM optimists and pessimists often committed to very different views of how these systems work. Overcoming stalemate requires agreement on fundamental questions, and the goal of this paper is to address one such question, namely: is LLM behavior driven partly by representation-based information processing of the sort implicated in biological cognition, or is it driven entirely by processes of memorization and stochastic table look-up? This is a question about what kind of algorithm LLMs implement, and the answer carries serious implications for higher level questions about whether these systems have beliefs, intentions, concepts, knowledge, and understanding. I argue that LLM behavior is partially driven by representation-based information processing, and then I describe and defend a series of practical techniques for investigating these representations and developing explanations on their basis. The resulting account provides a groundwork for future theorizing about language models and their successors.	 | 最近大规模语言模型（LLMs）在多种任务上的非凡成功导致了科学和哲学理论的大爆发，旨在解释它们是如何完成任务的。不幸的是，对基本理论问题的分歧导致了僵局，LLM 乐观派和悲观派阵营各持己见，坚持非常不同的系统工作方式观点。克服僵局需要在基本问题上达成共识，本文的目标是解决这样一个问题：即LLM的行为是部分由与生物认知相关的基于表示的信息处理驱动，还是完全由记忆和随机表查找过程驱动？这是一个关于LLMs实现何种算法的问题，答案对更高层次的问题有着严重的影响，例如这些系统是否有信念、意图、概念、知识和理解。我认为LLM的行为部分由基于表示的信息处理驱动，然后我描述并辩护了一系列实用技术，用于调查这些表示并基于它们发展解释。由此得出的解释为未来对语言模型及其继任者的理论研究打下了基础。
2501.00876	 | A Novel Approach using CapsNet and Deep Belief Network for Detection and Identification of Oral Leukopenia	 | Hirthik Mathesh GV,Kavin Chakravarthy M,Sentil Pandi S	 | 本研究通过收集全球临床专家标注的口腔病变图片，开发了一个自动化检测和分类系统，利用深度信念网络与CAPSNET结合的方法，实现了高准确率的病变检测和分类，有助于早期发现口腔癌。	 | Oral cancer constitutes a significant global health concern, resulting in 277,484 fatalities in 2023, with the highest prevalence observed in low- and middle-income nations. Facilitating automation in the detection of possibly malignant and malignant lesions in the oral cavity could result in cost-effective and early disease diagnosis. Establishing an extensive repository of meticulously annotated oral lesions is essential. In this research photos are being collected from global clinical experts, who have been equipped with an annotation tool to generate comprehensive labelling. This research presents a novel approach for integrating bounding box annotations from various doctors. Additionally, Deep Belief Network combined with CAPSNET is employed to develop automated systems that extracted intricate patterns to address this challenging problem. This study evaluated two deep learning-based computer vision methodologies for the automated detection and classification of oral lesions to facilitate the early detection of oral cancer: image classification utilizing CAPSNET. Image classification attained an F1 score of 94.23% for detecting photos with lesions 93.46% for identifying images necessitating referral. Object detection attained an F1 score of 89.34% for identifying lesions for referral. Subsequent performances are documented about classification based on the sort of referral decision. Our preliminary findings indicate that deep learning possesses the capability to address this complex problem.	 | 口腔癌是全球公共卫生的一大重要关切，2023年导致了277,484例死亡，且在低收入和中等收入国家的发病率最高。在口腔腔内检测可能恶性及恶性病变的自动化流程有助于实现成本效益高且早期的疾病诊断。建立一个包含详细注释的口腔病变广泛数据库至关重要。在此研究中，来自全球临床专家的图片正被收集，并配备了一个注释工具以生成全面的标签。该研究提出了一个新颖的方法，用于整合来自不同医生的边界框注释。此外，采用深度信念网络与CAPSNET结合的方法来开发自动系统，以提取复杂模式并解决这一具有挑战性的问题。此研究评估了两种基于深度学习的计算机视觉方法，以实现口腔病变的自动化检测和分类，从而促进口腔癌的早期发现：使用CAPSNET进行图像分类。图像分类的F1得分为94.23%，用于检测带有病变的图片，识别需要转诊的图片的准确率为93.46%。物体检测的F1得分为89.34%，用于识别需要转诊的病变。后续性能分析基于不同的转诊决定进行分类。初步结果显示，深度学习具有解决这一复杂问题的能力。
2501.00873	 | Exploring Structured Semantic Priors Underlying Diffusion Score for Test-time Adaptation	 | Mingjia Li,Shuang Li,Tongrui Su,Longhui Yuan,Jian Liang,Wei Li	 | 该研究揭示了评分生成模型内部的语义结构，并提出了一种名为DUSA的方法，利用扩散评分中的结构化语义先验来增强图像分类器或密集预测器的测试时适应性，该方法仅需从去噪扩散的一个时间步骤中提取知识，从而有效提高了模型在多种测试场景下的适应性。	 | Capitalizing on the complementary advantages of generative and discriminative models has always been a compelling vision in machine learning, backed by a growing body of research. This work discloses the hidden semantic structure within score-based generative models, unveiling their potential as effective discriminative priors. Inspired by our theoretical findings, we propose DUSA to exploit the structured semantic priors underlying diffusion score to facilitate the test-time adaptation of image classifiers or dense predictors. Notably, DUSA extracts knowledge from a single timestep of denoising diffusion, lifting the curse of Monte Carlo-based likelihood estimation over timesteps. We demonstrate the efficacy of our DUSA in adapting a wide variety of competitive pre-trained discriminative models on diverse test-time scenarios. Additionally, a thorough ablation study is conducted to dissect the pivotal elements in DUSA. Code is publicly available at https://github.com/BIT-DA/DUSA.	 | 充分利用生成模型和判别模型互补优势一直是机器学习中的一个迷人愿景，得到了越来越多研究的支持。本研究揭示了评分生成模型内部隐含的语义结构，展示了它们作为有效的判别先验的潜力。受我们理论发现的启发，我们提出了DUSA，以利用扩散评分背后的结构化语义先验来促进图像分类器或密集预测器的测试时适应。值得注意的是，DUSA 仅从去噪扩散的一个时间步骤中提取知识，从而克服了基于蒙特卡洛法的逐时间步似然估计的难题。我们展示了DUSA在多种测试场景下对各种预训练判别模型进行适应的有效性。此外，我们还进行了详尽的消融研究，以剖析DUSA中的关键元素。代码已公开发布在https://github.com/BIT-DA/DUSA。
2501.00865	 | Negative to Positive Co-learning with Aggressive Modality Dropout	 | Nicholas Magal,Minh Tran,Riku Arakawa,Suzanne Nie	 | 本文通过激进的模态丢弃方法将负协同学习转变为正协同学习，显著提高了多模态协同学习的有效性，在某些实验中准确率提升了高达20%。该技术不仅在负协同学习中表现出色，还能在正协同学习中改善模型性能，尽管效果相对较弱。	 | This paper aims to document an effective way to improve multimodal co-learning by using aggressive modality dropout. We find that by using aggressive modality dropout we are able to reverse negative co-learning (NCL) to positive co-learning (PCL). Aggressive modality dropout can be used to "prep" a multimodal model for unimodal deployment, and dramatically increases model performance during negative co-learning, where during some experiments we saw a 20% gain in accuracy. We also benchmark our modality dropout technique against PCL to show that our modality drop out technique improves co-learning during PCL, although it does not have as much as an substantial effect as it does during NCL. Github: https://github.com/nmagal/modality_drop_for_colearning	 | 本文旨在通过使用激进的模态丢弃方法记录一种提高多模态协同学习有效性的途径。我们发现，通过使用激进的模态丢弃，我们可以将负协同学习（NCL）逆转为正协同学习（PCL）。激进的模态丢弃可以用于“预处理”多模态模型以进行单模态部署，并在负协同学习期间显着提高模型性能，在某些实验中我们看到了高达20%的准确率提升。我们还使用PCL作为基准，展示了我们的模态丢弃技术在PCL期间改善了协同学习，尽管其效果不如在NCL期间显著。GitHub：https://github.com/nmagal/modality_drop_for_colearning
2501.00862	 | DiffETM: Diffusion Process Enhanced Embedded Topic Model	 | Wei Shao,Mingyang Liu,Linqi Song	 | 该研究提出了一种新的嵌入主题模型（ETM），在采样文档主题分布时引入扩散过程，以克服传统ETM假设逻辑正态分布的限制，实验表明该方法能有效提升主题建模性能。	 | The embedded topic model (ETM) is a widely used approach that assumes the sampled document-topic distribution conforms to the logistic normal distribution for easier optimization. However, this assumption oversimplifies the real document-topic distribution, limiting the model's performance. In response, we propose a novel method that introduces the diffusion process into the sampling process of document-topic distribution to overcome this limitation and maintain an easy optimization process. We validate our method through extensive experiments on two mainstream datasets, proving its effectiveness in improving topic modeling performance.	 | 嵌入主题模型（ETM）是一种广泛使用的方法，它假设采样文档主题分布遵循逻辑正态分布，以便于优化。然而，这种假设简化了实际的文档主题分布，限制了模型的性能。为应对这一问题，我们提出了一种新方法，在文档主题分布的采样过程中引入扩散过程，以克服这一限制并保持优化过程的简便性。我们通过在两个主流数据集上进行广泛的实验来验证该方法的有效性，证明了该方法在提高主题建模性能方面的有效性。
2501.00854	 | A Graphical Approach to State Variable Selection in Off-policy Learning	 | Joakim Blach Andersen,Qingyuan Zhao	 | 该研究利用因果推断中的有向无环混合图（ADMGs）理论，提出了一套适用于一般决策过程的图形识别准则，该准则既适用于动态治疗制度（DTRs），也适用于马尔可夫决策过程（MDPs），从而缩小了离策学习在DTR和RL领域之间的差距。通过集装箱物流中动态定价问题的现实模拟，研究展示了违反这些图形准则可能导致次优策略。	 | Sequential decision problems are widely studied across many areas of science. A key challenge when learning policies from historical data - a practice commonly referred to as off-policy learning - is how to ``identify'' the impact of a policy of interest when the observed data are not randomized. Off-policy learning has mainly been studied in two settings: dynamic treatment regimes (DTRs), where the focus is on controlling confounding in medical problems with short decision horizons, and offline reinforcement learning (RL), where the focus is on dimension reduction in closed systems such as games. The gap between these two well studied settings has limited the wider application of off-policy learning to many real-world problems. Using the theory for causal inference based on acyclic directed mixed graph (ADMGs), we provide a set of graphical identification criteria in general decision processes that encompass both DTRs and MDPs. We discuss how our results relate to the often implicit causal assumptions made in the DTR and RL literatures and further clarify several common misconceptions. Finally, we present a realistic simulation study for the dynamic pricing problem encountered in container logistics, and demonstrate how violations of our graphical criteria can lead to suboptimal policies.	 | Sequential决策问题在科学的许多领域中都有广泛的研究。在从历史数据中学习策略时——这一实践通常被称为离策学习——面临的一个关键挑战是如何在观察到的数据不是随机化的情况下“识别”目标策略的影响。离策学习主要在两个场景中得到了研究：动态治疗制度（DTRs），其中重点是在具有短期决策时间表的医疗问题中控制混淆变量；以及离策强化学习（offline RL），其中重点是在封闭系统（如游戏）中降低维度。这两个广泛研究的场景之间的差距限制了离策学习在许多实际问题中的更广泛应用。利用基于有向无环混合图（ADMGs）的因果推断理论，我们提供了一套适用于一般决策过程的图形识别准则，该准则既涵盖了DTRs，也涵盖了MDPs（马尔可夫决策过程）。我们讨论了我们的结果与DTR和RL文献中通常隐含的因果假设之间的关系，并进一步澄清了几种常见的误解。最后，我们进行了一项基于集装箱物流中动态定价问题的现实模拟研究，并展示了违反我们图形准则可能导致次优策略。
2501.00838	 | Spatially-guided Temporal Aggregation for Robust Event-RGB Optical Flow Estimation	 | Qianang Zhou,Junhui Hou,Meiyi Yang,Yongjian Deng,Youfu Li,Junlin Xiong	 | 本文提出了一种新的跨模态光流估计方法，通过事件摄像头捕捉的高时间分辨率运动信息和帧摄像头提供的丰富纹理信息的互补优势，实现了更有效的融合。实验结果表明，该方法在准确性和推理时间上均优于现有方法，特别是在DSEC-Flow数据集上达到了领先性能。	 | Current optical flow methods exploit the stable appearance of frame (or RGB) data to establish robust correspondences across time. Event cameras, on the other hand, provide high-temporal-resolution motion cues and excel in challenging scenarios. These complementary characteristics underscore the potential of integrating frame and event data for optical flow estimation. However, most cross-modal approaches fail to fully utilize the complementary advantages, relying instead on simply stacking information. This study introduces a novel approach that uses a spatially dense modality to guide the aggregation of the temporally dense event modality, achieving effective cross-modal fusion. Specifically, we propose an event-enhanced frame representation that preserves the rich texture of frames and the basic structure of events. We use the enhanced representation as the guiding modality and employ events to capture temporally dense motion information. The robust motion features derived from the guiding modality direct the aggregation of motion information from events. To further enhance fusion, we propose a transformer-based module that complements sparse event motion features with spatially rich frame information and enhances global information propagation. Additionally, a mix-fusion encoder is designed to extract comprehensive spatiotemporal contextual features from both modalities. Extensive experiments on the MVSEC and DSEC-Flow datasets demonstrate the effectiveness of our framework. Leveraging the complementary strengths of frames and events, our method achieves leading performance on the DSEC-Flow dataset. Compared to the event-only model, frame guidance improves accuracy by 10\%. Furthermore, it outperforms the state-of-the-art fusion-based method with a 4\% accuracy gain and a 45\% reduction in inference time.	 | 当前的光流方法利用帧（或RGB）数据的稳定外观在时间上建立稳健的对应关系。相比之下，事件摄像头提供了高时间分辨率的运动线索，并在具有挑战性的场景中表现出色。这些互补的特性强调了将帧和事件数据结合以进行光流估计的潜力。然而，大多数跨模态方法未能充分利用这些互补优势，而是简单地堆叠信息。本研究提出了一种新方法，通过密集的空间模态指导密集的时间模态的聚合，实现有效的跨模态融合。具体而言，我们提出了一种事件增强的帧表示，该表示保留了帧的丰富纹理和事件的基本结构。我们使用增强的表示作为指导模态，并利用事件捕捉密集的运动信息。来自指导模态的稳健运动特征引导事件运动信息的聚合。为了进一步增强融合，我们提出了一种基于transformer的模块，该模块用丰富的空间帧信息补充稀疏的事件运动特征，并增强全局信息传播。此外，我们设计了一种混合融合编码器，从两种模态中提取全面的空间-时间上下文特征。在MVSEC和DSEC-Flow数据集上的广泛实验表明，我们的框架有效。利用帧和事件的互补优势，我们的方法在DSEC-Flow数据集上取得了领先性能。与仅使用事件的模型相比，帧指导提高了10%的准确性。此外，与最先进的融合方法相比，它在准确性和推理时间上分别提高了4%和减少了45%。
2501.00811	 | Regression Guided Strategy to Automated Facial Beauty Optimization through Image Synthesis	 | Erik Nguyen,Spencer Htin	 | 该研究提出了一种基于生成对抗网络的潜在空间优化方法，用于自动实现面部美容编辑，通过一个新开发的回归网络指导优化过程，从而捕捉整体的美感模式，提供了一种无需依赖预定义规则的动态面部美化编辑方法。	 | The use of beauty filters on social media, which enhance the appearance of individuals in images, is a well-researched area, with existing methods proving to be highly effective. Traditionally, such enhancements are performed using rule-based approaches that leverage domain knowledge of facial features associated with attractiveness, applying very specific transformations to maximize these attributes. In this work, we present an alternative approach that projects facial images as points on the latent space of a pre-trained GAN, which are then optimized to produce beautiful faces. The movement of the latent points is guided by a newly developed facial beauty evaluation regression network, which learns to distinguish attractive facial features, outperforming many existing facial beauty evaluation models in this domain. By using this data-driven approach, our method can automatically capture holistic patterns in beauty directly from data rather than relying on predefined rules, enabling more dynamic and potentially broader applications of facial beauty editing. This work demonstrates a potential new direction for automated aesthetic enhancement, offering a complementary alternative to existing methods.	 | 社交媒体上使用的美容滤镜能够增强图像中个人的外观，这一领域已经得到了广泛的研究，现有方法证明非常有效。传统上，这些增强是通过基于规则的方法实现的，这些方法利用与吸引力相关的面部特征领域的知识，应用特定的变换来最大化这些特征。在本研究中，我们提出了一种替代方法，即将面部图像投影到预训练的生成对抗网络（GAN）的潜在空间中，并对其进行优化以生成美丽面孔。这种潜在点的移动由一个新开发的面部美丽评估回归网络指导，该网络能够学习区分吸引人的面部特征，在这一领域中，它在面部美丽评估模型中表现出色。通过使用数据驱动的方法，我们的方法可以从数据中自动捕捉整体的美感模式，而无需依赖预定义的规则，从而使得面部美丽编辑具有更动态和更广泛的应用前景。这项工作展示了自动化美学增强的潜在新方向，为现有方法提供了补充替代方案。
2501.00777	 | FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation	 | Qianli Wang,Nils Feldhus,Simon Ostermann,Luis Felipe Villa-Arenas,Sebastian Möller,Vera Schmitt	 | 该研究提出了ZeroCF和FitCF框架，用于在零样本设置下生成高质量的反事实例子，并通过标签翻转验证进一步优化这些例子，从而提高模型的可解释性。FitCF框架中的关键组件及其归因方法的有效性被详细评估，展示了其在提高反事实质量方面的显著效果。	 | Counterfactual examples are widely used in natural language processing (NLP) as valuable data to improve models, and in explainable artificial intelligence (XAI) to understand model behavior. The automated generation of counterfactual examples remains a challenging task even for large language models (LLMs), despite their impressive performance on many tasks. In this paper, we first introduce ZeroCF, a faithful approach for leveraging important words derived from feature attribution methods to generate counterfactual examples in a zero-shot setting. Second, we present a new framework, FitCF, which further verifies aforementioned counterfactuals by label flip verification and then inserts them as demonstrations for few-shot prompting, outperforming two state-of-the-art baselines. Through ablation studies, we identify the importance of each of FitCF's core components in improving the quality of counterfactuals, as assessed through flip rate, perplexity, and similarity measures. Furthermore, we show the effectiveness of LIME and Integrated Gradients as backbone attribution methods for FitCF and find that the number of demonstrations has the largest effect on performance. Finally, we reveal a strong correlation between the faithfulness of feature attribution scores and the quality of generated counterfactuals.	 | 以下是该摘要的中文翻译：  反事实例子在自然语言处理（NLP）中被广泛用作改进模型的宝贵数据，并在可解释的人工智能（XAI）中用于理解模型行为。尽管大型语言模型（LLM）在许多任务上表现出色，但自动化生成反事实例子仍是一个具有挑战性的任务。在本文中，我们首先介绍了ZeroCF，这是一种忠实的方法，用于利用特征归因方法提取的重要词汇，在零样本设置下生成反事实例子。其次，我们提出了一种新的框架FitCF，在此框架中，通过标签翻转验证进一步验证上述反事实例子，然后将它们作为少样本提示的演示，优于两个最先进的基线模型。通过消融研究，我们确定了FitCF核心组件中的每个组件对提高反事实质量的重要性，这通过反转率、困惑度和相似性指标进行评估。此外，我们展示了LIME和集成梯度作为FitCF的基础归因方法的有效性，并发现演示的数量对性能影响最大。最后，我们揭示了特征归因分数的真实性和生成的反事实质量之间存在强烈的相关性。
2501.00765	 | Beyond Words: AuralLLM and SignMST-C for Precise Sign Language Production and Bidirectional Accessibility	 | Yulong Li,Yuxuan Zhang,Feilong Tang,Mian Zhou,Zhixiang Lu,Haochen Xue,Yifang Wang,Kang Dang,Jionglong Su	 | 该研究开发了CNText2Sign和CNSign两个数据集，以及AuraLLM和SignMST-C两个模型，分别用于手语生成和翻译，以解决现有模型在准确性与姿势控制上的不足，并且提高了手语系统的实用性和适用性。	 | Although sign language recognition aids non-hearing-impaired understanding, many hearing-impaired individuals still rely on sign language alone due to limited literacy, underscoring the need for advanced sign language production and translation (SLP and SLT) systems. In the field of sign language production, the lack of adequate models and datasets restricts practical applications. Existing models face challenges in production accuracy and pose control, making it difficult to provide fluent sign language expressions across diverse scenarios. Additionally, data resources are scarce, particularly high-quality datasets with complete sign vocabulary and pose annotations. To address these issues, we introduce CNText2Sign and CNSign, comprehensive datasets to benchmark SLP and SLT, respectively, with CNText2Sign covering gloss and landmark mappings for SLP, and CNSign providing extensive video-to-text data for SLT. To improve the accuracy and applicability of sign language systems, we propose the AuraLLM and SignMST-C models. AuraLLM, incorporating LoRA and RAG techniques, achieves a BLEU-4 score of 50.41 on the CNText2Sign dataset, enabling precise control over gesture semantics and motion. SignMST-C employs self-supervised rapid motion video pretraining, achieving a BLEU-4 score of 31.03/32.08 on the PHOENIX2014-T benchmark, setting a new state-of-the-art. These models establish robust baselines for the datasets released for their respective tasks.	 | 尽管手语识别有助于非听力障碍人士的理解，但许多听力障碍人士仍主要依赖手语交流，这突显了需要高级的手语生成和翻译（SLP和SLT）系统的重要性。在手语生成领域，缺乏足够的模型和数据集限制了其实用应用。现有的模型在生成准确性和姿势控制方面面临挑战，使得在各种场景中提供流畅的手语表达变得困难。此外，数据资源稀缺，特别是包含完整手语词汇和姿势注释的高质量数据集。为了解决这些问题，我们推出了CNText2Sign和CNSign两个综合数据集，分别用于SLP和SLT的基准测试。CNText2Sign涵盖了SLP的词汇和关键点映射，而CNSign提供了丰富的视频到文本数据，用于SLT。  为了提高手语系统的准确性和适用性，我们提出了AuraLLM和SignMST-C两个模型。AuraLLM结合了LoRA和RAG技术，在CNText2Sign数据集上达到了BLEU-4得分为50.41，实现了对手势语义和动作的精确控制。SignMST-C采用了自监督的快速运动视频预训练方法，在PHOENIX2014-T基准上获得了31.03/32.08的BLEU-4得分，刷新了最新的技术水平。这些模型为各自任务发布的数据集建立了坚实的基础。
2501.00757	 | Beyond Static Datasets: A Behavior-Driven Entity-Specific Simulation to Overcome Data Scarcity and Train Effective Crypto Anti-Money Laundering Models	 | Dinesh Srivasthav P,Manoj Apte	 | 加密货币因多种原因被滥用，尤其是用于洗钱等非法活动，但由于缺乏足够的数据集和定制化，现有检测方法难以有效识别洗钱交易。为此，本文提出了一种实体特定的洗钱模拟器，以生成更贴近实际的交易数据，从而提高洗钱检测模型的性能。	 | For different factors/reasons, ranging from inherent characteristics and features providing decentralization, enhanced privacy, ease of transactions, etc., to implied external hardships in enforcing regulations, contradictions in data sharing policies, etc., cryptocurrencies have been severely abused for carrying out numerous malicious and illicit activities including money laundering, darknet transactions, scams, terrorism financing, arm trades. However, money laundering is a key crime to be mitigated to also suspend the movement of funds from other illicit activities. Billions of dollars are annually being laundered. It is getting extremely difficult to identify money laundering in crypto transactions owing to many layering strategies available today, and rapidly evolving tactics, and patterns the launderers use to obfuscate the illicit funds. Many detection methods have been proposed ranging from naive approaches involving complete manual investigation to machine learning models. However, there are very limited datasets available for effectively training machine learning models. Also, the existing datasets are static and class-imbalanced, posing challenges for scalability and suitability to specific scenarios, due to lack of customization to varying requirements. This has been a persistent challenge in literature. In this paper, we propose behavior embedded entity-specific money laundering-like transaction simulation that helps in generating various transaction types and models the transactions embedding the behavior of several entities observed in this space. The paper discusses the design and architecture of the simulator, a custom dataset we generated using the simulator, and the performance of models trained on this synthetic data in detecting real addresses involved in money laundering.	 | 由于不同的因素/原因，从提供去中心化、增强隐私、简化交易等特点，到外部执行监管时遇到的困难、数据共享政策中的矛盾等，加密货币被严重滥用，用于进行各种恶意和非法活动，包括洗钱、暗网交易、诈骗、恐怖融资和军火交易。然而，洗钱是需要优先减少的关键犯罪，以停止其他非法活动的资金流动。每年有数十亿美元的资金被洗钱。由于当今可用的许多层析策略，以及洗钱者不断演变的策略和模式，使得识别加密货币交易中的洗钱变得极其困难。许多检测方法被提出，从完全手动调查的初级方法到机器学习模型。然而，有效的训练机器学习模型的可用数据集非常有限。现有的数据集是静态且类别不平衡的，缺乏定制化以满足不同需求，这给可扩展性和特定场景的适用性带来了挑战。这一直是文献中的一个持久性挑战。在这篇论文中，我们提出了一种嵌入行为的实体特定洗钱似交易模拟，这有助于生成各种交易类型，并模拟嵌入了在该领域观察到的多个实体行为的交易。本文讨论了模拟器的设计和架构，我们使用模拟器生成的自定义数据集，以及在这些合成数据上训练的模型检测实际涉及洗钱的地址的性能。
2501.00755	 | An AI-powered Bayesian generative modeling approach for causal inference in observational studies	 | Qiao Liu,Wing Hung Wong	 | CausalBGM是一种基于AI的贝叶斯生成建模方法，能够有效捕捉高维协变量环境下治疗与结果之间的因果关系，并通过学习低维潜在特征的个体分布来估计个体治疗效应，克服了传统方法在处理混杂效应方面的局限性。CausalBGM在广泛的实验证明中表现优异，优于现有方法，并提供了可靠的不确定性量化。	 | Causal inference in observational studies with high-dimensional covariates presents significant challenges. We introduce CausalBGM, an AI-powered Bayesian generative modeling approach that captures the causal relationship among covariates, treatment, and outcome variables. The core innovation of CausalBGM lies in its ability to estimate the individual treatment effect (ITE) by learning individual-specific distributions of a low-dimensional latent feature set (e.g., latent confounders) that drives changes in both treatment and outcome. This approach not only effectively mitigates confounding effects but also provides comprehensive uncertainty quantification, offering reliable and interpretable causal effect estimates at the individual level. CausalBGM adopts a Bayesian model and uses a novel iterative algorithm to update the model parameters and the posterior distribution of latent features until convergence. This framework leverages the power of AI to capture complex dependencies among variables while adhering to the Bayesian principles. Extensive experiments demonstrate that CausalBGM consistently outperforms state-of-the-art methods, particularly in scenarios with high-dimensional covariates and large-scale datasets. Its Bayesian foundation ensures statistical rigor, providing robust and well-calibrated posterior intervals. By addressing key limitations of existing methods, CausalBGM emerges as a robust and promising framework for advancing causal inference in modern applications in fields such as genomics, healthcare, and social sciences. CausalBGM is maintained at the website https://causalbgm.readthedocs.io/.	 | 在高维协变量观测研究中进行因果推断存在显著挑战。我们提出了CausalBGM，一种基于AI的贝叶斯生成建模方法，能够捕捉协变量、治疗和结果变量之间的因果关系。CausalBGM的核心创新在于其能够通过学习驱动治疗和结果变化的低维潜在特征集合（例如潜在混杂因素）的个体特定分布来估计个体治疗效应（ITE）。这种方法不仅有效缓解了混杂效应，还提供了全面的不确定性量化，提供了可靠且可解释的个体层面因果效应估计。CausalBGM采用贝叶斯模型，并使用一个新颖的迭代算法更新模型参数和潜在特征的后验分布，直到收敛。该框架利用了AI的强大功能来捕捉变量间的复杂依赖关系，同时遵守贝叶斯原理。广泛的实验证明，CausalBGM在高维协变量和大规模数据集场景中始终优于最先进的方法。其贝叶斯基础确保了统计严谨性，提供了稳健且校准良好的后验间隔。通过解决现有方法的关键局限性，CausalBGM成为在基因组学、医疗保健和社会科学等现代应用中推进因果推断的强大且稳健框架。CausalBGM可在网站https://causalbgm.readthedocs.io/上维护。
2501.00744	 | A Distributional Evaluation of Generative Image Models	 | Edric Tam,Barbara E Engelhardt	 | 该研究关注图像生成模型的评估方法，指出现有度量标准如弗雷切特入神距离（FID）存在缺陷，提出了一种新的度量标准嵌入特征得分（ECS），能够更全面地评估生成样本与目标分布之间的分布匹配。	 | Generative models are ubiquitous in modern artificial intelligence (AI) applications. Recent advances have led to a variety of generative modeling approaches that are capable of synthesizing highly realistic samples. Despite these developments, evaluating the distributional match between the synthetic samples and the target distribution in a statistically principled way remains a core challenge. We focus on evaluating image generative models, where studies often treat human evaluation as the gold standard. Commonly adopted metrics, such as the Fréchet Inception Distance (FID), do not sufficiently capture the differences between the learned and target distributions, because the assumption of normality ignores differences in the tails. We propose the Embedded Characteristic Score (ECS), a comprehensive metric for evaluating the distributional match between the learned and target sample distributions, and explore its connection with moments and tail behavior. We derive natural properties of ECS and show its practical use via simulations and an empirical study.	 | 生成模型在现代人工智能（AI）应用中无处不在。近年来的进步催生了多种生成建模方法，能够合成高度逼真的样本。尽管取得了这些进展，用统计学上可靠的方式评估合成样本与目标分布之间的分布匹配仍然是一项核心挑战。我们专注于评估图像生成模型，其中许多研究将人类评价视为黄金标准。常见的度量标准，如弗雷切特 inception 距离（FID），由于正态性假设忽略了尾部差异，因此无法充分捕捉学习分布与目标分布之间的差异。我们提出了嵌入特征得分（ECS），这是一种全面的度量标准，用于评估学习样本分布与目标样本分布之间的分布匹配，并探讨了它与矩和尾行为之间的联系。我们推导了ECS的自然属性，并通过模拟和实证研究展示了其实际应用。
2501.00738	 | Learning Weather Models from Data with WSINDy	 | Seth Minor,Daniel A. Messenger,Vanja Dukic,David M. Bortz	 | 本文通过使用弱形式稀疏识别非线性动力学（WSINDy）算法，从模拟和同化数据中学习有效的天气模型，解决了数据驱动天气 modeling 中的可解释性问题，并将标准的WSINDy算法调整为适用于高维流体数据，同时提出了一种处理不满足部分积分条件的项的方法。	 | The multiscale and turbulent nature of Earth's atmosphere has historically rendered accurate weather modeling a hard problem. Recently, there has been an explosion of interest surrounding data-driven approaches to weather modeling, which in many cases show improved forecasting accuracy and computational efficiency when compared to traditional methods. However, many of the current data-driven approaches employ highly parameterized neural networks, often resulting in uninterpretable models and limited gains in scientific understanding. In this work, we address the interpretability problem by explicitly discovering partial differential equations governing various weather phenomena, identifying symbolic mathematical models with direct physical interpretations. The purpose of this paper is to demonstrate that, in particular, the Weak form Sparse Identification of Nonlinear Dynamics (WSINDy) algorithm can learn effective weather models from both simulated and assimilated data. Our approach adapts the standard WSINDy algorithm to work with high-dimensional fluid data of arbitrary spatial dimension. Moreover, we develop an approach for handling terms that are not integrable-by-parts, such as advection operators.	 | 地球大气的多尺度和湍流性质历来使得准确的天气 modeling 成为一个难题。最近，数据驱动的天气 modeling 方法引起了极大的兴趣，这些方法在许多情况下显示出比传统方法更高的预测准确性和计算效率。然而，当前许多数据驱动的方法使用高度参数化的神经网络，常常导致不可解释的模型和科学理解上的有限增益。在本文中，我们通过显式地发现支配各种天气现象的偏微分方程来解决可解释性问题，从而识别具有直接物理解释的符号数学模型。本文的目的在于证明，特别是弱形式稀疏识别非线性动力学（WSINDy）算法可以从模拟和同化数据中学习有效的天气模型。我们的方法将标准的WSINDy算法调整为适用于任意空间维度的高维流体数据。此外，我们还开发了一种处理不满足部分积分条件的项（如对流算子）的方法。
2501.00734	 | DDD: Discriminative Difficulty Distance for plant disease diagnosis	 | Yuji Arima,Satoshi Kagiwada,Hitoshi Iyatomi	 | 本文提出了区分难度距离（DDD），一种用于量化训练集与测试集之间领域差距的新度量指标，以评估测试数据的分类难度，并提高植物病害诊断的诊断性能。实验表明，使用DDD可以显著提高不同数据集上训练的图像编码器生成的距离度量与实际诊断难度的相关性。	 | Recent studies on plant disease diagnosis using machine learning (ML) have highlighted concerns about the overestimated diagnostic performance due to inappropriate data partitioning, where training and test datasets are derived from the same source (domain). Plant disease diagnosis presents a challenging classification task, characterized by its fine-grained nature, vague symptoms, and the extensive variability of image features within each domain. In this study, we propose the concept of Discriminative Difficulty Distance (DDD), a novel metric designed to quantify the domain gap between training and test datasets while assessing the classification difficulty of test data. DDD provides a valuable tool for identifying insufficient diversity in training data, thus supporting the development of more diverse and robust datasets. We investigated multiple image encoders trained on different datasets and examined whether the distances between datasets, measured using low-dimensional representations generated by the encoders, are suitable as a DDD metric. The study utilized 244,063 plant disease images spanning four crops and 34 disease classes collected from 27 domains. As a result, we demonstrated that even if the test images are from different crops or diseases than those used to train the encoder, incorporating them allows the construction of a distance measure for a dataset that strongly correlates with the difficulty of diagnosis indicated by the disease classifier developed independently. Compared to the base encoder, pre-trained only on ImageNet21K, the correlation higher by 0.106 to 0.485, reaching a maximum of 0.909.	 | 近年来，使用机器学习（ML）进行植物病害诊断的研究突显了由于数据分区不当导致的诊断性能高估的问题，其中训练集和测试集数据来源于相同的领域（domain）。植物病害诊断面临着一个具有挑战性的分类任务，其特点是细微差别较大、症状模糊以及每个领域内的图像特征具有广泛的变异性。在本研究中，我们提出了区分难度距离（DDD，Discriminative Difficulty Distance）的概念，这是一种新的度量指标，旨在量化训练集与测试集之间的领域差距，并评估测试数据的分类难度。DDD 提供了一个有价值的工具，用于识别训练数据中的不足多样性，从而支持开发更多样化和更稳健的数据集。我们研究了多个在不同数据集上训练的图像编码器，并检查了使用编码器生成的低维表示测量的数据集之间的距离是否适合作为 DDD 度量。本研究使用了来自27个领域、涵盖四种作物和34种病害类别的244,063张植物病害图像。结果表明，即使测试图像的作物或病害种类与训练编码器所使用的数据不同，将这些图像纳入考虑也能构建与独立开发的病害分类器所指示的诊断难度高度相关的距离度量。与仅在ImageNet21K上预训练的基线编码器相比，相关性提高了0.106到0.485，最高可达0.909。
2501.00733	 | On Importance of Layer Pruning for Smaller BERT Models and Low Resource Languages	 | Mayur Shirke,Amey Shembade,Madhushri Wagh,Pavan Thorat,Raviraj Joshi	 | 本研究评估了层剪枝技术在特定下游任务中对低资源语言BERT模型性能的影响，发现剪枝后的模型能够保持甚至超越从头训练的小模型的性能，特别是在单语BERT模型中效果更佳。	 | This study explores the effectiveness of layer pruning for developing more efficient BERT models tailored to specific downstream tasks in low-resource languages. Our primary objective is to evaluate whether pruned BERT models can maintain high performance while reducing model size and complexity. We experiment with several BERT variants, including MahaBERT-v2 and Google-Muril, applying different pruning strategies and comparing their performance to smaller, scratch-trained models like MahaBERT-Small and MahaBERT-Smaller. We fine-tune these models on Marathi datasets, specifically Short Headlines Classification (SHC), Long Paragraph Classification (LPC) and Long Document Classification (LDC), to assess their classification accuracy. Our findings demonstrate that pruned models, despite having fewer layers, achieve comparable performance to their fully-layered counterparts while consistently outperforming scratch-trained models of similar size. Notably, pruning layers from the middle of the model proves to be the most effective strategy, offering performance competitive with pruning from the top and bottom. However, there is no clear winner, as different pruning strategies perform better in different model and dataset combinations. Additionally, monolingual BERT models outperform multilingual ones in these experiments. This approach, which reduces computational demands, provides a faster and more efficient alternative to training smaller models from scratch, making advanced NLP models more accessible for low-resource languages without compromising classification accuracy.	 | 本研究探讨了层剪枝技术在开发针对特定下游任务的低资源语言BERT模型中的有效性。我们的主要目标是评估剪枝后的BERT模型是否能在减少模型大小和复杂度的同时保持高度的性能。我们对几种BERT变体进行了实验，包括MahaBERT-v2和Google-Muril，应用了不同的剪枝策略，并将其性能与更小的从头训练模型（如MahaBERT-Small和MahaBERT-Smaller）进行了比较。我们将这些模型在马拉地语数据集上进行微调，具体包括短标题分类（SHC）、长段落分类（LPC）和长文档分类（LDC），以评估其分类准确性。研究结果表明，尽管剪枝后的模型拥有较少的层，它们的性能与完全层化的模型相当，且在大多数情况下优于大小相似的从头训练模型。值得注意的是，从模型中间剪枝层的效果最佳，其性能与从顶部和底部剪枝相当。然而，没有明显的胜者，不同的剪枝策略在不同模型和数据集组合中表现出不同的效果。此外，单语BERT模型在这项实验中优于多语BERT模型。这种方法通过减少计算需求，提供了一种更快且更高效的替代方案，即训练较小的模型，使先进的NLP模型在低资源语言中更加可及，而不牺牲分类准确性。
2501.00726	 | Enhancing Unsupervised Feature Selection via Double Sparsity Constrained Optimization	 | Xianchao Xiu,Anning Yang,Chenyi Huang,Xinrong Li,Wanquan Liu	 | 本文提出了一种新的无监督特征选择方法DSCOFS，通过结合$\ell_{2,0}$-范数和$\ell_0$-范数实现双稀疏约束优化，从而提高特征选择的准确性和区分性，并通过数值实验验证了其在多个数据集上的有效性。	 | Unsupervised feature selection (UFS) is widely applied in machine learning and pattern recognition. However, most of the existing methods only consider a single sparsity, which makes it difficult to select valuable and discriminative feature subsets from the original high-dimensional feature set. In this paper, we propose a new UFS method called DSCOFS via embedding double sparsity constrained optimization into the classical principal component analysis (PCA) framework. Double sparsity refers to using $\ell_{2,0}$-norm and $\ell_0$-norm to simultaneously constrain variables, by adding the sparsity of different types, to achieve the purpose of improving the accuracy of identifying differential features. The core is that $\ell_{2,0}$-norm can remove irrelevant and redundant features, while $\ell_0$-norm can filter out irregular noisy features, thereby complementing $\ell_{2,0}$-norm to improve discrimination. An effective proximal alternating minimization method is proposed to solve the resulting nonconvex nonsmooth model. Theoretically, we rigorously prove that the sequence generated by our method globally converges to a stationary point. Numerical experiments on three synthetic datasets and eight real-world datasets demonstrate the effectiveness, stability, and convergence of the proposed method. In particular, the average clustering accuracy (ACC) and normalized mutual information (NMI) are improved by at least 3.34% and 3.02%, respectively, compared with the state-of-the-art methods. More importantly, two common statistical tests and a new feature similarity metric verify the advantages of double sparsity. All results suggest that our proposed DSCOFS provides a new perspective for feature selection.	 | 无监督特征选择（UFS）在机器学习和模式识别中得到了广泛的应用。然而，现有的大多数方法仅考虑单一稀疏性，这使得从原始高维特征集中选择有价值的和区分性强的特征子集变得困难。本文提出了一种新的UFS方法，称为DSCOFS，通过将双稀疏约束优化嵌入经典的主成分分析（PCA）框架中来实现。双稀疏性指的是同时利用$\ell_{2,0}$-范数和$\ell_0$-范数对变量进行约束，通过增加不同类型变量的稀疏性，以提高识别差异特征的准确性。核心在于$\ell_{2,0}$-范数可以移除无关和冗余的特征，而$\ell_0$-范数可以滤除不规则的噪声特征，从而补充$\ell_{2,0}$-范数以提高区分性。我们提出了一种有效的邻近交替最小化方法来解决由此产生的非凸非光滑模型。从理论上讲，我们严格证明了本方法生成的序列全局收敛到极小点。在三个合成数据集和八个真实世界数据集上的数值实验表明，所提出的方法具有有效性、稳定性和收敛性。特别是，与最先进的方法相比，平均聚类准确性（ACC）和归一化互信息（NMI）分别提高了至少3.34%和3.02%。更重要的是，两种常见的统计测试和一种新的特征相似度度量验证了双稀疏性的优势。所有结果表明，我们提出的DSCOFS为特征选择提供了一个新的视角。
2501.00712	 | Rethinking Addressing in Language Models via Contexualized Equivariant Positional Encoding	 | Jiajun Zhu,Peihao Wang,Ruisi Cai,Jason D. Lee,Pan Li,Zhangyang Wang	 | TAPE提出了一种新的位置嵌入框架，通过在每一层中引入序列内容来动态增强位置嵌入，从而提高模型对长时依赖性和多样任务的建模能力。实验表明，TAPE在语言建模、算术推理和长上下文检索任务中优于现有方法。	 | Transformers rely on both content-based and position-based addressing mechanisms to make predictions, but existing positional encoding techniques often diminish the effectiveness of position-based addressing. Many current methods enforce rigid patterns in attention maps, limiting the ability to model long-range dependencies and adapt to diverse tasks. Additionally, most positional encodings are learned as general biases, lacking the specialization required for different instances within a dataset. To address this, we propose con$\textbf{T}$extualized equivari$\textbf{A}$nt $\textbf{P}$osition $\textbf{E}$mbedding ($\textbf{TAPE}$), a novel framework that enhances positional embeddings by incorporating sequence content across layers. TAPE introduces dynamic, context-aware positional encodings, overcoming the constraints of traditional fixed patterns. By enforcing permutation and orthogonal equivariance, TAPE ensures the stability of positional encodings during updates, improving robustness and adaptability. Our method can be easily integrated into pre-trained transformers, offering parameter-efficient fine-tuning with minimal overhead. Extensive experiments shows that TAPE achieves superior performance in language modeling, arithmetic reasoning, and long-context retrieval tasks compared to existing positional embedding techniques.	 | 以下是该摘要的中文翻译：   Transformer模型依赖于内容导向和位置导向的双重寻址机制来进行预测，但现有的位置编码技术往往削弱了位置导向寻址的有效性。当前许多方法在注意图中强加了刚性模式，这限制了它们建模长时依赖性和适应多样任务的能力。此外，大多数位置编码被学习为通用偏置，缺乏针对数据集中不同实例所需的专门化能力。为了解决这一问题，我们提出了Con$\textbf{T}$extualized equiv$\textbf{A}$nt $\textbf{P}$osition $\textbf{E}$mbedding（TAPE），这是一种新颖的框架，通过在各层中引入序列内容来增强位置嵌入。TAPE引入了动态、上下文感知的位置编码，克服了传统固定模式的限制。通过强制执行置换和正交不变性，TAPE确保位置编码在更新过程中保持稳定，从而提高了稳定性和适应性。该方法可以轻松集成到预训练的Transformer中，提供参数效率高的微调，且开销较小。广泛的实验表明，与现有的位置嵌入技术相比，TAPE在语言建模、算术推理和长上下文检索任务中表现出更优的性能。
2501.00693	 | Beyond Model Scale Limits: End-Edge-Cloud Federated Learning with Self-Rectified Knowledge Agglomeration	 | Zhiyuan Wu,Sheng Sun,Yuwei Wang,Min Liu,Ke Xu,Quyang Pan,Bo Gao,Tian Wen	 | 端边云联邦学习与自校正知识聚集（FedEEC）是一种新型联邦学习框架，它通过在线蒸馏协议和自校正知识技术解决了端边云协作环境中层次联邦学习的性能瓶颈和知识传递问题，从而增强了训练模型的规模和泛化能力。	 | The rise of End-Edge-Cloud Collaboration (EECC) offers a promising paradigm for Artificial Intelligence (AI) model training across end devices, edge servers, and cloud data centers, providing enhanced reliability and reduced latency. Hierarchical Federated Learning (HFL) can benefit from this paradigm by enabling multi-tier model aggregation across distributed computing nodes. However, the potential of HFL is significantly constrained by the inherent heterogeneity and dynamic characteristics of EECC environments. Specifically, the uniform model structure bounded by the least powerful end device across all computing nodes imposes a performance bottleneck. Meanwhile, coupled heterogeneity in data distributions and resource capabilities across tiers disrupts hierarchical knowledge transfer, leading to biased updates and degraded performance. Furthermore, the mobility and fluctuating connectivity of computing nodes in EECC environments introduce complexities in dynamic node migration, further compromising the robustness of the training process. To address multiple challenges within a unified framework, we propose End-Edge-Cloud Federated Learning with Self-Rectified Knowledge Agglomeration (FedEEC), which is a novel EECC-empowered FL framework that allows the trained models from end, edge, to cloud to grow larger in size and stronger in generalization ability. FedEEC introduces two key innovations: (1) Bridge Sample Based Online Distillation Protocol (BSBODP), which enables knowledge transfer between neighboring nodes through generated bridge samples, and (2) Self-Knowledge Rectification (SKR), which refines the transferred knowledge to prevent suboptimal cloud model optimization. The proposed framework effectively handles both cross-tier resource heterogeneity and effective knowledge transfer between neighboring nodes, while satisfying the migration-resilient requirements of EECC.	 | 端边云协作（End-Edge-Cloud Collaboration, EECC）的兴起为在终端设备、边缘服务器和云数据中心之间进行人工智能（AI）模型训练提供了有前景的范式，提高了可靠性和减少了延迟。层次联邦学习（Hierarchical Federated Learning, HFL）可以从这种范式中受益，通过在分布式计算节点之间实现多层次模型聚合。然而，HFL 的潜力受到 EECC 环境内部在生性和动态特性的影响而受到显著限制。具体来说，所有计算节点中由最弱终端设备定义的统一模型结构形成了性能瓶颈。同时，不同层级之间数据分布和资源能力的联合异质性破坏了层次化的知识传递，导致偏斜的更新和性能降低。此外，EECC 环境中计算节点的移动性和不断变化的连接性引入了动态节点迁移的复杂性，进一步损害了训练过程的鲁棒性。为了在一个统一框架内解决这些挑战，我们提出了端边云联邦学习与自校正知识聚集（FedEEC），这是一种由 EECC 支撑的新型联邦学习框架，使得从终端、边缘到云的训练模型在规模和泛化能力上得到增强。FedEEC 引入了两个关键创新： （1）基于桥样本的在线蒸馏协议（Bridge Sample Based Online Distillation Protocol, BSBODP），它通过生成的桥样本在相邻节点之间实现知识传递，（2）自校正知识（Self-Knowledge Rectification, SKR），它精炼传递的知识以防止云模型优化的次优性。所提出框架有效处理了跨层级资源异质性和相邻节点之间有效的知识传递问题，同时满足了 EECC 的迁移鲁棒性要求。
2501.00691	 | Labels Generated by Large Language Model Helps Measuring People's Empathy in Vitro	 | Md Rakibul Hasan,Yue Yao,Md Zakir Hossain,Aneesh Krishna,Imre Rudas,Shafin Rahman,Tom Gedeon	 | 该研究探讨了大型语言模型即服务（LLMaaS）在情感计算领域的应用，通过利用LLM生成标签来提高预训练语言模型预测心理问卷结果的准确性，特别是在嘈杂标签环境下取得了显著改进。	 | Large language models (LLMs) have revolutionised numerous fields, with LLM-as-a-service (LLMSaaS) having a strong generalisation ability that offers accessible solutions directly without the need for costly training. In contrast to the widely studied prompt engineering for task solving directly (in vivo), this paper explores its potential in in-vitro applications. These involve using LLM to generate labels to help the supervised training of mainstream models by (1) noisy label correction and (2) training data augmentation with LLM-generated labels. In this paper, we evaluate this approach in the emerging field of empathy computing -- automating the prediction of psychological questionnaire outcomes from inputs like text sequences. Specifically, crowdsourced datasets in this domain often suffer from noisy labels that misrepresent underlying empathy. By leveraging LLM-generated labels to train pre-trained language models (PLMs) like RoBERTa, we achieve statistically significant accuracy improvements over baselines, achieving a state-of-the-art Pearson correlation coefficient of 0.648 on NewsEmp benchmarks. In addition, we bring insightful discussions, including current challenges in empathy computing, data biases in training data and evaluation metric selection. Code and LLM-generated data are available at https://github.com/hasan-rakibul/LLMPathy (available once the paper is accepted).	 | 大型语言模型（LLMs）已经彻底改变了多个领域，LLM即服务（LLMSaaS）具有强大的泛化能力，可以提供无需昂贵训练即可直接使用的解决方案。与广泛研究的任务直接解决的提示工程（in vivo）相比，本文探讨了其在体外应用中的潜力。这些应用涉及使用LLM生成标签以帮助主流模型的监督训练，包括（1）嘈杂标签的校正和（2）通过LLM生成的标签进行训练数据增强。在本文中，我们评估了这种方法在新兴的情感计算领域——自动化预测心理问卷结果（如文本序列输入）中的应用。具体来说，该领域中的众包数据集经常遭受代表不准确情绪的嘈杂标签的问题。通过利用LLM生成的标签来训练像RoBERTa这样的预训练语言模型（PLMs），我们实现了相对于基线的统计学上显著的准确率改进，在NewsEmp基准上取得了0.648的最新Pearson相关系数。此外，我们还进行了深入讨论，包括情感计算领域的当前挑战、训练数据中的数据偏差以及评估指标的选择。相关代码和LLM生成的数据可在 https://github.com/hasan-rakibul/LLMPathy 获取（论文被接受后即可访问）。
2501.00664	 | Grade Inflation in Generative Models	 | Phuc Nguyen,Miao Li,Alexandra Morgan,Rima Arnaout,Ramy Arnaout	 | 研究发现，广泛使用的生成模型质量评估分数在与真实数据的分布进行比较时往往会“评分膨胀”，而新的“等密度”评分方法，如Eden分数，能够避免这一问题并更好地符合人类对拟合优度的感知。	 | Generative models hold great potential, but only if one can trust the evaluation of the data they generate. We show that many commonly used quality scores for comparing two-dimensional distributions of synthetic vs. ground-truth data give better results than they should, a phenomenon we call the "grade inflation problem." We show that the correlation score, Jaccard score, earth-mover's score, and Kullback-Leibler (relative-entropy) score all suffer grade inflation. We propose that any score that values all datapoints equally, as these do, will also exhibit grade inflation; we refer to such scores as "equipoint" scores. We introduce the concept of "equidensity" scores, and present the Eden score, to our knowledge the first example of such a score. We found that Eden avoids grade inflation and agrees better with human perception of goodness-of-fit than the equipoint scores above. We propose that any reasonable equidensity score will avoid grade inflation. We identify a connection between equidensity scores and Rényi entropy of negative order. We conclude that equidensity scores are likely to outperform equipoint scores for generative models, and for comparing low-dimensional distributions more generally.	 | 生成模型具有巨大的潜力，但前提是必须信任它们生成的数据评估结果。我们展示了将合成数据与真实数据的二维分布进行比较时，广泛使用的质量得分往往比实际情况更好，我们称之为“评分膨胀问题”。我们证明了相关分数、Jaccard分数、搬运工距离分数以及Kullback-Leibler（相对熵）分数都存在评分膨胀。我们提出任何像这些分数一样以所有数据点同等对待的评分都会同样地存在评分膨胀；我们称之为“等点”评分。我们提出了“等密度”评分的概念，并引入了Eden分数，据我们所知，这是第一个此类评分的例子。我们发现Eden评分避免了评分膨胀，并且与人类对拟合优度感知的契合度优于上述等点评分。我们提议任何合理的等密度评分都将避免评分膨胀。我们发现了等密度评分与负阶Rényi熵之间的联系。我们得出结论，等密度评分很可能在生成模型中表现优于等点评分，并且在一般低维度分布的比较中也是如此。
2501.00656	 | 2 OLMo 2 Furious	 | Team OLMo,Pete Walsh,Luca Soldaini,Dirk Groeneveld,Kyle Lo,Shane Arora,Akshita Bhagia,Yuling Gu,Shengyi Huang,Matt Jordan,Nathan Lambert,Dustin Schwenk,Oyvind Tafjord,Taira Anderson,David Atkinson,Faeze Brahman,Christopher Clark,Pradeep Dasigi,Nouha Dziri,Michal Guerquin,Hamish Ivison,Pang Wei Koh,Jiacheng Liu,Saumya Malik,William Merrill,Lester James V. Miranda,Jacob Morrison,Tyler Murray,Crystal Nam,Valentina Pyatkin,Aman Rangapur,Michael Schmitz,Sam Skjonsberg,David Wadden,Christopher Wilhelm,Michael Wilson,Luke Zettlemoyer,Ali Farhadi,Noah A. Smith,Hannaneh Hajishirzi	 | OLMo 2是开源的下一代语言模型，结合了改进的模型架构、训练配方和新的数据混合方法，显著提升了训练稳定性和性能，并公开发布了所有相关数据、代码和检查点；此外，OLMo 2-Instruct版本通过强化学习和可验证奖励进一步优化，展示了在多个下游任务中的竞争力。	 | We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes dense autoregressive models with improved architecture and training recipe, pretraining data mixtures, and instruction tuning recipes. Our modified model architecture and training recipe achieve both better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e. specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from Tülu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to compute, often matching or outperforming open-weight only models like Llama 3.1 and Qwen 2.5 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with or surpassing open-weight only models of comparable size, including Qwen 2.5, Llama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly -- models at 7B and 13B scales, both pretrained and post-trained, including their full training data, training code and recipes, training logs and thousands of intermediate checkpoints. The final instruction model is available on the Ai2 Playground as a free research demo.	 | 我们提出了OLMo 2，这是我们完全开源语言模型的下一代产品。OLMo 2 包含密集的自回归模型，改进了架构和训练配方，混合了预训练数据，并且包含指令调优配方。我们修改的模型架构和训练配方在提高训练稳定性和单个词效率方面表现更佳。我们更新的预训练数据混合引入了一种新的专业化数据混合，称为Dolmino Mix 1124，这种数据混合在通过晚期课程训练（即预训练的退火阶段引入特定数据）引入时，显著提高了模型在许多下游任务基准测试中的性能。最后，我们采用了Tülu 3的最佳实践，开发了OLMo 2-Instruct，重点是宽松的数据和扩展我们最终阶段的强化学习，使用可验证奖励（RLVR）。我们的OLMo 2基础模型位于性能与计算资源的最佳权衡点，通常与或优于仅开放权重的模型，如Llama 3.1和Qwen 2.5，同时使用更少的FLOPs，并且具有完全透明的训练数据、代码和配方。我们的完全开源的OLMo 2-Instruct模型在大小相当的模型中竞争力强或超越，包括Qwen 2.5、Llama 3.1和Gemma 2。我们公开发布了所有OLMo 2的成果——包括7B和13B规模的模型，无论是预训练还是后训练的模型，以及它们的完整训练数据、训练代码和配方、训练日志以及数千个中间检查点。最终的指令模型可在Ai2 Playground上免费供研究演示使用。
2501.00655	 | Finding Missed Code Size Optimizations in Compilers using LLMs	 | Davide Italiano,Chris Cummins	 | 该研究利用差异测试技术结合大型语言模型，开发了一种简洁的方法来识别编译器中的代码大小优化缺失，从而在C/C++、Rust和Swift编译器中发现了多个真实存在的错误，证明了LLM辅助测试在发现编译器优化错误方面的有效性。	 | Compilers are complex, and significant effort has been expended on testing them. Techniques such as random program generation and differential testing have proved highly effective and have uncovered thousands of bugs in production compilers. The majority of effort has been expended on validating that a compiler produces correct code for a given input, while less attention has been paid to ensuring that the compiler produces performant code.   In this work we adapt differential testing to the task of identifying missed optimization opportunities in compilers. We develop a novel testing approach which combines large language models (LLMs) with a series of differential testing strategies and use them to find missing code size optimizations in C / C++ compilers.   The advantage of our approach is its simplicity. We offload the complex task of generating random code to an off-the-shelf LLM, and use heuristics and analyses to identify anomalous compiler behavior. Our approach requires fewer than 150 lines of code to implement. This simplicity makes it extensible. By simply changing the target compiler and initial LLM prompt we port the approach from C / C++ to Rust and Swift, finding bugs in both. To date we have reported 24 confirmed bugs in production compilers, and conclude that LLM-assisted testing is a promising avenue for detecting optimization bugs in real world compilers.	 | 编译器结构复杂，对它们的测试已经投入了大量精力。诸如随机程序生成和差异测试等技术已被证明非常有效，发现了数千个生产编译器中的错误。大多数努力集中在验证编译器是否能为给定输入生成正确的代码，但对于确保编译器生成高性能代码的关注较少。在这项工作中，我们将差异测试技术应用于识别编译器中遗漏的优化机会。我们开发了一种新颖的测试方法，结合了大型语言模型（LLMs）和一系列差异测试策略，并使用这些策略在C/C++编译器中查找缺失的代码大小优化。  我们方法的优势在于其简洁性。我们将生成随机代码的复杂任务移交给商用现货的LLM，使用启发式方法和分析来识别异常的编译器行为。我们的方法只需不到150行代码即可实现。这种简洁性使其易于扩展。通过简单更改目标编译器和初始LLM提示，我们将该方法从C/C++扩展到了Rust和Swift，同时在两者中都发现了错误。迄今为止，我们已在生产编译器中报告了24个确认的错误，并得出结论，LLM辅助测试是检测现实世界编译器优化错误的一个有前途的方法。
2501.00654	 | ICONS: Influence Consensus for Vision-Language Data Selection	 | Xindi Wu,Mengzhou Xia,Rulin Shao,Zhiwei Deng,Pang Wei Koh,Olga Russakovsky	 | 本文提出了一种基于梯度驱动的影响力共识选择方法（ICONs），用于从大量冗余的视觉-语言训练数据中选择高效的数据子集，以实现多任务训练。实验表明，使用选择后的数据训练模型能获得接近全数据集训练的性能，同时大大减少了计算成本。	 | Visual Instruction Tuning typically requires a large amount of vision-language training data. This data often containing redundant information that increases computational costs without proportional performance gains. In this work, we introduce ICONS, a gradient-driven Influence CONsensus approach for vision-language data Selection that selects a compact training dataset for efficient multi-task training. The key element of our approach is cross-task influence consensus, which uses majority voting across task-specific influence matrices to identify samples that are consistently valuable across multiple tasks, allowing us to effectively prioritize data that optimizes for overall performance. Experiments show that models trained on our selected data (20% of LLaVA-665K) achieve 98.6% of the relative performance obtained using the full dataset. Additionally, we release this subset, LLaVA-ICONS-133K, a compact yet highly informative subset of LLaVA-665K visual instruction tuning data, preserving high impact training data for efficient vision-language model development.	 | 视觉指令调优通常需要大量的视觉-语言训练数据。这些数据往往包含冗余信息，增加了计算成本但并未带来相应的性能提升。在本文中，我们提出了ICONs（基于梯度驱动的影响力共识选择方法），这是一种用于视觉-语言数据选择的方法，旨在选择一个紧凑的训练数据集，以实现高效的多任务训练。我们方法的关键元素是跨任务影响力共识，它通过在任务特定的影响力矩阵中使用多数投票来识别在多个任务中始终具有价值的样本，从而使我们可以有效优先选择优化整体性能的数据。实验表明，使用我们选择的数据（LLaVA-665K中的20%）训练的模型可以达到使用完整数据集获得的相对性能的98.6%。此外，我们还发布了这个子集——LLaVA-ICONS-133K，它是LLaVA-665K视觉指令调优数据的一个紧凑而高度信息丰富的子集，保留了高效的视觉-语言模型开发所需的重要训练数据。
2501.00651	 | Taming Feed-forward Reconstruction Models as Latent Encoders for 3D Generative Models	 | Suttisak Wizadwongsa,Jinfan Zhou,Edward Li,Jeong Joon Park	 | 本文提出了一种新颖的方法，利用预训练的前馈3D重建模型作为潜在编码器来训练3D生成模型，结合图像到3D重建和生成模型两种范式，通过改进的后处理管道和特定损失函数，实现高质量的文本条件3D内容生成。	 | Recent AI-based 3D content creation has largely evolved along two paths: feed-forward image-to-3D reconstruction approaches and 3D generative models trained with 2D or 3D supervision. In this work, we show that existing feed-forward reconstruction methods can serve as effective latent encoders for training 3D generative models, thereby bridging these two paradigms. By reusing powerful pre-trained reconstruction models, we avoid computationally expensive encoder network training and obtain rich 3D latent features for generative modeling for free. However, the latent spaces of reconstruction models are not well-suited for generative modeling due to their unstructured nature. To enable flow-based model training on these latent features, we develop post-processing pipelines, including protocols to standardize the features and spatial weighting to concentrate on important regions. We further incorporate a 2D image space perceptual rendering loss to handle the high-dimensional latent spaces. Finally, we propose a multi-stream transformer-based rectified flow architecture to achieve linear scaling and high-quality text-conditioned 3D generation. Our framework leverages the advancements of feed-forward reconstruction models to enhance the scalability of 3D generative modeling, achieving both high computational efficiency and state-of-the-art performance in text-to-3D generation.	 | 近年来，基于AI的3D内容生成主要沿着两条路径演化：前馈的图像到3D重建方法和通过2D或3D监督训练的3D生成模型。在本文中，我们展示了现有的前馈重建方法可以作为有效的潜在编码器，用于训练3D生成模型，从而连接这两种范式。通过重用强大的预训练重建模型，我们避免了耗时的编码器网络训练，并且可以免费获得丰富的3D潜在特征用于生成建模。然而，重建模型的潜在空间由于其结构化不佳，不适合生成建模。为了在这些潜在特征上进行流模型训练，我们开发了后处理管道，包括标准化特征的协议和空间加权以集中在重要区域。我们进一步引入了2D图像空间感知渲染损失来处理高维潜在空间。最后，我们提出了一种基于多流变压器的校正流架构，以实现线性扩展并生成高质量的文本条件3D内容。我们的框架利用前馈重建模型的进步来增强3D生成建模的可扩展性，同时实现高计算效率和最先进的文本到3D生成性能。
2501.00645	 | SoundBrush: Sound as a Brush for Visual Scene Editing	 | Kim Sung-Bin,Kim Jun-Seong,Junseok Ko,Yewon Kim,Tae-Hyun Oh	 | SoundBrush是一种利用声音作为画笔编辑和操控视觉场景的模型，它扩展了Latent Diffusion Model的生成能力，通过一个丰富的音频配对视觉场景数据集进行训练，能够准确地编辑或插入发声对象以匹配音频输入，并支持3D场景编辑。	 | We propose SoundBrush, a model that uses sound as a brush to edit and manipulate visual scenes. We extend the generative capabilities of the Latent Diffusion Model (LDM) to incorporate audio information for editing visual scenes. Inspired by existing image-editing works, we frame this task as a supervised learning problem and leverage various off-the-shelf models to construct a sound-paired visual scene dataset for training. This richly generated dataset enables SoundBrush to learn to map audio features into the textual space of the LDM, allowing for visual scene editing guided by diverse in-the-wild sound. Unlike existing methods, SoundBrush can accurately manipulate the overall scenery or even insert sounding objects to best match the audio inputs while preserving the original content. Furthermore, by integrating with novel view synthesis techniques, our framework can be extended to edit 3D scenes, facilitating sound-driven 3D scene manipulation. Demos are available at https://soundbrush.github.io/.	 | 我们提出了一种名为SoundBrush的模型，它利用声音作为画笔来编辑和操控视觉场景。我们将生成能力扩展到Latent Diffusion Model (LDM)，使其能够结合音频信息来编辑视觉场景。受到现有图像编辑工作的启发，我们将这一任务构想为监督学习问题，并利用多种现成模型构建了一个音频配对的视觉场景数据集用于训练。这个丰富生成的数据集使SoundBrush能够学习将音频特征映射到LDM的文本空间，从而允许根据多样化的野外声音进行视觉场景编辑。与现有方法不同，SoundBrush能够准确地操控整体场景或插入发声对象，以最好地匹配音频输入，同时保留原始内容。此外，通过结合新型视图合成技术，我们的框架可以扩展到编辑3D场景，从而实现基于声音的3D场景操控。相关演示可在https://soundbrush.github.io/查看。
2501.00642	 | Enabling New HDLs with Agents	 | Mark Zakharov,Farzaneh Rabiei Kashanaki,Jose Renau	 | 该研究探讨了如何利用大型语言模型（LLMs）来改进硬件描述语言（HDLs）的学习和应用，并提出了HDLAgent，一种针对未训练过的HDLs优化的AI代理，显著提升了LLMs在这些领域的能力。	 | Large Language Models (LLMs) based agents are transforming the programming language landscape by facilitating learning for beginners, enabling code generation, and optimizing documentation workflows. Hardware Description Languages (HDLs), with their smaller user community, stand to benefit significantly from the application of LLMs as tools for learning new HDLs. This paper investigates the challenges and solutions of enabling LLMs for HDLs, particularly for HDLs that LLMs have not been previously trained on. This work introduces HDLAgent, an AI agent optimized for LLMs with limited knowledge of various HDLs. It significantly enhances off-the-shelf LLMs.	 | 基于大型语言模型（LLMs）的代理正在通过促进初学者学习、实现代码生成以及优化文档工作流程，改变编程语言的格局。硬件描述语言（HDLs），由于其较小的用户群体，尤其可以从LLMs的应用中受益，作为学习新HDLs的工具。本文探讨了为HDLs启用LLMs的挑战与解决方案，特别是对于LLMs未曾训练过的HDLs。本文引入了HDLAgent，这是一种针对各种HDLs知识有限的LLMs进行了优化的AI代理，显著增强了现成的LLMs。
2501.00637	 | Flash-Split: 2D Reflection Removal with Flash Cues and Latent Diffusion Separation	 | Tianfu Wang,Mingyang Xie,Haoming Cai,Sachin Shah,Christopher A. Metzler	 | Flash-Split 是一种稳健的框架，能够仅使用一对（可能错位的）闪光/非闪光图像来分离透过光和反射光，通过一个条件编码的双分支扩散模型解决错位问题，并通过跨潜在解码过程恢复高分辨率细节，从而实现先进的反射分离性能。	 | Transparent surfaces, such as glass, create complex reflections that obscure images and challenge downstream computer vision applications. We introduce Flash-Split, a robust framework for separating transmitted and reflected light using a single (potentially misaligned) pair of flash/no-flash images. Our core idea is to perform latent-space reflection separation while leveraging the flash cues. Specifically, Flash-Split consists of two stages. Stage 1 separates apart the reflection latent and transmission latent via a dual-branch diffusion model conditioned on an encoded flash/no-flash latent pair, effectively mitigating the flash/no-flash misalignment issue. Stage 2 restores high-resolution, faithful details to the separated latents, via a cross-latent decoding process conditioned on the original images before separation. By validating Flash-Split on challenging real-world scenes, we demonstrate state-of-the-art reflection separation performance and significantly outperform the baseline methods.	 | 透明表面，如玻璃，会产生复杂的反射，遮挡图像并挑战下游的计算机视觉应用。我们提出了一种名为 Flash-Split 的稳健框架，用于仅使用一对（可能错位的）闪光/非闪光图像来分离透过光和反射光。我们的核心思想是在利用闪光线索的同时进行潜在空间反射分离。具体而言，Flash-Split 包含两个阶段。第一阶段通过一个条件于编码的闪光/非闪光潜在对的双分支扩散模型，将反射潜在和穿透潜在区分开来，有效地解决了闪光/非闪光错位的问题。第二阶段通过一个跨潜在解码过程，在分离后的潜在中恢复高分辨率、忠实的细节，该解码过程是基于分离前的原始图像。通过在具有挑战性的实际场景上验证 Flash-Split，我们展示了最先进的反射分离性能，并显著优于基础方法。
2501.00632	 | Different thresholding methods on Nearest Shrunken Centroid algorithm	 | Mohammad Omar Sahtout,Haiyan Wang,Santosh Ghimire	 | 本文探讨了不同阈值方法对最近收缩质心算法（PAM）在高维分类问题中的影响，并通过引入硬阈值化和顺序阈值化方法以及深度搜索算法，改进了PAM算法，提高了癌症状态预测的准确性并减少了特征数量。	 | This article considers the impact of different thresholding methods to the Nearest Shrunken Centroid algorithm, which is popularly referred as the Prediction Analysis of Microarrays (PAM) for high-dimensional classification. PAM uses soft thresholding to achieve high computational efficiency and high classification accuracy but in the price of retaining too many features. When applied to microarray human cancers, PAM selected 2611 features on average from 10 multi-class datasets. Such a large number of features make it difficult to perform follow up study. One reason behind this problem is the soft thresholding, which is known to produce biased parameter estimate in regression analysis. In this article, we extend the PAM algorithm with two other thresholding methods, hard and order thresholding, and a deep search algorithm to achieve better thresholding parameter estimate. The modified algorithms are extensively tested and compared to the original one based on real data and Monte Carlo studies. In general, the modification not only gave better cancer status prediction accuracy, but also resulted in more parsimonious models with significantly smaller number of features.	 | 本文探讨了不同阈值方法对最近收缩质心算法（通常称为微阵列预测分析PAM）的影响，该算法适用于高维分类问题。PAM 使用软阈值化以实现高效计算和高分类准确性，但代价是保留了过多的特征。当应用于微阵列人类癌症数据时，PAM 平均从 10 个多类数据集中选择了 2611 个特征。如此多的特征使得后续研究变得困难。造成这一问题的一个原因是软阈值化，在回归分析中已知会产生有偏参数估计。本文中，我们扩展了 PAM 算法，引入了两种其他阈值方法（硬阈值化和顺序阈值化）以及一种深度搜索算法，以实现更好的阈值参数估计。修改后的算法在实际数据和蒙特卡洛研究的基础上进行了广泛测试和比较。总体而言，这些修改不仅提高了癌症状态预测的准确性，还产生了更为简洁的模型，特征数量显著减少。
2501.00603	 | DiC: Rethinking Conv3x3 Designs in Diffusion Models	 | Yuchuan Tian,Jing Han,Chengcheng Wang,Yuchen Liang,Chao Xu,Hanting Chen	 | 本文提出了一种新的扩散模型DiC（Diffusion CNN），它基于大规模的纯卷积结构，通过引入稀疏跳跃连接和条件增强机制，既提高了模型速度又保持了高性能，显著优于现有的扩散Transformer模型。	 | Diffusion models have shown exceptional performance in visual generation tasks. Recently, these models have shifted from traditional U-Shaped CNN-Attention hybrid structures to fully transformer-based isotropic architectures. While these transformers exhibit strong scalability and performance, their reliance on complicated self-attention operation results in slow inference speeds. Contrary to these works, we rethink one of the simplest yet fastest module in deep learning, 3x3 Convolution, to construct a scaled-up purely convolutional diffusion model. We first discover that an Encoder-Decoder Hourglass design outperforms scalable isotropic architectures for Conv3x3, but still under-performing our expectation. Further improving the architecture, we introduce sparse skip connections to reduce redundancy and improve scalability. Based on the architecture, we introduce conditioning improvements including stage-specific embeddings, mid-block condition injection, and conditional gating. These improvements lead to our proposed Diffusion CNN (DiC), which serves as a swift yet competitive diffusion architecture baseline. Experiments on various scales and settings show that DiC surpasses existing diffusion transformers by considerable margins in terms of performance while keeping a good speed advantage. Project page: https://github.com/YuchuanTian/DiC	 | 扩散模型在视觉生成任务中展现了卓越的性能。近年来，这些模型从传统的U型CNN-注意力混合结构转向了完全基于Transformer的各向同性架构。虽然这些Transformer表现出强大的可扩展性和性能，但它们依赖复杂的自我注意操作导致推理速度较慢。与这些工作不同，我们重新思考了深度学习中既简单又快速的模块——3x3卷积，构建了一个大规模的纯卷积扩散模型。我们首先发现，编码器-解码器Hourglass设计在Conv3x3中优于可扩展的各向同性架构，但仍然未达到我们的期望。进一步改进架构，我们引入了稀疏跳跃连接来减少冗余并提高可扩展性。基于此架构，我们提出了条件增强改进，包括阶段特定嵌入、中间块条件注入和条件门控。这些改进促使我们提出了DiC（Diffusion CNN）扩散架构，这是一种既迅速又具有竞争力的基线模型。在不同规模和设置下的实验表明，DiC在性能上显著优于现有扩散Transformer，同时保持了良好的速度优势。项目页面：https://github.com/YuchuanTian/DiC
2501.00602	 | STORM: Spatio-Temporal Reconstruction Model for Large-Scale Outdoor Scenes	 | Jiawei Yang,Jiahui Huang,Yuxiao Chen,Yan Wang,Boyi Li,Yurong You,Apoorva Sharma,Maximilian Igl,Peter Karkus,Danfei Xu,Boris Ivanovic,Yue Wang,Marco Pavone	 | STORM模型是一种时空重建模型，通过一个数据驱动的Transformer架构直接从稀疏观测中推断动态3D场景表示，解决了现有方法在优化时间、泛化能力和重建质量上的问题，显著提高了动态场景重建的性能并支持实时渲染。	 | We present STORM, a spatio-temporal reconstruction model designed for reconstructing dynamic outdoor scenes from sparse observations. Existing dynamic reconstruction methods often rely on per-scene optimization, dense observations across space and time, and strong motion supervision, resulting in lengthy optimization times, limited generalization to novel views or scenes, and degenerated quality caused by noisy pseudo-labels for dynamics. To address these challenges, STORM leverages a data-driven Transformer architecture that directly infers dynamic 3D scene representations--parameterized by 3D Gaussians and their velocities--in a single forward pass. Our key design is to aggregate 3D Gaussians from all frames using self-supervised scene flows, transforming them to the target timestep to enable complete (i.e., "amodal") reconstructions from arbitrary viewpoints at any moment in time. As an emergent property, STORM automatically captures dynamic instances and generates high-quality masks using only reconstruction losses. Extensive experiments on public datasets show that STORM achieves precise dynamic scene reconstruction, surpassing state-of-the-art per-scene optimization methods (+4.3 to 6.6 PSNR) and existing feed-forward approaches (+2.1 to 4.7 PSNR) in dynamic regions. STORM reconstructs large-scale outdoor scenes in 200ms, supports real-time rendering, and outperforms competitors in scene flow estimation, improving 3D EPE by 0.422m and Acc5 by 28.02%. Beyond reconstruction, we showcase four additional applications of our model, illustrating the potential of self-supervised learning for broader dynamic scene understanding.	 | 我们提出了STORM模型，这是一种专门设计用于从稀疏观察中重建动态户外场景的时空重建模型。现有的动态重建方法通常依赖于每场景优化、空间和时间上的密集观测以及强大的运动监督，这导致了优化时间过长、对新视角或新场景的泛化能力有限，以及由于动态伪标签噪声导致的重建质量退化。为了解决这些问题，STORM利用了一个数据驱动的Transformer架构，能够在单次前向传递中直接推断出动态的3D场景表示——这些表示由3D高斯分布及其速度参数化。我们的关键设计是使用自监督场景流来聚合所有帧中的3D高斯分布，并将它们转换为目标时间步，从而在任何时间点从任意视角实现完整的（即，"无遮挡的"）重建。作为一种衍生特性，STORM仅使用重建损失就可以自动捕捉动态实例并生成高质量的掩码。在公共数据集上的大量实验表明，STORM在动态区域的动态场景重建上优于最先进的每场景优化方法（PSNR提高4.3到6.6），并且也优于现有的前馈方法（PSNR提高2.1到4.7）。STORM可以在200毫秒内重建大规模的户外场景，支持实时渲染，并在场景流估计中超越竞争对手，提高了3D EPE分数0.422米和Acc5分数28.02%。除了重建之外，我们展示了模型的四种额外应用，说明了自监督学习在更广泛的动态场景理解中的潜在价值。
2501.00599	 | VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM	 | Yuqian Yuan,Hang Zhang,Wentong Li,Zesen Cheng,Boqiang Zhang,Long Li,Xin Li,Deli Zhao,Wenqiao Zhang,Yueting Zhuang,Jianke Zhu,Lidong Bing	 | 该研究提出了VideoRefer套件，旨在增强视频大语言模型（Video LLMs）在细粒度空间-时间视频理解方面的能力，通过构建大规模高质量数据集VideoRefer-700K、开发多功能的空间-时间对象编码器的VideoRefer模型以及建立全面的基准测试VideoRefer-Bench来实现这一目标。	 | Video Large Language Models (Video LLMs) have recently exhibited remarkable capabilities in general video understanding. However, they mainly focus on holistic comprehension and struggle with capturing fine-grained spatial and temporal details. Besides, the lack of high-quality object-level video instruction data and a comprehensive benchmark further hinders their advancements. To tackle these challenges, we introduce the VideoRefer Suite to empower Video LLM for finer-level spatial-temporal video understanding, i.e., enabling perception and reasoning on any objects throughout the video. Specially, we thoroughly develop VideoRefer Suite across three essential aspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent data engine to meticulously curate a large-scale, high-quality object-level video instruction dataset, termed VideoRefer-700K. Next, we present the VideoRefer model, which equips a versatile spatial-temporal object encoder to capture precise regional and sequential representations. Finally, we meticulously create a VideoRefer-Bench to comprehensively assess the spatial-temporal understanding capability of a Video LLM, evaluating it across various aspects. Extensive experiments and analyses demonstrate that our VideoRefer model not only achieves promising performance on video referring benchmarks but also facilitates general video understanding capabilities.	 | 视频大语言模型（Video LLMs）近年来在通用视频理解方面展现了显著的能力。然而，它们主要关注整体理解，而在捕捉细粒度的空间和时间细节方面存在困难。此外，高质量的对象级视频指令数据的缺乏以及缺乏全面的基准数据集进一步阻碍了它们的发展。为了解决这些挑战，我们引入了VideoRefer套件，以增强Video LLM在更细粒度的空间-时间视频理解方面的能力，即在视频中的任何对象上实现感知和推理。特别地，我们从三个关键方面全面开发了VideoRefer套件：数据集、模型和基准。首先，我们介绍了一个多代理数据引擎，以精心维护一个大规模、高质量的对象级视频指令数据集，称为VideoRefer-700K。接下来，我们提出了VideoRefer模型，该模型配备了多功能的空间-时间对象编码器，以捕获精确的区域和序列表示。最后，我们精心构建了一个VideoRefer-Bench，以全面评估Video LLM的空间-时间理解能力，并从多个方面对其进行评估。广泛的实验和分析表明，我们的VideoRefer模型不仅在视频参考基准测试中取得了令人鼓舞的性能，而且还提升了通用视频理解能力。
2501.00597	 | Per Subject Complexity in Eye Movement Prediction	 | Kateryna Melnyk,Dmytro Katrychuk,Lee Friedman,Oleg Komogortsev	 | 本研究全面分析了眼动预测任务的复杂性，使用了三种不同模型（轻量级LSTM、基于变压器的TST和嵌入卡尔曼滤波器的OPKF），并发现不同模型对受试者的预测性能有相似的趋势，但某些受试者的数据对模型构成更大挑战，研究还探讨了这些个体复杂性的来源及潜在解决方案。	 | Eye movement prediction is a promising area of research to compensate for the latency introduced by eye-tracking systems in virtual reality devices. In this study, we comprehensively analyze the complexity of the eye movement prediction task associated with subjects. We use three fundamentally different models within the analysis: the lightweight Long Short-Term Memory network (LSTM), the transformer-based network for multivariate time series representation learning (TST), and the Oculomotor Plant Mathematical Model wrapped in the Kalman Filter framework (OPKF). Each solution is assessed following a sample-to-event evaluation strategy and employing the new event-to-subject metrics. Our results show that the different models maintained similar prediction performance trends pertaining to subjects. We refer to these outcomes as per-subject complexity since some subjects' data pose a more significant challenge for models. Along with the detailed correlation analysis, this report investigates the source of the per-subject complexity and discusses potential solutions to overcome it.	 | 眼动预测是补偿眼动追踪系统在虚拟现实设备中引入的延迟的一种有前景的研究领域。在本研究中，我们全面分析了与受试者相关的眼动预测任务的复杂性。我们在此分析中使用了三种根本不同的模型：轻量级长短期记忆网络（LSTM）、基于变压器的多变量时间序列表示学习网络（TST），以及嵌套在卡尔曼滤波器框架中的眼动植物数学模型（OPKF）。每一项解决方案都遵循样本到事件的评估策略，并采用新的事件到受试者的度量标准进行评估。我们的结果显示，不同模型在受试者的预测性能趋势上保持一致。我们将这些结果称为针对个体的复杂性，因为有些受试者的数据对模型构成了更大的挑战。除了详细的相关性分析外，本报告还探讨了针对个体复杂性的来源，并讨论了潜在的解决方案以克服这些问题。
2501.00586	 | Advanced Lung Nodule Segmentation and Classification for Early Detection of Lung Cancer using SAM and Transfer Learning	 | Asha V,Bhavanishankar K	 | 本研究提出了一种结合Segment Anything Model（SAM）和迁移学习的肺结节分割方法，有效提高了CT图像中肺结节的分割精度，并显著提升了肺癌诊断的计算机辅助检测系统性能。结果显示，该模型在分割和分类准确性方面均优于现有技术，具有重要的临床应用价值。	 | Lung cancer is an extremely lethal disease primarily due to its late-stage diagnosis and significant mortality rate, making it the major cause of cancer-related demises globally. Machine Learning (ML) and Convolution Neural network (CNN) based Deep Learning (DL) techniques are primarily used for precise segmentation and classification of cancerous nodules in the CT (Computed Tomography) or MRI images. This study introduces an innovative approach to lung nodule segmentation by utilizing the Segment Anything Model (SAM) combined with transfer learning techniques. Precise segmentation of lung nodules is crucial for the early detection of lung cancer. The proposed method leverages Bounding Box prompts and a vision transformer model to enhance segmentation performance, achieving high accuracy, Dice Similarity Coefficient (DSC) and Intersection over Union (IoU) metrics. The integration of SAM and Transfer Learning significantly improves Computer-Aided Detection (CAD) systems in medical imaging, particularly for lung cancer diagnosis. The findings demonstrate the proposed model effectiveness in precisely segmenting lung nodules from CT scans, underscoring its potential to advance early detection and improve patient care outcomes in lung cancer diagnosis. The results show SAM Model with transfer learning achieving a DSC of 97.08% and an IoU of 95.6%, for segmentation and accuracy of 96.71% for classification indicates that ,its performance is noteworthy compared to existing techniques.	 | 肺癌是一种极其致命的疾病，主要原因在于其晚期诊断以及较高的死亡率，使其成为全球癌症相关死亡的主要原因。机器学习（ML）和卷积神经网络（CNN）为基础的深度学习（DL）技术主要用于精确分割和分类CT（计算机断层扫描）或MRI图像中的癌性结节。本研究提出了一种新的肺结节分割方法，结合了Segment Anything Model（SAM）和迁移学习技术。精确分割肺结节对于早期发现肺癌至关重要。所提出的方法利用边界框提示和视觉变换器模型来增强分割性能，实现了高精度、Dice相似系数（DSC）和交并比（IoU）指标。结合SAM和迁移学习显著提高了医学影像中的计算机辅助检测（CAD）系统，特别是在肺癌诊断中的表现。研究结果表明，所提出的模型在精确分割CT扫描中的肺结节方面非常有效，突显了其在肺癌诊断中早期发现和改善患者护理结果的潜力。结果显示，结合迁移学习的SAM模型在分割上的DSC达到97.08%，IoU达到95.6%，分类准确率达到96.71%，表明其性能优于现有技术。
2501.00585	 | Sidewalk Hazard Detection Using Variational Autoencoder and One-Class SVM	 | Edgar Guzman,Robert D. Howe	 | 本文提出了一种结合变分自编码器（VAE）和单类支持向量机（OCSVM）的系统，用于检测人行道上的异常情况，从而保障行人的安全导航。该系统通过高准确率（AUC 达到 0.94）有效地区分了危险和非危险异常，并在部署中展现出高度可靠的性能。	 | The unpredictable nature of outdoor settings introduces numerous safety concerns, making hazard detection crucial for safe navigation. This paper introduces a novel system for sidewalk safety navigation utilizing a hybrid approach that combines a Variational Autoencoder (VAE) with a One-Class Support Vector Machine (OCSVM). The system is designed to detect anomalies on sidewalks that could potentially pose walking hazards. A dataset comprising over 15,000 training frames and 5,000 testing frames was collected using video recordings, capturing various sidewalk scenarios, including normal and hazardous conditions. During deployment, the VAE utilizes its reconstruction mechanism to detect anomalies within a frame. Poor reconstruction by the VAE implies the presence of an anomaly, after which the OCSVM is used to confirm whether the anomaly is hazardous or non-hazardous. The proposed VAE model demonstrated strong performance, with a high Area Under the Curve (AUC) of 0.94, effectively distinguishing anomalies that could be potential hazards. The OCSVM is employed to reduce the detection of false hazard anomalies, such as manhole or water valve covers. This approach achieves an accuracy of 91.4%, providing a highly reliable system for distinguishing between hazardous and non-hazardous scenarios. These results suggest that the proposed system offers a robust solution for hazard detection in uncertain environments.	 | 户外环境的不可预测性引入了大量安全问题，因此危害检测对于安全导航至关重要。本文提出了一种结合变分自编码器（VAE）和单类支持向量机（OCSVM）的混合方法的新型系统，用于人行道安全导航。该系统旨在检测可能构成步行隐患的人行道上的异常情况。通过视频录制收集了一个包含超过15,000个训练帧和5,000个测试帧的数据集，涵盖了正常和危险的各种人行道场景。在部署过程中，VAE 利用其重构机制检测帧内的异常。VAE 的重构效果不佳意味着存在异常，随后 OCSVM 用于确认该异常是否为危险或非危险异常。所提出的 VAE 模型表现出色，AUC 达到 0.94，有效地区分了可能构成隐患的异常。OCSVM 用于减少误报的异常检测，例如井盖或水阀盖。这种方法在准确率方面达到了 91.4%，提供了一个高度可靠的系统来区分危险和非危险场景。这些结果表明，所提出的系统为不确定环境中的危害检测提供了一个稳健的解决方案。
2501.00584	 | Online Video Understanding: A Comprehensive Benchmark and Memory-Augmented Method	 | Zhenpeng Huang,Xinhao Li,Jiaqi Li,Jing Wang,Xiangyu Zeng,Cheng Liang,Tao Wu,Xi Chen,Liang Li,Limin Wang	 | 本文针对多模态大型语言模型在自动驾驶和人机交互等实时场景中的应用挑战，提出了一种新的金字塔记忆库和离线到在线的学习范式，并开发了VideoChat-Online模型，该模型在流行的离线视频基准和OVBench上表现出色。	 | Multimodal Large Language Models (MLLMs) have shown significant progress in offline video understanding. However, applying these models to real-world scenarios, such as autonomous driving and human-computer interaction, presents unique challenges due to the need for real-time processing of continuous online video streams. To this end, this paper presents systematic efforts from three perspectives: evaluation benchmark, model architecture, and training strategy. First, we introduce OVBench, a comprehensive question-answering benchmark specifically designed to evaluate models' ability to perceive, memorize, and reason within online video contexts. It features six core task types across three temporal contexts-past, present, and future-forming 16 subtasks from diverse datasets. Second, we propose a new Pyramid Memory Bank (PMB) that effectively retains key spatiotemporal information in video streams. Third, we proposed an offline-to-online learning paradigm, designing an interleaved dialogue format for online video data and constructing an instruction-tuning dataset tailored for online video training. This framework led to the development of VideoChat-Online, a robust and efficient model for online video understanding. Despite the lower computational cost and higher efficiency, VideoChat-Online outperforms existing state-of-the-art offline and online models across popular offline video benchmarks and OVBench, demonstrating the effectiveness of our model architecture and training strategy.	 | 多模态大型语言模型（MLLMs）在离线视频理解方面已经取得了显著进展。然而，将这些模型应用于自动驾驶和人机交互等真实世界场景时，由于需要实时处理连续的在线视频流，因此面临着独特的挑战。为此，本文从三个维度进行了系统性研究：评价基准、模型架构和训练策略。首先，我们介绍了OVBench，这是一个全面的问题回答基准，专门用于评估模型在在线视频上下文中的感知、记忆和推理能力。它包括三个时间上下文（过去、现在、未来）下的六种核心任务类型，形成了16个子任务，这些子任务来自多个数据集。其次，我们提出了一种新的金字塔记忆库（PMB），能够有效地保留视频流中的关键时空信息。第三，我们提出了一个离线到在线的学习范式，设计了一种交错对话格式来处理在线视频数据，并构建了一个针对在线视频训练的指令调优数据集。该框架促成了VideoChat-Online模型的开发，这是一种用于在线视频理解的稳健且高效的模型。尽管计算成本较低且效率更高，VideoChat-Online在流行的离线视频基准和OVBench上仍然优于现有的离线和在线模型，这证明了我们模型架构和训练策略的有效性。
2501.00581	 | Causal Graph Guided Steering of LLM Values via Prompts and Sparse Autoencoders	 | Yipeng Kang,Junqi Wang,Yexin Li,Fangwei Zhong,Xue Feng,Mengmeng Wang,Wenming Tu,Quansen Wang,Hengli Li,Zilong Zheng	 | 本文提出了一种新的方法，通过挖掘因果图来揭示大型语言模型内部各种价值观之间的隐含关系，进而实现更有效和可控的价值引导，相比现有方法更加轻量且能够充分利用价值观之间的相关性。实验结果表明，该方法在Gemma-2B-IT和Llama3-8B-IT上表现出了有效性和可控性。	 | As large language models (LLMs) become increasingly integrated into critical applications, aligning their behavior with human values presents significant challenges. Current methods, such as Reinforcement Learning from Human Feedback (RLHF), often focus on a limited set of values and can be resource-intensive. Furthermore, the correlation between values has been largely overlooked and remains underutilized. Our framework addresses this limitation by mining a causal graph that elucidates the implicit relationships among various values within the LLMs. Leveraging the causal graph, we implement two lightweight mechanisms for value steering: prompt template steering and Sparse Autoencoder feature steering, and analyze the effects of altering one value dimension on others. Extensive experiments conducted on Gemma-2B-IT and Llama3-8B-IT demonstrate the effectiveness and controllability of our steering methods.	 | 随着大型语言模型（LLMs）在关键应用中的集成越来越广泛，使它们的行为与人类价值观保持一致面临着重大挑战。目前的方法，如从人类反馈中进行强化学习（RLHF），往往关注于少数几类价值观，并且资源消耗较大。此外，价值观之间的相关性已经被很大程度上忽视，并且尚未得到充分利用。我们的框架通过挖掘一个因果图来解决这一局限性，该因果图阐明了LLMs内部各种价值观之间的隐含关系。利用因果图，我们实现了两种轻量级的价值引导机制：提示模板引导和稀疏自动编码器特征引导，并分析了改变一个价值观维度对其他维度的影响。在Gemma-2B-IT和Llama3-8B-IT上的广泛实验表明，我们引导方法的有效性和可控性。
2501.00574	 | VideoChat-Flash: Hierarchical Compression for Long-Context Video Modeling	 | Xinhao Li,Yi Wang,Jiashuo Yu,Xiangyu Zeng,Yuhan Zhu,Haian Huang,Jianfei Gao,Kunchang Li,Yinan He,Chenting Wang,Yu Qiao,Yali Wang,Limin Wang	 | 本文提出了一种名为HiCo的分层视觉令牌压缩方法和VideoChat-Flash系统，通过压缩冗余视觉信息来提高多模态大规模语言模型处理长视频的能力，显著减少了计算量并在多种基准测试中表现出色，特别是在“视频中的细针”（NIAH）任务上达到了99.1%的高准确性。	 | Long-context modeling is a critical capability for multimodal large language models (MLLMs), enabling them to process long-form contents with implicit memorization. Despite its advances, handling extremely long videos remains challenging due to the difficulty in maintaining crucial features over extended sequences. This paper introduces a Hierarchical visual token Compression (HiCo) method designed for high-fidelity representation and a practical context modeling system VideoChat-Flash tailored for multimodal long-sequence processing. HiCo capitalizes on the redundancy of visual information in long videos to compress long video context from the clip-level to the video-level, reducing the compute significantly while preserving essential details. VideoChat-Flash features a multi-stage short-to-long learning scheme, a rich dataset of real-world long videos named LongVid, and an upgraded "Needle-In-A-video-Haystack" (NIAH) for evaluating context capacities. In extensive experiments, VideoChat-Flash shows the leading performance on both mainstream long and short video benchmarks at the 7B model scale. It firstly gets 99.1% accuracy over 10,000 frames in NIAH among open-source models.	 | 长上下文建模是多模态大规模语言模型（MLLMs）的一项关键能力，使它们能够处理长格式内容并隐式记忆。尽管取得了进展，但在处理非常长的视频时仍面临挑战，因为难以在整个序列中保持关键特征。本文介绍了HiCo（分层视觉令牌压缩）方法，这是一种用于高保真表示的分层视觉令牌压缩方法，以及针对多模态长序列处理定制的实用上下文建模系统VideoChat-Flash。HiCo利用长视频中的冗余视觉信息，将视频上下文从片段级压缩到视频级，显著减少了计算量，同时保留了关键细节。VideoChat-Flash包含一个多阶段的短到长学习方案、名为LongVid的真实世界长视频丰富数据集以及升级的“视频中的细针”（NIAH, Needle-In-A-video-Haystack）以评估上下文容量。在广泛的实验中，VideoChat-Flash在7B模型规模下的主流长视频和短视频基准测试中表现出卓越性能。它首次在NIAH中，在超过10,000帧的开源模型中达到了99.1%的准确性。
2501.00569	 | Probing Visual Language Priors in VLMs	 | Tiange Luo,Ang Cao,Gunhee Lee,Justin Johnson,Honglak Lee	 | 该研究引入了ViLP基准测试，旨在评估视觉语言模型的视觉推理能力，发现尽管人类可以高效完成任务，现代VLMs表现不佳；为此提出了一种自我改进框架，通过生成和破坏图像配对来提升模型的视觉推理能力。	 | Despite recent advances in Vision-Language Models (VLMs), many still over-rely on visual language priors present in their training data rather than true visual reasoning. To examine the situation, we introduce ViLP, a visual question answering (VQA) benchmark that pairs each question with three potential answers and three corresponding images: one image whose answer can be inferred from text alone, and two images that demand visual reasoning. By leveraging image generative models, we ensure significant variation in texture, shape, conceptual combinations, hallucinated elements, and proverb-based contexts, making our benchmark images distinctly out-of-distribution. While humans achieve near-perfect accuracy, modern VLMs falter; for instance, GPT-4 achieves only 66.17% on ViLP. To alleviate this, we propose a self-improving framework in which models generate new VQA pairs and images, then apply pixel-level and semantic corruptions to form "good-bad" image pairs for self-training. Our training objectives compel VLMs to focus more on actual visual inputs and have demonstrated their effectiveness in enhancing the performance of open-source VLMs, including LLaVA-v1.5 and Cambrian.	 | 尽管在视觉语言模型（VLMs）方面取得了一些进展，但许多模型仍然过度依赖训练数据中存在的视觉语言先验，而不是真正的视觉推理。为了考察这一情况，我们引入了ViLP，这是一个视觉问答（VQA）基准测试，每个问题与三个潜在答案和三个相应的图像配对：一个图像的答 案可以从文本中单独推断，而两个图像则需要视觉推理。通过利用图像生成模型，我们确保了纹理、形状、概念组合、幻觉元素和谚语基础背景方面的显著变化，使我们的基准图像具有明显的离群值特征。尽管人类可以接近完美地完成任务，但现代VLMs却表现不佳；例如，GPT-4在ViLP上的准确率仅为66.17%。为了缓解这一问题，我们提出了一种自我改进框架，在该框架中，模型生成新的VQA配对和图像，然后对图像进行像素级和语义破坏，形成“好-坏”图像配对，用于自我训练。我们的训练目标促使VLMs更关注实际的视觉输入，并已证明其在增强开源VLMs，包括LLaVA-v1.5和Cambrian等性能方面具有有效性。
2501.00565	 | Polynomial time sampling from log-smooth distributions in fixed dimension under semi-log-concavity of the forward diffusion with application to strongly dissipative distributions	 | Adrien Vacher,Omar Chehab,Anna Korba	 | 该研究提出了一种在固定维度上具有多项式复杂度的随机采样算法，该算法利用了扩散模型的进展，通过低成本估计器逼近中间得分来实现从特定类型的分布中采样，并证明了在特定假设条件下，算法在期望误差为$\varepsilon$时的时间复杂度为$O(d^7L^{d+2}\varepsilon^{-2(d+3)} (L+β)^2d^{2(d+1)})$，从而解决了球外为$\alpha$-强对数凹的$L$-对数光滑分布的采样问题并带来了指数级的复杂度改进。	 | In this article we provide a stochastic sampling algorithm with polynomial complexity in fixed dimension that leverages the recent advances on diffusion models where it is shown that under mild conditions, sampling can be achieved via an accurate estimation of intermediate scores across the marginals $(p_t)_{t\ge 0}$ of the standard Ornstein-Uhlenbeck process started at the density we wish to sample from. The heart of our method consists into approaching these scores via a computationally cheap estimator and relating the variance of this estimator to the smoothness properties of the forward process. Under the assumption that the density to sample from is $L$-log-smooth and that the forward process is semi-log-concave: $-\nabla^2 \log(p_t) \succeq -βI_d$ for some $β\geq 0$, we prove that our algorithm achieves an expected $ε$ error in $\text{KL}$ divergence in $O(d^7L^{d+2}ε^{-2(d+3)} (L+β)^2d^{2(d+1)})$ time. In particular, our result allows to fully transfer the problem of sampling from a log-smooth distribution into a regularity estimate problem. As an application, we derive an exponential complexity improvement for the problem of sampling from a $L$-log-smooth distribution that is $α$-strongly log-concave distribution outside some ball of radius $R$: after proving that such distributions verify the semi-log-concavity assumption, a result which might be of independent interest, we recover a $poly(R,L,α^{-1}, ε^{-1})$ complexity in fixed dimension which exponentially improves upon the previously known $poly(e^{RL^2}, L,α^{-1}, \log(ε^{-1}))$ complexity in the low precision regime.	 | 在这篇文章中，我们提供了一种在固定维度上具有多项式复杂度的随机采样算法，该算法利用了最近关于扩散模型的进展。在这些进展中，表明在温和条件下，可以通过对标准Ornstein-Uhlenbeck过程（从我们希望采样的密度出发）的边缘分布$(p_t)_{t\ge 0}$中的中间得分进行准确估计来实现采样。我们的方法的核心在于通过一个计算成本低廉的估计器来逼近这些得分，并将此估计器的方差与前向过程的光滑性特性联系起来。假设我们需要采样的密度是$L$-对数光滑的，并且前向过程是半对数凹的：$-\nabla^2 \log(p_t) \succeq -βI_d$，其中$β\geq 0$，我们证明我们的算法在$\text{KL}$散度上的期望误差为$\varepsilon$时，在$O(d^7L^{d+2}\varepsilon^{-2(d+3)} (L+β)^2d^{2(d+1)})$时间内可以实现。特别地，我们的结果使得从一个对数光滑分布中采样的问题可以完全转化为一个正则性估计问题。作为应用，我们为从一个在某个半径为$R$的球外为$\alpha$-强对数凹的$L$-对数光滑分布中采样的问题带来了指数级的复杂度改进：在证明此类分布满足半对数凹假设之后（这一结果可能具有独立兴趣），我们得到了一个固定维度下的$poly(R,L,α^{-1}, \varepsilon^{-1})$复杂度，这在低精度情况下比之前已知的$poly(e^{RL^2}, L,α^{-1}, \log(\varepsilon^{-1}))$复杂度有了指数级的改进。
2501.00560	 | Re-evaluating Automatic LLM System Ranking for Alignment with Human Preference	 | Mingqi Gao,Yixin Liu,Xinyu Hu,Xiaojun Wan,Jonathan Bragg,Arman Cohan	 | 该研究通过控制实验探讨了自动语言模型评估框架的关键组成部分（输入集、评估模型、评估类型和聚合方法）的选择策略，并发现当前框架在评估性能相近的语言模型时表现不佳，强调了需要对测评框架进行专门的系统级评估。	 | Evaluating and ranking the capabilities of different LLMs is crucial for understanding their performance and alignment with human preferences. Due to the high cost and time-consuming nature of human evaluations, an automatic LLM bencher (i.e., an automatic evaluation framework that aims to rank LLMs based on their alignment with human preferences) is indispensable. An automatic LLM bencher consists of four components: the input set (e.g., a user instruction), the evaluation model (e.g., an LLM), the evaluation type (e.g., pairwise comparison), and the aggregation method (e.g., the ELO rating system). However, previous work has not thoroughly explored how to select these components or how their different combinations influence the results. In this work, through controlled experiments, we provide a series of recommendations on how to choose each component to better automate the evaluation of LLMs. Furthermore, we discovered that when evaluating LLMs with similar performance, the performance of the automatic LLM bencher declines sharply, underscoring the limitations of current benchers and calling for future work. Lastly, we found that the evaluation models' performance at the instance level (e.g., the accuracy of selecting the best output) does not always align with their effectiveness when used as a component of a bencher, highlighting the importance of dedicated system-level evaluation of benchers.	 | 评估和排名不同语言模型（LLM）的能力对于理解其性能及其与人类偏好的一致性至关重要。由于人工评估成本高昂且耗时，因此自动LLM测评框架（即旨在根据人类偏好对LLM进行排名的自动评估框架）是必不可少的。一个自动LLM测评框架由四个组成部分构成：输入集（例如，用户指令）、评估模型（例如，LLM）、评估类型（例如，成对比较）和聚合方法（例如，ELO评分系统）。然而，先前的研究并没有充分探讨如何选择这些组成部分，或不同组合如何影响结果。在本项工作中，通过控制实验，我们提供了一系列关于如何选择每个组成部分以更好地自动化LLM评估的建议。此外，我们发现，当评估具有相似性能的LLM时，自动LLM测评框架的表现急剧下降，这揭示了当前测评框架的局限性，并呼吁未来的工作进行改进。最后，我们发现，评估模型在实例级别的性能（例如，选择最佳输出的准确性）并不总是与其用作测评框架组成部分时的有效性一致，这强调了对测评框架进行专门的系统级评估的重要性。
2501.00556	 | Finding the Underlying Viscoelastic Constitutive Equation via Universal Differential Equations and Differentiable Physics	 | Elias C. Rodrigues,Roney L. Thompson,Dário A.B. Oliveira,Roberto F. Ausas	 | 本研究利用通用微分方程（UDEs）和可微物理学建模黏弹性流体，并通过合成数据集分析了UCM、Johnson-Segalman、Giesekus和ePTT四种模型，结果显示UDEs在各类条件下都能有效预测剪切应力和正应力，但在ePTT模型上存在局限性；研究还采用了模型蒸馏方法提取简化模型，突显了UDEs在流体力学中的应用潜力。	 | This research employs Universal Differential Equations (UDEs) alongside differentiable physics to model viscoelastic fluids, merging conventional differential equations, neural networks and numerical methods to reconstruct missing terms in constitutive models. This study focuses on analyzing four viscoelastic models: Upper Convected Maxwell (UCM), Johnson-Segalman, Giesekus, and Exponential Phan-Thien-Tanner (ePTT), through the use of synthetic datasets. The methodology was tested across different experimental conditions, including oscillatory and startup flows. While the UDE framework effectively predicts shear and normal stresses for most models, it demonstrates some limitations when applied to the ePTT model. The findings underscore the potential of UDEs in fluid mechanics while identifying critical areas for methodological improvement. Also, a model distillation approach was employed to extract simplified models from complex ones, emphasizing the versatility and robustness of UDEs in rheological modeling.	 | 本研究采用通用微分方程（UDEs）和可微物理学来建模黏弹性流体，结合传统微分方程、神经网络和数值方法，以重建构成模型中的缺失项。本研究主要通过合成数据集分析了四种黏弹性模型：上科恩氏麦克斯韦（UCM）、约翰逊-塞甘（Johnson-Segalman）、盖斯库斯（Giesekus）和指数芬-廷纳-坦纳（ePTT）。该方法在不同的实验条件下进行了测试，包括振荡流和启动流。虽然UDE框架在大多数模型中有效预测了剪切应力和正应力，但在应用于ePTT模型时表现出一定的局限性。研究结果突显了UDEs在流体力学中的应用潜力，并指出了方法改进的关键领域。此外，还采用模型蒸馏方法从复杂模型中提取简化模型，强调了UDEs在 rheological 模型中的灵活性和稳健性。
2501.00539	 | MCP-Solver: Integrating Language Models with Constraint Programming Systems	 | Stefan Szeider	 | MCP-Solver 是一种将大型语言模型与约束编程系统集成的原型实现，旨在通过确保模型一致性并在每次修改步骤中维护持久化的建模见解知识库，有效结合自然语言理解和约束求解能力。这项工作展示了形式推理系统与LLMs集成的可行性，并为自然语言处理与基于约束的推理的集成迈出了重要一步。	 | While Large Language Models (LLMs) perform exceptionally well at natural language tasks, they often struggle with precise formal reasoning and the rigorous specification of problems. We present MCP-Solver, a prototype implementation of the Model Context Protocol that demonstrates the potential for systematic integration between LLMs and constraint programming systems. Our implementation provides interfaces for the creation, editing, and validation of a constraint model. Through an item-based editing approach with integrated validation, the system ensures model consistency at every modification step and enables structured iterative refinement. The system handles concurrent solving sessions and maintains a persistent knowledge base of modeling insights. Initial experiments suggest that this integration can effectively combine LLMs' natural language understanding with constraint-solving capabilities. Our open-source implementation is proof of concept for integrating formal reasoning systems with LLMs through standardized protocols. While further research is needed to establish comprehensive formal guarantees, this work takes a first step toward principled integration of natural language processing with constraint-based reasoning.	 | 尽管大型语言模型（LLMs）在自然语言任务上表现优异，但在精确的形式推理和问题的严格规范方面常常面临挑战。我们提出了MCP-Solver，这是一种模型上下文协议的原型实现，展示了LLMs与约束编程系统系统性集成的潜力。我们的实现提供了创建、编辑和验证约束模型的接口。通过基于项的编辑方法并集成验证，系统在每次修改步骤中确保模型一致性，并允许结构化的迭代细化。该系统能够处理并行求解会话，并维护一个持久化的建模见解知识库。初步实验表明，这种集成可以有效结合LLMs的自然语言理解和约束求解能力。我们的开源实现证明了通过标准化协议将形式推理系统与LLMs进行集成的可行性。虽然还需要进一步的研究来建立全面的形式保证，但这项工作为自然语言处理与基于约束的推理原则性集成迈出了第一步。
2501.00537	 | Extending XReason: Formal Explanations for Adversarial Detection	 | Amira Jemaa,Adnan Rashid,Sofiene Tahar	 | 该研究扩展了XReason工具，使其能够为LightGBM模型生成正式的实例级和类级解释，并且实现了对抗样本的生成和检测机制，评估结果显示该方法在CICIDS-2017数据集上的效率和准确性较高。	 | Explainable Artificial Intelligence (XAI) plays an important role in improving the transparency and reliability of complex machine learning models, especially in critical domains such as cybersecurity. Despite the prevalence of heuristic interpretation methods such as SHAP and LIME, these techniques often lack formal guarantees and may produce inconsistent local explanations. To fulfill this need, few tools have emerged that use formal methods to provide formal explanations. Among these, XReason uses a SAT solver to generate formal instance-level explanation for XGBoost models. In this paper, we extend the XReason tool to support LightGBM models as well as class-level explanations. Additionally, we implement a mechanism to generate and detect adversarial examples in XReason. We evaluate the efficiency and accuracy of our approach on the CICIDS-2017 dataset, a widely used benchmark for detecting network attacks.	 | 可解释的人工智能（Explainable Artificial Intelligence, XAI）在提高复杂机器学习模型的透明度和可靠性方面发挥着重要作用，尤其是在网络安全等关键领域。尽管存在诸如SHAP和LIME等启发式解释方法，但这些技术往往缺乏正式保证，可能会产生不一致的地方性解释。为了满足这一需求，一些工具开始使用正式方法来提供正式解释。其中，XReason使用SAT求解器为XGBoost模型生成正式实例级解释。在本文中，我们扩展了XReason工具，使其支持LightGBM模型以及类级解释。此外，我们实现了一种机制来生成和检测XReason中的对抗样本。我们在CICIDS-2017数据集上评估了我们方法的效率和准确性，该数据集是广泛用于检测网络攻击的基准数据集。
2501.00527	 | Exploiting Boundary Loss for the Hierarchical Panoptic Segmentation of Plants and Leaves	 | Madeleine Darbyshire,Elizabeth Sklar,Simon Parsons	 | 该研究提出了一种分层全景分割方法，结合焦点损失和边界损失来精确识别作物叶片和杂草，以优化精准农业中的资源使用和环境影响。该方法不仅提高了小实例分割性能，在标准数据集上的PQ+达到81.89，还增强了叶片计数的准确性。	 | Precision agriculture leverages data and machine learning so that farmers can monitor their crops and target interventions precisely. This enables the precision application of herbicide only to weeds, or the precision application of fertilizer only to undernourished crops, rather than to the entire field. The approach promises to maximize yields while minimizing resource use and harm to the surrounding environment. To this end, we propose a hierarchical panoptic segmentation method that simultaneously determines leaf count (as an identifier of plant growth)and locates weeds within an image. In particular, our approach aims to improve the segmentation of smaller instances like the leaves and weeds by incorporating focal loss and boundary loss. Not only does this result in competitive performance, achieving a PQ+ of 81.89 on the standard training set, but we also demonstrate we can improve leaf-counting accuracy with our method. The code is available at https://github.com/madeleinedarbyshire/HierarchicalMask2Former.	 | 精准农业利用数据和机器学习技术，使农民能够精确监测作物并精准实施干预措施。这种方法允许仅对杂草施用除草剂，或将肥料仅施用到营养不足的作物上，而不是整个田地。该方法承诺在最大限度提高产量的同时，最大限度地减少资源使用和对周围环境的损害。为此，我们提出了一种分层全景分割方法，该方法同时确定叶片数量（作为植物生长的标识符）并定位图像中的杂草。特别是，我们的方法旨在通过结合焦点损失和边界损失来提高较小实例（如叶片和杂草）的分割性能。这不仅导致了竞争力的表现，实现了标准训练集上的PQ+为81.89，而且我们还证明可以通过我们的方法提高叶片计数的准确性。相关代码可在https://github.com/madeleinedarbyshire/HierarchicalMask2Former获取。
2501.00520	 | Innovative Silicosis and Pneumonia Classification: Leveraging Graph Transformer Post-hoc Modeling and Ensemble Techniques	 | Bao Q. Bui,Tien T.T. Nguyen,Duy M. Le,Cong Tran,Cuong Pham	 | 本文创建了一个专门用于矽肺相关肺炎症检测的胸片数据集SVBCX，并提出了一种结合图变换网络和传统深度神经网络的新架构，以及平衡交叉熵损失函数，以提高矽肺和肺炎分类的准确性和可靠性。实验结果表明，该方法在宏F1分数和AUC ROC分数上均显著优于基线模型。	 | This paper presents a comprehensive study on the classification and detection of Silicosis-related lung inflammation. Our main contributions include 1) the creation of a newly curated chest X-ray (CXR) image dataset named SVBCX that is tailored to the nuances of lung inflammation caused by distinct agents, providing a valuable resource for silicosis and pneumonia research community; and 2) we propose a novel deep-learning architecture that integrates graph transformer networks alongside a traditional deep neural network module for the effective classification of silicosis and pneumonia. Additionally, we employ the Balanced Cross-Entropy (BalCE) as a loss function to ensure more uniform learning across different classes, enhancing the model's ability to discern subtle differences in lung conditions. The proposed model architecture and loss function selection aim to improve the accuracy and reliability of inflammation detection, particularly in the context of Silicosis. Furthermore, our research explores the efficacy of an ensemble approach that combines the strengths of diverse model architectures. Experimental results on the constructed dataset demonstrate promising outcomes, showcasing substantial enhancements compared to baseline models. The ensemble of models achieves a macro-F1 score of 0.9749 and AUC ROC scores exceeding 0.99 for each class, underscoring the effectiveness of our approach in accurate and robust lung inflammation classification.	 | 本文详细研究了矽肺相关肺炎症的分类与检测。我们的主要贡献包括：1）创建了一个专门为不同致病因子引起的肺炎症特点定制的胸片（CXR）图像数据集，名为SVBCX，为矽肺和肺炎研究社区提供了宝贵的资源；以及2）提出了一种新的深度学习架构，该架构将图变换网络与传统的深度神经网络模块相结合，以有效地对矽肺和肺炎进行分类。此外，我们采用了平衡交叉熵（BalCE）作为损失函数，以确保不同类别的均匀学习，从而提高模型识别肺部状况细微差异的能力。所提出的模型架构和损失函数选择旨在提高炎症检测的准确性和可靠性，特别是在矽肺的背景下。此外，我们的研究探讨了结合多种模型架构优点的集成方法的有效性。在构建的数据集上进行的实验结果显示了有希望的结果，显示出与基线模型相比，显著的改进。集成模型在每类上的宏F1分数达到0.9749，AUC ROC分数超过0.99，进一步证明了我们方法在准确和稳健的肺部炎症分类中的有效性。
2501.00514	 | H-Net: A Multitask Architecture for Simultaneous 3D Force Estimation and Stereo Semantic Segmentation in Intracardiac Catheters	 | Pedram Fekri,Mehrdad Zadeh,Javad Dargahi	 | 该研究提出了一种新颖的轻量级多输入多输出编码器-解码器架构，能够同时从两个视角分割医疗导管并在三维空间中估计施加的力，这是首次整合力估计与导管分割的端到端模型。该模型利用双平面X射线图像，通过共享参数的并行子网络和立体视觉技术实现高效力测量与高精度分割。	 | The success rate of catheterization procedures is closely linked to the sensory data provided to the surgeon. Vision-based deep learning models can deliver both tactile and visual information in a sensor-free manner, while also being cost-effective to produce. Given the complexity of these models for devices with limited computational resources, research has focused on force estimation and catheter segmentation separately. However, there is a lack of a comprehensive architecture capable of simultaneously segmenting the catheter from two different angles and estimating the applied forces in 3D. To bridge this gap, this work proposes a novel, lightweight, multi-input, multi-output encoder-decoder-based architecture. It is designed to segment the catheter from two points of view and concurrently measure the applied forces in the x, y, and z directions. This network processes two simultaneous X-Ray images, intended to be fed by a biplane fluoroscopy system, showing a catheter's deflection from different angles. It uses two parallel sub-networks with shared parameters to output two segmentation maps corresponding to the inputs. Additionally, it leverages stereo vision to estimate the applied forces at the catheter's tip in 3D. The architecture features two input channels, two classification heads for segmentation, and a regression head for force estimation through a single end-to-end architecture. The output of all heads was assessed and compared with the literature, demonstrating state-of-the-art performance in both segmentation and force estimation. To the best of the authors' knowledge, this is the first time such a model has been proposed	 | 医疗导管操作的成功率与提供的感觉数据密切相关。基于视觉的深度学习模型能够在不依赖传感器的情况下同时提供触觉和视觉信息，且成本相对较低。由于这些模型对计算资源有限的设备来说复杂度较高，研究主要集中在分别进行力估计和导管分割。然而，目前缺乏一种能够同时从两个角度分割导管并在三维空间中估计施加力的综合架构。为填补这一空白，本研究提出了一种新颖的、轻量级的、多输入多输出的编码器-解码器架构。该架构旨在从两个视角分割导管，并同时测量在 x、y 和 z 方向上的施加力。该网络处理两个同时的 X 射线图像，旨在由双平面透视系统提供，分别从不同角度显示导管的弯曲情况。该网络利用两个具有共享参数的并行子网络输出两个分割图，对应于输入。此外，该架构利用立体视觉估计导管尖端在三维空间中的施加力。该架构具有两个输入通道、两个用于分割的分类头以及一个用于通过单个端到端架构进行力估计的回归头。所有头的输出均进行了评估和比较，显示出在分割和力估计方面的先进性能。据作者所知，这是首次提出此类模型。
2501.00513	 | Fine-grained Video-Text Retrieval: A New Benchmark and Method	 | Yifan Xu,Xinhao Li,Yichun Yang,Rui Huang,Limin Wang	 | 为了评估视频-语言模型在精细粒度检索中的能力，研究提出了FIBER基准，该基准包含1,000个来自FineAction数据集的视频，并提供了详细的空间和时间注释。实验表明，新的Video Large Language Encoder (VLLE) 在传统基准上表现与CLIP相当，且具有更低的空间-时间偏见和更强的精细粒度表示能力。	 | The ability of perceiving fine-grained spatial and temporal information is crucial for video-language retrieval. However, the existing video retrieval benchmarks, such as MSRVTT and MSVD, fail to efficiently evaluate the fine-grained retrieval ability of video-language models (VLMs) due to a lack of detailed annotations. To address this problem, we present FIBER, a FIne-grained BEnchmark for text to video Retrieval, containing 1,000 videos sourced from the FineAction dataset. Uniquely, our FIBER benchmark provides detailed human-annotated spatial annotations and temporal annotations for each video, making it possible to independently evaluate the spatial and temporal bias of VLMs on video retrieval task. Besides, we employ a text embedding method to unlock the capability of fine-grained video-language understanding of Multimodal Large Language Models (MLLMs). Surprisingly, the experiment results show that our Video Large Language Encoder (VLLE) performs comparably to CLIP-based models on traditional benchmarks and has a stronger capability of fine-grained representation with lower spatial-temporal bias. Project page: https://fiber-bench.github.io.	 | 感知精细时空信息的能力对于视频-语言检索至关重要。然而，现有的视频检索基准，如MSRVTT和MSVD，由于缺乏详细的注释，无法有效地评估视频-语言模型（VLMs）的精细粒度检索能力。为了解决这个问题，我们提出了FIBER（FIne-grained BEnchmark for text to video Retrieval），该基准包含来自FineAction数据集的1,000个视频。特别地，我们的FIBER基准为每个视频提供了详细的人工标注的空间注释和时间注释，使得可以独立评估VLMs在视频检索任务中的空间和时间偏见。此外，我们使用了一种文本嵌入方法，以解锁多模态大规模语言模型（MLLMs）对细粒度视频-语言理解的能力。令人惊讶的是，实验结果表明，我们的Video Large Language Encoder（VLLE）在传统基准上的表现与CLIP基模型相当，且具有较低的空间-时间偏见和更强的精细粒度表示能力。项目页面：https://fiber-bench.github.io。
2501.00461	 | Efficient support ticket resolution using Knowledge Graphs	 | Sherwin Varghese,James Tian	 | 通过对16万个客户案例的回顾，发现90%的时间用于解决10%具有挑战性的问题，这些问题往往需要多位工程师协作或转交开发支持。研究提出了一种基于知识图嵌入的新方法，通过多数据源建模来改进排序学习任务，显著提升了工程师推荐系统的性能。	 | A review of over 160,000 customer cases indicates that about 90% of time is spent by the product support for solving around 10% of subset of tickets where a trivial solution may not exist. Many of these challenging cases require the support of several engineers working together within a "swarm", and some also need to go to development support as bugs. These challenging customer issues represent a major opportunity for machine learning and knowledge graph that identifies the ideal engineer / group of engineers(swarm) that can best address the solution, reducing the wait times for the customer. The concrete ML task we consider here is a learning-to-rank(LTR) task that given an incident and a set of engineers currently assigned to the incident (which might be the empty set in the non-swarming context), produce a ranked list of engineers best fit to help resolve that incident. To calculate the rankings, we may consider a wide variety of input features including the incident description provided by the customer, the affected component(s), engineer ratings of their expertise, knowledge base article text written by engineers, response to customer text written by engineers, and historic swarming data. The central hypothesis test is that by including a holistic set of contextual data around which cases an engineer has solved, we can significantly improve the LTR algorithm over benchmark models. The article proposes a novel approach of modelling Knowledge Graph embeddings from multiple data sources, including the swarm information. The results obtained proves that by incorporating this additional context, we can improve the recommendations significantly over traditional machine learning methods like TF-IDF.	 | 对超过16万个客户案例的回顾表明，大约90%的时间被产品支持团队用于解决大约10%的子集票务问题，其中可能不存在简单的解决方案。许多具有挑战性的案例需要多位工程师共同协作的“蜂群”支持，有些还需要将问题转交给开发支持，因为可能存在缺陷。这些具有挑战性的客户问题代表了机器学习和知识图谱的一个重大机会，这些技术能够识别最合适的工程师（或一组工程师，即“蜂群”），从而最好地解决这些问题，从而减少客户的等待时间。我们在这里考虑的具体机器学习任务是排序学习（Learning-to-Rank，LTR）任务，给定一个事件和当前分配给该事件的一组工程师（在非蜂群化情景中可能是空集），生成一个最佳工程师列表，他们最适合解决该事件。为了计算排名，我们可以考虑多种输入特征，包括客户提供的事件描述、受影响的组件、工程师的专长评级、工程师编写的知识库文章文本、工程师编写的客户回复文本，以及历史蜂群化数据。核心假设测试是通过包括关于某个工程师解决了哪些案件的全面上下文数据，我们可以显著改进LTR算法相对于基准模型的表现。文章提出了一种新的方法，即从多个数据源（包括蜂群信息）建模知识图嵌入。实验结果证明，通过引入这种额外的上下文，我们可以在传统机器学习方法（如TF-IDF）上显著提高推荐系统的性能。
2501.00452	 | Unrolled Creative Adversarial Network For Generating Novel Musical Pieces	 | Pratik Nag	 | 本文探讨了使用生成对抗网络（GANs）及其变体Creative Adversarial Networks（CAN）生成创意音乐的方法，其中CAN通过学习不同作曲家的风格来生成新颖的音乐作品，以克服传统GAN在创意输出方面的局限性。	 | Music generation has been established as a prominent topic in artificial intelligence and machine learning over recent years. In most recent works on RNN-based neural network methods have been applied for sequence generation. In contrast, generative adversarial networks (GANs) and their counterparts have been explored by very few researchersfor music generation.   In this paper, a classical system was employed alongside a new system to generate creative music. Both systems were designed based on adversarial networks to generate music by learning from examples. The classical system was trained to learn a set of music pieces without differentiating between classes, whereas the new system was trained to learn the different composers and their styles to generate a creative music piece by deviating from the learned composers' styles.   The base structure utilized was generative adversarial networks (GANs), which are capable of generating novel outputs given a set of inputs to learn from and mimic their distribution. It has been shown in previous work that GANs are limited in their original design with respect to creative outputs. Building on the Creative Adversarial Networks (CAN) , this work applied them in the music domain rather than the visual art domain. Additionally, unrolled CAN was introduced to prevent mode collapse. Experiments were conducted on both GAN and CAN for generating music, and their capabilities were measured in terms of deviation from the input set.	 | 音乐生成在过去几年中已经成为人工智能和机器学习领域的突出话题。在最近的工作中，基于循环神经网络（RNN）的神经网络方法被广泛应用于序列生成。相比之下，生成对抗网络（GANs）及其变体在音乐生成方面的研究却很少。在本文中，我们采用了一个传统的系统和一个新系统来生成创意音乐。这两个系统都基于对抗网络设计，通过学习示例来生成音乐。传统的系统被训练学习一组音乐作品，而不会区分不同类别；而新的系统则被训练学习不同的作曲家及其风格，从而生成一种新的、创意性的音乐作品，偏离已学习的作曲家风格。  所采用的基本结构是生成对抗网络（GANs），这种网络能够在给定一组输入数据以学习和模仿其分布的情况下产生新颖的输出。先前的研究表明，GANs在创意输出方面存在局限性。在此基础上，本文将Creative Adversarial Networks（CAN）应用于音乐领域，而不是视觉艺术领域。此外，引入了展开的CAN来防止模式崩溃。我们分别在GAN和CAN上进行了生成音乐的实验，并从输入集的偏差角度评估了它们的能力。
2501.00437	 | Unleashing Text-to-Image Diffusion Prior for Zero-Shot Image Captioning	 | Jianjie Luo,Jingwen Chen,Yehao Li,Yingwei Pan,Jianlin Feng,Hongyang Chao,Ting Yao	 | 该研究提出了一种局部跨模态特征混合机制（PCM），通过自适应地减轻合成图像中的缺陷细节影响，显著提升了零样本图像字幕生成的性能。实验结果显示，PCM-Net在MSCOCO和Flickr30k数据集上优于现有最先进的视觉-语言模型，并且在零样本图像字幕生成任务中排名第一。	 | Recently, zero-shot image captioning has gained increasing attention, where only text data is available for training. The remarkable progress in text-to-image diffusion model presents the potential to resolve this task by employing synthetic image-caption pairs generated by this pre-trained prior. Nonetheless, the defective details in the salient regions of the synthetic images introduce semantic misalignment between the synthetic image and text, leading to compromised results. To address this challenge, we propose a novel Patch-wise Cross-modal feature Mix-up (PCM) mechanism to adaptively mitigate the unfaithful contents in a fine-grained manner during training, which can be integrated into most of encoder-decoder frameworks, introducing our PCM-Net. Specifically, for each input image, salient visual concepts in the image are first detected considering the image-text similarity in CLIP space. Next, the patch-wise visual features of the input image are selectively fused with the textual features of the salient visual concepts, leading to a mixed-up feature map with less defective content. Finally, a visual-semantic encoder is exploited to refine the derived feature map, which is further incorporated into the sentence decoder for caption generation. Additionally, to facilitate the model training with synthetic data, a novel CLIP-weighted cross-entropy loss is devised to prioritize the high-quality image-text pairs over the low-quality counterparts. Extensive experiments on MSCOCO and Flickr30k datasets demonstrate the superiority of our PCM-Net compared with state-of-the-art VLMs-based approaches. It is noteworthy that our PCM-Net ranks first in both in-domain and cross-domain zero-shot image captioning. The synthetic dataset SynthImgCap and code are available at https://jianjieluo.github.io/SynthImgCap.	 | 最近，零样本图像字幕生成领域获得了越来越多的关注，其中仅在训练时可用文本数据。文本到图像的扩散模型的显著进展表明，可以通过使用由预训练先验生成的合成图像-字幕对来解决这一任务。然而，合成图像中显著区域的缺陷细节会在合成图像与文本之间引入语义对齐问题，导致生成结果质量下降。为了解决这一挑战，我们提出了一种新颖的局部跨模态特征混合机制（Patch-wise Cross-modal feature Mix-up, PCM），该机制在训练过程中以精细的方式自适应地减轻不忠实内容的影响，可以集成到大多数编码器-解码器框架中，引入了我们的PCM-Net。具体而言，对于每个输入图像，我们首先在CLIP空间中考虑图像-文本相似性，检测图像中的显著视觉概念。接着，根据这些显著视觉概念的文本特征，选择性地融合输入图像的局部视觉特征，生成一个带有更少缺陷内容的混合特征图。最后，采用视觉-语义编码器来细化提取的特征图，将其进一步融入句子解码器以生成字幕。此外，为了方便使用合成数据训练模型，我们设计了一种新颖的CLIP加权交叉熵损失，以优先考虑高质量的图像-文本对，而非低质量的图像-文本对。在MSCOCO和Flickr30k数据集上的广泛实验表明，我们的PCM-Net在与现有最先进的视觉-语言模型（VLM）相比时表现优越。值得注意的是，我们的PCM-Net在领域内和跨领域的零样本图像字幕生成中均排名第一。合成数据集SynthImgCap及其代码可在https://jianjieluo.github.io/SynthImgCap 获取。
2501.00432	 | OV-HHIR: Open Vocabulary Human Interaction Recognition Using Cross-modal Integration of Large Language Models	 | Lala Shakti Swarup Ray,Bo Zhou,Sungho Suh,Paul Lukowicz	 | 本文提出了一种基于开放词汇的人类间互动识别（OV-HHIR）框架，该框架利用大型语言模型生成开放性的文本描述，识别已知和未知的互动，克服了传统系统固定词汇和预定义标签的限制，并通过创建大规模数据集作为基准，证明了其在视频理解上的优越性。	 | Understanding human-to-human interactions, especially in contexts like public security surveillance, is critical for monitoring and maintaining safety. Traditional activity recognition systems are limited by fixed vocabularies, predefined labels, and rigid interaction categories that often rely on choreographed videos and overlook concurrent interactive groups. These limitations make such systems less adaptable to real-world scenarios, where interactions are diverse and unpredictable. In this paper, we propose an open vocabulary human-to-human interaction recognition (OV-HHIR) framework that leverages large language models to generate open-ended textual descriptions of both seen and unseen human interactions in open-world settings without being confined to a fixed vocabulary. Additionally, we create a comprehensive, large-scale human-to-human interaction dataset by standardizing and combining existing public human interaction datasets into a unified benchmark. Extensive experiments demonstrate that our method outperforms traditional fixed-vocabulary classification systems and existing cross-modal language models for video understanding, setting the stage for more intelligent and adaptable visual understanding systems in surveillance and beyond.	 | 理解人类之间的互动，尤其是在公共安全监控等情境中，对于监视和维持安全至关重要。传统的活动识别系统受到固定词汇、预定义标签和刚性互动类别等限制，这些类别往往依赖于编排好的视频，并且忽略了同时进行的互动群体。这些限制使得这些系统在实际场景中不够灵活，因为实际中的互动既多样又不可预测。在本文中，我们提出了一种基于开放词汇的人类间互动识别（OV-HHIR）框架，该框架利用大型语言模型生成开放性的文本描述，不仅涵盖了已知的互动，还包括未知的互动，而不受固定词汇的限制。此外，我们通过标准化和结合现有的公共人类互动数据集，创建了一个全面的大规模人类间互动数据集，作为统一的基准。广泛的实验表明，我们的方法在视频理解方面优于传统的固定词汇分类系统和现有的跨模态语言模型，为在监控以及其他领域实现更智能和适应性强的视觉理解系统奠定了基础。
2501.00426	 | B2Net: Camouflaged Object Detection via Boundary Aware and Boundary Fusion	 | Junmin Cai,Han Sun,Ningzhong Liu	 | 本文提出了一种名为B2Net的新网络，通过在不同阶段重新利用边界感知模块来增强边界的准确性，并结合低级和高级特征信息以提高伪装目标检测的性能。实验结果表明，B2Net在三个基准数据集上的表现显著优于现有方法。	 | Camouflaged object detection (COD) aims to identify objects in images that are well hidden in the environment due to their high similarity to the background in terms of texture and color. However, existing most boundary-guided camouflage object detection algorithms tend to generate object boundaries early in the network, and inaccurate edge priors often introduce noises in object detection. Address on this issue, we propose a novel network named B2Net aiming to enhance the accuracy of obtained boundaries by reusing boundary-aware modules at different stages of the network. Specifically, we present a Residual Feature Enhanced Module (RFEM) with the goal of integrating more discriminative feature representations to enhance detection accuracy and reliability. After that, the Boundary Aware Module (BAM) is introduced to explore edge cues twice by integrating spatial information from low-level features and semantic information from high-level features. Finally, we design the Cross-scale Boundary Fusion Module(CBFM) that integrate information across different scales in a top-down manner, merging boundary features with object features to obtain a comprehensive feature representation incorporating boundary information. Extensive experimental results on three challenging benchmark datasets demonstrate that our proposed method B2Net outperforms 15 state-of-art methods under widely used evaluation metrics. Code will be made publicly available.	 | 伪装目标检测（COD）旨在识别由于在纹理和颜色方面与背景高度相似而在环境中被很好地隐藏的对象。然而，现有的大多数基于边界引导的伪装目标检测算法倾向于在网络早期生成目标边界，不准确的边缘先验往往会在目标检测中引入噪声。针对这一问题，我们提出了一种名为B2Net的新型网络，旨在通过在网络的不同阶段重新利用边界感知模块来增强获得边界的准确性。具体而言，我们提出了一个残差特征增强模块（RFEM），目标是整合更多有区别的特征表示以提高检测准确性和可靠性。之后，我们引入了一个边界感知模块（BAM），通过结合低级特征的空间信息和高级特征的语义信息，两次探索边缘线索。最后，我们设计了一个跨尺度边界融合模块（CBFM），以自上而下的方式整合不同尺度的信息，将边界特征与对象特征相结合，以获得包含边界信息的综合特征表示。在三个具有挑战性的基准数据集上进行的广泛实验结果表明，我们提出的方法B2Net在广泛使用的评估指标下优于15种最先进的方法。代码将公开发布。
2501.00421	 | Outlier-Robust Linear System Identification Under Heavy-tailed Noise	 | Vinay Kanakeri,Aritra Mitra	 | 该研究在假设噪声仅具有有限四阶矩的情况下，通过开发一种新的鲁棒系统识别算法，估计线性时不变系统的状态转移矩阵，并证明了其复杂性界接近于次高斯噪声下的界。工作还表明噪声的峰态影响实现期望估计误差所需的轨迹数量，并展示了方法对部分数据被对手篡改情况的鲁棒性。	 | We consider the problem of estimating the state transition matrix of a linear time-invariant (LTI) system, given access to multiple independent trajectories sampled from the system. Several recent papers have conducted a non-asymptotic analysis of this problem, relying crucially on the assumption that the process noise is either Gaussian or sub-Gaussian, i.e., "light-tailed". In sharp contrast, we work under a significantly weaker noise model, assuming nothing more than the existence of the fourth moment of the noise distribution. For this setting, we provide the first set of results demonstrating that one can obtain sample-complexity bounds for linear system identification that are nearly of the same order as under sub-Gaussian noise. To achieve such results, we develop a novel robust system identification algorithm that relies on constructing multiple weakly-concentrated estimators, and then boosting their performance using suitable tools from high-dimensional robust statistics. Interestingly, our analysis reveals how the kurtosis of the noise distribution, a measure of heavy-tailedness, affects the number of trajectories needed to achieve desired estimation error bounds. Finally, we show that our algorithm and analysis technique can be easily extended to account for scenarios where an adversary can arbitrarily corrupt a small fraction of the collected trajectory data. Our work takes the first steps towards building a robust statistical learning theory for control under non-ideal assumptions on the data-generating process.	 | 我们研究了在可以访问系统生成的多个独立轨迹的情况下估计线性时不变（LTI）系统的状态转移矩阵的问题。几篇最近的论文对这一问题进行了非渐近分析，关键假设是过程噪声要么是高斯分布要么是次高斯分布，即“轻尾”分布。与此形成鲜明对比的是，我们采用了一个显著更弱的噪声模型，仅仅假设噪声分布的四阶矩存在。在这种设置下，我们提供了第一个结果集，表明可以得到的状态复杂性界与次高斯噪声下的界几乎相同数量级。为了实现这些结果，我们开发了一种新颖的鲁棒系统识别算法，该算法依赖于构建多个弱集中估计器，然后通过高维鲁棒统计中的适当工具来提升这些估计器的性能。有趣的是，我们的分析揭示了噪声分布的峰态（衡量重尾程度的指标）如何影响实现期望估计误差边界所需的轨迹数量。最后，我们展示了我们的算法和分析技术可以很容易地扩展以适应收集的数据中有一部分数据由对手任意篡改的情况。我们的工作朝着在不理想的生成数据假设下构建控制的稳健统计学习理论迈出了第一步。
2501.00398	 | TSPE: Task-Specific Prompt Ensemble for Improved Zero-Shot Audio Classification	 | Nishit Anand,Ashish Seth,Ramani Duraiswami,Dinesh Manocha	 | TSPE（任务特定提示集合）通过为不同音频分类任务定制上下文丰富的提示，显著提升了语音-语言模型（ALMs）在零样本音频分类任务中的性能，相较于通用模板提示，TSPE提升了1.23%-16.36%的零样本识别准确率。	 | Audio-language models (ALMs) excel in zero-shot audio classification, a task where models classify previously unseen audio clips at test time by leveraging descriptive natural language prompts. We introduce TSPE (Task-Specific Prompt Ensemble), a simple, training-free hard prompting method that boosts ALEs' zero-shot performance by customizing prompts for diverse audio classification tasks. Rather than using generic template-based prompts like "Sound of a car" we generate context-rich prompts, such as "Sound of a car coming from a tunnel". Specifically, we leverage label information to identify suitable sound attributes, such as "loud" and "feeble", and appropriate sound sources, such as "tunnel" and "street" and incorporate this information into the prompts used by Audio-Language Models (ALMs) for audio classification. Further, to enhance audio-text alignment, we perform prompt ensemble across TSPE-generated task-specific prompts. When evaluated on 12 diverse audio classification datasets, TSPE improves performance across ALMs by showing an absolute improvement of 1.23-16.36% over vanilla zero-shot evaluation.	 | 以下是对给出的摘要进行的中文翻译：  语音-语言模型（ALMs）在零样本音频分类任务中表现出色，这是一种在测试时通过利用描述性的自然语言提示对未见过的音频片段进行分类的任务。我们引入了TSPE（任务特定提示集合），这是一种简单且无需训练的硬提示方法，通过为各种音频分类任务量身定制提示来提升ALMs的零样本性能。与使用通用模板提示（如“汽车的声音”）不同，我们生成了富有上下文的提示，例如“从隧道传来的汽车声音”。具体而言，我们利用标签信息来识别适合的声音属性，如“响亮”和“微弱”，以及适当的声音来源，如“隧道”和“街道”，并将这些信息整合到用于音频分类的语音-语言模型（ALMs）的提示中。进一步地，为了增强音频-文本对齐，我们在TSPE生成的任务特定提示之间进行提示集合。在对12个不同的音频分类数据集进行评估时，TSPE通过在纯零样本评估的基础上提高1.23%-16.36%的性能，提升了ALMs的表现。
2501.00378	 | STARFormer: A Novel Spatio-Temporal Aggregation Reorganization Transformer of FMRI for Brain Disorder Diagnosis	 | Wenhao Dong,Yueyang Li,Weiming Zeng,Lei Chen,Hongjie Yan,Wai Ting Siok,Nizhuan Wang	 | 本文提出了一个时空聚合变换器（STARFormer）模型，该模型通过有效捕捉功能性磁共振成像（fMRI）中的BOLD信号的空间和时间特征，提高了对自闭症谱系障碍（ASD）和注意缺陷多动障碍（ADHD）的分类准确性，并在公开数据集上取得了最先进的性能。	 | Many existing methods that use functional magnetic resonance imaging (fMRI) classify brain disorders, such as autism spectrum disorder (ASD) and attention deficit hyperactivity disorder (ADHD), often overlook the integration of spatial and temporal dependencies of the blood oxygen level-dependent (BOLD) signals, which may lead to inaccurate or imprecise classification results. To solve this problem, we propose a Spatio-Temporal Aggregation eorganization ransformer (STARFormer) that effectively captures both spatial and temporal features of BOLD signals by incorporating three key modules. The region of interest (ROI) spatial structure analysis module uses eigenvector centrality (EC) to reorganize brain regions based on effective connectivity, highlighting critical spatial relationships relevant to the brain disorder. The temporal feature reorganization module systematically segments the time series into equal-dimensional window tokens and captures multiscale features through variable window and cross-window attention. The spatio-temporal feature fusion module employs a parallel transformer architecture with dedicated temporal and spatial branches to extract integrated features. The proposed STARFormer has been rigorously evaluated on two publicly available datasets for the classification of ASD and ADHD. The experimental results confirm that the STARFormer achieves state-of-the-art performance across multiple evaluation metrics, providing a more accurate and reliable tool for the diagnosis of brain disorders and biomedical research. The codes will be available at: https://github.com/NZWANG/STARFormer.	 | 许多现有的使用功能性磁共振成像（fMRI）分类脑部疾病的算法，如自闭症谱系障碍（ASD）和注意缺陷多动障碍（ADHD），往往忽视了对血氧水平依赖（BOLD）信号的空间和时间依赖性的整合，这可能导致分类结果不准确或不够精确。为了解决这个问题，我们提出了一个时空聚合变换器（STARFormer），该模型通过结合三个关键模块能够有效捕捉BOLD信号的空间和时间特征。兴趣区域（ROI）的空间结构分析模块使用特征向量中心性（EC）重新组织脑区，根据有效连接突出脑疾病相关的关键空间关系。时间特征重新组织模块系统地将时间序列分割成等维度的时间窗口标记，并通过可变窗口和跨窗口注意力捕捉多尺度特征。时空特征融合模块采用具有专门时间分支和空间分支的并行变压器架构，提取整合特征。STARFormer已经在两个公开可用的数据集上对ASD和ADHD的分类进行了严格的评估。实验结果证实，STARFormer在多个评估指标上达到了最先进的性能，为脑部疾病的诊断和生物医学研究提供了更准确可靠的新工具。代码将在 https://github.com/NZWANG/STARFormer 上提供。
2501.00375	 | Token Pruning for Caching Better: 9 Times Acceleration on Stable Diffusion for Free	 | Evelyn Zhang,Bang Xiao,Jiayi Tang,Qianli Ma,Chang Zou,Xuefei Ning,Xuming Hu,Linfeng Zhang	 | 本文提出了一种动态意识标记剪枝（DaTo）方法，通过有选择地剪枝低动态标记，以在时间步之间延续特征动态，从而在保持图像质量的同时加速稳定扩散模型的生成过程。实验结果表明，DaTo 方法在 ImageNet 和 COCO-30k 上分别实现了9倍和7倍的加速，并且降低了 FID 分数，证明了其有效性和效率。	 | Stable Diffusion has achieved remarkable success in the field of text-to-image generation, with its powerful generative capabilities and diverse generation results making a lasting impact. However, its iterative denoising introduces high computational costs and slows generation speed, limiting broader adoption. The community has made numerous efforts to reduce this computational burden, with methods like feature caching attracting attention due to their effectiveness and simplicity. Nonetheless, simply reusing features computed at previous timesteps causes the features across adjacent timesteps to become similar, reducing the dynamics of features over time and ultimately compromising the quality of generated images. In this paper, we introduce a dynamics-aware token pruning (DaTo) approach that addresses the limitations of feature caching. DaTo selectively prunes tokens with lower dynamics, allowing only high-dynamic tokens to participate in self-attention layers, thereby extending feature dynamics across timesteps. DaTo combines feature caching with token pruning in a training-free manner, achieving both temporal and token-wise information reuse. Applied to Stable Diffusion on the ImageNet, our approach delivered a 9$\times$ speedup while reducing FID by 0.33, indicating enhanced image quality. On the COCO-30k, we observed a 7$\times$ acceleration coupled with a notable FID reduction of 2.17.	 | 稳定扩散（Stable Diffusion）在文本到图像生成领域取得了显著的成功，其强大的生成能力和多样的生成结果产生了深远影响。然而，其迭代去噪过程引入了高昂的计算成本，从而减慢了生成速度，限制了更广泛的采用。社区为此做出了多项努力，试图减轻这一计算负担，其中特征缓存（feature caching）方法因其有效性和简便性而受到广泛关注。尽管如此，简单地在前一时间步计算的特征上进行重用会导致相邻时间步特征相似，减少了特征随时间的变化性，最终影响生成图像的质量。本文介绍了一种动态意识标记剪枝（DaTo）方法来解决特征缓存的局限性。DaTo 选择性地剪枝动态较低的标记，仅允许高动态标记参与自注意力层，从而在时间步之间延续特征动态。DaTo 以无训练的方式结合了特征缓存和标记剪枝，实现了时间和标记级别的信息重用。将其应用于 ImageNet 上的稳定扩散，我们的方法实现了9倍的加速，并将FID降低了0.33，表明图像质量得到提升。在COCO-30k上，我们观察到了7倍的加速，并且FID降低了2.17。
2501.00360	 | A Novel Shape Guided Transformer Network for Instance Segmentation in Remote Sensing Images	 | Dawen Yu,Shunping Ji	 | 本文提出了一种新颖的形状引导变压器网络（SGTN），它结合了高效的LSwin编码器和形状引导模块（SGM），以提高遥感图像（RSI）中的实例分割性能。实验结果表明，SGTN在多个公开数据集上优于现有方法，特别是在平均精度（AP）得分上表现最佳。	 | Instance segmentation performance in remote sensing images (RSIs) is significantly affected by two issues: how to extract accurate boundaries of objects from remote imaging through the dynamic atmosphere, and how to integrate the mutual information of related object instances scattered over a vast spatial region. In this study, we propose a novel Shape Guided Transformer Network (SGTN) to accurately extract objects at the instance level. Inspired by the global contextual modeling capacity of the self-attention mechanism, we propose an effective transformer encoder termed LSwin, which incorporates vertical and horizontal 1D global self-attention mechanisms to obtain better global-perception capacity for RSIs than the popular local-shifted-window based Swin Transformer. To achieve accurate instance mask segmentation, we introduce a shape guidance module (SGM) to emphasize the object boundary and shape information. The combination of SGM, which emphasizes the local detail information, and LSwin, which focuses on the global context relationships, achieve excellent RSI instance segmentation. Their effectiveness was validated through comprehensive ablation experiments. Especially, LSwin is proved better than the popular ResNet and Swin transformer encoder at the same level of efficiency. Compared to other instance segmentation methods, our SGTN achieves the highest average precision (AP) scores on two single-class public datasets (WHU dataset and BITCC dataset) and a multi-class public dataset (NWPU VHR-10 dataset). Code will be available at http://gpcv.whu.edu.cn/data/.	 | 遥感图像（RSI）中的实例分割性能受到两个问题的显著影响：如何通过动态大气准确提取遥感成像中的物体边界，以及如何整合分布在广阔空间区域中的相关物体实例的相互信息。在这项研究中，我们提出了一种新颖的形状引导变压器网络（SGTN），以准确地在实例级别提取物体。受自我注意机制的全局上下文建模能力启发，我们提出了一种高效的变压器编码器LSwin，它结合了垂直和水平的1D全局自我注意机制，以获得比流行的基于局部移位窗口的Swin Transformer更好的RSI全局感知能力。为了实现准确的实例掩码分割，我们引入了一种形状引导模块（SGM），以强调物体边界和形状信息。SGM强调局部细节信息与LSwin关注全局上下文关系的结合，实现了卓越的RSI实例分割效果。其有效性通过全面的消融实验得到了验证。特别是，LSwin在效率相同的情况下证明优于流行的ResNet和Swin变压器编码器。与其它实例分割方法相比，我们的SGTN在两个单类公开数据集（WHU数据集和BITCC数据集）和一个多类公开数据集（NWPU VHR-10数据集）上取得了最高的平均精度（AP）得分。代码将在http://gpcv.whu.edu.cn/data/获得。
2501.00353	 | RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented Instructions	 | Wanlong Liu,Junying Chen,Ke Ji,Li Zhou,Wenyu Chen,Benyou Wang	 | RAG-Instruct提出了一种通用方法，利用多种RAG范式和指令模拟从维基百科合成高质量的RAG指令数据集，有效增强了大型语言模型的检索增强生成能力，并在多种任务上优于现有基线。	 | Retrieval-Augmented Generation (RAG) has emerged as a key paradigm for enhancing large language models (LLMs) by incorporating external knowledge. However, current RAG methods face two limitations: (1) they only cover limited RAG scenarios. (2) They suffer from limited task diversity due to the lack of a general RAG dataset. To address these limitations, we propose RAG-Instruct, a general method for synthesizing diverse and high-quality RAG instruction data based on any source corpus. Our approach leverages (1) five RAG paradigms, which encompass diverse query-document relationships, and (2) instruction simulation, which enhances instruction diversity and quality by utilizing the strengths of existing instruction datasets. Using this method, we construct a 40K instruction dataset from Wikipedia, comprehensively covering diverse RAG scenarios and tasks. Experiments demonstrate that RAG-Instruct effectively enhances LLMs' RAG capabilities, achieving strong zero-shot performance and significantly outperforming various RAG baselines across a diverse set of tasks. RAG-Instruct is publicly available at https://github.com/FreedomIntelligence/RAG-Instruct.	 | 检索增强生成（RAG）已成为通过引入外部知识来增强大型语言模型（LLMs）的关键范式。然而，当前的RAG方法存在两个限制：（1）它们只涵盖有限的RAG场景。（2）由于缺乏通用的RAG数据集，它们在任务多样性方面受到限制。为了解决这些限制，我们提出了RAG-Instruct，这是一种基于任何源语料库合成多样且高质量RAG指令数据的通用方法。我们的方法利用了（1）五种RAG范式，这涵盖了不同的查询-文档关系，以及（2）指令模拟，通过利用现有指令数据集的优势来增强指令的多样性和质量。使用这种方法，我们从维基百科构建了一个包含4万个指令的数据集，全面覆盖了多样化的RAG场景和任务。实验表明，RAG-Instruct有效地增强了LLMs的RAG能力，实现了强大的零样本性能，并在多种任务上显著优于各种RAG基线。RAG-Instruct已公开发布在[https://github.com/FreedomIntelligence/RAG-Instruct](https://github.com/FreedomIntelligence/RAG-Instruct)。
2501.00346	 | CNC: Cross-modal Normality Constraint for Unsupervised Multi-class Anomaly Detection	 | Xiaolei Wang,Xiaoyang Wang,Huihui Bai,Eng Gee Lim,Jimin Xiao	 | 该研究指出，现有无监督蒸馏方法中解码器的过度泛化导致了对异常区域的误检测问题，特别是在多类异常检测任务中更为明显。为此，作者提出了一种新的方法，通过类自适应的学习提示和门控专家混合模块来抑制解码器的过度泛化，从而提高异常检测的准确性。	 | Existing unsupervised distillation-based methods rely on the differences between encoded and decoded features to locate abnormal regions in test images. However, the decoder trained only on normal samples still reconstructs abnormal patch features well, degrading performance. This issue is particularly pronounced in unsupervised multi-class anomaly detection tasks. We attribute this behavior to over-generalization(OG) of decoder: the significantly increasing diversity of patch patterns in multi-class training enhances the model generalization on normal patches, but also inadvertently broadens its generalization to abnormal patches. To mitigate OG, we propose a novel approach that leverages class-agnostic learnable prompts to capture common textual normality across various visual patterns, and then apply them to guide the decoded features towards a normal textual representation, suppressing over-generalization of the decoder on abnormal patterns. To further improve performance, we also introduce a gated mixture-of-experts module to specialize in handling diverse patch patterns and reduce mutual interference between them in multi-class training. Our method achieves competitive performance on the MVTec AD and VisA datasets, demonstrating its effectiveness.	 | 现有的无监督蒸馏方法依赖于编码和解码特征之间的差异来定位测试图像中的异常区域。然而，仅在正常样本上训练的解码器仍然能够很好地重建异常块特征，这会降低性能。这个问题在无监督多类异常检测任务中尤为明显。我们将这种行为归因于解码器的过度泛化(OG)：多类训练中块模式显著增加的多样性提升了模型在正常块上的泛化能力，但也无意中增强了其对异常块的泛化能力。为了缓解OG，我们提出了一种新的方法，该方法利用类自适应的学习提示来捕捉各种视觉模式下的共同文本正常性，然后使用这些提示来引导解码特征向正常文本表示靠拢，抑制解码器对异常模式的过度泛化。为了进一步提高性能，我们还引入了一个门控专家混合模块，专门处理各种块模式，并在多类训练中减少它们之间的相互干扰。我们的方法在MVTec AD和VisA数据集上实现了竞争力的表现，证明了其有效性。
2501.00340	 | Dynamic Prompt Adjustment for Multi-Label Class-Incremental Learning	 | Haifeng Zhao,Yuguang Jin,Leilei Ma	 | 该研究针对多标签类别增量学习（MLCIL）问题，结合改进的数据重播放机制和提示损失，以减少灾难性遗忘并提高模型性能；实验表明该方法在多个基准数据集上显著提升了MLCIL任务的表现。	 | Significant advancements have been made in single label incremental learning (SLCIL),yet the more practical and challenging multi label class incremental learning (MLCIL) remains understudied. Recently,visual language models such as CLIP have achieved good results in classification tasks. However,directly using CLIP to solve MLCIL issue can lead to catastrophic forgetting. To tackle this issue, we integrate an improved data replay mechanism and prompt loss to curb knowledge forgetting. Specifically,our model enhances the prompt information to better adapt to multi-label classification tasks and employs confidence-based replay strategy to select representative samples. Moreover, the prompt loss significantly reduces the model's forgetting of previous knowledge. Experimental results demonstrate that our method has substantially improved the performance of MLCIL tasks across multiple benchmark datasets,validating its effectiveness.	 | 在单标签增量学习（SLCIL）方面已经取得了显著进展，但更具实践性和挑战性的多标签类别增量学习（MLCIL）仍研究不足。最近，视觉语言模型如CLIP在分类任务中取得了良好效果。然而，直接将CLIP用于解决MLCIL问题可能会导致灾难性遗忘。为了解决这一问题，我们结合了改进的数据重播放机制和提示损失，以遏制知识遗忘。具体来说，我们的模型增强了提示信息，使其更好地适应多标签分类任务，并采用基于置信度的重播放策略选择代表性样本。此外，提示损失显著减少了模型对先前知识的遗忘。实验结果表明，我们的方法在多个基准数据集上显著提高了MLCIL任务的性能，验证了其有效性。
2501.00339	 | Rethinking Layer Removal: Preserving Critical Components with Task-Aware Singular Value Decomposition	 | Kainan Liu,Yong Zhang,Ning Cheng,Zhitao Li,Shaojun Wang,Jing Xiao	 | Taco-SVD 是一种任务感知的框架，能够保留关键的奇异值方向并保持内部一致性，从而在压缩大型语言模型时减少性能下降；该方法通过将奇异值对齐到下游任务目标，优于现有方法并在不同架构下实现了更好的困惑度和任务性能。	 | Layer removal has emerged as a promising approach for compressing large language models (LLMs) by leveraging redundancy within layers to reduce model size and accelerate inference. However, this technique often compromises internal consistency, leading to performance degradation and instability, with varying impacts across different model architectures. In this work, we propose Taco-SVD, a task-aware framework that retains task-critical singular value directions, preserving internal consistency while enabling efficient compression. Unlike direct layer removal, Taco-SVD preserves task-critical transformations to mitigate performance degradation. By leveraging gradient-based attribution methods, Taco-SVD aligns singular values with downstream task objectives. Extensive evaluations demonstrate that Taco-SVD outperforms existing methods in perplexity and task performance across different architectures while ensuring minimal computational overhead.	 | 层移除已经作为一种潜在的方法，通过利用层内的冗余来压缩大型语言模型（LLMs），减少模型大小并加速推理。然而，这种方法往往会牺牲内部一致性，导致性能下降和不稳定，不同模型架构的影响程度各异。本文中，我们提出了Taco-SVD，这是一种任务感知框架，能够保留任务关键的奇异值方向，同时保持内部一致性并实现高效的压缩。与直接移除层不同，Taco-SVD 保留了任务关键的变换，以减轻性能下降。通过利用基于梯度的归因方法，Taco-SVD 将奇异值对齐到下游任务目标。广泛的评估表明，在不同架构下，Taco-SVD 在困惑度和任务性能方面均优于现有方法，同时确保了最小的计算开销。
2501.00326	 | OVGaussian: Generalizable 3D Gaussian Segmentation with Open Vocabularies	 | Runnan Chen,Xiangyu Sun,Zhaoqing Wang,Youquan Liu,Jiepeng Wang,Lingdong Kong,Jiankang Deng,Mingming Gong,Liang Pan,Wenping Wang,Tongliang Liu	 | 本文提出了OVGaussian，一种基于3D高斯表示的通用开放词汇3D语义分割框架，通过构建大规模的SegGaussian数据集和引入通用语义光栅化（GSR）及交叉模态一致性学习（CCL）框架，显著提高了跨场景和新视角的泛化能力。	 | Open-vocabulary scene understanding using 3D Gaussian (3DGS) representations has garnered considerable attention. However, existing methods mostly lift knowledge from large 2D vision models into 3DGS on a scene-by-scene basis, restricting the capabilities of open-vocabulary querying within their training scenes so that lacking the generalizability to novel scenes. In this work, we propose \textbf{OVGaussian}, a generalizable \textbf{O}pen-\textbf{V}ocabulary 3D semantic segmentation framework based on the 3D \textbf{Gaussian} representation. We first construct a large-scale 3D scene dataset based on 3DGS, dubbed \textbf{SegGaussian}, which provides detailed semantic and instance annotations for both Gaussian points and multi-view images. To promote semantic generalization across scenes, we introduce Generalizable Semantic Rasterization (GSR), which leverages a 3D neural network to learn and predict the semantic property for each 3D Gaussian point, where the semantic property can be rendered as multi-view consistent 2D semantic maps. In the next, we propose a Cross-modal Consistency Learning (CCL) framework that utilizes open-vocabulary annotations of 2D images and 3D Gaussians within SegGaussian to train the 3D neural network capable of open-vocabulary semantic segmentation across Gaussian-based 3D scenes. Experimental results demonstrate that OVGaussian significantly outperforms baseline methods, exhibiting robust cross-scene, cross-domain, and novel-view generalization capabilities. Code and the SegGaussian dataset will be released. (https://github.com/runnanchen/OVGaussian).	 | 使用3D高斯（3DGS）表示的开放词汇场景理解引起了广泛关注。然而，现有方法大多是从大规模的2D视觉模型中提取知识并逐场景地转移到3DGS上，这限制了开放词汇查询在其训练场景内的能力，缺乏对新场景的泛化能力。在本文中，我们提出了\textbf{OVGaussian}，这是一种基于3D高斯表示的通用开放词汇3D语义分割框架。我们首先基于3DGS构建了一个大规模的3D场景数据集，命名为\textbf{SegGaussian}，它为高斯点和多视图图像提供了详细的语义和实例注释。为了促进跨场景的语义泛化，我们引入了通用语义光栅化（GSR），这是一种利用3D神经网络学习和预测每个3D高斯点的语义属性的方法，其中语义属性可以渲染成多视图一致的2D语义图。随后，我们提出了交叉模态一致性学习（CCL）框架，该框架利用\textbf{SegGaussian}中的2D图像和3D高斯点的开放词汇注释来训练能够在基于高斯的3D场景中进行开放词汇语义分割的3D神经网络。实验结果表明，OVGaussian显著优于基线方法，展示了跨场景、跨域和新视角的泛化能力。我们还将发布代码和SegGaussian数据集。 (https://github.com/runnanchen/OVGaussian)
2501.00318	 | Improving Text-based Person Search via Part-level Cross-modal Correspondence	 | Jicheol Park,Boseung Jeong,Dongwon Kim,Suha Kwak	 | 该研究提出了一种高效的编码-解码模型，用于基于文本的人像搜索任务，该模型能够在无监督条件下学习图像和文本之间的语义对齐，并引入了一种基于共性度的边界排名损失来捕捉细微的身体部位差异，从而在公开基准数据集上取得了最佳性能。	 | Text-based person search is the task of finding person images that are the most relevant to the natural language text description given as query. The main challenge of this task is a large gap between the target images and text queries, which makes it difficult to establish correspondence and distinguish subtle differences across people. To address this challenge, we introduce an efficient encoder-decoder model that extracts coarse-to-fine embedding vectors which are semantically aligned across the two modalities without supervision for the alignment. There is another challenge of learning to capture fine-grained information with only person IDs as supervision, where similar body parts of different individuals are considered different due to the lack of part-level supervision. To tackle this, we propose a novel ranking loss, dubbed commonality-based margin ranking loss, which quantifies the degree of commonality of each body part and reflects it during the learning of fine-grained body part details. As a consequence, it enables our method to achieve the best records on three public benchmarks.	 | 基于文本的人像搜索是指根据给定的自然语言文本描述，在图像数据库中找到最相关的个人图像的任务。这项任务的主要挑战在于目标图像与文本查询之间存在巨大差距，这使得难以建立对应关系并区分不同个人的细微差异。为了应对这一挑战，我们引入了一种高效的编码-解码模型，该模型可以提取从粗到细的嵌入向量，这些向量在两个模态之间是语义对齐的，且在没有监督的情况下进行了对齐。  另一个挑战是在仅有个人ID作为监督的情况下学习捕捉细微信息，由于缺乏部分级别的监督，不同个体的相似身体部位被视为不同。为此，我们提出了一种新颖的排名损失，称为基于共性度的边界排名损失，该损失量化了每个身体部位的共性程度，并在学习细微的身体部位细节时反映了这种共性。因此，我们的方法在三个公开基准上取得了最佳记录。
2501.00317	 | Spatio-Temporal Multi-Subgraph GCN for 3D Human Motion Prediction	 | Jiexin Wang,Yiju Guo,Bing Su	 | 本文提出了一种时空多子图图卷积网络（STMS-GCN），该网络通过解耦时间和空间依赖性并在多个尺度上实现跨域知识转移来捕捉复杂的人类运动的时空依赖关系。同时，利用多个子图和同构信息约束机制来增强不同子图之间的学习关联，实验结果表明该方法在标准人类运动预测基准数据集上具有优越性。	 | Human motion prediction (HMP) involves forecasting future human motion based on historical data. Graph Convolutional Networks (GCNs) have garnered widespread attention in this field for their proficiency in capturing relationships among joints in human motion. However, existing GCN-based methods tend to focus on either temporal-domain or spatial-domain features, or they combine spatio-temporal features without fully leveraging the complementarity and cross-dependency of these two features. In this paper, we propose the Spatial-Temporal Multi-Subgraph Graph Convolutional Network (STMS-GCN) to capture complex spatio-temporal dependencies in human motion. Specifically, we decouple the modeling of temporal and spatial dependencies, enabling cross-domain knowledge transfer at multiple scales through a spatio-temporal information consistency constraint mechanism. Besides, we utilize multiple subgraphs to extract richer motion information and enhance the learning associations of diverse subgraphs through a homogeneous information constraint mechanism. Extensive experiments on the standard HMP benchmarks demonstrate the superiority of our method.	 | 人类运动预测（HMP）涉及根据历史数据预测未来的人类运动。图卷积网络（GCNs）在这一领域引起了广泛关注，因为它们能够捕捉人类运动中关节之间的关系。然而，现有的基于GCN的方法要么侧重于时间域特征，要么侧重于空间域特征，或者结合时空特征但未能充分利用这两种特征之间的互补性和交叉依赖性。在本文中，我们提出了一种时空多子图图卷积网络（STMS-GCN），以捕捉人类运动中的复杂时空依赖关系。具体来说，我们将时间和空间依赖性建模解耦，通过时空信息一致性约束机制在多个尺度上实现跨域知识转移。此外，我们利用多个子图来提取更丰富的运动信息，并通过同构信息约束机制增强不同子图之间的学习关联。在标准HMP基准数据集上的广泛实验表明了我们方法的优越性。
2501.00309	 | Retrieval-Augmented Generation with Graphs (GraphRAG)	 | Haoyu Han,Yu Wang,Harry Shomer,Kai Guo,Jiayuan Ding,Yongjia Lei,Mahantesh Halappanavar,Ryan A. Rossi,Subhabrata Mukherjee,Xianfeng Tang,Qi He,Zhigang Hua,Bo Long,Tong Zhao,Neil Shah,Amin Javari,Yinglong Xia,Jiliang Tang	 | 本文全面综述了GraphRAG（图检索增强生成）技术，这是一种将图结构数据与RAG（检索增强生成）集成的技术，并探讨了其关键组件和专门针对不同领域的设计挑战。	 | Retrieval-augmented generation (RAG) is a powerful technique that enhances downstream task execution by retrieving additional information, such as knowledge, skills, and tools from external sources. Graph, by its intrinsic "nodes connected by edges" nature, encodes massive heterogeneous and relational information, making it a golden resource for RAG in tremendous real-world applications. As a result, we have recently witnessed increasing attention on equipping RAG with Graph, i.e., GraphRAG. However, unlike conventional RAG, where the retriever, generator, and external data sources can be uniformly designed in the neural-embedding space, the uniqueness of graph-structured data, such as diverse-formatted and domain-specific relational knowledge, poses unique and significant challenges when designing GraphRAG for different domains. Given the broad applicability, the associated design challenges, and the recent surge in GraphRAG, a systematic and up-to-date survey of its key concepts and techniques is urgently desired. Following this motivation, we present a comprehensive and up-to-date survey on GraphRAG. Our survey first proposes a holistic GraphRAG framework by defining its key components, including query processor, retriever, organizer, generator, and data source. Furthermore, recognizing that graphs in different domains exhibit distinct relational patterns and require dedicated designs, we review GraphRAG techniques uniquely tailored to each domain. Finally, we discuss research challenges and brainstorm directions to inspire cross-disciplinary opportunities. Our survey repository is publicly maintained at https://github.com/Graph-RAG/GraphRAG/.	 | ## RAG增强生成技术（RAG）  RAG（Retrieval-Augmented Generation，检索增强生成）是一种强大的技术，通过从外部来源检索额外信息（如知识、技能和工具），来增强下游任务的执行。图因其固有的“节点通过边连接”的本质，能够编码大量的异构和关系信息，使其成为RAG在大量实际应用中的优质资源。因此，我们最近见证了越来越多的研究将图与RAG集成，即GraphRAG。然而，与传统的RAG不同，传统的RAG中检索器、生成器和外部数据源可以在神经嵌入空间中统一设计，而图结构数据的独特性，如多样化的格式和特定领域的关系知识，为设计GraphRAG带来了独特的和重大的挑战。鉴于其广泛的应用性、相关的设计挑战以及GraphRAG的近期迅猛发展，对GraphRAG的关键概念和技术进行全面而最新的综述变得迫切。  基于这一动机，我们全面综述了GraphRAG。我们的综述首先提出了一种全面的GraphRAG框架，定义了其关键组件，包括查询处理器、检索器、组织器、生成器和数据源。进一步地，我们认识到不同领域的图表现出不同的关系模式，并需要专门的设计，因此我们回顾了专门针对每个领域的GraphRAG技术。最后，我们讨论了研究挑战并展望了新的研究方向，以激发跨学科的机会。我们的综述仓库已公开维护在<https://github.com/Graph-RAG/GraphRAG/>。
2501.00303	 | SAM-Aware Graph Prompt Reasoning Network for Cross-Domain Few-Shot Segmentation	 | Shi-Feng Peng,Guolei Sun,Yong Li,Hongsong Wang,Guo-Sen Xie	 | 本文提出了一种SAM意识的图提示推理网络（GPRN），通过利用大规模预训练模型SAM来指导跨领域少样本分割特征表示学习，提高分割准确性。该方法包括SAM意识的提示初始化模块和图提示推理模块，分别解决生成不一致视觉提示和增强语义一致性的问题，并在四个标准数据集上取得了最先进的性能。	 | The primary challenge of cross-domain few-shot segmentation (CD-FSS) is the domain disparity between the training and inference phases, which can exist in either the input data or the target classes. Previous models struggle to learn feature representations that generalize to various unknown domains from limited training domain samples. In contrast, the large-scale visual model SAM, pre-trained on tens of millions of images from various domains and classes, possesses excellent generalizability. In this work, we propose a SAM-aware graph prompt reasoning network (GPRN) that fully leverages SAM to guide CD-FSS feature representation learning and improve prediction accuracy. Specifically, we propose a SAM-aware prompt initialization module (SPI) to transform the masks generated by SAM into visual prompts enriched with high-level semantic information. Since SAM tends to divide an object into many sub-regions, this may lead to visual prompts representing the same semantic object having inconsistent or fragmented features. We further propose a graph prompt reasoning (GPR) module that constructs a graph among visual prompts to reason about their interrelationships and enable each visual prompt to aggregate information from similar prompts, thus achieving global semantic consistency. Subsequently, each visual prompt embeds its semantic information into the corresponding mask region to assist in feature representation learning. To refine the segmentation mask during testing, we also design a non-parameter adaptive point selection module (APS) to select representative point prompts from query predictions and feed them back to SAM to refine inaccurate segmentation results. Experiments on four standard CD-FSS datasets demonstrate that our method establishes new state-of-the-art results. Code: https://github.com/CVL-hub/GPRN.	 | 跨领域少样本分割（CD-FSS）的主要挑战是在训练和推理阶段存在领域差异，这种差异可能存在于输入数据或目标类别中。先前的模型难以从有限的训练领域样本中学习到能够泛化到各种未知领域的特征表示。相比之下，大规模视觉模型SAM在数千万来自不同领域和类别的图像上进行了预训练，具有出色的泛化能力。在这项工作中，我们提出了一种SAM意识的图提示推理网络（GPRN），它充分利用SAM来指导CD-FSS特征表示学习，并提高预测准确性。具体而言，我们提出了一种SAM意识的提示初始化模块（SPI），将由SAM生成的掩码转换为带有高级语义信息的视觉提示。由于SAM倾向于将一个物体分割成许多子区域，这可能导致表示相同语义对象的视觉提示具有不一致或碎片化的特征。为进一步解决这一问题，我们提出了一种图提示推理（GPR）模块，该模块构建了视觉提示之间的图来推理它们之间的关系，并使每个视觉提示能够从相似的提示中聚合信息，从而实现全局语义一致性。随后，每个视觉提示将其语义信息嵌入到相应的掩码区域中，以辅助特征表示学习。为了在测试过程中细化分割掩码，我们还设计了一种非参数自适应点选择模块（APS），从查询预测中选择具有代表性的点提示，并将它们反馈给SAM以修正不准确的分割结果。在四个标准CD-FSS数据集上的实验结果表明，我们的方法建立了新的最先进水平。代码：https://github.com/CVL-hub/GPRN。
2501.00300	 | Research on vehicle detection based on improved YOLOv8 network	 | Haocheng Guo,Yaqiong Zhang,Lieyang Chen,Arfat Ahmad Khan	 | 本文提出了一种改进的YOLOv8车辆检测方法，通过使用FasterNet网络、加入CBAM注意力机制以及修改损失函数为WIoU，提高了车辆、行人和摩托车的检测精度，分别达到了98.3%、89.1%和88.4%，在多个指标上优于改进前的YOLOv9模型和其他六个指标。	 | The key to ensuring the safe obstacle avoidance function of autonomous driving systems lies in the use of extremely accurate vehicle recognition techniques. However, the variability of the actual road environment and the diverse characteristics of vehicles and pedestrians together constitute a huge obstacle to improving detection accuracy, posing a serious challenge to the realization of this goal. To address the above issues, this paper proposes an improved YOLOv8 vehicle detection method. Specifically, taking the YOLOv8n-seg model as the base model, firstly, the FasterNet network is used to replace the backbone network to achieve the purpose of reducing the computational complexity and memory while improving the detection accuracy and speed; secondly, the feature enhancement is achieved by adding the attention mechanism CBAM to the Neck; and lastly, the loss function CIoU is modified to WIoU, which optimizes the detection box localization while improving the segmentation accuracy. The results show that the improved model achieves 98.3%, 89.1% and 88.4% detection accuracy for car, Person and Motorcycle. Compared with the pre-improvement and YOLOv9 models in six metrics such as Precision.	 | 确保自动驾驶系统安全避障功能的关键在于使用极其精确的车辆识别技术。然而，实际道路环境的多变性和车辆及行人的多样特性共同构成了提高检测准确性的巨大障碍，对实现这一目标构成了严峻挑战。为解决上述问题，本文提出了一种改进的YOLOv8车辆检测方法。具体来说，以YOLOv8n-seg模型为基础模型，首先使用FasterNet网络替代骨干网络，以达到降低计算复杂性和内存使用，同时提高检测准确性和速度的目的；其次，在Neck中加入了注意力机制CBAM进行特征增强；最后，将损失函数CIoU修改为WIoU，优化了检测框的定位同时提高了分割精度。结果显示，改进后的模型在车辆、行人和摩托车的检测精度分别达到了98.3%、89.1%和88.4%。与改进前的YOLOv9模型及其他六个指标（如精确度）相比，改进后的模型表现更为优异。
2501.00296	 | Predicate Invention from Pixels via Pretrained Vision-Language Models	 | Ashay Athalye,Nishanth Kumar,Tom Silver,Yichao Liang,Tomás Lozano-Pérez,Leslie Pack Kaelbling	 | 本研究旨在学习在高度复杂的机器人环境中，直接从原始传感器输入（图像形式）中解决长期决策问题。通过利用预训练的视觉-语言模型的能力，该研究提出了一种新的方法pix2pred，能够从少量演示中直接在图像上发明具有语义意义的谓词，从而使得模型能够适应新的复杂任务。	 | Our aim is to learn to solve long-horizon decision-making problems in highly-variable, combinatorially-complex robotics domains given raw sensor input in the form of images. Previous work has shown that one way to achieve this aim is to learn a structured abstract transition model in the form of symbolic predicates and operators, and then plan within this model to solve novel tasks at test time. However, these learned models do not ground directly into pixels from just a handful of demonstrations. In this work, we propose to invent predicates that operate directly over input images by leveraging the capabilities of pretrained vision-language models (VLMs). Our key idea is that, given a set of demonstrations, a VLM can be used to propose a set of predicates that are potentially relevant for decision-making and then to determine the truth values of these predicates in both the given demonstrations and new image inputs. We build upon an existing framework for predicate invention, which generates feature-based predicates operating on object-centric states, to also generate visual predicates that operate on images. Experimentally, we show that our approach -- pix2pred -- is able to invent semantically meaningful predicates that enable generalization to novel, complex, and long-horizon tasks across two simulated robotic environments.	 | 我们的目标是在高度可变的组合复杂机器人领域中，给定原始传感器输入（以图像形式），学习解决长期决策问题。之前的工作表明，实现这一目标的一种方法是学习一种结构化的抽象过渡模型，形式为符号谓词和操作符，然后在该模型中进行规划，以在测试时解决新的任务。然而，这些学习到的模型并不能直接从少量的演示中将这些模型映射到像素上。在本工作中，我们提出利用预训练的视觉-语言模型（VLM）的能力来直接在输入图像上发明谓词。我们的核心思想是，给定一组演示，可以使用VLM建议一组潜在相关的决策谓词，然后在给定的演示和新图像输入中确定这些谓词的真值。我们在此前基于谓词发明的框架基础上进行扩展，该框架生成基于特征的谓词，操作于以对象为中心的状态，现在也生成操作图像的视觉谓词。实验上，我们展示了我们的方法——pix2pred——能够发明具有语义意义的谓词，使模型能够在两个仿真的机器人环境中推广到新的复杂和长期的任务上。
2501.00289	 | Dual Diffusion for Unified Image Generation and Understanding	 | Zijie Li,Henry Li,Yichun Shi,Amir Barati Farimani,Yuval Kluger,Linjie Yang,Peng Wang	 | 本文提出了一种大规模且端到端的多模态扩散模型，该模型结合了多模态扩散变换器和离散扩散语言模型的最新进展，能够在图像生成、描述和视觉问答等多种多模态任务上取得优异表现，并且在视觉理解任务上优于现有的自回归视觉语言模型。	 | Diffusion models have gained tremendous success in text-to-image generation, yet still lag behind with visual understanding tasks, an area dominated by autoregressive vision-language models. We propose a large-scale and fully end-to-end diffusion model for multi-modal understanding and generation that significantly improves on existing diffusion-based multimodal models, and is the first of its kind to support the full suite of vision-language modeling capabilities. Inspired by the multimodal diffusion transformer (MM-DiT) and recent advances in discrete diffusion language modeling, we leverage a cross-modal maximum likelihood estimation framework that simultaneously trains the conditional likelihoods of both images and text jointly under a single loss function, which is back-propagated through both branches of the diffusion transformer. The resulting model is highly flexible and capable of a wide range of tasks including image generation, captioning, and visual question answering. Our model attained competitive performance compared to recent unified image understanding and generation models, demonstrating the potential of multimodal diffusion modeling as a promising alternative to autoregressive next-token prediction models.	 | 扩散模型在文本到图像生成任务中取得了巨大的成功，但在视觉理解任务上仍显滞后，这一领域主要由自回归的视觉语言模型占据。我们提出了一种大规模且完全端到端的多模态扩散模型，该模型在现有的多模态扩散模型上显著提升了多模态理解与生成的能力，是首个能够支持全面的视觉语言建模功能的模型。受多模态扩散变换器（MM-DiT）和离散扩散语言模型的最新进展启发，我们利用了一种跨模态最大似然估计框架，该框架在同一损失函数下同时训练图像和文本的条件似然性，损失函数通过扩散变换器的两个分支进行反向传播。该模型具有高度的灵活性，并能完成包括图像生成、描述和视觉问答等一系列任务。与最近的统一图像理解和生成模型相比，我们的模型取得了具有竞争力的性能，这表明多模态扩散建模有可能作为一种有前景的替代自回归下一个词预测模型的方式。
2501.00288	 | Solving Partial Differential Equations with Random Feature Models	 | Chunyang Liao	 | 本文提出了一种基于随机特征的方法来高效求解偏微分方程（PDEs），该方法被视为浅层神经网络，并在多个PDE基准测试中展示了优于传统求解器的性能，尤其是在处理大量插值点时。	 | Machine learning based partial differential equations (PDEs) solvers have received great attention in recent years. Most progress in this area has been driven by deep neural networks such as physics-informed neural networks (PINNs) and kernel method. In this paper, we introduce a random feature based framework toward efficiently solving PDEs. Random feature method was originally proposed to approximate large-scale kernel machines and can be viewed as a shallow neural network as well. We provide an error analysis for our proposed method along with comprehensive numerical results on several PDE benchmarks. In contrast to the state-of-the-art solvers that face challenges with a large number of collocation points, our proposed method reduces the computational complexity. Moreover, the implementation of our method is simple and does not require additional computational resources. Due to the theoretical guarantee and advantages in computation, our approach is proven to be efficient for solving PDEs.	 | 基于机器学习的偏微分方程（PDEs）求解器近年来受到了广泛关注。这一领域的主要进展是由物理感知神经网络（PINNs）和核方法等深度神经网络推动的。在本文中，我们介绍了一种基于随机特征的方法，以高效地求解PDEs。随机特征方法最初用于近似大规模核机，也可以被视为浅层神经网络。我们为所提出的方法提供了误差分析，并在几个PDE基准上提供了全面的数值结果。与现有的求解器相比，这些求解器在面对大量插值点时会遇到挑战，我们提出的方法降低了计算复杂性。此外，我们方法的实现简单，不需要额外的计算资源。由于理论保证和计算上的优势，我们的方法被证明适用于PDEs的求解。
2501.00277	 | Efficient Human-in-the-Loop Active Learning: A Novel Framework for Data Labeling in AI Systems	 | Yiran Huang,Jian-Feng Yang,Haoda Fu	 | 本文提出了一种新的主动学习框架，能够在现代AI系统中高效利用专家标注未标记数据，通过结合数据驱动的探索与利用以及创新的查询方案，该框架在五个真实世界数据集上表现出了优于传统方法的准确性和效率。	 | Modern AI algorithms require labeled data. In real world, majority of data are unlabeled. Labeling the data are costly. this is particularly true for some areas requiring special skills, such as reading radiology images by physicians. To most efficiently use expert's time for the data labeling, one promising approach is human-in-the-loop active learning algorithm. In this work, we propose a novel active learning framework with significant potential for application in modern AI systems. Unlike the traditional active learning methods, which only focus on determining which data point should be labeled, our framework also introduces an innovative perspective on incorporating different query scheme. We propose a model to integrate the information from different types of queries. Based on this model, our active learning frame can automatically determine how the next question is queried. We further developed a data driven exploration and exploitation framework into our active learning method. This method can be embedded in numerous active learning algorithms. Through simulations on five real-world datasets, including a highly complex real image task, our proposed active learning framework exhibits higher accuracy and lower loss compared to other methods.	 | 现代AI算法需要标记的数据。在现实世界中，大多数数据都是未标记的。标记数据成本高昂。特别是在需要特殊技能的领域，如医生阅读放射影像图像时尤其如此。为了高效利用专家的时间进行数据标记，一种有前景的方法是基于人工在环路的主动学习算法。在本研究中，我们提出了一种新的主动学习框架，具有在现代AI系统中广泛应用的巨大潜力。与传统的主动学习方法仅专注于确定应标记哪些数据点不同，我们的框架还引入了一种创新的查询方案视角。我们提出了一种模型，以整合不同类型的查询信息。基于该模型，我们的主动学习框架可以自动决定下一个问题如何被询问。我们进一步将数据驱动的探索与利用框架融入到我们的主动学习方法中。该方法可以嵌入到众多的主动学习算法中。通过在五个真实世界数据集上的模拟，包括一项高度复杂的实际图像任务，我们提出的主动学习框架在准确性和损失方面优于其他方法。
2501.00237	 | Make Domain Shift a Catastrophic Forgetting Alleviator in Class-Incremental Learning	 | Wei Chen,Yi Zhou	 | 本文揭示通过引入领域偏移可以显著降低类增量学习中的灾难性遗忘，并提出了一种简单有效的DisCo方法，该方法通过对比学习促进任务间特征分布的独特性，从而减少任务间的干扰，有效提升CIL性能。	 | In the realm of class-incremental learning (CIL), alleviating the catastrophic forgetting problem is a pivotal challenge. This paper discovers a counter-intuitive observation: by incorporating domain shift into CIL tasks, the forgetting rate is significantly reduced. Our comprehensive studies demonstrate that incorporating domain shift leads to a clearer separation in the feature distribution across tasks and helps reduce parameter interference during the learning process. Inspired by this observation, we propose a simple yet effective method named DisCo to deal with CIL tasks. DisCo introduces a lightweight prototype pool that utilizes contrastive learning to promote distinct feature distributions for the current task relative to previous ones, effectively mitigating interference across tasks. DisCo can be easily integrated into existing state-of-the-art class-incremental learning methods. Experimental results show that incorporating our method into various CIL methods achieves substantial performance improvements, validating the benefits of our approach in enhancing class-incremental learning by separating feature representation and reducing interference. These findings illustrate that DisCo can serve as a robust fashion for future research in class-incremental learning.	 | 在类增量学习（Class-Incremental Learning, CIL）领域，缓解灾难性遗忘（Catastrophic Forgetting）问题是关键性的挑战。本文揭示了一个令人意外的观察：通过将领域偏移（Domain Shift）引入CIL任务中，能够显著降低遗忘率。我们的全面研究显示，引入领域偏移有助于任务间特征分布更加清晰的分离，并在学习过程中减少参数间的干扰。受到这一观察的启发，我们提出了一种简单而有效的处理CIL任务的方法，称为DisCo（Domain Shift and Contrastive Optimization）。DisCo引入了一个轻量级的原型池，利用对比学习促进当前任务相对于先前任务的特征分布独特性，从而有效减少任务间的干扰。DisCo可以轻松集成到现有的最先进的CIL方法中。实验结果表明，在各种CIL方法中加入我们的方法可以显著提升性能，验证了通过分离特征表示和减少干扰来增强CIL方法的有效性。这些发现表明，DisCo可以作为一种稳健的方法，用于未来CIL研究的发展。
2501.00224	 | Extracting effective solutions hidden in large language models via generated comprehensive specialists: case studies in developing electronic devices	 | Hikari Tomita,Nobuhiro Nakamura,Shoichi Ishida,Toshio Kamiya,Kei Terayama	 | 该研究提出了一种名为SELLM的框架，利用大规模语言模型和MECE原则来生成跨学科的有效解决方案，并将其应用于提高OLED照明效率和开发下一代存储材料电极的挑战，显示出在解决复杂问题时生成有效解决方案的潜力。	 | Recently, many studies have increasingly explored the use of large language models (LLMs) to generate research ideas and scientific hypotheses. However, real-world research and development often require solving complex, interdisciplinary challenges where solutions may not be readily found through existing knowledge related to the problem. Therefore, it is desirable to leverage the vast, comprehensive knowledge of LLMs to generate effective, breakthrough solutions by integrating various perspectives from other disciplines. Here, we propose SELLM (Solution Enumeration via comprehensive List and LLM), a framework leveraging LLMs and structured guidance using MECE (Mutually Exclusive, Collectively Exhaustive) principles, such as International Patent Classification (IPC) and the periodic table of elements. SELLM systematically constructs comprehensive expert agents from the list to generate cross-disciplinary and effective solutions. To evaluate SELLM's practicality, we applied it to two challenges: improving light extraction in organic light-emitting diode (OLED) lighting and developing electrodes for next-generation memory materials. The results demonstrate that SELLM significantly facilitates the generation of effective solutions compared to cases without specific customization or effort, showcasing the potential of SELLM to enable LLMs to generate effective solutions even for challenging problems.	 | 近年来，许多研究开始探索使用大规模语言模型（LLMs）生成研究想法和科学假设。然而，实际研究和发展往往需要解决复杂的、跨学科的挑战，而这些问题的解决方案可能无法通过现有与问题相关的知识轻易找到。因此，利用LLMs庞大的综合知识来生成有效的突破性解决方案，结合其他学科的不同视角，变得尤为重要。在此基础上，我们提出了SELLM（基于MECE原则的综合列表和LLM解决方案列举）框架，利用LLMs和MECE（互斥且穷尽）原则，例如国际专利分类（IPC）和元素周期表。SELLM系统地构建了综合专家代理，从列表中生成跨学科且有效的解决方案。为了评估SELLM的实用性，我们将其应用于两个挑战：提高有机发光二极管（OLED）照明中的光提取效率以及开发下一代存储材料的电极。结果表明，与没有特定定制或努力的情况相比，SELLM在生成有效解决方案方面显著更有效，展示了SELLM的潜力，即即使面对复杂问题，也能使LLMs生成有效的解决方案。
2501.00223	 | CancerKG.ORG A Web-scale, Interactive, Verifiable Knowledge Graph-LLM Hybrid for Assisting with Optimal Cancer Treatment and Care	 | Michael Gubanov,Anna Pyayt,Aleksandra Karolak	 | 该研究介绍了一种新的混合系统，结合了最新的同行评审医学知识图谱（CancerKG）和大规模语言模型（LLM），用于结直肠癌的医学研究和临床信息检索，旨在克服现有LLM的幻觉和灾难性遗忘问题，并提供更高效、精准的服务。	 | Here, we describe one of the first Web-scale hybrid Knowledge Graph (KG)-Large Language Model (LLM), populated with the latest peer-reviewed medical knowledge on colorectal Cancer. It is currently being evaluated to assist with both medical research and clinical information retrieval tasks at Moffitt Cancer Center, which is one of the top Cancer centers in the U.S. and in the world. Our hybrid is remarkable as it serves the user needs better than just an LLM, KG or a search-engine in isolation. LLMs as is are known to exhibit hallucinations and catastrophic forgetting as well as are trained on outdated corpora. The state of the art KGs, such as PrimeKG, cBioPortal, ChEMBL, NCBI, and other require manual curation, hence are quickly getting stale. CancerKG is unsupervised and is capable of automatically ingesting and organizing the latest medical findings. To alleviate the LLMs shortcomings, the verified KG serves as a Retrieval Augmented Generation (RAG) guardrail. CancerKG exhibits 5 different advanced user interfaces, each tailored to serve different data modalities better and more convenient for the user.	 | 这里，我们描述了一个早期的大规模混合知识图谱（KG）-大型语言模型（LLM），该知识图谱填充了最新的同行评审医学知识，涉及结直肠癌。目前，该混合系统正在莫菲特癌症中心进行评估，以协助医学研究和临床信息检索任务，莫菲特癌症中心是美国乃至世界顶级的癌症中心之一。我们的混合系统尤为出色，因为它能够更好地满足用户需求，而不仅仅是独立使用LLM、KG或搜索引擎。现有的LLM已知会表现出幻觉和灾难性遗忘，并且是基于过时的语料库进行训练。最新的KG，如PrimeKG、cBioPortal、ChEMBL、NCBI等，需要手动策展，因此很快就会变得过时。CancerKG则是一种无监督系统，能够自动吸收和整理最新的医学发现。为了弥补LLM的不足，经过验证的KG充当了检索增强生成（RAG）的护栏。CancerKG展示了5种不同的高级用户界面，每种界面都针对不同数据模态进行了优化，以更方便地为用户服务。
2501.00220	 | DecoratingFusion: A LiDAR-Camera Fusion Network with the Combination of Point-level and Feature-level Fusion	 | Zixuan Yin,Han Sun,Ningzhong Liu,Huiyu Zhou,Jiaquan Shen	 | 本文提出了一种结合特征级和点级融合的激光雷达与相机融合方法，通过校准矩阵建立硬关联来引导生成对象查询，并在KITTI和Waymo数据集上取得了优越性能。	 | Lidars and cameras play essential roles in autonomous driving, offering complementary information for 3D detection. The state-of-the-art fusion methods integrate them at the feature level, but they mostly rely on the learned soft association between point clouds and images, which lacks interpretability and neglects the hard association between them. In this paper, we combine feature-level fusion with point-level fusion, using hard association established by the calibration matrices to guide the generation of object queries. Specifically, in the early fusion stage, we use the 2D CNN features of images to decorate the point cloud data, and employ two independent sparse convolutions to extract the decorated point cloud features. In the mid-level fusion stage, we initialize the queries with a center heatmap and embed the predicted class labels as auxiliary information into the queries, making the initial positions closer to the actual centers of the targets. Extensive experiments conducted on two popular datasets, i.e. KITTI, Waymo, demonstrate the superiority of DecoratingFusion.	 | 激光雷达和相机在自动驾驶中扮演着重要角色，它们提供了互补的信息以实现3D检测。最先进的融合方法在特征层面整合了这两种技术，但这些方法主要依赖于从点云到图像的软关联，缺乏可解释性，并且忽略了它们之间的硬关联。在这篇论文中，我们结合了特征级融合与点级融合方法，通过校准矩阵建立的硬关联来引导生成对象查询。具体来说，在早期融合阶段，我们使用图像的2D CNN特征来装饰点云数据，并采用两个独立的稀疏卷积来提取装饰后的点云特征。在中期融合阶段，我们用中心热图初始化查询，并将预测的类别标签嵌入到查询中作为辅助信息，从而使查询的初始位置更接近目标的实际中心。在KITTI和Waymo这两个流行数据集上的广泛实验表明，DecoratingFusion具有优越性。
2501.00192	 | MLLM-as-a-Judge for Image Safety without Human Labeling	 | Zhenting Wang,Shuming Hu,Shiyu Zhao,Xiaowen Lin,Felix Juefei-Xu,Zhuowei Li,Ligong Han,Harihar Subramanyam,Li Chen,Jianfa Chen,Nan Jiang,Lingjuan Lyu,Shiqing Ma,Dimitris N. Metaxas,Ankit Jain	 | 该研究探讨了在零样本设置下使用预训练多模态大型语言模型检测不安全图像的方法，提出了一种结合客观化安全规则、去偏见处理和逻辑推理的技术，有效解决了图像安全性判断的挑战。	 | Image content safety has become a significant challenge with the rise of visual media on online platforms. Meanwhile, in the age of AI-generated content (AIGC), many image generation models are capable of producing harmful content, such as images containing sexual or violent material. Thus, it becomes crucial to identify such unsafe images based on established safety rules. Pre-trained Multimodal Large Language Models (MLLMs) offer potential in this regard, given their strong pattern recognition abilities. Existing approaches typically fine-tune MLLMs with human-labeled datasets, which however brings a series of drawbacks. First, relying on human annotators to label data following intricate and detailed guidelines is both expensive and labor-intensive. Furthermore, users of safety judgment systems may need to frequently update safety rules, making fine-tuning on human-based annotation more challenging. This raises the research question: Can we detect unsafe images by querying MLLMs in a zero-shot setting using a predefined safety constitution (a set of safety rules)? Our research showed that simply querying pre-trained MLLMs does not yield satisfactory results. This lack of effectiveness stems from factors such as the subjectivity of safety rules, the complexity of lengthy constitutions, and the inherent biases in the models. To address these challenges, we propose a MLLM-based method includes objectifying safety rules, assessing the relevance between rules and images, making quick judgments based on debiased token probabilities with logically complete yet simplified precondition chains for safety rules, and conducting more in-depth reasoning with cascaded chain-of-thought processes if necessary. Experiment results demonstrate that our method is highly effective for zero-shot image safety judgment tasks.	 | 随着在线平台上视觉媒体的兴起，图像内容的安全性已成为一个重大挑战。同时，在AI生成内容（AIGC）的时代，许多图像生成模型能够生成有害内容，如包含色情或暴力的图像。因此，基于已建立的安全规则识别此类不安全图像变得至关重要。预训练的多模态大型语言模型（MLLMs）在这方面具有潜在的应用前景，因为它们具有强大的模式识别能力。现有方法通常通过人类标注的数据对MLLMs进行微调，然而这种方法带来了许多问题。首先，依赖人类注释员按照复杂而详细的指南标注数据既昂贵又耗时。此外，安全判断系统的用户可能需要频繁更新安全规则，这使得基于人类标注的微调更加困难。这引发了一个研究问题：我们是否可以在零样本设置下通过查询预训练的MLLMs并使用预定义的安全宪法（一组安全规则）来检测不安全图像？我们的研究表明，仅仅查询预训练的MLLMs并不能取得满意的结果。这种无效性源于诸如安全规则的主观性、宪法内容的复杂性以及模型本身的固有偏见等因素。为了解决这些挑战，我们提出了一种基于MLLM的方法，包括客观化安全规则、评估规则与图像的相关性、基于去偏见的令牌概率进行快速判断，并在必要时使用逻辑完整且简化的前提链来简化安全规则，以及采取更深层次的推理链式思维过程。实验结果表明，我们的方法在零样本图像安全性判断任务中非常有效。
2501.00149	 | LASSE: Learning Active Sampling for Storm Tide Extremes in Non-Stationary Climate Regimes	 | Grace Jiang,Jiangchao Qiu,Sai Ravela	 | 该研究提出了一种使用代理模型和在线学习方法来高效识别能产生破坏性风暴潮的热带气旋，即使在资源有限的情况下也能实现高精度和高召回率，适用于大型风暴目录和气候变化情景的分析。	 | Identifying tropical cyclones that generate destructive storm tides for risk assessment, such as from large downscaled storm catalogs for climate studies, is often intractable because it entails many expensive Monte Carlo hydrodynamic simulations. Here, we show that surrogate models are promising from accuracy, recall, and precision perspectives, and they ``generalize" to novel climate scenarios. We then present an informative online learning approach to rapidly search for extreme storm tide-producing cyclones using only a few hydrodynamic simulations. Starting from a minimal subset of TCs with detailed storm tide hydrodynamic simulations, a surrogate model selects informative data to retrain online and iteratively improves its predictions of damaging TCs. Results on an extensive catalog of downscaled TCs indicate a 100% precision retrieving the rare destructive storms training using less than 20% of the simulations as training. The informative sampling approach is efficient, scalable to large storm catalogs, and generalizable to climate scenarios.	 | 以下是该摘要的中文翻译：  在风险评估中识别产生破坏性风暴潮的热带气旋，例如来自大型下标风暴目录的气候变化研究，通常难以解决，因为它涉及许多昂贵的蒙特卡洛水动力模拟。在这里，我们展示了代理模型在准确度、召回率和精准度方面极具前景，并且它们能够“泛化”到新的气候情景中。我们随后提出了一种信息丰富的在线学习方法，仅使用少量水动力模拟即可快速搜索产生极端风暴潮的热带气旋。从一个具有详细风暴潮水动力模拟的最小子集开始，代理模型选择具有信息性的数据进行在线重新训练，并逐步提高其对破坏性气旋的预测能力。在广泛的数据集上进行的结果表明，在使用不到20%的模拟作为训练集的情况下，能够100%精准地检索到罕见的破坏性风暴。这种具有信息性的采样方法高效、可以扩展到大型风暴目录，并且适用于气候变化情景。
2501.00136	 | Detection-Fusion for Knowledge Graph Extraction from Videos	 | Taniya Das,Louis Mahon,Thomas Lukasiewicz	 | 本文提出了一种使用知识图谱标注视频的方法，以避免现有系统依赖语言模型和难以评估的问题；该方法通过深度学习模型预测视频中的个体及其关系，并可扩展以纳入背景知识。	 | One of the challenging tasks in the field of video understanding is extracting semantic content from video inputs. Most existing systems use language models to describe videos in natural language sentences, but this has several major shortcomings. Such systems can rely too heavily on the language model component and base their output on statistical regularities in natural language text rather than on the visual contents of the video. Additionally, natural language annotations cannot be readily processed by a computer, are difficult to evaluate with performance metrics and cannot be easily translated into a different natural language. In this paper, we propose a method to annotate videos with knowledge graphs, and so avoid these problems. Specifically, we propose a deep-learning-based model for this task that first predicts pairs of individuals and then the relations between them. Additionally, we propose an extension of our model for the inclusion of background knowledge in the construction of knowledge graphs.	 | 视频理解领域的一个挑战性任务是从视频输入中提取语义内容。现有大多数系统使用语言模型来用自然语言句子描述视频，但这有几个重大不足之处。这类系统可能会过度依赖语言模型组件，并基于自然语言文本中的统计规律来生成输出，而不是基于视频的视觉内容。此外，自然语言注释无法被计算机直接处理，难以用性能指标进行评估，也无法轻松翻译成其他自然语言。在本文中，我们提出了一种使用知识图谱标注视频的方法，以避免这些问题。具体而言，我们提出了一种基于深度学习的模型，首先预测个体对，然后预测它们之间的关系。此外，我们还提出了扩展模型，使其能够将背景知识纳入知识图谱的构建中。
2501.00135	 | GroverGPT: A Large Language Model with 8 Billion Parameters for Quantum Searching	 | Haoran Wang,Pingzhi Li,Min Chen,Jinglei Cheng,Junyu Liu,Tianlong Chen	 | 研究人员开发了GroverGPT，一种基于大规模语言模型的专门模型，用于模拟量子图灵机的输出，特别是在Grover量子电路方面表现出色，提供了对经典对应算法的二次加速，并在多种量子比特数据集上实现了高准确率和泛化能力。	 | Quantum computing is an exciting non-Von Neumann paradigm, offering provable speedups over classical computing for specific problems. However, the practical limits of classical simulatability for quantum circuits remain unclear, especially with current noisy quantum devices. In this work, we explore the potential of leveraging Large Language Models (LLMs) to simulate the output of a quantum Turing machine using Grover's quantum circuits, known to provide quadratic speedups over classical counterparts. To this end, we developed GroverGPT, a specialized model based on LLaMA's 8-billion-parameter architecture, trained on over 15 trillion tokens. Unlike brute-force state-vector simulations, which demand substantial computational resources, GroverGPT employs pattern recognition to approximate quantum search algorithms without explicitly representing quantum states. Analyzing 97K quantum search instances, GroverGPT consistently outperformed OpenAI's GPT-4o (45\% accuracy), achieving nearly 100\% accuracy on 6- and 10-qubit datasets when trained on 4-qubit or larger datasets. It also demonstrated strong generalization, surpassing 95\% accuracy for systems with over 20 qubits when trained on 3- to 6-qubit data. Analysis indicates GroverGPT captures quantum features of Grover's search rather than classical patterns, supported by novel prompting strategies to enhance performance. Although accuracy declines with increasing system size, these findings offer insights into the practical boundaries of classical simulatability. This work suggests task-specific LLMs can surpass general-purpose models like GPT-4o in quantum algorithm learning and serve as powerful tools for advancing quantum research.	 | 量子计算是一种令人兴奋的非冯·诺伊曼范式，对于特定问题提供可证明的经典计算加速。然而，当前嘈杂的量子设备使得量子电路的经典模拟的实际极限仍然模糊不清。在本研究中，我们探索了利用大规模语言模型（LLMs）模拟量子图灵机输出的可能性，特别是通过Grover量子电路，这些电路提供了经典对应算法的二次加速。为此，我们开发了GroverGPT，这是一种基于LLaMA 80亿参数架构的专门模型，并使用超过15万亿个标记进行训练。与需要大量计算资源的暴力状态向量模拟不同，GroverGPT利用模式识别来近似量子搜索算法，无需明确表示量子态。通过对97,000个量子搜索实例的分析，GroverGPT在准确率方面始终优于OpenAI的GPT-4o（准确率为45%），当使用4比特或更大量子比特数据集进行训练时，其准确率接近100%。对于6-和10-量子比特数据集，准确率也达到了99%。此外，GroverGPT展示了强大的泛化能力，在使用3-至6-量子比特数据集进行训练时，对于超过20个量子比特的系统，准确率超过了95%。分析表明，GroverGPT捕捉到了Grover搜索的量子特征，而不是经典的模式，新型提示策略的使用进一步增强了其性能。尽管随系统规模增加，准确率会下降，但这些发现为经典模拟的实际界限提供了有价值的见解。本研究表明，针对特定任务的语言模型可以超越通用模型如GPT-4o，在量子算法学习中表现出色，并成为推动量子研究的强大工具。
2501.00124	 | PQD: Post-training Quantization for Efficient Diffusion Models	 | Jiaojiao Ye,Zhen Wang,Linnan Jiang	 | 本文提出了一种名为PQD的后训练量化方法，通过时间感知优化框架将全精度扩散模型高效地量化为8位或4位模型，同时保持无条件图像生成性能，首次实现该方法在512x512文本指导图像生成中的应用。	 | Diffusionmodels(DMs)havedemonstratedremarkableachievements in synthesizing images of high fidelity and diversity. However, the extensive computational requirements and slow generative speed of diffusion models have limited their widespread adoption. In this paper, we propose a novel post-training quantization for diffusion models (PQD), which is a time-aware optimization framework for diffusion models based on post-training quantization. The proposed framework optimizes the inference process by selecting representative samples and conducting time-aware calibration. Experimental results show that our proposed method is able to directly quantize full-precision diffusion models into 8-bit or 4-bit models while maintaining comparable performance in a training-free manner, achieving a few FID change on ImageNet for unconditional image generation. Our approach demonstrates compatibility and can also be applied to 512x512 text-guided image generation for the first time.	 | 扩散模型（DMs）在高保真和多样性图像合成方面取得了显著成就。然而，扩散模型的大量计算需求和缓慢的生成速度限制了它们的广泛应用。本文提出了一种新颖的后训练量化方法（PQD），这是一种基于后训练量化的时间感知优化框架。该框架通过选择代表性样本并进行时间感知校准来优化推理过程。实验结果表明，我们的方法能够在无训练的情况下直接将全精度扩散模型量化为8位或4位模型，同时在无条件图像生成方面保持相当的性能，ImageNet上的FID变化很小。我们的方法具有兼容性，并且首次可以应用于512x512文本指导图像生成。
2501.00119	 | Post Launch Evaluation of Policies in a High-Dimensional Setting	 | Shima Nassiri,Mohsen Bayati,Joe Cooprider	 | 本文探讨了在大规模实验中使用合成控制方法作为A/B测试的替代方案，通过结合最近邻匹配和监督学习来提高反事实结果的估计准确性，并指出机器学习偏差可能影响实验结果，提出去偏差技术以应对这一挑战。	 | A/B tests, also known as randomized controlled experiments (RCTs), are the gold standard for evaluating the impact of new policies, products, or decisions. However, these tests can be costly in terms of time and resources, potentially exposing users, customers, or other test subjects (units) to inferior options. This paper explores practical considerations in applying methodologies inspired by "synthetic control" as an alternative to traditional A/B testing in settings with very large numbers of units, involving up to hundreds of millions of units, which is common in modern applications such as e-commerce and ride-sharing platforms. This method is particularly valuable in settings where the treatment affects only a subset of units, leaving many units unaffected. In these scenarios, synthetic control methods leverage data from unaffected units to estimate counterfactual outcomes for treated units. After the treatment is implemented, these estimates can be compared to actual outcomes to measure the treatment effect. A key challenge in creating accurate counterfactual outcomes is interpolation bias, a well-documented phenomenon that occurs when control units differ significantly from treated units. To address this, we propose a two-phase approach: first using nearest neighbor matching based on unit covariates to select similar control units, then applying supervised learning methods suitable for high-dimensional data to estimate counterfactual outcomes. Testing using six large-scale experiments demonstrates that this approach successfully improves estimate accuracy. However, our analysis reveals that machine learning bias -- which arises from methods that trade off bias for variance reduction -- can impact results and affect conclusions about treatment effects. We document this bias in large-scale experimental settings and propose effective de-biasing techniques to address this challenge.	 | A/B 测试，也被称为随机对照试验（RCTs），是评估新政策、产品或决策影响的黄金标准。然而，这些测试在时间和资源方面可能非常昂贵，可能会将用户、客户或其他测试对象（实验单位）暴露于较差的选择。本文探讨了在涉及数百万乃至数亿个实验单位的大型场景中，如何应用受“合成控制法”启发的方法作为传统 A/B 测试的替代方案。这种方法在治疗仅影响一部分单位，而许多单位未受影响的情境下特别有价值。在这种情况下，合成控制方法会利用未受影响单位的数据来估计受治疗单位的反事实结果。在治疗实施后，可以将这些估计值与实际结果进行比较，以衡量治疗效果。创建准确反事实结果的关键挑战之一是插值偏差，这是一种已广泛记录的现象，当控制单位与受治疗单位有显著差异时会引发这种偏差。为解决这个问题，我们提出了一种两阶段的方法：首先基于单位协变量使用最近邻匹配来选择相似的控制单位，然后应用适用于高维数据的监督学习方法来估计反事实结果。使用六个大规模实验进行测试表明，这种方法能够有效提高估计准确性。然而，我们的分析指出，机器学习偏差——这是由那些权衡偏差以减少变异性的方法引起的——可能会影响结果，并影响关于治疗效果的结论。我们详细记录了这种偏差在大规模实验中的出现，并提出有效的去偏差技术来应对这一挑战。
2501.00116	 | Text-to-Image GAN with Pretrained Representations	 | Xiaozhou You,Jian Zhang	 | 该研究提出了TIGER，一种结合视觉增强判别器和高容量生成器的文本到图像生成网络，以提高复杂场景下的生成效率和质量；TIGER在标准和零样本文本到图像合成任务中均表现出色，达到了最先进的性能，并且具有更快的推理速度和更少的模型参数。	 | Generating desired images conditioned on given text descriptions has received lots of attention. Recently, diffusion models and autoregressive models have demonstrated their outstanding expressivity and gradually replaced GAN as the favored architectures for text-to-image synthesis. However, they still face some obstacles: slow inference speed and expensive training costs. To achieve more powerful and faster text-to-image synthesis under complex scenes, we propose TIGER, a text-to-image GAN with pretrained representations. To be specific, we propose a vision-empowered discriminator and a high-capacity generator. (i) The vision-empowered discriminator absorbs the complex scene understanding ability and the domain generalization ability from pretrained vision models to enhance model performance. Unlike previous works, we explore stacking multiple pretrained models in our discriminator to collect multiple different representations. (ii) The high-capacity generator aims to achieve effective text-image fusion while increasing the model capacity. The high-capacity generator consists of multiple novel high-capacity fusion blocks (HFBlock). And the HFBlock contains several deep fusion modules and a global fusion module, which play different roles to benefit our model. Extensive experiments demonstrate the outstanding performance of our proposed TIGER both on standard and zero-shot text-to-image synthesis tasks. On the standard text-to-image synthesis task, TIGER achieves state-of-the-art performance on two challenging datasets, which obtain a new FID 5.48 (COCO) and 9.38 (CUB). On the zero-shot text-to-image synthesis task, we achieve comparable performance with fewer model parameters, smaller training data size and faster inference speed. Additionally, more experiments and analyses are conducted in the Supplementary Material.	 | 基于给定文本描述生成所需图像的领域受到了广泛关注。最近，扩散模型和自回归模型展示了其出色的表征能力，并逐渐取代了GAN成为文本到图像合成的首选架构。然而，它们仍然面临一些障碍：推理速度慢和训练成本高。为了在复杂场景下实现更强大的快速文本到图像合成，我们提出了一种预训练表示的文本到图像GAN——TIGER。具体而言，我们提出了一个视觉增强的判别器和一个高容量生成器。(i) 视觉增强的判别器从预训练视觉模型中吸收复杂场景理解和领域泛化能力，以增强模型性能。与先前的工作不同，我们在判别器中探索堆叠多个预训练模型，以收集多种不同的表示。(ii) 高容量生成器旨在通过增加模型容量来实现有效的文本图像融合。高容量生成器由多个新型高容量融合块（HFBlock）组成。HFBlock包含多个深度融合模块和一个全局融合模块，这些模块在不同程度上发挥作用，以利于我们的模型。广泛实验表明，我们在标准和零样本文本到图像合成任务中均表现出色。在标准文本到图像合成任务中，TIGER在两个挑战性数据集上达到了最先进的性能，分别获得了新的FID 5.48（COCO）和9.38（CUB）。在零样本文本到图像合成任务中，我们使用更少的模型参数、更小的训练数据集和更快的推理速度实现了可媲美的性能。此外，我们在补充材料中进行了更多实验和分析。
2501.00097	 | CaseSumm: A Large-Scale Dataset for Long-Context Summarization from U.S. Supreme Court Opinions	 | Mourad Heddaya,Kyle MacMillan,Anup Malani,Hongyuan Mei,Chenhao Tan	 | 本文介绍了CaseSumm，一个包含25.6万份美国最高法院裁决及其官方摘要的新数据集，用于法律领域长上下文摘要，并展示了基于LLM的摘要生成及其与自动和人工评估的对比，揭示了当前自动评估方法的局限性和人类评估的重要性。	 | This paper introduces CaseSumm, a novel dataset for long-context summarization in the legal domain that addresses the need for longer and more complex datasets for summarization evaluation. We collect 25.6K U.S. Supreme Court (SCOTUS) opinions and their official summaries, known as "syllabuses." Our dataset is the largest open legal case summarization dataset, and is the first to include summaries of SCOTUS decisions dating back to 1815.   We also present a comprehensive evaluation of LLM-generated summaries using both automatic metrics and expert human evaluation, revealing discrepancies between these assessment methods. Our evaluation shows Mistral 7b, a smaller open-source model, outperforms larger models on most automatic metrics and successfully generates syllabus-like summaries. In contrast, human expert annotators indicate that Mistral summaries contain hallucinations. The annotators consistently rank GPT-4 summaries as clearer and exhibiting greater sensitivity and specificity. Further, we find that LLM-based evaluations are not more correlated with human evaluations than traditional automatic metrics. Furthermore, our analysis identifies specific hallucinations in generated summaries, including precedent citation errors and misrepresentations of case facts. These findings demonstrate the limitations of current automatic evaluation methods for legal summarization and highlight the critical role of human evaluation in assessing summary quality, particularly in complex, high-stakes domains.   CaseSumm is available at https://huggingface.co/datasets/ChicagoHAI/CaseSumm	 | 本文介绍了CaseSumm，这是一个用于法律领域长上下文摘要的新数据集，旨在解决摘要评估中对更长、更复杂数据集的需求。我们收集了25.6万份美国最高法院（SCOTUS）裁决及其官方摘要，即“大纲”。我们的数据集是最大的开放法律案例摘要数据集，并且是第一个包含从1815年至今最高法院裁决摘要的数据集。此外，我们还呈现了LLM生成摘要的全面评估，使用自动指标和专家人工评估相结合的方法，揭示了这些评估方法之间的差异。评估结果显示，较小的开源模型Mistral 7b在大多数自动指标上表现优于大型模型，并成功生成了类似大纲的摘要。相比之下，人工专家注释者认为Mistral摘要包含幻觉。注释者一致将GPT-4摘要评为更清晰，并且更具敏感性和特异性。此外，我们发现，基于LLM的评估与传统自动指标的相关性并不更高。我们的分析还指出了生成摘要中的特定幻觉，包括先例引用错误和对案件事实的误述。这些发现表明，当前的自动评估方法对法律摘要的局限性，并强调了在复杂、高风险领域评估摘要质量时人类评估的关键作用。CaseSumm数据集可在以下链接获取：https://huggingface.co/datasets/ChicagoHAI/CaseSumm
2501.00093	 | Machine Learning Gravity Compactifications on Negatively Curved Manifolds	 | G. Bruno De Luca	 | 本文利用机器学习技术解决了一般扭曲引力紧化方程组，并通过神经网络在非平凡三维流形上求解爱因斯坦方程，展示了其在高维负曲率爱因斯坦度量构建和M理论紧化中的潜在应用。	 | Constructing the landscape of vacua of higher-dimensional theories of gravity by directly solving the low-energy (semi-)classical equations of motion is notoriously difficult. In this work, we investigate the feasibility of Machine Learning techniques as tools for solving the equations of motion for general warped gravity compactifications. As a proof-of-concept we use Neural Networks to solve the Einstein PDEs on non-trivial three manifolds obtained by filling one or more cusps of hyperbolic manifolds. While in three dimensions an Einstein metric is also locally hyperbolic, the generality and scalability of Machine Learning methods, the availability of explicit families of hyperbolic manifolds in higher dimensions, and the universality of the filling procedure strongly suggest that the methods and code developed in this work can be of broader applicability. Specifically, they can be used to tackle both the geometric problem of numerically constructing novel higher-dimensional negatively curved Einstein metrics, as well as the physical problem of constructing four-dimensional de Sitter compactifications of M-theory on the same manifolds.	 | 直接通过求解低能（半）经典方程来构建高维引力理论的真空景观历来十分困难。在本文中，我们研究了机器学习技术作为一种工具，用于解决一般扭曲引力紧化方程组的能力。作为概念验证，我们使用神经网络在填充双曲流形的一个或多个尖点后获得的非平凡三维流形上求解爱因斯坦偏微分方程（PDEs）。尽管在三维中，爱因斯坦度量也是局部双曲的，但机器学习方法的普遍性和可扩展性、高维中显式的双曲流形族以及填充程序的普适性强烈表明，本文中开发的方法和代码可以具有更广泛的应用。具体而言，它们可以用于解决几何问题——通过数值方法构建新的高维负曲率爱因斯坦度量，以及解决物理问题——在相同的流形上构造M理论的四维dS紧化。
2501.00089	 | Insights on Galaxy Evolution from Interpretable Sparse Feature Networks	 | John F. Wu	 | 本文提出了一种名为稀疏特征网络（SFNet）的新神经网络架构，能够在不影响预测准确性的前提下生成可解释的星系图像特征，从而有助于理解星系的物理属性及其演化规律。	 | Galaxy appearances reveal the physics of how they formed and evolved. Machine learning models can now exploit galaxies' information-rich morphologies to predict physical properties directly from image cutouts. Learning the relationship between pixel-level features and galaxy properties is essential for building a physical understanding of galaxy evolution, but we are still unable to explicate the details of how deep neural networks represent image features. To address this lack of interpretability, we present a novel neural network architecture called a Sparse Feature Network (SFNet). SFNets produce interpretable features that can be linearly combined in order to estimate galaxy properties like optical emission line ratios or gas-phase metallicity. We find that SFNets do not sacrifice accuracy in order to gain interpretability, and that they perform comparably well to cutting-edge models on astronomical machine learning tasks. Our novel approach is valuable for finding physical patterns in large datasets and helping astronomers interpret machine learning results.	 | 星系的外观揭示了它们形成和演化的物理规律。现在的机器学习模型能够利用星系信息丰富的形态直接从图像切片中预测物理属性。学习像素级特征与星系属性之间的关系对于构建星系演化的真实理解至关重要，但我们仍然无法解释深度神经网络如何表示图像特征的细节。为了解决这种不可解释性的问题，我们提出了一种名为稀疏特征网络（Sparse Feature Network，SFNet）的新神经网络架构。SFNets生成可解释的特征，这些特征可以线性组合以估算如光学发射线比或气态金属丰度等星系属性。我们发现，SFNets在获得可解释性的同时并未牺牲准确性，并且它们在天文学机器学习任务上的表现与当前最先进的模型相当。我们的新型方法对于在大数据集中找到物理模式并对天文学家解释机器学习结果具有很高的价值。
2501.00087	 | High-Dimensional Markov-switching Ordinary Differential Processes	 | Katherine Tsai,Mladen Kolar,Sanmi Koyejo	 | 本文研究了从离散观测中恢复马尔可夫切换的非线性常微分过程参数，并开发了一种两阶段算法来估计这些参数，同时提供了在β-混合条件下的理论保证。研究还应用该模型分析了ADHD组和正常控制组的静息状态脑网络差异。	 | We investigate the parameter recovery of Markov-switching ordinary differential processes from discrete observations, where the differential equations are nonlinear additive models. This framework has been widely applied in biological systems, control systems, and other domains; however, limited research has been conducted on reconstructing the generating processes from observations. In contrast, many physical systems, such as human brains, cannot be directly experimented upon and rely on observations to infer the underlying systems. To address this gap, this manuscript presents a comprehensive study of the model, encompassing algorithm design, optimization guarantees, and quantification of statistical errors. Specifically, we develop a two-stage algorithm that first recovers the continuous sample path from discrete samples and then estimates the parameters of the processes. We provide novel theoretical insights into the statistical error and linear convergence guarantee when the processes are $β$-mixing. Our analysis is based on the truncation of the latent posterior processes and demonstrates that the truncated processes approximate the true processes under mixing conditions. We apply this model to investigate the differences in resting-state brain networks between the ADHD group and normal controls, revealing differences in the transition rate matrices of the two groups.	 | 我们研究了从离散观测中恢复马尔可夫切换常微分过程的参数，其中常微分方程是非线性加性模型。这种框架在生物系统、控制系统和其他领域得到了广泛应用；然而，关于从观测数据重构生成过程的研究还较少。相比之下，许多物理系统，如人类大脑，无法直接进行实验，只能依赖观测数据来推断其内部系统。为了填补这一空白，本文对这一模型进行了全面研究，包括算法设计、优化保证和统计误差的量化。具体而言，我们开发了一个两阶段算法，首先从离散样本中恢复连续样本路径，然后估计过程参数。我们对当过程是$β$-混合同时状态误差和线性收敛保证提供了新的理论见解。我们的分析基于潜在后验过程的截断，并证明在混合同条件下，截断过程可以逼近真实的过程。我们应用该模型来探究ADHD组和正常控制组的静息状态脑网络之间的差异，揭示了两组之间的转换率矩阵的差异。
2501.00078	 | Human-like Bots for Tactical Shooters Using Compute-Efficient Sensors	 | Niels Justesen(<a href="http://modl.ai" rel="external noopener nofollow" class="link-external link-http">this http URL</a>),Maria Kaselimi(<a href="http://modl.ai" rel="external noopener nofollow" class="link-external link-http">this http URL</a>),Sam Snodgrass(<a href="http://modl.ai" rel="external noopener nofollow" class="link-external link-http">this http URL</a>),Miruna Vozaru(<a href="http://modl.ai" rel="external noopener nofollow" class="link-external link-http">this http URL</a>),Matthew Schlegel(<a href="http://modl.ai" rel="external noopener nofollow" class="link-external link-http">this http URL</a>),Jonas Wingren(<a href="http://modl.ai" rel="external noopener nofollow" class="link-external link-http">this http URL</a>),Gabriella A. B. Barros(<a href="http://modl.ai" rel="external noopener nofollow" class="link-external link-http">this http URL</a>),Tobias Mahlmann(<a href="http://modl.ai" rel="external noopener nofollow" class="link-external link-http">this http URL</a>),Shyam Sudhakaran(<a href="http://modl.ai" rel="external noopener nofollow" class="link-external link-http">this http URL</a>),Wesley Kerr(Riot Games),Albert Wang(Riot Games),Christoffer Holmgård(<a href="http://modl.ai" rel="external noopener nofollow" class="link-external link-http">this http URL</a>),Georgios N. Yannakakis(<a href="http://modl.ai" rel="external noopener nofollow" class="link-external link-http">this http URL</a>),Sebastian Risi(<a href="http://modl.ai" rel="external noopener nofollow" class="link-external link-http">this http URL</a>),Julian Togelius(<a href="http://modl.ai" rel="external noopener nofollow" class="link-external link-http">this http URL</a>)	 | 本文介绍了一种新的方法，通过模仿学习训练神经网络来使AI代理在商业标准的2v2战术射击游戏中表现类似人类，该方法在有限的计算资源下运行高效，并通过少量射线投射传感器高效捕捉关键空间信息。	 | Artificial intelligence (AI) has enabled agents to master complex video games, from first-person shooters like Counter-Strike to real-time strategy games such as StarCraft II and racing games like Gran Turismo. While these achievements are notable, applying these AI methods in commercial video game production remains challenging due to computational constraints. In commercial scenarios, the majority of computational resources are allocated to 3D rendering, leaving limited capacity for AI methods, which often demand high computational power, particularly those relying on pixel-based sensors. Moreover, the gaming industry prioritizes creating human-like behavior in AI agents to enhance player experience, unlike academic models that focus on maximizing game performance. This paper introduces a novel methodology for training neural networks via imitation learning to play a complex, commercial-standard, VALORANT-like 2v2 tactical shooter game, requiring only modest CPU hardware during inference. Our approach leverages an innovative, pixel-free perception architecture using a small set of ray-cast sensors, which capture essential spatial information efficiently. These sensors allow AI to perform competently without the computational overhead of traditional methods. Models are trained to mimic human behavior using supervised learning on human trajectory data, resulting in realistic and engaging AI agents. Human evaluation tests confirm that our AI agents provide human-like gameplay experiences while operating efficiently under computational constraints. This offers a significant advancement in AI model development for tactical shooter games and possibly other genres.	 | 人工智能（AI）已使代理能够掌握复杂的电子游戏，从第一人称射击游戏如《反恐精英》（Counter-Strike）到即时战略游戏如《星际争霸II》（StarCraft II）以及赛车游戏如《狂热赛车》（Gran Turismo）。虽然这些成就令人瞩目，但在商业视频游戏生产中应用这些AI方法仍然面临挑战，主要原因在于计算资源的限制。在商业场景中，大部分计算资源被分配到3D渲染，留给AI方法的资源有限，而后者往往需要高计算能力，尤其是那些依赖像素传感器的方法。此外，游戏行业更注重在AI代理中创造类似人类的行为，以提升玩家体验，而学术模型则更关注最大化游戏性能。本文介绍了一种新的方法，通过模仿学习训练神经网络来玩一个复杂的、符合商业标准的类似于《 VALORANT》的2v2战术射击游戏，此方法在推理过程中仅需要适度的CPU硬件。我们的方法利用了一个创新的、无像素感知架构，采用少量射线投射传感器，这些传感器能够高效地捕捉到关键的空间信息。这些传感器使AI能够在不使用传统方法的大量计算开销的情况下表现良好。模型通过监督学习模仿人类行为在人类轨迹数据上的训练，从而产生真实且具有吸引力的AI代理。人类评估测试证实，我们的AI代理提供了类似人类的游戏体验，同时在计算资源受限的情况下运行高效。这为战术射击游戏和其他类型的游戏中的AI模型开发提供了重大进展。
2501.00073	 | Position Information Emerges in Causal Transformers Without Positional Encodings via Similarity of Nearby Embeddings	 | Chunsheng Zuo,Pavel Guerzhoy,Michael Guerzhoy	 | 该研究探讨了在不使用位置编码的情况下，如何存储位置信息以解决需要位置感知的任务，发现相邻嵌入的相似性可以被变换器利用以重建标记的位置，并且这种模式在因果注意机制和随机初始化模型中普遍出现。	 | Transformers with causal attention can solve tasks that require positional information without using positional encodings. In this work, we propose and investigate a new hypothesis about how positional information can be stored without using explicit positional encoding. We observe that nearby embeddings are more similar to each other than faraway embeddings, allowing the transformer to potentially reconstruct the positions of tokens. We show that this pattern can occur in both the trained and the randomly initialized Transformer models with causal attention and no positional encodings over a common range of hyperparameters.	 | 具有因果注意机制的变换器可以在不使用位置编码的情况下解决需要位置信息的任务。在本文中，我们提出了一个新的假设，探讨如何在不使用显式位置编码的情况下存储位置信息。我们观察到，相邻的嵌入彼此更为相似，而距离较远的嵌入则更不相似，这使得变换器有可能重建标记的位置。我们展示了这种模式可以在具有因果注意机制和未使用位置编码的训练模型和随机初始化模型中，在一组共同的超参数范围内同时出现。
2501.00070	 | ICLR: In-Context Learning of Representations	 | Core Francisco Park,Andrew Lee,Ekdeep Singh Lubana,Yongyi Yang,Maya Okawa,Kento Nishi,Martin Wattenberg,Hidenori Tanaka	 | 这项研究通过设计一个“图跟踪”任务，发现大型语言模型在面对与预训练语义不同的上下文指定语义时，能够重新组织其表示方式，随着上下文规模的增加，这种重新组织会出现突变。此外，研究还揭示了模型在特定相关性上下文中能够保留部分上下文结构，但总体仍受预训练结构的影响。	 | Recent work has demonstrated that semantics specified by pretraining data influence how representations of different concepts are organized in a large language model (LLM). However, given the open-ended nature of LLMs, e.g., their ability to in-context learn, we can ask whether models alter these pretraining semantics to adopt alternative, context-specified ones. Specifically, if we provide in-context exemplars wherein a concept plays a different role than what the pretraining data suggests, do models reorganize their representations in accordance with these novel semantics? To answer this question, we take inspiration from the theory of conceptual role semantics and define a toy "graph tracing" task wherein the nodes of the graph are referenced via concepts seen during training (e.g., apple, bird, etc.) and the connectivity of the graph is defined via some predefined structure (e.g., a square grid). Given exemplars that indicate traces of random walks on the graph, we analyze intermediate representations of the model and find that as the amount of context is scaled, there is a sudden re-organization from pretrained semantic representations to in-context representations aligned with the graph structure. Further, we find that when reference concepts have correlations in their semantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure is still present in the representations, but is unable to dominate the pretrained structure. To explain these results, we analogize our task to energy minimization for a predefined graph topology, providing evidence towards an implicit optimization process to infer context-specified semantics. Overall, our findings indicate scaling context-size can flexibly re-organize model representations, possibly unlocking novel capabilities.	 | 近年来的研究已经证明，预训练数据指定的语义会影响大型语言模型（LLM）中不同概念表示的组织方式。然而，由于LLM的开放性质，例如它们的上下文学习能力，我们可以询问模型是否会改变这些预训练语义，以适应上下文指定的其他语义。特别是，如果我们提供一种在上下文中示例，其中概念在预训练数据中所起的作用不同，模型是否会根据这些新的语义重新组织其表示？为了回答这个问题，我们从概念角色语义的理论中汲取灵感，定义了一个简单的“图跟踪”任务，其中图的节点通过训练期间看到的概念（如苹果、鸟等）来引用，图的连接性通过某种预定义结构来定义（如一个平方网格）。鉴于表明图上随机行走轨迹的示例，我们分析了模型的中间表示，并发现随着上下文量的增加，会发生突然从预训练语义表示到与图结构对齐的上下文指定表示的重新组织。此外，我们发现当引用概念在其语义上有相关性（如星期一、星期二等）时，上下文指定的图结构仍然存在于表示中，但无法主导预训练结构。为了解释这些结果，我们将我们的任务类比为对于预定义图拓扑的能最优化，这提供了隐含优化过程的证据，以推断上下文指定的语义。总体而言，我们的发现表明，扩大上下文大小可以灵活地重新组织模型表示，可能解锁新的能力。
2501.00068	 | Dynamic Optimization of Storage Systems Using Reinforcement Learning Techniques	 | Chiyu Cheng,Chang Zhou,Yang Zhao,Jin Cao	 | 本文提出了一种名为RL-Storage的新型强化学习框架，旨在动态优化存储系统的配置参数，通过深度Q学习算法实时学习I/O模式并调整缓存大小、队列深度和预读设置，从而显著提升存储性能并减少资源浪费。实验结果显示，与传统启发式方法相比，RL-Storage在提高吞吐量和降低延迟方面表现出色，且具有极低的CPU和内存开销。	 | The exponential growth of data-intensive applications has placed unprecedented demands on modern storage systems, necessitating dynamic and efficient optimization strategies. Traditional heuristics employed for storage performance optimization often fail to adapt to the variability and complexity of contemporary workloads, leading to significant performance bottlenecks and resource inefficiencies. To address these challenges, this paper introduces RL-Storage, a novel reinforcement learning (RL)-based framework designed to dynamically optimize storage system configurations. RL-Storage leverages deep Q-learning algorithms to continuously learn from real-time I/O patterns and predict optimal storage parameters, such as cache size, queue depths, and readahead settings[1]. The proposed framework operates within the storage kernel, ensuring minimal latency and low computational overhead. Through an adaptive feedback mechanism, RL-Storage dynamically adjusts critical parameters, achieving efficient resource utilization across a wide range of workloads. Experimental evaluations conducted on a range of benchmarks, including RocksDB and PostgreSQL, demonstrate significant improvements, with throughput gains of up to 2.6x and latency reductions of 43% compared to baseline heuristics. Additionally, RL-Storage achieves these performance enhancements with a negligible CPU overhead of 0.11% and a memory footprint of only 5 KB, making it suitable for seamless deployment in production environments. This work underscores the transformative potential of reinforcement learning techniques in addressing the dynamic nature of modern storage systems. By autonomously adapting to workload variations in real time, RL-Storage provides a robust and scalable solution for optimizing storage performance, paving the way for next-generation intelligent storage infrastructures.	 | 随着数据密集型应用的指数级增长，现代存储系统面临着前所未有的需求，迫切需要动态和高效的优化策略。传统用于存储性能优化的启发式方法往往难以适应现代工作负载的多样性和复杂性，导致了显著的性能瓶颈和资源浪费。为应对这些挑战，本文提出了一种名为RL-Storage的新型强化学习（RL）框架，旨在动态优化存储系统配置。RL-Storage利用深度Q学习算法从实时I/O模式中持续学习，并预测最优的存储参数，如缓存大小、队列深度和预读设置[1]。该提出的框架运行在存储内核中，确保了最小的延迟和低计算开销。通过自适应反馈机制，RL-Storage动态调整关键参数，在广泛的应用场景下实现了高效的资源利用率。在RocksDB和PostgreSQL等基准测试中的实验评估显示，与基线启发式方法相比，吞吐量提高了2.6倍，延迟减少了43%。此外，RL-Storage通过极低的CPU开销（0.11%）和仅有5 KB的内存占用实现了这些性能提升，使其适合在生产环境中无缝部署。这项工作突显了强化学习技术在应对现代存储系统动态特性的潜力。通过实时自主适应工作负载的变化，RL-Storage提供了一种稳健且可扩展的解决方案，以优化存储性能，为下一代智能化存储基础设施铺平了道路。
2501.00066	 | On Adversarial Robustness of Language Models in Transfer Learning	 | Bohdan Turbal,Anastasiia Mazur,Jiaxu Zhao,Mykola Pechenizkiy	 | 研究发现，虽然大语言模型在迁移学习中能提升标准性能，但往往也会增加模型对对抗攻击的脆弱性，而较大的模型在这其中表现出了更强的抗性。研究强调了在迁移学习中考虑对抗鲁棒性的关键重要性，并为开发高性能且安全的大语言模型提供了见解。	 | We investigate the adversarial robustness of LLMs in transfer learning scenarios. Through comprehensive experiments on multiple datasets (MBIB Hate Speech, MBIB Political Bias, MBIB Gender Bias) and various model architectures (BERT, RoBERTa, GPT-2, Gemma, Phi), we reveal that transfer learning, while improving standard performance metrics, often leads to increased vulnerability to adversarial attacks. Our findings demonstrate that larger models exhibit greater resilience to this phenomenon, suggesting a complex interplay between model size, architecture, and adaptation methods. Our work highlights the crucial need for considering adversarial robustness in transfer learning scenarios and provides insights into maintaining model security without compromising performance. These findings have significant implications for the development and deployment of LLMs in real-world applications where both performance and robustness are paramount.	 | 我们研究了大语言模型（LLMs）在迁移学习场景中的对抗鲁棒性。通过在多个数据集（MBIB仇恨言论、MBIB政治偏见、MBIB性别偏见）和多种模型架构（BERT、RoBERTa、GPT-2、Gemma、Phi）上进行全面实验，我们揭示了尽管迁移学习能够提升标准性能指标，但往往也会导致模型对对抗攻击更加脆弱。我们的研究结果表明，较大的模型在这一现象中表现出了更强的抗性，这提示了模型大小、架构和适应方法之间复杂的关系。我们的工作强调了在迁移学习场景中考虑对抗鲁棒性的关键重要性，并提供了在不牺牲性能的情况下保持模型安全性的见解。这些发现对于在实际应用中同时追求高性能和鲁棒性的大语言模型的开发和部署具有重要意义。
2501.00064	 | Lungmix: A Mixup-Based Strategy for Generalization in Respiratory Sound Classification	 | Shijia Ge,Weixiang Zhang,Shuzhao Xie,Baixu Yan,Zhi Wang	 | 研究发现，深度学习模型在呼吸音分类任务中难以跨数据集泛化，主要因数据采集和标注一致性问题。为此，提出了一种名为Lungmix的数据增强技术，通过波形混合和标签插值提高模型泛化能力，实验表明Lungmix显著提升了模型性能，特别是在未见数据集上的分类准确性。	 | Respiratory sound classification plays a pivotal role in diagnosing respiratory diseases. While deep learning models have shown success with various respiratory sound datasets, our experiments indicate that models trained on one dataset often fail to generalize effectively to others, mainly due to data collection and annotation \emph{inconsistencies}. To address this limitation, we introduce \emph{Lungmix}, a novel data augmentation technique inspired by Mixup. Lungmix generates augmented data by blending waveforms using loudness and random masks while interpolating labels based on their semantic meaning, helping the model learn more generalized representations. Comprehensive evaluations across three datasets, namely ICBHI, SPR, and HF, demonstrate that Lungmix significantly enhances model generalization to unseen data. In particular, Lungmix boosts the 4-class classification score by up to 3.55\%, achieving performance comparable to models trained directly on the target dataset.	 | 呼吸音分类在诊断呼吸系统疾病中起着关键作用。虽然深度学习模型在多种呼吸音数据集中已显示出成功，但我们的实验表明，训练于一个数据集上的模型往往无法有效地泛化到其他数据集，主要原因在于数据采集和标注的一致性问题。为了解决这一限制，我们引入了名为Lungmix的新颖数据增强技术，该技术灵感来源于Mixup。Lungmix通过使用响度和随机遮罩混合波形，并基于语义意义插值标签，生成增强数据，帮助模型学习更泛化的表示。在ICBHI、SPR和HF三个数据集上的全面评估表明，Lungmix显著提升了模型对未见数据的泛化能力。特别是，Lungmix将4类分类分数提高了高达3.55%，其性能与直接在目标数据集上训练的模型相当。
2501.00053	 | Implementing Trust in Non-Small Cell Lung Cancer Diagnosis with a Conformalized Uncertainty-Aware AI Framework in Whole-Slide Images	 | Xiaoge Zhang,Tao Wang,Chao Yan,Fedaa Najdawi,Kai Zhou,Yuan Ma,Yiu-ming Cheung,Bradley A. Malin	 | TRUECAM是一种框架，旨在解决数字病理学AI模型在非小细胞肺癌分类中的信任度问题，通过光谱归一化神经高斯过程、模糊区域过滤和一致性预测等功能提升分类准确性、鲁棒性和可解释性。	 | Ensuring trustworthiness is fundamental to the development of artificial intelligence (AI) that is considered societally responsible, particularly in cancer diagnostics, where a misdiagnosis can have dire consequences. Current digital pathology AI models lack systematic solutions to address trustworthiness concerns arising from model limitations and data discrepancies between model deployment and development environments. To address this issue, we developed TRUECAM, a framework designed to ensure both data and model trustworthiness in non-small cell lung cancer subtyping with whole-slide images. TRUECAM integrates 1) a spectral-normalized neural Gaussian process for identifying out-of-scope inputs and 2) an ambiguity-guided elimination of tiles to filter out highly ambiguous regions, addressing data trustworthiness, as well as 3) conformal prediction to ensure controlled error rates. We systematically evaluated the framework across multiple large-scale cancer datasets, leveraging both task-specific and foundation models, illustrate that an AI model wrapped with TRUECAM significantly outperforms models that lack such guidance, in terms of classification accuracy, robustness, interpretability, and data efficiency, while also achieving improvements in fairness. These findings highlight TRUECAM as a versatile wrapper framework for digital pathology AI models with diverse architectural designs, promoting their responsible and effective applications in real-world settings.	 | 确保人工智能（AI）的信任度是使其在社会上负责任地发展的重要基础，特别是在癌症诊断中，误诊可能会导致严重的后果。当前的数字病理学AI模型缺乏系统性的解决方案来解决由于模型限制和部署环境与开发环境之间数据不一致所引发的信任度问题。为了解决这一问题，我们开发了TRUECAM，这是一种旨在确保非小细胞肺癌亚型分类中全切片图像的数据和模型信任度的框架。TRUECAM集成了以下功能： 1) 光谱归一化神经高斯过程，用于识别超出范围的输入； 2) 通过引导去除高度模糊区域，以过滤出高度模糊的区域，解决数据信任度问题； 3) 一致性预测，以确保可控的错误率。  我们系统地在多个大规模癌症数据集上评估了该框架，利用了任务特定模型和基础模型，证明了使用TRUECAM包裹的AI模型在分类准确性、鲁棒性、可解释性和数据效率方面明显优于缺乏此类指导的模型，同时还在公平性方面取得了改进。这些发现突显了TRUECAM作为适用于具有不同架构设计的数字病理学AI模型的多功能包装框架的作用，促进了它们在实际应用中的负责任和有效使用。
2501.00050	 | Learning in Multiple Spaces: Few-Shot Network Attack Detection with Metric-Fused Prototypical Networks	 | Fernando Martinez-Lopez,Lesther Santana,Mohamed Rahouti	 | 该研究提出了一种多空间原型学习（MSPL）框架，用于少样本攻击检测，通过在多个度量空间上运作并结合受限权重方案，提高了对罕见和零日攻击的识别能力；实验结果显示，MSPL在检测低轮廓和新型攻击类型方面优于传统方法，证明了其在网络安全中的应用潜力。	 | Network intrusion detection systems face significant challenges in identifying emerging attack patterns, especially when limited data samples are available. To address this, we propose a novel Multi-Space Prototypical Learning (MSPL) framework tailored for few-shot attack detection. The framework operates across multiple metric spaces-Euclidean, Cosine, Chebyshev, and Wasserstein distances-integrated through a constrained weighting scheme to enhance embedding robustness and improve pattern recognition. By leveraging Polyak-averaged prototype generation, the framework stabilizes the learning process and effectively adapts to rare and zero-day attacks. Additionally, an episodic training paradigm ensures balanced representation across diverse attack classes, enabling robust generalization. Experimental results on benchmark datasets demonstrate that MSPL outperforms traditional approaches in detecting low-profile and novel attack types, establishing it as a robust solution for zero-day attack detection.	 | 网络入侵检测系统在识别新兴攻击模式时面临着重大挑战，尤其是在数据样本有限的情况下。为了应对这一挑战，我们提出了一种新的多空间原型学习（MSPL）框架，专门用于少样本攻击检测。该框架在多个度量空间（欧几里得距离、余弦距离、切比雪夫距离和威耳斯特拉斯距离）上运作，并通过受限权重方案进行整合，以增强嵌入的鲁棒性和提高模式识别能力。通过利用Polyak平均原型生成，该框架稳定了学习过程，并有效地适应了罕见和零日攻击。此外，采用情景训练范式确保了对不同攻击类别的平衡表示，从而实现稳健的泛化。在基准数据集上的实验结果表明，MSPL在检测低轮廓和新型攻击类型方面优于传统方法，确立了其作为零日攻击检测稳健解决方案的地位。
2501.00045	 | Cross-Linguistic Examination of Machine Translation Transfer Learning	 | Saughmon Boujkian	 | 本研究评估了五种不同语言对在跨语言家族机器翻译中的迁移学习效果，发现迁移学习在不同语言家族中都是有效的，尽管超参数的影响有所不同；研究强调了使用中等批量大小和适当的学习率可以提高多语言模型训练的效率。	 | This study investigates the effectiveness of transfer learning in machine translation across diverse linguistic families by evaluating five distinct language pairs. Leveraging pre-trained models on high-resource languages, these models were fine-tuned on low-resource languages, examining variations in hyperparameters such as learning rate, batch size, number of epochs, and weight decay. The research encompasses language pairs from different linguistic backgrounds: Semitic (Modern Standard Arabic - Levantine Arabic), Bantu (Hausa - Zulu), Romance (Spanish - Catalan), Slavic (Slovakian - Macedonian), and language isolates (Eastern Armenian - Western Armenian). Results demonstrate that transfer learning is effective across different language families, although the impact of hyperparameters varies. A moderate batch size (e.g., 32) is generally more effective, while very high learning rates can disrupt model training. The study highlights the universality of transfer learning in multilingual contexts and suggests that consistent hyperparameter settings can simplify and enhance the efficiency of multilingual model training.	 | 本研究通过评估五种不同的语言对，探讨了迁移学习在跨不同语言家族的机器翻译中的有效性。利用高资源语言的预训练模型，并在此基础上对低资源语言进行微调，考察了学习率、批量大小、迭代次数和权重衰减等超参数的变化。研究涵盖了不同语言背景的语言对：闪米特语系（现代标准阿拉伯语 - 莱凡特阿拉伯语）、班图语系（豪萨语 - 拉苏鲁语）、罗曼语系（西班牙语 - 加泰罗尼亚语）、斯拉夫语系（斯洛伐克语 - 马其顿语）和孤立语系（东方亚美尼亚语 - 西部亚美尼亚语）。研究结果表明，迁移学习在不同语言家族中都是有效的，尽管超参数的影响有所不同。一般来说，中等批量大小（例如，32）更有效，而非常高的学习率可能会干扰模型训练。研究强调了迁移学习在多语言环境中的普适性，并指出一致的超参数设置可以简化和提高多语言模型训练的效率。
2501.00039	 | Speech Recognition With LLMs Adapted to Disordered Speech Using Reinforcement Learning	 | Chirag Nagpal,Subhashini Venugopalan,Jimmy Tobin,Marilyn Ladewig,Katherine Heller,Katrin Tomanek	 | 该研究提出了一种通过强化学习改进的大语言模型，能够更好地处理紊乱语音输入，并通过人类偏好的强化学习微调提升了模型的适应性，尽管最终性能未超越现有系统，但展示了在特定场景下监督微调之外的有效优化策略。	 | We introduce a large language model (LLM) capable of processing speech inputs and show that tuning it further with reinforcement learning on human preference (RLHF) enables it to adapt better to disordered speech than traditional fine-tuning. Our method replaces low-frequency text tokens in an LLM's vocabulary with audio tokens and enables the model to recognize speech by fine-tuning it on speech with transcripts. We then use RL with rewards based on syntactic and semantic accuracy measures generalizing the LLM further to recognize disordered speech. While the resulting LLM does not outperform existing systems for speech recognition, we find that tuning with reinforcement learning using custom rewards leads to substantially better performance than supervised fine-tuning of the language model, specifically when adapting to speech in a different setting. This presents a compelling alternative tuning strategy for speech recognition using large language models.	 | 我们介绍了一种能够处理语音输入的大语言模型（LLM），并通过进一步使用基于人类偏好的强化学习（RLHF）微调，展示了它比传统微调更能适应紊乱语音的能力。我们的方法用音频标记替换LLM词汇表中的低频文本标记，并通过使用带转录的语音对其进行微调，使模型能够识别语音。然后，我们使用基于语法规则和语义准确性的奖励进行RL训练，进一步推广LLM以识别紊乱语音。尽管最终生成的LLM在语音识别方面并未超过现有系统，但我们发现，使用自定义奖励进行强化学习微调比监督微调语言模型在特定场景下适应语音时，性能有了显著提升。这为使用大语言模型进行语音识别的调优策略提供了一个有说服力的替代方案。
2501.00034	 | Time Series Feature Redundancy Paradox: An Empirical Study Based on Mortgage Default Prediction	 | Chengyue Huang,Yahe Yang	 | 本文通过实证研究发现，在抵押贷款违约预测中，增加训练数据时间和非关键特征反而会降低预测效果，建议使用较短的时间窗口和关键特征以提高模型性能，从而挑战了“多多益善”的传统观点。	 | With the widespread application of machine learning in financial risk management, conventional wisdom suggests that longer training periods and more feature variables contribute to improved model performance. This paper, focusing on mortgage default prediction, empirically discovers a phenomenon that contradicts traditional knowledge: in time series prediction, increased training data timespan and additional non-critical features actually lead to significant deterioration in prediction effectiveness. Using Fannie Mae's mortgage data, the study compares predictive performance across different time window lengths (2012-2022) and feature combinations, revealing that shorter time windows (such as single-year periods) paired with carefully selected key features yield superior prediction results. The experimental results indicate that extended time spans may introduce noise from historical data and outdated market patterns, while excessive non-critical features interfere with the model's learning of core default factors. This research not only challenges the traditional "more is better" approach in data modeling but also provides new insights and practical guidance for feature selection and time window optimization in financial risk prediction.	 | 随着机器学习在金融风险管理中的广泛应用，普遍认为更长的训练时间和更多的特征变量有助于提升模型性能。本文聚焦于抵押贷款违约预测，通过实证研究发现了一个与传统知识相悖的现象：在时间序列预测中，增加训练数据时间跨度和添加非关键特征实际上会导致预测效果显著下降。利用房利美（Fannie Mae）的抵押贷款数据，研究比较了不同时间窗口长度（2012-2022）和特征组合下的预测性能，结果显示，较短的时间窗口（如单年度周期）与精心选择的关键特征相结合，能够获得更好的预测结果。实验结果表明，长的时间跨度可能会引入历史数据中的噪声和过时的市场模式，而过多的非关键特征会干扰模型对核心违约因素的学习。这项研究不仅挑战了数据建模中的“多多益善”传统观点，还为金融风险预测中的特征选择和时间窗口优化提供了新的见解和实践指导。
2501.00029	 | A Breadth-First Catalog of Text Processing, Speech Processing and Multimodal Research in South Asian Languages	 | Pranav Gupta	 | 该研究回顾了2022年1月至2024年10月期间南亚语言中关于文本处理、多模态模型和语音处理的最新文献，并重点分析了21种低资源南亚语言的技术进展，旨在为相关研究人员提供全面的技术概览，支持未来的研究工作。	 | We review the recent literature (January 2022- October 2024) in South Asian languages on text-based language processing, multimodal models, and speech processing, and provide a spotlight analysis focused on 21 low-resource South Asian languages, namely Saraiki, Assamese, Balochi, Bhojpuri, Bodo, Burmese, Chhattisgarhi, Dhivehi, Gujarati, Kannada, Kashmiri, Konkani, Khasi, Malayalam, Meitei, Nepali, Odia, Pashto, Rajasthani, Sindhi, and Telugu. We identify trends, challenges, and future research directions, using a step-wise approach that incorporates relevance classification and clustering based on large language models (LLMs). Our goal is to provide a breadth-first overview of the recent developments in South Asian language technologies to NLP researchers interested in working with South Asian languages.	 | 我们回顾了2022年1月至2024年10月期间南亚语言中关于基于文本的语言处理、多模态模型以及语音处理的最新文献，并对21种低资源南亚语言，即旁遮普语（Saraiki）、阿萨姆语（Assamese）、俾路其语（Balochi）、薄普里语（Bhojpuri）、邦达语（Bodo）、缅甸语（Burmese）、恰蒂斯加尔语（Chhattisgarhi）、 Divehi语（Dhivehi）、古吉拉特语（Gujarati）、卡纳达语（Kannada）、克什米尔语（Kashmiri）、古吉拉特语（Konkani）、卡西语（Khasi）、马拉雅拉姆语（Malayalam）、曼尼普尔语（Meitei）、尼泊尔语（Nepali）、奥里亚语（Odia）、普什图语（Pashto）、拉贾斯坦语（Rajasthani）、信德语（Sindhi）和特伦甘语（Telugu）进行了重点分析。我们通过逐步方法，结合大型语言模型（LLMs）的关联分类和聚类，识别趋势、挑战和未来研究方向。我们的目标是为对南亚语言感兴趣的研究人员提供一个全面的南亚语言技术最新进展概览，助力他们在南亚语言方向的研究工作。
2501.00020	 | Magnetic Field Data Calibration with Transformer Model Using Physical Constraints: A Scalable Method for Satellite Missions, Illustrated by Tianwen-1	 | Beibei Li(Deep Space Exploration Laboratory),Yutian Chi(Deep Space Exploration Laboratory),Yuming Wang(Deep Space Exploration Laboratory and School of Earth and Space Sciences University of Science and Technology of China)	 | 本文提出了一种将基于Transformer的神经网络与麦克斯韦方程物理原理结合的方法，用于校正天问一号火星任务的磁场数据，显著提高了校正效率和数据准确性。这种方法相比传统方法大幅缩短了校准和预测时间，加速了空间天气建模和行星磁层研究。	 | This study introduces a novel approach that integrates the magnetic field data correction from the Tianwen-1 Mars mission with a neural network architecture constrained by physical principles derived from Maxwell's equation equations. By employing a Transformer based model capable of efficiently handling sequential data, the method corrects measurement anomalies caused by satellite dynamics, instrument interference, and environmental noise. As a result, it significantly improves both the accuracy and the physical consistency of the calibrated data. Compared to traditional methods that require long data segments and manual intervention often taking weeks or even months to complete this new approach can finish calibration in just minutes to hours, and predictions are made within seconds. This innovation not only accelerates the process of space weather modeling and planetary magnetospheric studies but also provides a robust framework for future planetary exploration and solar wind interaction research.	 | 本文介绍了一种新颖的方法，该方法将天问一号火星任务的磁场数据校正与基于物理原理的神经网络架构相结合，这些物理原理源自麦克斯韦方程组。通过采用一种基于Transformer的模型，该模型能够高效处理序列数据，这种方法可以校正由卫星动力学、仪器干扰和环境噪声引起的测量异常。因此，它显著提高了校正数据的准确性和物理一致性。与传统的需要长时间数据段并经常需要数周甚至数月的人工干预相比，新方法可以在几分钟到几小时内完成校准，预测结果也可以在几秒钟内得出。这种创新不仅加速了空间天气建模和行星磁层研究的过程，还为未来的行星探测和太阳风相互作用研究提供了一个稳健的框架。
2501.00015	 | Energy-Efficient Sampling Using Stochastic Magnetic Tunnel Junctions	 | Nicolas Alder,Shivam Nitin Kajale,Milin Tunsiricharoengul,Deblina Sarkar,Ralf Herbrich	 | 研究人员提出了一种利用室温自旋磁隧道结器件进行高效均匀Float16采样的算法，该算法在能效上比最先进算法提升了至少9721倍，能够生成真正随机的浮点数，并将任意分布分解为非重叠的近似均匀分布进行采样。	 | (Pseudo)random sampling, a costly yet widely used method in (probabilistic) machine learning and Markov Chain Monte Carlo algorithms, remains unfeasible on a truly large scale due to unmet computational requirements. We introduce an energy-efficient algorithm for uniform Float16 sampling, utilizing a room-temperature stochastic magnetic tunnel junction device to generate truly random floating-point numbers. By avoiding expensive symbolic computation and mapping physical phenomena directly to the statistical properties of the floating-point format and uniform distribution, our approach achieves a higher level of energy efficiency than the state-of-the-art Mersenne-Twister algorithm by a minimum factor of 9721 and an improvement factor of 5649 compared to the more energy-efficient PCG algorithm. Building on this sampling technique and hardware framework, we decompose arbitrary distributions into many non-overlapping approximative uniform distributions along with convolution and prior-likelihood operations, which allows us to sample from any 1D distribution without closed-form solutions. We provide measurements of the potential accumulated approximation errors, demonstrating the effectiveness of our method.	 | 伪随机采样，作为一种在概率机器学习和马尔可夫链蒙特卡洛算法中广泛使用但成本高昂的方法，由于计算需求未得到满足，使其在真正大规模应用中不可行。我们提出了一种利用室温自旋磁隧道结器件进行均匀Float16采样的能效算法，以生成真正随机的浮点数。通过避免昂贵的符号计算，并直接将物理现象映射到浮点格式和均匀分布的统计特性，我们方法的能效比目前最先进的梅森旋转算法至少提高了9721倍，并且与更节能的PCG算法相比，能效提高了5649倍。基于此采样技术及其硬件框架，我们将任意分布分解为许多非重叠的近似均匀分布，并结合卷积和先验似然操作，从而可以在没有任何闭式解的情况下从任何1D分布中进行采样。我们提供了潜在累积近似误差的测量结果，证明了我们方法的有效性。
2501.00013	 | Relation-Aware Equivariant Graph Networks for Epitope-Unknown Antibody Design and Specificity Optimization	 | Lirong Wu,Haitao Lin,Yufei Huang,Zhangyang Gao,Cheng Tan,Yunfan Liu,Tailin Wu,Stan Z. Li	 | 本文提出了一种名为“关系感知抗体设计”（RAAD）的新型框架，通过动态建模抗原-抗体相互作用来协同设计CDRs的序列和结构，并通过新的评估指标和对比度增强的特异性约束优化抗体的特异性，从而有效应对现有方法面临的挑战。	 | Antibodies are Y-shaped proteins that protect the host by binding to specific antigens, and their binding is mainly determined by the Complementary Determining Regions (CDRs) in the antibody. Despite the great progress made in CDR design, existing computational methods still encounter several challenges: 1) poor capability of modeling complex CDRs with long sequences due to insufficient contextual information; 2) conditioned on pre-given antigenic epitopes and their static interaction with the target antibody; 3) neglect of specificity during antibody optimization leads to non-specific antibodies. In this paper, we take into account a variety of node features, edge features, and edge relations to include more contextual and geometric information. We propose a novel Relation-Aware Antibody Design (RAAD) framework, which dynamically models antigen-antibody interactions for co-designing the sequences and structures of antigen-specific CDRs. Furthermore, we propose a new evaluation metric to better measure antibody specificity and develop a contrasting specificity-enhancing constraint to optimize the specificity of antibodies. Extensive experiments have demonstrated the superior capability of RAAD in terms of antibody modeling, generation, and optimization across different CDR types, sequence lengths, pre-training strategies, and input contexts.	 | 抗体是Y形蛋白质，通过与特定抗原结合来保护宿主，而其结合主要由抗体中的互补决定区（CDRs）决定。尽管在CDR设计方面取得了显著进展，但现有的计算方法仍然面临几个挑战：1) 无法很好地建模长序列的复杂CDRs，因为缺乏足够的上下文信息；2) 预先给定抗原表位及其与目标抗体的静态相互作用；3) 在抗体优化过程中忽视特异性，导致非特异性抗体。本文中，我们考虑了多种节点特征、边特征和边关系，以纳入更多的上下文和几何信息。我们提出了一种新型的“关系感知抗体设计”（RAAD）框架，该框架动态建模抗原-抗体相互作用，以协同设计抗原特异性CDRs的序列和结构。此外，我们提出了一种新的评估指标，更好地衡量抗体特异性，并开发了一种对比度增强的特异性约束来优化抗体的特异性。广泛的实验表明，RAAD在不同CDR类型、序列长度、预训练策略和输入上下文方面，具有优越的抗体建模、生成和优化能力。
2501.00003	 | Machine learning models for Si nanoparticle growth in nonthermal plasma	 | Matt Raymond,Paolo Elvati,Jacob C. Saldinger,Jonathan Lin,Xuetao Shi,Angela Violi	 | 该研究探讨了利用机器学习加速非热等离子体中纳米颗粒生长模型参数计算的方法，并在硅烷片段反应性经典分子动力学数据上进行了验证，结果显示适当的选择损失函数和不变性可以显著提高预测性能，同时减少计算成本。	 | Nanoparticles (NPs) formed in nonthermal plasmas (NTPs) can have unique properties and applications. However, modeling their growth in these environments presents significant challenges due to the non-equilibrium nature of NTPs, making them computationally expensive to describe. In this work, we address the challenges associated with accelerating the estimation of parameters needed for these models. Specifically, we explore how different machine learning models can be tailored to improve prediction outcomes. We apply these methods to reactive classical molecular dynamics data, which capture the processes associated with colliding silane fragments in NTPs. These reactions exemplify processes where qualitative trends are clear, but their quantification is challenging, hard to generalize, and requires time-consuming simulations. Our results demonstrate that good prediction performance can be achieved when appropriate loss functions are implemented and correct invariances are imposed. While the diversity of molecules used in the training set is critical for accurate prediction, our findings indicate that only a fraction (15-25\%) of the energy and temperature sampling is required to achieve high levels of accuracy. This suggests a substantial reduction in computational effort is possible for similar systems.	 | 非热等离子体（NTPs）中形成的纳米颗粒（NPs）具有独特的特性和应用潜力。然而，由于NTPs处于非平衡状态，这使得对其生长过程的建模成为一项重大挑战，从而增加了计算成本。在本工作中，我们探讨了如何加快估算这些模型所需参数的计算速度。具体来说，我们研究了如何通过不同的机器学习模型来改进预测结果。我们应用这些方法到反应性经典分子动力学数据中，这些数据捕捉到了NTP中硅烷片段碰撞相关的过程。这些反应体现了虽然定性趋势明显，但量化却困难重重、难以泛化，并需要耗时的模拟。我们的结果表明，当实现了合适的损失函数并施加正确的不变性时，可以实现良好的预测性能。虽然训练集中的分子多样性对于准确预测至关重要，但我们的发现表明，实现高精度所需的能量和温度采样只需15%-25%即可，这表明对于类似系统，计算成本可以大幅度减少。
