| ID | Title | Authors | Summary (zh) | Abstract (en) | Abstract (zh) | 
| --- | --- | --- | --- | --- | --- |
2501.01957	 | VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction	 | Chaoyou Fu,Haojia Lin,Xiong Wang,Yi-Fan Zhang,Yunhang Shen,Xiaoyu Liu,Yangze Li,Zuwei Long,Heting Gao,Ke Li,Xiawu Zheng,Rongrong Ji,Xing Sun,Caifeng Shan,Ran He	 | 本文提出了一种多阶段训练方法，使大型语言模型逐步理解视觉和语音信息，从而实现流畅的多模态交互，该方法不仅保留了强大的视觉语言能力，还提高了多模态端到端的响应速度，能够在图像、视频和语音任务上与最先进的方法竞争，展示出接近实时的交互性能。	 | Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction.	 | 近年来，多模态大型语言模型（MLLMs）通常侧重于整合视觉和文本模态，而较少关注语音在增强交互中的作用。然而，语音在多模态对话系统中扮演着至关重要的角色，同时在视觉任务和语音任务上实现高性能仍然是一个重大挑战，原因在于不同模态的根本差异。在本文中，我们提出了一种仔细设计的多阶段训练方法，该方法逐步训练LLM以理解视觉和语音信息，最终实现流畅的视觉和语音交互。我们的方法不仅保留了强大的视觉语言能力，还能够实现高效的语音到语音对话能力，无需单独的ASR和TTS模块，从而显著加快了多模态端到端的响应速度。通过在图像、视频和语音任务基准上与最先进的方法进行比较，我们证明我们的模型具有强大的视觉和语音能力，能够实现接近实时的视觉和语音交互。
2501.01949	 | VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo Alignment	 | Wenyan Cong,Kevin Wang,Jiahui Lei,Colton Stearns,Yuanhao Cai,Dilin Wang,Rakesh Ranjan,Matt Feiszli,Leonidas Guibas,Zhangyang Wang,Weiyao Wang,Zhiwen Fan	 | VideoLifter是一种新的框架，它利用几何先验直接从视频序列中优化全局稀疏到密集的3D表示，通过跨帧和片段传播稀疏点对应关系来逐步细化相机姿态和3D结构，从而显著提高重建速度和准确性。	 | Efficiently reconstructing accurate 3D models from monocular video is a key challenge in computer vision, critical for advancing applications in virtual reality, robotics, and scene understanding. Existing approaches typically require pre-computed camera parameters and frame-by-frame reconstruction pipelines, which are prone to error accumulation and entail significant computational overhead. To address these limitations, we introduce VideoLifter, a novel framework that leverages geometric priors from a learnable model to incrementally optimize a globally sparse to dense 3D representation directly from video sequences. VideoLifter segments the video sequence into local windows, where it matches and registers frames, constructs consistent fragments, and aligns them hierarchically to produce a unified 3D model. By tracking and propagating sparse point correspondences across frames and fragments, VideoLifter incrementally refines camera poses and 3D structure, minimizing reprojection error for improved accuracy and robustness. This approach significantly accelerates the reconstruction process, reducing training time by over 82% while surpassing current state-of-the-art methods in visual fidelity and computational efficiency.	 | 从单目视频高效地重建准确的3D模型是计算机视觉中的一个关键挑战，对于推动虚拟现实、机器人技术和场景理解的应用至关重要。现有方法通常需要预先计算的相机参数和逐帧重建管道，这容易累积误差并带来巨大的计算开销。为了克服这些限制，我们引入了VideoLifter这一新的框架，它利用可学习模型中的几何先验来直接从视频序列中逐步优化一个全局稀疏到密集的3D表示。VideoLifter将视频序列分割成局部窗口，在这些窗口中匹配和注册帧、构建一致的片段，并逐级对齐它们以生成统一的3D模型。通过跨帧和片段跟踪和传播稀疏点对应关系，VideoLifter逐步细化相机姿态和3D结构，最小化重投影误差以提高准确性和鲁棒性。这种方法显著加快了重建过程，训练时间减少了超过82%，同时在视觉保真度和计算效率方面超越了当前最先进的方法。
2501.01932	 | Bridging Classification and Segmentation in Osteosarcoma Assessment via Foundation and Discrete Diffusion Models	 | Manh Duong Nguyen,Dac Thai Nguyen,Trung Viet Nguyen,Homi Yamada,Huy Hieu Pham,Phi Le Nguyen	 | FDDM是一种新颖的框架，通过结合局部区域分类和区域分割来提高骨肉瘤WSI中坏死评估的准确性，相较于最先进的方法，其分割性能和坏死率估计均有显著提升。	 | Osteosarcoma, the most common primary bone cancer, often requires accurate necrosis assessment from whole slide images (WSIs) for effective treatment planning and prognosis. However, manual assessments are subjective and prone to variability. In response, we introduce FDDM, a novel framework bridging the gap between patch classification and region-based segmentation. FDDM operates in two stages: patch-based classification, followed by region-based refinement, enabling cross-patch information intergation. Leveraging a newly curated dataset of osteosarcoma images, FDDM demonstrates superior segmentation performance, achieving up to a 10% improvement mIOU and a 32.12% enhancement in necrosis rate estimation over state-of-the-art methods. This framework sets a new benchmark in osteosarcoma assessment, highlighting the potential of foundation models and diffusion-based refinements in complex medical imaging tasks.	 | 以下是该摘要的中文翻译：  骨肉瘤是最常见的原发性骨癌，通常需要从全玻片图像（WSI）中进行准确的坏死评估，以实现有效的治疗规划和预后。然而，手动评估主观性强且容易产生变异。为应对这一挑战，我们提出了一种名为FDDM的新颖框架，该框架在局部区域分类和区域分割之间架起桥梁。FDDM分为两个阶段：基于局部区域的分类，随后是基于区域的细化，从而实现跨局部区域信息的整合。利用新整理的一组骨肉瘤图像数据集，FDDM在分割性能上表现出优越性，相比最先进的方法，其mIOU提高了10%，坏死率估计提高了32.12%。该框架在骨肉瘤评估中设立了新的基准，突显了基础模型和基于扩散的细化方法在复杂医学成像任务中的潜力。
2501.01926	 | Mitigating Hallucination for Large Vision Language Model by Inter-Modality Correlation Calibration Decoding	 | Jiaming Li,Jiacheng Zhang,Zequn Jie,Lin Ma,Guanbin Li	 | 本文提出了一种名为跨模态相关校准解码（IMCCD）的方法，旨在减少大型跨模态语言模型在生成任务中的幻觉问题，通过设计跨模态值增强解码（CMVED）模块和内容驱动的注意力精炼（CDAR）模块来缓解幻觉，实验结果表明该方法优于现有技术。	 | Large vision-language models (LVLMs) have shown remarkable capabilities in visual-language understanding for downstream multi-modal tasks. Despite their success, LVLMs still suffer from generating hallucinations in complex generation tasks, leading to inconsistencies between visual inputs and generated content. To address this issue, some approaches have introduced inference-time interventions, such as contrastive decoding and attention rectification, to reduce overreliance on language priors. However, these approaches overlook hallucinations stemming from spurious inter-modality correlations. In this paper, we propose an Inter-Modality Correlation Calibration Decoding (IMCCD) method to mitigate hallucinations in LVLMs in a training-free manner. In this method, we design a Cross-Modal Value-Enhanced Decoding(CMVED) module to alleviate hallucination by a novel contrastive decoding mechanism. During the estimation of distorted distribution, CMVED masks the value vectors associated with significant cross-modal attention weights, which address both uni-modality overreliance and misleading inter-modality correlations. Additionally, a Content-Driven Attention Refinement(CDAR) module refines cross-modal attention weights, guiding LVLMs to focus on important visual content. Experimental results on diverse hallucination benchmarks validate the superiority of our method over existing state-of-the-art techniques in reducing hallucinations in LVLM text generation. Our code will be available at https://github.com/lijm48/IMCCD.	 | 大型跨模态语言模型（Large Vision-Language Models, LVLMs）在下游多模态任务中的视觉语言理解方面展现出了显著的能力。尽管取得了成功，但在复杂的生成任务中，LVLMs 仍然会生成幻觉，导致视觉输入与生成内容之间的一致性问题。为了解决这一问题，一些方法在推理阶段引入了干预机制，如对比解码和注意力修正，以减少对语言先验的过度依赖。然而，这些方法忽视了源自虚假跨模态相关性的幻觉。在本文中，我们提出了一种名为跨模态相关校准解码（Inter-Modality Correlation Calibration Decoding, IMCCD）的方法，旨在在无需训练的情况下减轻LVLMs中的幻觉。在该方法中，我们设计了一个跨模态值增强解码（Cross-Modal Value-Enhanced Decoding, CMVED）模块，通过一种新颖的对比解码机制来缓解幻觉。在估计扭曲分布时，CMVED通过掩盖与显著跨模态注意力权重相关的价值向量来解决单模态过度依赖和误导性的跨模态相关性问题。此外，内容驱动的注意力精炼（Content-Driven Attention Refinement, CDAR）模块进一步精炼跨模态注意力权重，引导LVLMs关注重要的视觉内容。在多种幻觉基准测试上的实验结果验证了我们的方法在减少LVLM文本生成中的幻觉方面优于现有最先进的技术。我们的代码将在 https://github.com/lijm48/IMCCD 可用。
2501.01924	 | Transformer-Driven Inverse Problem Transform for Fast Blind Hyperspectral Image Dehazing	 | Po-Wei Tang,Chia-Hsiang Lin,Yangrui Liu	 | 本文提出了一种基于变换器驱动的逆问题变换（T2HyDHZ）方法，旨在通过将混合超光谱去雾（HyDHZ）问题重新表述为光谱超分辨率（SSR）问题来解决复杂逆问题，从而实现自动化的超光谱图像去雾处理，显著改善了色差表现。	 | Hyperspectral dehazing (HyDHZ) has become a crucial signal processing technology to facilitate the subsequent identification and classification tasks, as the airborne visible/infrared imaging spectrometer (AVIRIS) data portal reports a massive portion of haze-corrupted areas in typical hyperspectral remote sensing images. The idea of inverse problem transform (IPT) has been proposed in recent remote sensing literature in order to reformulate a hardly tractable inverse problem (e.g., HyDHZ) into a relatively simple one. Considering the emerging spectral super-resolution (SSR) technique, which spectrally upsamples multispectral data to hyperspectral data, we aim to solve the challenging HyDHZ problem by reformulating it as an SSR problem. Roughly speaking, the proposed algorithm first automatically selects some uncorrupted/informative spectral bands, from which SSR is applied to spectrally upsample the selected bands in the feature space, thereby obtaining a clean hyperspectral image (HSI). The clean HSI is then further refined by a deep transformer network to obtain the final dehazed HSI, where a global attention mechanism is designed to capture nonlocal information. There are very few HyDHZ works in existing literature, and this article introduces the powerful spatial-spectral transformer into HyDHZ for the first time. Remarkably, the proposed transformer-driven IPT-based HyDHZ (T2HyDHZ) is a blind algorithm without requiring the user to manually select the corrupted region. Extensive experiments demonstrate the superiority of T2HyDHZ with less color distortion.	 | 超光谱去雾（HyDHZ）已成为促进后续识别和分类任务的关键信号处理技术，因为机载可见/红外成像光谱仪（AVIRIS）数据门户报告了典型超光谱遥感图像中大量受雾污染的区域。近年来，在遥感文献中提出了逆问题变换（IPT）的想法，旨在将一个难以解决的逆问题（例如HyDHZ）重新表述为相对简单的问题。鉴于新兴的光谱超分辨率（SSR）技术，该技术将多光谱数据上采样为超光谱数据，我们希望通过将该难题重新表述为SSR问题来解决具有挑战性的HyDHZ问题。简而言之，所提出的算法首先自动选择一些未受污染/信息丰富的光谱带，在这些带中应用SSR技术在特征空间中光谱上采样所选的带，从而获得一个干净的超光谱图像（HSI）。然后通过深度变换网络进一步细化干净的HSI，以获得最终的去雾HSI，其中设计了全局注意力机制以捕获非局部信息。目前文献中很少有关于HyDHZ的研究，本文首次将强大而强大的空间光谱变换器引入到HyDHZ中。值得注意的是，所提出的基于变换器驱动的IPT的HyDHZ（T2HyDHZ）算法是一个盲处理算法，不需要用户手动选择受污染区域。广泛的实验表明，T2HyDHZ在色差方面具有明显的优势。
2501.01908	 | Detecting and Mitigating Adversarial Attacks on Deep Learning-Based MRI Reconstruction Without Any Retraining	 | Mahdi Saberi,Chi Zhang,Mehmet Akcakaya	 | 本文提出了一种无需重新训练即可检测和缓解下采样磁共振成像（MRI）重建模型对抗性攻击的方法，该方法基于循环测量一致性理念，并通过新的目标函数进一步优化，实验结果表明该方法在多个数据集和攻击类型下均优于需要重新训练的传统缓解方法。	 | Deep learning (DL) methods, especially those based on physics-driven DL, have become the state-of-the-art for reconstructing sub-sampled magnetic resonance imaging (MRI) data. However, studies have shown that these methods are susceptible to small adversarial input perturbations, or attacks, resulting in major distortions in the output images. Various strategies have been proposed to reduce the effects of these attacks, but they require retraining and may lower reconstruction quality for non-perturbed/clean inputs. In this work, we propose a novel approach for detecting and mitigating adversarial attacks on MRI reconstruction models without any retraining. Our detection strategy is based on the idea of cyclic measurement consistency. The output of the model is mapped to another set of MRI measurements for a different sub-sampling pattern, and this synthesized data is reconstructed with the same model. Intuitively, without an attack, the second reconstruction is expected to be consistent with the first, while with an attack, disruptions are present. Subsequently, this idea is extended to devise a novel objective function, which is minimized within a small ball around the attack input for mitigation. Experimental results show that our method substantially reduces the impact of adversarial perturbations across different datasets, attack types/strengths and PD-DL networks, and qualitatively and quantitatively outperforms conventional mitigation methods that involve retraining.	 | 深度学习（DL）方法，尤其是基于物理驱动的DL方法，已经成为重建下采样磁共振成像（MRI）数据的最先进的技术。然而，研究表明这些方法对小的对抗性输入扰动或攻击非常敏感，导致输出图像出现重大失真。已经提出了多种策略来减少这些攻击的影响，但这些策略需要重新训练，并可能降低非扰动/干净输入的重建质量。在这项工作中，我们提出了一种无需重新训练即可检测和缓解MRI重建模型对抗性攻击的新型方法。我们的检测策略基于循坏测量一致性（cyclic measurement consistency）的想法。模型的输出被映射到另一组不同的下采样模式的MRI测量值上，并使用相同的模型对合成数据进行重建。直观上，在没有攻击的情况下，第二次重建应与第一次保持一致，而有攻击的情况下则会出现破坏。随后，这一想法被扩展以设计一种新的目标函数，在攻击输入周围的较小球体内最小化该目标函数以进行缓解。实验结果表明，我们的方法显著减少了不同数据集、不同类型/强度的攻击以及PD-DL网络中的对抗性扰动的影响，并在定性和定量上优于涉及重新训练的传统缓解方法。
2501.01904	 | Virgo: A Preliminary Exploration on Reproducing o1-like MLLM	 | Yifan Du,Zikang Liu,Yifan Li,Wayne Xin Zhao,Yuqi Huo,Bingning Wang,Weipeng Chen,Zheng Liu,Zhongyuan Wang,Ji-Rong Wen	 | 本文提出了一种通过微调多模态大型语言模型（MLLMs）构建多模态慢思考系统的简单方法，并构建了名为Virgo的系统，发现基于文本的长形式推理数据能更有效地激发MLLMs的慢思考能力。	 | Recently, slow-thinking reasoning systems, built upon large language models (LLMs), have garnered widespread attention by scaling the thinking time during inference. There is also growing interest in adapting this capability to multimodal large language models (MLLMs). Given that MLLMs handle more complex data semantics across different modalities, it is intuitively more challenging to implement multimodal slow-thinking systems.   To address this issue, in this paper, we explore a straightforward approach by fine-tuning a capable MLLM with a small amount of textual long-form thought data, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning with long thought). We find that these long-form reasoning processes, expressed in natural language, can be effectively transferred to MLLMs. Moreover, it seems that such textual reasoning data can be even more effective than visual reasoning data in eliciting the slow-thinking capacities of MLLMs. While this work is preliminary, it demonstrates that slow-thinking capacities are fundamentally associated with the language model component, which can be transferred across modalities or domains. This finding can be leveraged to guide the development of more powerful slow-thinking reasoning systems. We release our resources at https://github.com/RUCAIBox/Virgo.	 | 近年来，基于大型语言模型（LLMs）的慢思考推理系统通过在推断过程中扩展思考时间而引起了广泛关注。人们也越来越对将这种能力应用于多模态大型语言模型（MLLMs）产生了兴趣。由于MLLMs处理的是不同模态下更为复杂的语义数据，因此构建多模态慢思考系统的挑战性更为直观。为了解决这一问题，本文探索了一种简单的方法，即通过少量文本长形式的思考数据微调一个能力较强的MLLM，从而构建了一个多模态慢思考系统，名为Virgo（视觉推理与长思考）。我们发现，这些用自然语言表达的长形式推理过程可以有效地转移到MLLMs中。此外，我们发现，这种文本推理数据甚至比视觉推理数据更能激发MLLMs的慢思考能力。尽管这项工作尚处于初步阶段，但它表明慢思考能力本质上与语言模型组件有关，并且可以在不同模态或领域之间转移。这一发现可以用于指导更强大慢思考推理系统的开发。我们的资源已发布在https://github.com/RUCAIBox/Virgo。
2501.01877	 | ANTHROPOS-V: benchmarking the novel task of Crowd Volume Estimation	 | Luca Collorone,Stefano D'Arrigo,Massimiliano Pappa,Guido Maria D'Amely di Melendugno,Giovanni Ficarra,Fabio Galasso	 | 该研究提出了人群体积估计（CVE）任务，即仅通过RGB图像估计人群整体体积，并构建了首个CVE基准，包含合成的高逼真度视频数据集ANTHROPOS-V，同时还介绍了评估方法和超越现有基线的特定方法。	 | We introduce the novel task of Crowd Volume Estimation (CVE), defined as the process of estimating the collective body volume of crowds using only RGB images. Besides event management and public safety, CVE can be instrumental in approximating body weight, unlocking weight sensitive applications such as infrastructure stress assessment, and assuring even weight balance. We propose the first benchmark for CVE, comprising ANTHROPOS-V, a synthetic photorealistic video dataset featuring crowds in diverse urban environments. Its annotations include each person's volume, SMPL shape parameters, and keypoints. Also, we explore metrics pertinent to CVE, define baseline models adapted from Human Mesh Recovery and Crowd Counting domains, and propose a CVE specific methodology that surpasses baselines. Although synthetic, the weights and heights of individuals are aligned with the real-world population distribution across genders, and they transfer to the downstream task of CVE from real images. Benchmark and code are available at github.com/colloroneluca/Crowd-Volume-Estimation.	 | 我们介绍了新的任务——人群体积估计（Crowd Volume Estimation, CVE），定义为仅使用RGB图像估计人群整体体积的过程。除了事件管理和公共安全之外，CVE还可以用于估算体型重量，解锁基于体重的应用程序，如基础设施应力评估，并确保体重平衡。我们提出了第一个CVE基准，其中包括ANTHROPOS-V，这是一个合成的高逼真度视频数据集，涵盖各种城市环境中的人群。其注释包括每个人的体积、SMPL形状参数和关键点。此外，我们探讨了与CVE相关的度量标准，定义了来自人类网格恢复和人群计数领域的基线模型，并提出了超越基线的CVE特定方法。尽管是合成的，但个体的体重和身高与现实世界人口分布中的性别分布相匹配，并且可以在从真实图像到CVE下游任务的迁移中发挥作用。基准测试和代码可在github.com/colloroneluca/Crowd-Volume-Estimation 获取。
2501.01864	 | Towards Hard and Soft Shadow Removal via Dual-Branch Separation Network and Vision Transformer	 | Jiajia Liang	 | 本文提出了一种双路径模型，通过专门设计的损失函数分别处理硬阴影和软阴影，从而在ISTD数据集上实现了优于现有方法的阴影去除效果，RMSE值为2.905。该模型结合了Vision Transformer和UNet++，增强了边缘细节和特征融合。	 | Image shadow removal is a crucial task in computer vision. In real-world scenes, shadows alter image color and brightness, posing challenges for perception and texture recognition. Traditional and deep learning methods often overlook the distinct needs for handling hard and soft shadows, thereby lacking detailed processing to specifically address each type of shadow in images.We propose a dual-path model that processes these shadows separately using specially designed loss functions to accomplish the hard and soft shadow removal. The model classifies shadow types and processes them through appropriate paths to produce shadow-free outputs, integrating a Vision Transformer with UNet++ for enhanced edge detail and feature fusion. Our model outperforms state-of-the-art methods and achieves 2.905 RMSE value on the ISTD dataset, which demonstrates greater effectiveness than typical single-path approaches.	 | 图像阴影去除是计算机视觉中的一个关键任务。在现实场景中，阴影会改变图像的颜色和亮度，给感知和纹理识别带来挑战。传统和深度学习方法往往忽视了处理硬阴影和软阴影的差异化需求，缺乏针对每种类型的阴影进行详细处理的方法。我们提出了一种双路径模型，使用专门设计的损失函数分别处理这两种阴影以完成阴影去除。该模型通过对阴影类型进行分类并分别通过适当的路径处理，从而生成无阴影的输出。该模型结合了Vision Transformer和UNet++，以增强边缘细节和特征融合。我们的模型在ISTD数据集上优于最先进的方法，取得了2.905的RMSE值，这表明其效果优于典型的单路径方法。
2501.01855	 | UAV-DETR: Efficient End-to-End Object Detection for Unmanned Aerial Vehicle Imagery	 | Huaxiang Zhang,Kai Liu,Zhongxue Gan,Guo-Niu Zhu	 | 本文提出了一种专门针对无人机图像的高效检测变换器（UAV-DETR）框架，通过引入多尺度特征融合、频率增强模块以及频率聚焦下采样模块，以提高无人机图像检测的性能。实验结果表明，该方法在多个无人机图像数据集上均表现出色，尤其是在VisDrone和UAVVaste数据集上分别提升了3.1%的AP和4.2%的$\text{AP}_{50}$。	 | Unmanned aerial vehicle object detection (UAV-OD) has been widely used in various scenarios. However, most existing UAV-OD algorithms rely on manually designed components, which require extensive tuning. End-to-end models that do not depend on such manually designed components are mainly designed for natural images, which are less effective for UAV imagery. To address such challenges, this paper proposes an efficient detection transformer (DETR) framework tailored for UAV imagery, i.e., UAV-DETR. The framework includes a multi-scale feature fusion with frequency enhancement module, which captures both spatial and frequency information at different scales. In addition, a frequency-focused down-sampling module is presented to retain critical spatial details during down-sampling. A semantic alignment and calibration module is developed to align and fuse features from different fusion paths. Experimental results demonstrate the effectiveness and generalization of our approach across various UAV imagery datasets. On the VisDrone dataset, our method improves AP by 3.1\% and $\text{AP}_{50}$ by 4.2\% over the baseline. Similar enhancements are observed on the UAVVaste dataset. The project page: https://github.com/ValiantDiligent/UAV-DETR	 | 无人机对象检测（UAV-OD）已在各种场景中广泛应用。然而，现有的大多数UAV-OD算法依赖于手动设计的组件，这需要大量的调优。旨在自然图像的端到端模型较少适用于无人机图像，效果也不尽如人意。为了解决这些挑战，本文提出了一种专门针对无人机图像的高效检测变换器（DETR）框架，即UAV-DETR。该框架包括多尺度特征融合和频率增强模块，能够从不同尺度捕捉空间和频率信息。此外，还提出了一种频率聚焦下采样模块，以在下采样过程中保留关键的空间细节。为此开发了一个语义对齐和校准模块，用于对齐和融合来自不同融合路径的特征。实验结果表明，我们的方法在各种无人机图像数据集上具有有效性和泛化性。在VisDrone数据集上，我们的方法在基础模型上提高了3.1%的AP和4.2%的$\text{AP}_{50}$。在UAVVaste数据集上也观察到了类似的改进。项目页面：https://github.com/ValiantDiligent/UAV-DETR
2501.01845	 | Semantic Segmentation for Sequential Historical Maps by Learning from Only One Map	 | Yunshuang Yuan,Frank Thiemann,Monika Sester	 | 本文提出了一种弱监督年代追溯策略，利用相邻时期历史地图的相似性对扫描的历史地图进行自动化的语义分割标注，显著提高了分割性能，平均交并比（mIoU）达到了77.3%，相较于基线方法提高了约20%。	 | Historical maps are valuable resources that capture detailed geographical information from the past. However, these maps are typically available in printed formats, which are not conducive to modern computer-based analyses. Digitizing these maps into a machine-readable format enables efficient computational analysis. In this paper, we propose an automated approach to digitization using deep-learning-based semantic segmentation, which assigns a semantic label to each pixel in scanned historical maps. A key challenge in this process is the lack of ground-truth annotations required for training deep neural networks, as manual labeling is time-consuming and labor-intensive. To address this issue, we introduce a weakly-supervised age-tracing strategy for model fine-tuning. This approach exploits the similarity in appearance and land-use patterns between historical maps from neighboring time periods to guide the training process. Specifically, model predictions for one map are utilized as pseudo-labels for training on maps from adjacent time periods. Experiments conducted on our newly curated \textit{Hameln} dataset demonstrate that the proposed age-tracing strategy significantly enhances segmentation performance compared to baseline models. In the best-case scenario, the mean Intersection over Union (mIoU) achieved 77.3\%, reflecting an improvement of approximately 20\% over baseline methods. Additionally, the fine-tuned model achieved an average overall accuracy of 97\%, highlighting the effectiveness of our approach for digitizing historical maps.	 | 历史地图是捕捉过去详细地理信息的重要资源。然而，这些地图通常以纸质形式存在，不利于现代基于计算机的分析。将这些地图数字化为机器可读格式可以促进高效的计算分析。本文提出了一种使用基于深度学习的语义分割的自动化方法，对扫描的历史地图中的每个像素进行语义标注。这一过程中面临的一个关键挑战是缺乏用于训练深度神经网络的真实标注数据，因为手动标注既耗时又费力。为了解决这一问题，我们提出了一种弱监督的年代追溯策略，用于模型的微调。该方法利用相邻时期历史地图在外观和用地模式上的相似性来指导训练过程。具体来说，一个地图的模型预测被用作相邻时期地图上的伪标签进行训练。我们在新构建的Hameln数据集上进行的实验表明，所提出的年代追溯策略在分割性能方面显著优于基线模型。在最佳情况下，平均交并比（mIoU）达到了77.3%，相较于基线方法提高了约20%。此外，微调后的模型平均整体准确率达到了97%，突显了我们方法在历史地图数字化方面的有效性。
2501.01841	 | Dedicated Inference Engine and Binary-Weight Neural Networks for Lightweight Instance Segmentation	 | Tse-Wei Chen,Wei Tao,Dongyue Zhao,Kazuhiro Mima,Tadayuki Ito,Kinya Osa,Masami Kato	 | 本文提出了一种针对现代二进制权重神经网络的硬件架构设计方法，通过使用位操作替代乘法操作来简化MAC操作，从而有效减少推理引擎的门电路计数，并证明了该推理引擎能够高效处理实例分割任务，具有更高的准确性和较低的成本。	 | Reducing computational costs is an important issue for development of embedded systems. Binary-weight Neural Networks (BNNs), in which weights are binarized and activations are quantized, are employed to reduce computational costs of various kinds of applications. In this paper, a design methodology of hardware architecture for inference engines is proposed to handle modern BNNs with two operation modes. Multiply-Accumulate (MAC) operations can be simplified by replacing multiply operations with bitwise operations. The proposed method can effectively reduce the gate count of inference engines by removing a part of computational costs from the hardware system. The architecture of MAC operations can calculate the inference results of BNNs efficiently with only 52% of hardware costs compared with the related works. To show that the inference engine can handle practical applications, two lightweight networks which combine the backbones of SegNeXt and the decoder of SparseInst for instance segmentation are also proposed. The output results of the lightweight networks are computed using only bitwise operations and add operations. The proposed inference engine has lower hardware costs than related works. The experimental results show that the proposed inference engine can handle the proposed instance-segmentation networks and achieves higher accuracy than YOLACT on the "Person" category although the model size is 77.7$\times$ smaller compared with YOLACT.	 | 降低计算成本是嵌入式系统发展中的一个重要问题。二进制权重神经网络（BNNs），其中权重是二进制化且激活值是量化，被用来减少各种应用的计算成本。在本文中，我们提出了一种针对现代BNNs的推理引擎硬件架构设计方法，该方法包含两种操作模式。通过用位操作替代乘法操作，乘积累加（MAC）操作可以简化。所提出的方法可以通过从硬件系统中移除一部分计算成本来有效减少推理引擎的门电路计数。MAC操作的架构仅使用相关工作52%的硬件成本即可高效计算BNNs的推理结果。为了证明推理引擎可以处理实际应用，我们还提出了两个轻量级网络，它们结合了SegNeXt的骨干和SparseInst的解码器以进行实例分割。轻量级网络的输出结果仅使用位操作和加法操作计算。所提出的推理引擎的硬件成本低于相关工作。实验结果表明，所提出的推理引擎能够处理所提出的实例分割网络，并在“Person”类别上比YOLACT具有更高的准确性，尽管模型大小仅为YOLACT的77.7%。
2501.01834	 | MoColl: Agent-Based Specific and General Model Collaboration for Image Captioning	 | Pu Yang,Bin Dong	 | 本文提出了一种名为MoColl的新代理增强模型协作框架，旨在有效整合特定领域的图像字幕数据集与通用知识，通过将复杂任务分解为一系列问答子任务，并利用视觉问答模型和大规模语言模型的合作来生成更准确的图像字幕。	 | Image captioning is a critical task at the intersection of computer vision and natural language processing, with wide-ranging applications across various domains. For complex tasks such as diagnostic report generation, deep learning models require not only domain-specific image-caption datasets but also the incorporation of relevant general knowledge to provide contextual accuracy. Existing approaches exhibit inherent limitations: specialized models excel in capturing domain-specific details but lack generalization, while vision-language models (VLMs) built on large language models (LLMs) leverage general knowledge but struggle with domain-specific adaptation. To address these limitations, this paper proposes a novel agent-enhanced model collaboration framework, which we called \textbf{MoColl}, designed to effectively integrate domain-specific and general knowledge. Specifically, our approach is to decompose complex image captioning tasks into a series of interconnected question-answer subtasks. A trainable visual question answering (VQA) model is employed as a specialized tool to focus on domain-specific visual analysis, answering task-specific questions based on image content. Concurrently, an LLM-based agent with general knowledge formulates these questions and synthesizes the resulting question-answer pairs into coherent captions. Beyond its role in leveraging the VQA model, the agent further guides its training to enhance its domain-specific capabilities. Experimental results on radiology report generation validate the effectiveness of the proposed framework, demonstrating significant improvements in the quality of generated reports.	 | 图像字幕是计算机视觉和自然语言处理交叉领域中的关键任务，具有广泛的应用领域。对于诸如诊断报告生成这样复杂的任务，深度学习模型不仅需要特定领域的图像字幕数据集，还需要融合相关的一般知识以提供上下文准确性。现有的方法存在固有的局限性：专门模型擅长捕捉特定领域的细节，但缺乏泛化能力；基于大规模语言模型（LLMs）的视觉语言模型（VLMs）则利用了通用知识，但在领域特定适应方面存在困难。为了解决这些局限性，本文提出了一种新的代理增强模型协作框架，我们称之为**MoColl**，旨在有效整合特定领域和通用知识。具体而言，我们的方法是将复杂的图像字幕任务分解为一系列相互连接的问答子任务。我们采用可训练的视觉问答（VQA）模型作为专门工具，专注于特定领域的视觉分析，根据图像内容回答特定任务的问题。同时，一个基于大规模语言模型的代理将其所需的一般知识转化为这些问题，并将生成的问题-答案对合成为连贯的字幕。除了利用VQA模型之外，该代理还进一步指导其训练，以增强其特定领域的功能。在医学影像报告生成实验中验证了所提出框架的有效性，显示了生成报告质量的显著提升。
2501.01816	 | Uncertainty-Aware Label Refinement on Hypergraphs for Personalized Federated Facial Expression Recognition	 | Hu Ding,Yan Yan,Yang Lu,Jing-Hao Xue,Hanzi Wang	 | 本文研究了在个性化联邦学习框架下的面部表情识别问题，提出了一种基于超图的不确定性感知标签精炼方法（AMY），能够在保护隐私的同时，提升面部表情识别的准确性和鲁棒性。实验结果表明，该方法在多个最新方法中表现出色，特别是在具有挑战性的现实世界面部表情数据库上。	 | Most facial expression recognition (FER) models are trained on large-scale expression data with centralized learning. Unfortunately, collecting a large amount of centralized expression data is difficult in practice due to privacy concerns of facial images. In this paper, we investigate FER under the framework of personalized federated learning, which is a valuable and practical decentralized setting for real-world applications. To this end, we develop a novel uncertainty-Aware label refineMent on hYpergraphs (AMY) method. For local training, each local model consists of a backbone, an uncertainty estimation (UE) block, and an expression classification (EC) block. In the UE block, we leverage a hypergraph to model complex high-order relationships between expression samples and incorporate these relationships into uncertainty features. A personalized uncertainty estimator is then introduced to estimate reliable uncertainty weights of samples in the local client. In the EC block, we perform label propagation on the hypergraph, obtaining high-quality refined labels for retraining an expression classifier. Based on the above, we effectively alleviate heterogeneous sample uncertainty across clients and learn a robust personalized FER model in each client. Experimental results on two challenging real-world facial expression databases show that our proposed method consistently outperforms several state-of-the-art methods. This indicates the superiority of hypergraph modeling for uncertainty estimation and label refinement on the personalized federated FER task. The source code will be released at https://github.com/mobei1006/AMY.	 | 大多数面部表情识别（FER）模型是在大规模表达数据上通过集中式学习进行训练的。然而，由于面部图像隐私问题，在实践中收集大量集中式表达数据是非常困难的。本文中，我们研究了在个性化联邦学习框架下的面部表情识别问题，这是一种对于实际应用非常有价值且实用的去中心化设置。为此，我们开发了一种新颖的基于超图的不确定性感知标签精炼（AMY）方法。对于局部训练，每个本地模型由骨干网络、不确定性估计（UE）模块和表情分类（EC）模块组成。在UE模块中，我们利用超图来建模表情样本间的复杂高阶关系，并将这些关系融入到不确定性特征中。然后引入一个个性化的不确定性估计器来估计本地客户端样本的可靠不确定性权重。在EC模块中，我们对超图进行标签传播，获取高质量的精炼标签以重新训练表情分类器。基于上述方法，我们有效地缓解了客户端间异质样本的不确定性，并在每个客户端中学习到一个鲁棒的个性化面部表情识别模型。在两个具有挑战性的现实世界面部表情数据库上的实验结果表明，我们提出的方法在多个最新方法中表现出色。这表明了超图建模在个性化联邦面部表情识别任务中进行不确定性估计和标签精炼方面的优越性。源代码将在https://github.com/mobei1006/AMY发布。
2501.01808	 | MoEE: Mixture of Emotion Experts for Audio-Driven Portrait Animation	 | Huaize Liu,Wenzhang Sun,Donglin Di,Shibo Sun,Jiahui Yang,Changqing Zou,Hujun Bao	 | 该研究提出了情绪专家混合模型（MoEE）和DH-FaceEmoVid-150数据集，以解决生成逼真说话头部视频时的情绪表达和数据集缺乏问题，显著提升了复杂情绪表达的生成能力。	 | The generation of talking avatars has achieved significant advancements in precise audio synchronization. However, crafting lifelike talking head videos requires capturing a broad spectrum of emotions and subtle facial expressions. Current methods face fundamental challenges: a)the absence of frameworks for modeling single basic emotional expressions, which restricts the generation of complex emotions such as compound emotions; b)the lack of comprehensive datasets rich in human emotional expressions, which limits the potential of models. To address these challenges, we propose the following innovations: 1)the Mixture of Emotion Experts (MoEE) model, which decouples six fundamental emotions to enable the precise synthesis of both singular and compound emotional states; 2)the DH-FaceEmoVid-150 dataset, specifically curated to include six prevalent human emotional expressions as well as four types of compound emotions, thereby expanding the training potential of emotion-driven models. Furthermore, to enhance the flexibility of emotion control, we propose an emotion-to-latents module that leverages multimodal inputs, aligning diverse control signals-such as audio, text, and labels-to ensure more varied control inputs as well as the ability to control emotions using audio alone. Through extensive quantitative and qualitative evaluations, we demonstrate that the MoEE framework, in conjunction with the DH-FaceEmoVid-150 dataset, excels in generating complex emotional expressions and nuanced facial details, setting a new benchmark in the field. These datasets will be publicly released.	 | 生成会说话的虚拟形象已经实现了在精确音频同步方面的重要进展。然而，制作逼真的说话头部视频需要捕捉广泛的情绪范围和微妙的面部表情。当前的方法面临着根本性的挑战：a) 缺乏用于建模单一基本情绪表达的框架，这限制了复杂情绪如复合情绪的生成；b) 缺乏丰富的人类情绪表达数据集，这限制了模型的潜在能力。为了解决这些挑战，我们提出了以下创新：1) 情绪专家混合模型（MoEE），该模型将六个基本情绪分离，以实现对单一情绪状态和复合情绪状态的精确合成；2) DH-FaceEmoVid-150 数据集，这是一个特别为包括六个常见的人类情绪表达以及四种类型的复合情绪而精心策划的数据集，从而扩展了情绪驱动模型的训练潜力。此外，为了增强情绪控制的灵活性，我们提出了一个情绪到潜在特征的模块，该模块利用多模态输入，将各种控制信号（如音频、文本和标签）对齐，以确保更丰富的控制输入以及仅使用音频控制情绪的能力。通过广泛的定量和定性评估，我们证明了MoEE框架与DH-FaceEmoVid-150数据集相结合，在生成复杂情绪表达和细腻面部细节方面表现出色，确立了该领域的最新基准。这些数据集将公开发布。
2501.01798	 | JoyGen: Audio-Driven 3D Depth-Aware Talking-Face Video Editing	 | Qili Wang,Dajiang Wu,Zihang Xu,Junshi Huang,Jun Lv	 | 本文提出了JoyGen，一种两阶段框架用于基于输入音频生成高质量的说话人面部视频，该框架能够实现精细的唇音同步和高质量的视觉效果。通过3D重建和音频到运动模型预测面部表情，并结合音频特征和面部深度图，JoyGen显著提升了唇音同步精度和视觉质量。	 | Significant progress has been made in talking-face video generation research; however, precise lip-audio synchronization and high visual quality remain challenging in editing lip shapes based on input audio. This paper introduces JoyGen, a novel two-stage framework for talking-face generation, comprising audio-driven lip motion generation and visual appearance synthesis. In the first stage, a 3D reconstruction model and an audio2motion model predict identity and expression coefficients respectively. Next, by integrating audio features with a facial depth map, we provide comprehensive supervision for precise lip-audio synchronization in facial generation. Additionally, we constructed a Chinese talking-face dataset containing 130 hours of high-quality video. JoyGen is trained on the open-source HDTF dataset and our curated dataset. Experimental results demonstrate superior lip-audio synchronization and visual quality achieved by our method.	 | 在说话人面部视频生成的研究中取得了显著进展，但在基于输入音频编辑唇形时，精细的唇音同步和高质量视觉效果仍然是挑战。本文介绍了JoyGen，这是一种新颖的两阶段框架，用于说话人面部生成，包括基于音频的唇部运动生成和视觉外观合成。在第一阶段，3D重建模型和音频到运动模型分别预测身份系数和表情系数。接下来，通过将音频特征与面部深度图结合，我们为面部生成提供了全面的监督，以实现精确的唇音同步。此外，我们构建了一个包含130小时高质量视频的中文说话人面部数据集。JoyGen在开源HDTF数据集和我们策划的数据集上进行训练。实验结果表明，我们的方法实现了更优的唇音同步和视觉质量。
2501.01791	 | A Minimal Subset Approach for Efficient and Scalable Loop Closure	 | Nikolaos Stathoulopoulos,Christoforos Kanellakis,George Nikolakopoulos	 | 本文提出了一种最小子集方法（MSA），该方法在滑动窗口框架内优化关键帧采样，以减少冗余并保留必要信息，从而提高姿态图优化和环闭检测的可扩展性和计算效率。MSA在多种环境中表现优异，且无需手动调整参数。	 | Loop closure detection in large-scale and long-term missions can be computationally demanding due to the need to identify, verify, and process numerous candidate pairs to establish edge connections for the pose graph optimization. Keyframe sampling mitigates this by reducing the number of frames stored and processed in the back-end system. In this article, we address the gap in optimized keyframe sampling for the combined problem of pose graph optimization and loop closure detection. Our Minimal Subset Approach (MSA) employs an optimization strategy with two key factors, redundancy minimization and information preservation, within a sliding window framework to efficiently reduce redundant keyframes, while preserving essential information. This method delivers comparable performance to baseline approaches, while enhancing scalability and reducing computational overhead. Finally, we evaluate MSA on relevant publicly available datasets, showcasing that it consistently performs across a wide range of environments, without requiring any manual parameter tuning.	 | 在大规模和长期任务中，环闭检测可能会因需要识别、验证和处理大量候选对以建立姿态图优化中的边连接而变得计算密集型。关键帧采样的方法可以减少后端系统中存储和处理的帧的数量，从而减轻这一负担。本文旨在解决姿态图优化和环闭检测联合问题中的优化关键帧采样的空白。我们的最小子集方法（Minimal Subset Approach, MSA）在滑动窗口框架内采用了一种优化策略，通过最小化冗余并保留信息来高效地减少冗余关键帧，同时保留必要的信息。该方法在性能上与基线方法相当，但提高了可扩展性并减少了计算开销。最后，我们使用相关公开数据集评估了MSA，结果显示该方法在多种环境中都能稳定地表现优异，且无需手动调整参数。
2501.01790	 | Ingredients: Blending Custom Photos with Video Diffusion Transformers	 | Zhengcong Fei,Debang Li,Di Qiu,Changqian Yu,Mingyuan Fan	 | 本文介绍了一种名为\texttt{Ingredients}的强大框架，该框架能通过结合多种身份照片定制视频创作，主要通过面部提取器、多尺度投影器和ID路由器三个核心模块实现。实验结果表明，\texttt{Ingredients}在生成动态个性化视频内容方面表现出色，并且可在GitHub上公开获取相关数据、代码和模型权重。	 | This paper presents a powerful framework to customize video creations by incorporating multiple specific identity (ID) photos, with video diffusion Transformers, referred to as \texttt{Ingredients}. Generally, our method consists of three primary modules: (\textbf{i}) a facial extractor that captures versatile and precise facial features for each human ID from both global and local perspectives; (\textbf{ii}) a multi-scale projector that maps face embeddings into the contextual space of image query in video diffusion transformers; (\textbf{iii}) an ID router that dynamically combines and allocates multiple ID embedding to the corresponding space-time regions. Leveraging a meticulously curated text-video dataset and a multi-stage training protocol, \texttt{Ingredients} demonstrates superior performance in turning custom photos into dynamic and personalized video content. Qualitative evaluations highlight the advantages of proposed method, positioning it as a significant advancement toward more effective generative video control tools in Transformer-based architecture, compared to existing methods. The data, code, and model weights are publicly available at: \url{https://github.com/feizc/Ingredients}.	 | 本文提出了一种强大的框架，通过结合多种特定身份（ID）照片来定制视频创作，该框架称为 \texttt{Ingredients}。我们的方法主要由三个核心模块组成：（\textbf{i}）面部提取器，用于从全局和局部视角捕捉每个人ID的多变且精确的面部特征；（\textbf{ii}）多尺度投影器，将面部嵌入映射到视频扩散变压器中的图像查询上下文空间；（\textbf{iii}）ID路由器，动态组合并分配多个ID嵌入到相应的空间-时间区域。借助精心策划的文本-视频数据集和多阶段训练协议，\texttt{Ingredients} 在将自定义照片转化为动态且个性化的视频内容方面表现出卓越性能。定性的评估强调了所提出方法的优势，将其定位为在Transformer架构中更有效的生成视频控制工具的一项重要进步，与现有方法相比更具优势。数据、代码和模型权重可在 \url{https://github.com/feizc/Ingredients} 公开获取。
2501.01770	 | TCPFormer: Learning Temporal Correlation with Implicit Pose Proxy for 3D Human Pose Estimation	 | Jiajie Liu,Mengyuan Liu,Hong Liu,Wenhao Li	 | 本文提出了一种TCPFormer方法，通过引入隐式姿态代理作为中间表示，解决了现有方法忽视2D姿态序列中复杂依赖关系的问题。TCPFormer包括代理更新模块（PUM）、代理调用模块（PIM）和代理注意模块（PAM），能够更好地捕捉人体动作的时间相关性，并在两个基准数据集上达到了最优性能。	 | Recent multi-frame lifting methods have dominated the 3D human pose estimation. However, previous methods ignore the intricate dependence within the 2D pose sequence and learn single temporal correlation. To alleviate this limitation, we propose TCPFormer, which leverages an implicit pose proxy as an intermediate representation. Each proxy within the implicit pose proxy can build one temporal correlation therefore helping us learn more comprehensive temporal correlation of human motion. Specifically, our method consists of three key components: Proxy Update Module (PUM), Proxy Invocation Module (PIM), and Proxy Attention Module (PAM). PUM first uses pose features to update the implicit pose proxy, enabling it to store representative information from the pose sequence. PIM then invocates and integrates the pose proxy with the pose sequence to enhance the motion semantics of each pose. Finally, PAM leverages the above mapping between the pose sequence and pose proxy to enhance the temporal correlation of the whole pose sequence. Experiments on the Human3.6M and MPI-INF-3DHP datasets demonstrate that our proposed TCPFormer outperforms the previous state-of-the-art methods.	 | 近年来，多帧提升方法在三维人体姿态估计中占据主导地位。然而，之前的方法忽视了2D姿态序列中的复杂依赖关系，并仅学习单一的时间相关性。为了解决这一限制，我们提出了一种TCPFormer方法，该方法利用隐式姿态代理作为中间表示。隐式姿态代理中的每个代理可以建立一种时间相关性，从而帮助我们学习更全面的人体动作的时间相关性。具体而言，我们的方法包括三个关键组件：代理更新模块（PUM）、代理调用模块（PIM）和代理注意模块（PAM）。PUM 首先使用姿态特征更新隐式姿态代理，使其能够存储姿态序列中的代表性信息。PIM 然后调用并整合姿态代理与姿态序列，以增强每个姿态的动作语义。最后，PAM 利用姿态序列与姿态代理之间的上述映射关系，增强整个姿态序列的时间相关性。在Human3.6M和MPI-INF-3DHP数据集上的实验结果表明，我们提出的TCPFormer方法优于之前的最先进的方法。
2501.01767	 | LogicAD: Explainable Anomaly Detection via VLM-based Text Feature Extraction	 | Er Jin,Qihui Feng,Yongli Mou,Stefan Decker,Gerhard Lakemeyer,Oliver Simons,Johannes Stegmaier	 | 这项研究探索了自回归多模态视觉语言模型（AVLMs）在逻辑异常检测中的应用，并在MVTec LOCO AD基准测试中取得了最优性能，证明了AVLMs在解释和推理图像中关系和一致性方面的优越性，显著优于现有方法。	 | Logical image understanding involves interpreting and reasoning about the relationships and consistency within an image's visual content. This capability is essential in applications such as industrial inspection, where logical anomaly detection is critical for maintaining high-quality standards and minimizing costly recalls. Previous research in anomaly detection (AD) has relied on prior knowledge for designing algorithms, which often requires extensive manual annotations, significant computing power, and large amounts of data for training. Autoregressive, multimodal Vision Language Models (AVLMs) offer a promising alternative due to their exceptional performance in visual reasoning across various domains. Despite this, their application to logical AD remains unexplored. In this work, we investigate using AVLMs for logical AD and demonstrate that they are well-suited to the task. Combining AVLMs with format embedding and a logic reasoner, we achieve SOTA performance on public benchmarks, MVTec LOCO AD, with an AUROC of 86.0% and F1-max of 83.7%, along with explanations of anomalies. This significantly outperforms the existing SOTA method by a large margin.	 | 逻辑图像理解涉及对图像视觉内容中的关系和一致性进行解释和推理。这种能力在工业检测等应用中至关重要，其中逻辑异常检测对于维持高质量标准并减少昂贵的召回至关重要。先前在异常检测（AD）领域的研究依赖于先验知识来设计算法，这通常需要大量的手动标注、强大的计算能力以及大量的训练数据。自回归多模态视觉语言模型（AVLMs）因其在各个领域视觉推理方面的出色表现提供了有前景的替代方案。然而，它们在逻辑AD中的应用尚未得到探索。在这项工作中，我们研究了使用AVLMs进行逻辑AD的方法，并证明它们非常适合这一任务。通过结合AVLMs、格式嵌入和逻辑推理器，我们在公共基准测试MVTec LOCO AD上取得了SOTA性能，AUROC达到了86.0%，F1-max达到了83.7%，并提供了异常的解释。这一结果显著优于现有的SOTA方法。
2501.01761	 | Adverse Weather Conditions Augmentation of LiDAR Scenes with Latent Diffusion Models	 | Andrea Matteazzi,Pascal Colling,Michael Arnold,Dietmar Tutsch	 | 该研究提出了一种由自编码器和潜在扩散模型构成的方法，用于生成恶劣天气条件下的LiDAR场景，以增强自动驾驶应用在各种天气条件下的稳健性和可靠性。通过后处理清晰条件的LiDAR场景，提高了生成场景的逼真度。	 | LiDAR scenes constitute a fundamental source for several autonomous driving applications. Despite the existence of several datasets, scenes from adverse weather conditions are rarely available. This limits the robustness of downstream machine learning models, and restrains the reliability of autonomous driving systems in particular locations and seasons. Collecting feature-diverse scenes under adverse weather conditions is challenging due to seasonal limitations. Generative models are therefore essentials, especially for generating adverse weather conditions for specific driving scenarios. In our work, we propose a latent diffusion process constituted by autoencoder and latent diffusion models. Moreover, we leverage the clear condition LiDAR scenes with a postprocessing step to improve the realism of the generated adverse weather condition scenes.	 | LiDAR场景是多种自动驾驶应用的基本数据来源。尽管存在多个数据集，但恶劣天气条件下的场景却很少见。这限制了下游机器学习模型的稳健性，特别是在特定位置和季节中限制了自动驾驶系统的可靠性。由于季节限制，收集具有特征多样性的恶劣天气条件下的场景具有挑战性。因此，生成模型变得尤为重要，尤其是用于生成特定驾驶场景下的恶劣天气条件。在我们的工作中，我们提出了一种由自编码器和潜在扩散模型构成的潜在扩散过程。此外，我们通过后处理步骤利用清晰条件的LiDAR场景，以提高生成的恶劣天气条件场景的逼真度。
2501.01760	 | From Age Estimation to Age-Invariant Face Recognition: Generalized Age Feature Extraction Using Order-Enhanced Contrastive Learning	 | Haoyi Wang,Victor Sanchez,Chang-Tsun Li,Nathan Clarke	 | 本文提出了一种名为增强对比学习（OrdCon）的方法，用于提取广义年龄特征，以最小化不同数据集之间的域差异，从而提高年龄估计和年龄不变面部识别的性能。实验结果表明，该方法在同质数据集上与最新方法表现相当，并在跨数据集评估中显著提高了年龄估计的准确性和年龄不变面部识别的性能。	 | Generalized age feature extraction is crucial for age-related facial analysis tasks, such as age estimation and age-invariant face recognition (AIFR). Despite the recent successes of models in homogeneous-dataset experiments, their performance drops significantly in cross-dataset evaluations. Most of these models fail to extract generalized age features as they only attempt to map extracted features with training age labels directly without explicitly modeling the natural progression of aging. In this paper, we propose Order-Enhanced Contrastive Learning (OrdCon), which aims to extract generalized age features to minimize the domain gap across different datasets and scenarios. OrdCon aligns the direction vector of two features with either the natural aging direction or its reverse to effectively model the aging process. The method also leverages metric learning which is incorporated with a novel soft proxy matching loss to ensure that features are positioned around the center of each age cluster with minimum intra-class variance. We demonstrate that our proposed method achieves comparable results to state-of-the-art methods on various benchmark datasets in homogeneous-dataset evaluations for both age estimation and AIFR. In cross-dataset experiments, our method reduces the mean absolute error by about 1.38 in average for age estimation task and boosts the average accuracy for AIFR by 1.87%.	 | 广义年龄特征提取对于年龄相关的面部分析任务（如年龄估计和年龄不变面部识别，AIFR）至关重要。尽管最近在同质数据集实验中模型取得了成功，但在跨数据集评估中其性能显著下降。大多数这些模型未能提取广义年龄特征，因为它们只是直接将提取的特征映射到训练年龄标签，而没有明确建模老化过程的自然进展。在本文中，我们提出了增强对比学习（OrdCon），该方法旨在提取广义年龄特征，以最小化不同数据集和场景之间的域差异。OrdCon 将两个特征的方向矢量与自然老化方向或其反方向对齐，从而有效建模老化过程。该方法还利用了度量学习，结合一种新颖的软代理匹配损失，以确保特征在每个年龄簇的中心周围定位，且具有最小的类内方差。我们证明，在同质数据集评估的基准数据集上，我们的方法在年龄估计和 AIFR 方面都达到了与最新方法相当的结果。在跨数据集实验中，我们的方法将年龄估计任务的平均绝对误差降低了约 1.38，且 AIFR 的平均准确率提升了 1.87 个百分点。
2501.01733	 | Augmentation Matters: A Mix-Paste Method for X-Ray Prohibited Item Detection under Noisy Annotations	 | Ruikang Chen,Yan Yan,Jing-Hao Xue,Yang Lu,Hanzi Wang	 | 本文提出了一种标签感知混合补丁粘贴增强方法（Mix-Paste），用于在错误标注存在的条件下训练自动X射线违禁品检测器，通过模拟项目重叠和增加正确标注图像的概率来提高检测性能，并展示了该方法在带有噪声标注的数据集上的优越性和泛化能力。	 | Automatic X-ray prohibited item detection is vital for public safety. Existing deep learning-based methods all assume that the annotations of training X-ray images are correct. However, obtaining correct annotations is extremely hard if not impossible for large-scale X-ray images, where item overlapping is ubiquitous.As a result, X-ray images are easily contaminated with noisy annotations, leading to performance deterioration of existing methods.In this paper, we address the challenging problem of training a robust prohibited item detector under noisy annotations (including both category noise and bounding box noise) from a novel perspective of data augmentation, and propose an effective label-aware mixed patch paste augmentation method (Mix-Paste). Specifically, for each item patch, we mix several item patches with the same category label from different images and replace the original patch in the image with the mixed patch. In this way, the probability of containing the correct prohibited item within the generated image is increased. Meanwhile, the mixing process mimics item overlapping, enabling the model to learn the characteristics of X-ray images. Moreover, we design an item-based large-loss suppression (LLS) strategy to suppress the large losses corresponding to potentially positive predictions of additional items due to the mixing operation. We show the superiority of our method on X-ray datasets under noisy annotations. In addition, we evaluate our method on the noisy MS-COCO dataset to showcase its generalization ability. These results clearly indicate the great potential of data augmentation to handle noise annotations. The source code is released at https://github.com/wscds/Mix-Paste.	 | 自动X射线违禁品检测对于公共安全至关重要。现有的基于深度学习的方法都假设训练X射线图像的标注是正确的。然而，大规模X射线图像中项目重叠普遍存在，获得正确的标注几乎是不可能的，因此X射线图像很容易被错误的标注污染，导致现有方法的性能下降。在本文中，我们从数据增强的新视角解决在错误标注（包括类别噪声和边界框噪声）下的鲁棒违禁品检测器训练这一具有挑战性的问题，并提出了一种有效的标签感知混合补丁粘贴增强方法（Mix-Paste）。具体来说，对于每个项目补丁，我们从不同的图像中混合具有相同类别标签的多个项目补丁，并用混合补丁替换图像中的原始补丁。这样可以增加生成图像中包含正确违禁品的概率。同时，混合过程模拟了项目重叠，使模型能够学习X射线图像的特点。此外，我们设计了一种基于项目的大型损失抑制（LLS）策略，以抑制由于混合操作可能导致的附加项目潜在正预测的大型损失。我们在带有噪声标注的X射线数据集上展示了我们方法的优势，并在带有噪声的MS-COCO数据集上评估了我们的方法，以展示其泛化能力。这些结果清楚地表明数据增强在处理噪声标注方面的巨大潜力。源代码已发布在https://github.com/wscds/Mix-Paste。
2501.01728	 | Multi-modal classification of forest biodiversity potential from 2D orthophotos and 3D airborne laser scanning point clouds	 | Simon B. Jensen,Stefan Oehmcke,Andreas Møgelmose,Meysam Madadi,Christian Igel,Sergio Escalera,Thomas B. Moeslund	 | 本研究利用2D正射影像和空中激光扫描点云数据，结合深度学习方法，有效提升了森林生物多样性的评估精度，特征级拼接策略达到了75.5%的平均准确率，显示了光谱信息与结构信息的有效互补。	 | Accurate assessment of forest biodiversity is crucial for ecosystem management and conservation. While traditional field surveys provide high-quality assessments, they are labor-intensive and spatially limited. This study investigates whether deep learning-based fusion of close-range sensing data from 2D orthophotos (12.5 cm resolution) and 3D airborne laser scanning (ALS) point clouds (8 points/m^2) can enhance biodiversity assessment. We introduce the BioVista dataset, comprising 44.378 paired samples of orthophotos and ALS point clouds from temperate forests in Denmark, designed to explore multi-modal fusion approaches for biodiversity potential classification. Using deep neural networks (ResNet for orthophotos and PointVector for ALS point clouds), we investigate each data modality's ability to assess forest biodiversity potential, achieving mean accuracies of 69.4% and 72.8%, respectively. We explore two fusion approaches: a confidence-based ensemble method and a feature-level concatenation strategy, with the latter achieving a mean accuracy of 75.5%. Our results demonstrate that spectral information from orthophotos and structural information from ALS point clouds effectively complement each other in forest biodiversity assessment.	 | 准确评估森林生物多样性对于生态系统管理和保护至关重要。尽管传统的实地调查可以提供高质量的评估，但它们劳动密集且空间限制较大。本研究探讨了是否可以通过融合2D正射影像（分辨率12.5厘米）和空中激光扫描（ALS）点云数据（每平方米8个点），利用基于深度学习的方法来提升生物多样性评估。我们引入了BioVista数据集，包含来自丹麦温带森林的44,378对正射影像和ALS点云配对样本，旨在探索多模态融合方法在生物多样性潜力分类中的应用。使用深度神经网络（使用ResNet处理正射影像和PointVector处理ALS点云），我们研究了每种数据模式评估森林生物多样性潜力的能力，分别达到了69.4%和72.8%的平均准确率。我们探讨了两种融合方法：基于信心的集成方法和特征级拼接策略，后者达到了75.5%的平均准确率。我们的结果显示，正射影像的光谱信息和空中激光扫描点云的结构信息在森林生物多样性评估中能够有效互补。
2501.01723	 | IGAF: Incremental Guided Attention Fusion for Depth Super-Resolution	 | Athanasios Tragakis,Chaitanya Kaul,Kevin J. Mitchell,Hang Dai,Roderick Murray-Smith,Daniele Faccio	 | 本文提出了一种基于递增引导注意力融合（IGAF）模块的深度超分辨率（GDSR）方法，该方法结合了低分辨率深度图和高分辨率结构化输入，以生成详细的高分辨率深度图。实验结果表明，该模型在多个基准数据集上均优于现有基线模型。	 | Accurate depth estimation is crucial for many fields, including robotics, navigation, and medical imaging. However, conventional depth sensors often produce low-resolution (LR) depth maps, making detailed scene perception challenging. To address this, enhancing LR depth maps to high-resolution (HR) ones has become essential, guided by HR-structured inputs like RGB or grayscale images. We propose a novel sensor fusion methodology for guided depth super-resolution (GDSR), a technique that combines LR depth maps with HR images to estimate detailed HR depth maps. Our key contribution is the Incremental guided attention fusion (IGAF) module, which effectively learns to fuse features from RGB images and LR depth maps, producing accurate HR depth maps. Using IGAF, we build a robust super-resolution model and evaluate it on multiple benchmark datasets. Our model achieves state-of-the-art results compared to all baseline models on the NYU v2 dataset for $\times 4$, $\times 8$, and $\times 16$ upsampling. It also outperforms all baselines in a zero-shot setting on the Middlebury, Lu, and RGB-D-D datasets. Code, environments, and models are available on GitHub.	 | 准确的深度估计在许多领域中至关重要，包括机器人技术、导航和医学成像等。然而，传统的深度传感器往往生成低分辨率（LR）的深度图，使得对场景的详细感知变得困难。为了解决这个问题，增强LR深度图到高分辨率（HR）深度图变得非常重要，这一过程可以由HR结构化的输入（如RGB或灰度图像）来指导。我们提出了一种新的传感器融合方法，用于指导下的深度超分辨率（GDSR），该技术结合了LR深度图和HR图像以估计详细的HR深度图。我们的主要贡献是提出了一种递增引导注意力融合（IGAF）模块，该模块有效地学习从RGB图像和LR深度图中融合特征，生成准确的HR深度图。利用IGAF，我们构建了一个稳健的超分辨率模型，并在多个基准数据集上进行了评估。在NYU v2数据集上，我们的模型在4倍、8倍和16倍上采样的情况下，均优于所有基线模型。在Middlebury、Lu和RGB-D-D数据集的零样本设置中，我们的模型也优于所有基线模型。代码、环境和模型已经在GitHub上提供。
2501.01722	 | AR4D: Autoregressive 4D Generation from Monocular Videos	 | Hanxin Zhu,Tianyu He,Xiqian Yu,Junliang Guo,Zhibo Chen,Jiang Bian	 | 该研究提出了AR4D，一种无SDS的新型4D生成范式，通过三个阶段生成高质量的3D内容，实现更高的多样性、更好的时空一致性和更好的输入提示对齐。AR4D利用预训练模型和自回归方法生成3D表示，并引入了逐步视角采样和几何精炼阶段以防止过拟合和外观漂移。	 | Recent advancements in generative models have ignited substantial interest in dynamic 3D content creation (\ie, 4D generation). Existing approaches primarily rely on Score Distillation Sampling (SDS) to infer novel-view videos, typically leading to issues such as limited diversity, spatial-temporal inconsistency and poor prompt alignment, due to the inherent randomness of SDS. To tackle these problems, we propose AR4D, a novel paradigm for SDS-free 4D generation. Specifically, our paradigm consists of three stages. To begin with, for a monocular video that is either generated or captured, we first utilize pre-trained expert models to create a 3D representation of the first frame, which is further fine-tuned to serve as the canonical space. Subsequently, motivated by the fact that videos happen naturally in an autoregressive manner, we propose to generate each frame's 3D representation based on its previous frame's representation, as this autoregressive generation manner can facilitate more accurate geometry and motion estimation. Meanwhile, to prevent overfitting during this process, we introduce a progressive view sampling strategy, utilizing priors from pre-trained large-scale 3D reconstruction models. To avoid appearance drift introduced by autoregressive generation, we further incorporate a refinement stage based on a global deformation field and the geometry of each frame's 3D representation. Extensive experiments have demonstrated that AR4D can achieve state-of-the-art 4D generation without SDS, delivering greater diversity, improved spatial-temporal consistency, and better alignment with input prompts.	 | 最近在生成模型方面的进步激发了对动态3D内容生成（即4D生成）的广泛关注。现有的方法主要依赖于得分蒸馏采样（Score Distillation Sampling, SDS）来生成新视角视频，但这类方法往往会导致多样性有限、空间时间一致性差以及提示对齐不佳等问题，这是由于SDS方法固有的随机性。为了解决这些问题，我们提出了AR4D，这是一种无SDS的新型4D生成范式。具体来说，我们的范式包括三个阶段。  首先，对于一个由生成或捕获的单视角视频，我们利用预训练的专家模型创建第一个帧的3D表示，并进一步微调以作为基准空间。其次，鉴于视频在自然情况下以自回归的方式发生，我们提出根据前一帧的3D表示生成当前帧的3D表示，这种方式可以促进更准确的几何和运动估计。同时，为了防止在这一过程中出现过拟合，我们引入了一种逐步视角采样策略，利用大规模3D重建模型中的先验知识。为了防止自回归生成引入外观漂移，我们进一步引入了一个基于全局变形场和每个帧的3D表示几何结构的精炼阶段。大量实验表明，AR4D能够在不依赖SDS的情况下实现最先进的4D生成，提供更大的多样性、更好的空间时间一致性和更好的输入提示对齐。
2501.01720	 | Interpretable Face Anti-Spoofing: Enhancing Generalization with Multimodal Large Language Models	 | Guosheng Zhang,Keyao Wang,Haixiao Yue,Ajian Liu,Gang Zhang,Kun Yao,Errui Ding,Jingdong Wang	 | 本文提出了一种名为I-FAS的可解释人脸防欺骗方法，将FAS任务转化为可解释的视觉问答范式，并通过引入欺骗感知的图caption和过滤策略及不平衡语言模型损失函数等技术，显著提升了模型的解释性和泛化能力。	 | Face Anti-Spoofing (FAS) is essential for ensuring the security and reliability of facial recognition systems. Most existing FAS methods are formulated as binary classification tasks, providing confidence scores without interpretation. They exhibit limited generalization in out-of-domain scenarios, such as new environments or unseen spoofing types. In this work, we introduce a multimodal large language model (MLLM) framework for FAS, termed Interpretable Face Anti-Spoofing (I-FAS), which transforms the FAS task into an interpretable visual question answering (VQA) paradigm. Specifically, we propose a Spoof-aware Captioning and Filtering (SCF) strategy to generate high-quality captions for FAS images, enriching the model's supervision with natural language interpretations. To mitigate the impact of noisy captions during training, we develop a Lopsided Language Model (L-LM) loss function that separates loss calculations for judgment and interpretation, prioritizing the optimization of the former. Furthermore, to enhance the model's perception of global visual features, we design a Globally Aware Connector (GAC) to align multi-level visual representations with the language model. Extensive experiments on standard and newly devised One to Eleven cross-domain benchmarks, comprising 12 public datasets, demonstrate that our method significantly outperforms state-of-the-art methods.	 | 人脸防欺骗（Face Anti-Spoofing, FAS）对于确保面部识别系统的安全性和可靠性至关重要。大多数现有的FAS方法被形式化为二分类任务，提供没有解释性的置信分数。它们在新的领域场景下，如新环境或未见过的欺骗类型中表现出有限的泛化能力。在本文中，我们引入了一种多模态大型语言模型（MLLM）框架，称为可解释的人脸防欺骗（I-FAS），将FAS任务转化为可解释的视觉问答（VQA）范式。具体来说，我们提出了一种欺骗感知的图 captioning 和过滤（SCF）策略，为FAS图像生成高质量的 caption，从而丰富模型的监督并提供自然语言解释。为了减轻训练过程中嘈杂 caption 的影响，我们开发了一种不平衡语言模型（L-LM）损失函数，将判断和解释的损失计算分开，优先优化判断部分。此外，为了增强模型对全局视觉特征的感知，我们设计了一种全局感知连接器（GAC），将多级视觉表示与语言模型对齐。在包含12个公开数据集的标准和新设计的 One to Eleven 跨域基准上的广泛实验表明，我们的方法显著优于最先进的方法。
2501.01717	 | KeyNode-Driven Geometry Coding for Real-World Scanned Human Dynamic Mesh Compression	 | Huong Hoang,Truong Nguyen,Pamela Cosman	 | 该研究提出了一种基于嵌入式关键节点的3D人体动态网格压缩方法，通过传输关键节点的变换并结合八叉树残差编码和双向预测模式来提高预测质量，从而在低比特率下显著提升了压缩效果，平均比特率节省了24.51%。	 | The compression of real-world scanned 3D human dynamic meshes is an emerging research area, driven by applications such as telepresence, virtual reality, and 3D digital streaming. Unlike synthesized dynamic meshes with fixed topology, scanned dynamic meshes often not only have varying topology across frames but also scan defects such as holes and outliers, increasing the complexity of prediction and compression. Additionally, human meshes often combine rigid and non-rigid motions, making accurate prediction and encoding significantly more difficult compared to objects that exhibit purely rigid motion. To address these challenges, we propose a compression method designed for real-world scanned human dynamic meshes, leveraging embedded key nodes. The temporal motion of each vertex is formulated as a distance-weighted combination of transformations from neighboring key nodes, requiring the transmission of solely the key nodes' transformations. To enhance the quality of the KeyNode-driven prediction, we introduce an octree-based residual coding scheme and a Dual-direction prediction mode, which uses I-frames from both directions. Extensive experiments demonstrate that our method achieves significant improvements over the state-of-the-art, with an average bitrate saving of 24.51% across the evaluated sequences, particularly excelling at low bitrates.	 | 以下是该摘要的中文翻译：  3D人体动态网格的现实扫描压缩是一个新兴的研究领域，其驱动力来自于诸如远程存在、虚拟现实和三维数字流传输等应用。与具有固定拓扑结构的合成动态网格不同，扫描得到的动态网格不仅在各个帧之间具有变化的拓扑结构，还可能存在孔洞和离群点等扫描缺陷，这增加了预测和压缩的复杂性。此外，人体网格通常包含刚性和非刚性运动的结合，这使得准确的预测和编码比仅表现出刚性运动的对象要困难得多。为了解决这些挑战，我们提出了一种基于嵌入式关键节点的压缩方法。每个顶点的时间运动被形式化为来自相邻关键节点的变换的加权距离组合，只需要传输关键节点的变换。为了提高关键节点驱动的预测质量，我们引入了一种基于八叉树的残差编码方案和双向预测模式，该模式使用来自两个方向的I帧。大量的实验表明，与现有技术相比，我们的方法取得了显著的改进，特别是在低比特率下表现出色，平均比特率节省了24.51%。
2501.01715	 | Cloth-Splatting: 3D Cloth State Estimation from RGB Supervision	 | Alberta Longhini,Marcel Büsching,Bardienus P. Duisterhof,Jens Lundell,Jeffrey Ichnowski,Mårten Björkman,Danica Kragic	 | Cloth-Splatting方法通过预测-更新框架从RGB图像估计布料的3D状态，利用基于动作条件的动力学模型进行预测并用3D高斯斑点更新状态，从而通过可微映射和基于梯度的优化技术实现精确的布料状态估计，优于现有基线并在收敛时间上有所改进。	 | We introduce Cloth-Splatting, a method for estimating 3D states of cloth from RGB images through a prediction-update framework. Cloth-Splatting leverages an action-conditioned dynamics model for predicting future states and uses 3D Gaussian Splatting to update the predicted states. Our key insight is that coupling a 3D mesh-based representation with Gaussian Splatting allows us to define a differentiable map between the cloth state space and the image space. This enables the use of gradient-based optimization techniques to refine inaccurate state estimates using only RGB supervision. Our experiments demonstrate that Cloth-Splatting not only improves state estimation accuracy over current baselines but also reduces convergence time.	 | 我们提出了Cloth-Splatting方法，这是一种通过预测-更新框架从RGB图像估计布料3D状态的方法。Cloth-Splatting利用了基于动作条件的动力学模型来进行未来状态的预测，并使用3D高斯斑点更新预测状态。我们的关键洞察是，将基于3D网格的表示与高斯斑点相结合，可以定义从布料状态空间到图像空间的可微映射。这使得我们可以利用基于梯度的优化技术仅使用RGB监督来细化不准确的状态估计。我们的实验表明，Cloth-Splatting不仅在状态估计准确性上优于当前基线，还在收敛时间上有所改进。
2501.01710	 | Enhancing Large Vision Model in Street Scene Semantic Understanding through Leveraging Posterior Optimization Trajectory	 | Wei-Bin Kou,Qingfeng Lin,Ming Tang,Shuai Wang,Rongguang Ye,Guangxu Zhu,Yik-Chung Wu	 | 为了提高自动驾驶感知模型的泛化能力，该研究提出使用预训练的大规模视觉模型作为骨干，并结合下游感知头部来理解AD语义信息，同时引入基于后验优化轨迹的优化方案加速收敛，从而有效提升了模型性能并加快了收敛速度。	 | To improve the generalization of the autonomous driving (AD) perception model, vehicles need to update the model over time based on the continuously collected data. As time progresses, the amount of data fitted by the AD model expands, which helps to improve the AD model generalization substantially. However, such ever-expanding data is a double-edged sword for the AD model. Specifically, as the fitted data volume grows to exceed the the AD model's fitting capacities, the AD model is prone to under-fitting. To address this issue, we propose to use a pretrained Large Vision Models (LVMs) as backbone coupled with downstream perception head to understand AD semantic information. This design can not only surmount the aforementioned under-fitting problem due to LVMs' powerful fitting capabilities, but also enhance the perception generalization thanks to LVMs' vast and diverse training data. On the other hand, to mitigate vehicles' computational burden of training the perception head while running LVM backbone, we introduce a Posterior Optimization Trajectory (POT)-Guided optimization scheme (POTGui) to accelerate the convergence. Concretely, we propose a POT Generator (POTGen) to generate posterior (future) optimization direction in advance to guide the current optimization iteration, through which the model can generally converge within 10 epochs. Extensive experiments demonstrate that the proposed method improves the performance by over 66.48\% and converges faster over 6 times, compared to the existing state-of-the-art approach.	 | 为了提高自动驾驶（AD）感知模型的泛化能力，车辆需要根据不断收集的数据更新模型。随着时间的推移，AD模型拟合的数据量不断扩大，这有助于显著提高AD模型的泛化能力。然而，这种不断增长的数据对AD模型而言既是机遇也是挑战。具体来说，当拟合数据量增长到超过AD模型的拟合能力时，AD模型容易出现过拟合的问题。为了解决这一问题，我们提出使用预训练的大规模视觉模型（LVMs）作为骨干，并结合下游感知头部来理解AD语义信息。这一设计不仅可以通过LVMs强大的拟合能力克服上述过拟合问题，还能由于LVMs广泛的训练数据增强感知泛化能力。另一方面，为了减轻车辆在运行LVM骨干时训练感知头部的计算负担，我们引入了一种基于后验优化轨迹（POT）的优化方案（POTGui）来加速收敛。具体来说，我们提出了一个POT生成器（POTGen）来预先生成后续（未来）优化方向以指导当前优化迭代，从而使模型一般在10个epochs内即可收敛。广泛的实验结果表明，与现有最先进的方法相比，所提出的方法在性能上提高了66.48%以上，并且收敛速度提高了6倍以上。
2501.01709	 | MoVE-KD: Knowledge Distillation for VLMs with Mixture of Visual Encoders	 | Jiajun Cao,Yuan Zhang,Tao Huang,Ming Lu,Qizhe Zhang,Ruichuan An,Ningning MA,Shanghang Zhang	 | 本文提出了一种名为MoVE-KD的新框架，通过结合多个视觉编码器的独特能力并利用低秩适应和混合专家技术，将这些能力提炼到一个高效编码器模型中，从而提高视觉-语言模型的性能。该方法采用基于注意力的知识蒸馏策略，根据不同的视觉编码器自适应地分配权重，减少特征复制负担，有效提升了模型的适应性和效率。	 | Visual encoders are fundamental components in vision-language models (VLMs), each showcasing unique strengths derived from various pre-trained visual foundation models. To leverage the various capabilities of these encoders, recent studies incorporate multiple encoders within a single VLM, leading to a considerable increase in computational cost. In this paper, we present Mixture-of-Visual-Encoder Knowledge Distillation (MoVE-KD), a novel framework that distills the unique proficiencies of multiple vision encoders into a single, efficient encoder model. Specifically, to mitigate conflicts and retain the unique characteristics of each teacher encoder, we employ low-rank adaptation (LoRA) and mixture-of-experts (MoEs) to selectively activate specialized knowledge based on input features, enhancing both adaptability and efficiency. To regularize the KD process and enhance performance, we propose an attention-based distillation strategy that adaptively weighs the different visual encoders and emphasizes valuable visual tokens, reducing the burden of replicating comprehensive but distinct features from multiple teachers. Comprehensive experiments on popular VLMs, such as LLaVA and LLaVA-NeXT, validate the effectiveness of our method. The code will be released.	 | 视觉编码器是视觉-语言模型（VLMs）中的基本组件，每个编码器都源自不同的预训练视觉基础模型，展现出独特的优势。为了充分利用这些编码器的多种能力，最近的研究在单一VLM中引入了多个编码器，这导致了计算成本的显著增加。在本文中，我们提出了一种名为Mixture-of-Visual-Encoder Knowledge Distillation (MoVE-KD)的新型框架，该框架将多个视觉编码器的独特能力提炼到一个高效编码器模型中。具体而言，为了缓解冲突并保持每个教师编码器的独特特性，我们利用低秩适应（LoRA）和混合专家（MoEs）来根据输入特征选择性地激活专门的知识，从而提高适应性和效率。为了规范知识蒸馏过程并增强性能，我们提出了一种基于注意力的知识蒸馏策略，该策略根据不同的视觉编码器自适应地分配权重，并强调有价值的视觉令牌，从而减少复制多个教师的全面但不同的特征的负担。在流行VLMs，如LLaVA和LLaVA-NeXT上进行的全面实验验证了我们方法的有效性。代码将开源。
2501.01700	 | Aesthetic Matters in Music Perception for Image Stylization: A Emotion-driven Music-to-Visual Manipulation	 | Junjie Xu,Xingjiao Wu,Tanren Yao,Zihao Zhang,Jiayang Bei,Wu Wen,Liang He	 | EmoMV是一种情感驱动的音乐到视觉的操控方法，能够根据音乐中的情感来操控图像，有效将音乐的情感内容转化为视觉上引人注目的图像，促进多模态情感整合。	 | Emotional information is essential for enhancing human-computer interaction and deepening image understanding. However, while deep learning has advanced image recognition, the intuitive understanding and precise control of emotional expression in images remain challenging. Similarly, music research largely focuses on theoretical aspects, with limited exploration of its emotional dimensions and their integration with visual arts. To address these gaps, we introduce EmoMV, an emotion-driven music-to-visual manipulation method that manipulates images based on musical emotions. EmoMV combines bottom-up processing of music elements-such as pitch and rhythm-with top-down application of these emotions to visual aspects like color and lighting. We evaluate EmoMV using a multi-scale framework that includes image quality metrics, aesthetic assessments, and EEG measurements to capture real-time emotional responses. Our results demonstrate that EmoMV effectively translates music's emotional content into visually compelling images, advancing multimodal emotional integration and opening new avenues for creative industries and interactive technologies.	 | 情感信息对于增强人机交互和深化图像理解至关重要。然而，尽管深度学习在图像识别方面取得了进展，但在直观理解和精确控制图像中的情感表达方面仍面临挑战。同样，音乐研究主要集中在理论层面，对情感维度及其与视觉艺术的整合研究较少。为了弥补这些空白，我们提出了EmoMV，一种情感驱动的音乐到视觉的操控方法，能够根据音乐中的情感来操控图像。EmoMV将音乐元素（如音高和节奏）的自下而上处理与这些情感在视觉方面（如颜色和照明）的应用进行结合。我们使用多尺度框架进行评估，该框架包括图像质量指标、审美评估和EEG测量，以捕捉实时的情感反应。研究结果表明，EmoMV能够有效地将音乐的情感内容转化为视觉上引人注目的图像，促进多模态情感整合，并为创意产业和交互技术开辟新的途径。
2501.01699	 | Robust Self-Paced Hashing for Cross-Modal Retrieval with Noisy Labels	 | Ruitao Pu,Yuan Sun,Yang Qin,Zhenwen Ren,Xiaomin Song,Huiming Zheng,Dezhong Peng	 | 本文提出了一种新的认知跨模态检索方法——鲁棒的嘈杂标签自我引导哈希（RSHNL），以缓解嘈杂标签对哈希模型的影响。该方法通过对比哈希学习、中心聚合学习和噪声容忍自我引导哈希等方案，提高了多模态数据的一致性和对类内变异的鲁棒性。	 | Cross-modal hashing (CMH) has appeared as a popular technique for cross-modal retrieval due to its low storage cost and high computational efficiency in large-scale data. Most existing methods implicitly assume that multi-modal data is correctly labeled, which is expensive and even unattainable due to the inevitable imperfect annotations (i.e., noisy labels) in real-world scenarios. Inspired by human cognitive learning, a few methods introduce self-paced learning (SPL) to gradually train the model from easy to hard samples, which is often used to mitigate the effects of feature noise or outliers. It is a less-touched problem that how to utilize SPL to alleviate the misleading of noisy labels on the hash model. To tackle this problem, we propose a new cognitive cross-modal retrieval method called Robust Self-paced Hashing with Noisy Labels (RSHNL), which can mimic the human cognitive process to identify the noise while embracing robustness against noisy labels. Specifically, we first propose a contrastive hashing learning (CHL) scheme to improve multi-modal consistency, thereby reducing the inherent semantic gap. Afterward, we propose center aggregation learning (CAL) to mitigate the intra-class variations. Finally, we propose Noise-tolerance Self-paced Hashing (NSH) that dynamically estimates the learning difficulty for each instance and distinguishes noisy labels through the difficulty level. For all estimated clean pairs, we further adopt a self-paced regularizer to gradually learn hash codes from easy to hard. Extensive experiments demonstrate that the proposed RSHNL performs remarkably well over the state-of-the-art CMH methods.	 | 跨模态哈希（CMH）由于其在大规模数据中较低的存储成本和高效的计算能力，已经成为跨模态检索的一种流行技术。大多数现有方法隐含地假设多模态数据具有正确的标签，但在现实环境中，由于不可避免的不完美注释（即嘈杂标签），获取准确标签可能非常昂贵甚至不可能。受到人类认知学习的启发，一些方法引入了自我指导学习（SPL），逐渐从简单到困难的样本训练模型，这通常用于减轻特征噪声或异常值的影响。如何利用SPL来减轻嘈杂标签对哈希模型的误导是一个较少研究的问题。为了解决这个问题，我们提出了一种新的认知跨模态检索方法，称为鲁棒的嘈杂标签自我引导哈希（RSHNL），它可以模拟人类的认知过程来识别噪声并增强对嘈杂标签的鲁棒性。具体而言，我们首先提出了一种对比哈希学习（CHL）方案，以提高多模态一致性，从而减少固有的语义差距。随后，我们提出了一种中心聚合学习（CAL）方案，以减轻类内变异。最后，我们提出了噪声容忍自我引导哈希（NSH），动态估计每个实例的学习难度，并通过难度级别区分嘈杂标签。对于所有估计的干净成对，我们进一步采用自我引导正则化器，逐渐从简单到困难学习哈希编码。广泛的实验表明，所提出的RSHNL在最先进的CMH方法上表现出色。
2501.01695	 | CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene Reconstruction	 | Chenhao Zhang,Yuanping Cao,Lei Zhang	 | 本文提出了一种基于双分支融合的跨视图高斯插值方法，用于大规模场景重建，通过独立的空中和地面视图分支重建模型，并引入梯度感知正则化和高斯补充策略以应对显著视点差异带来的挑战，实验结果表明该方法在新视图合成方面优于现有方法。	 | 3D Gaussian Splatting (3DGS) has emerged as a prominent method for scene representation and reconstruction, leveraging densely distributed Gaussian primitives to enable real-time rendering of high-resolution images. While existing 3DGS methods perform well in scenes with minor view variation, large view changes in cross-view scenes pose optimization challenges for these methods. To address these issues, we propose a novel cross-view Gaussian Splatting method for large-scale scene reconstruction, based on dual-branch fusion. Our method independently reconstructs models from aerial and ground views as two independent branches to establish the baselines of Gaussian distribution, providing reliable priors for cross-view reconstruction during both initialization and densification. Specifically, a gradient-aware regularization strategy is introduced to mitigate smoothing issues caused by significant view disparities. Additionally, a unique Gaussian supplementation strategy is utilized to incorporate complementary information of dual-branch into the cross-view model. Extensive experiments on benchmark datasets demonstrate that our method achieves superior performance in novel view synthesis compared to state-of-the-art methods.	 | 3D高斯插值（3DGS）已成为场景表示和重建的一个突出方法，利用密集分布的高斯原语，使其能够实时渲染高分辨率图像。虽然现有的3DGS方法在视点变化较小的场景中表现良好，但在跨视图场景中大的视点变化却给这些方法带来了优化挑战。为了解决这些问题，我们提出了一种基于双分支融合的跨视图高斯插值方法，用于大规模场景重建。我们的方法独立地从空中和地面视图两个独立分支中重建模型，以建立高斯分布的基础，为跨视图重建在初始化和密集化过程中提供可靠的先验信息。具体而言，引入了一种梯度感知正则化策略，以缓解由于显著视点差异引起的平滑问题。此外，还利用了一种独特的高斯补充策略，将双分支的互补信息融入到跨视图模型中。在基准数据集上的广泛实验表明，与最先进的方法相比，我们的方法在新视图合成方面取得了更好的性能。
2501.01691	 | VidFormer: A novel end-to-end framework fused by 3DCNN and Transformer for Video-based Remote Physiological Measurement	 | Jiachen Li,Shisheng Guo,Longzhen Tang,Cuolong Cui,Lingjiang Kong,Xiaobo Yang	 | 本文提出了一种名为VidFormer的端到端框架，该框架结合了3DCNN和Transformer以处理基于面部视频的远程光体积描记术（rPPG）任务，并在多个公开数据集上优于现有方法。VidFormer通过定制的时空注意力机制增强了时空特征提取能力，并设计了一个模块促进两种模型之间的信息交换和融合。	 | Remote physiological signal measurement based on facial videos, also known as remote photoplethysmography (rPPG), involves predicting changes in facial vascular blood flow from facial videos. While most deep learning-based methods have achieved good results, they often struggle to balance performance across small and large-scale datasets due to the inherent limitations of convolutional neural networks (CNNs) and Transformer. In this paper, we introduce VidFormer, a novel end-to-end framework that integrates 3-Dimension Convolutional Neural Network (3DCNN) and Transformer models for rPPG tasks. Initially, we conduct an analysis of the traditional skin reflection model and subsequently introduce an enhanced model for the reconstruction of rPPG signals. Based on this improved model, VidFormer utilizes 3DCNN and Transformer to extract local and global features from input data, respectively. To enhance the spatiotemporal feature extraction capabilities of VidFormer, we incorporate temporal-spatial attention mechanisms tailored for both 3DCNN and Transformer. Additionally, we design a module to facilitate information exchange and fusion between the 3DCNN and Transformer. Our evaluation on five publicly available datasets demonstrates that VidFormer outperforms current state-of-the-art (SOTA) methods. Finally, we discuss the essential roles of each VidFormer module and examine the effects of ethnicity, makeup, and exercise on its performance.	 | 基于面部视频的远程生理信号测量，也称为远程光体积描记术（rPPG），涉及从面部视频中预测面部血管血流的变化。虽然大多数基于深度学习的方法取得了良好效果，但由于卷积神经网络（CNN）和Transformer固有的局限性，它们在小规模和大规模数据集之间的性能平衡往往难以实现。在本文中，我们介绍了VidFormer，这是一种新颖的端到端框架，结合了三维卷积神经网络（3DCNN）和Transformer模型以处理rPPG任务。首先，我们对传统的皮肤反射模型进行了分析，并随后引入了改进的模型以重建rPPG信号。基于这个改进模型，VidFormer利用3DCNN和Transformer分别从输入数据中提取局部和全局特征。为了增强VidFormer的时空特征提取能力，我们结合了针对3DCNN和Transformer定制的时空注意力机制。此外，我们设计了一个模块，以促进3DCNN和Transformer之间的信息交换和融合。在五个公开可用的数据集上的评估表明，VidFormer优于当前的最先进的（SOTA）方法。最后，我们讨论了每个VidFormer模块的必要作用，并探讨了种族、化妆和运动对其性能的影响。
2501.01689	 | Quantitative Gait Analysis from Single RGB Videos Using a Dual-Input Transformer-Based Network	 | Hiep Dinh,Son Le,My Than,Minh Ho,Nicolas Vuillerme,Hieu Pham	 | 本文提出了一种基于双模式输入卷积Transformer网络的高效步态分析方法，能够从单视角摄像机拍摄的单色RGB视频中准确估计关键的步态参数，如步态偏差指数、膝关节弯曲角度、步长和步行节律，并在资源有限的环境中优于现有方法。	 | Gait and movement analysis have become a well-established clinical tool for diagnosing health conditions, monitoring disease progression for a wide spectrum of diseases, and to implement and assess treatment, surgery and or rehabilitation interventions. However, quantitative motion assessment remains limited to costly motion capture systems and specialized personnel, restricting its accessibility and broader application. Recent advancements in deep neural networks have enabled quantitative movement analysis using single-camera videos, offering an accessible alternative to conventional motion capture systems. In this paper, we present an efficient approach for clinical gait analysis through a dual-pattern input convolutional Transformer network. The proposed system leverages a dual-input Transformer model to estimate essential gait parameters from single RGB videos captured by a single-view camera. The system demonstrates high accuracy in estimating critical metrics such as the gait deviation index (GDI), knee flexion angle, step length, and walking cadence, validated on a dataset of individuals with movement disorders. Notably, our approach surpasses state-of-the-art methods in various scenarios, using fewer resources and proving highly suitable for clinical application, particularly in resource-constrained environments.	 | 步行和运动分析已成为临床诊断健康状况、监测多种疾病的病情进展，并实施和评估治疗、手术或康复干预措施的有效工具。然而，定量运动评估仍然局限于昂贵的运动捕捉系统和专门的技术人员，限制了其可访问性和更广泛的应用。近年来，深度神经网络的进步使得通过单摄像头视频进行定量运动分析成为可能，这为传统运动捕捉系统的替代方案提供了新的途径。本文介绍了一种基于双模式输入卷积Transformer网络的高效临床步态分析方法。该系统利用双输入Transformer模型，从单视角摄像机拍摄的单色RGB视频中估计关键的步态参数。该系统在估计关键指标（如步态偏差指数（GDI）、膝关节弯曲角度、步长和步行节律）方面表现出高度准确性，并在患有运动障碍的个体数据集上得到了验证。值得注意的是，我们的方法在各种场景中超越了现有最先进的方法，同时使用更少的资源，并且特别适合临床应用，特别是在资源有限的环境中。
2501.01685	 | IAM: Enhancing RGB-D Instance Segmentation with New Benchmarks	 | Aecheon Jung,Soyun Choi,Junhong Min,Sungeun Hong	 | 该研究介绍了三个新的实例级RGB-D分割基准数据集，旨在填补现有方法在细粒度物体识别方面的不足，并对多种基线模型进行了评估，提出了有效的RGB-D数据集成方法以推动更深入的场景理解。	 | Image segmentation is a vital task for providing human assistance and enhancing autonomy in our daily lives. In particular, RGB-D segmentation-leveraging both visual and depth cues-has attracted increasing attention as it promises richer scene understanding than RGB-only methods. However, most existing efforts have primarily focused on semantic segmentation and thus leave a critical gap. There is a relative scarcity of instance-level RGB-D segmentation datasets, which restricts current methods to broad category distinctions rather than fully capturing the fine-grained details required for recognizing individual objects. To bridge this gap, we introduce three RGB-D instance segmentation benchmarks, distinguished at the instance level. These datasets are versatile, supporting a wide range of applications from indoor navigation to robotic manipulation. In addition, we present an extensive evaluation of various baseline models on these benchmarks. This comprehensive analysis identifies both their strengths and shortcomings, guiding future work toward more robust, generalizable solutions. Finally, we propose a simple yet effective method for RGB-D data integration. Extensive evaluations affirm the effectiveness of our approach, offering a robust framework for advancing toward more nuanced scene understanding.	 | 图像分割是为提供人类辅助并增强日常生活中的自主性而进行的一项关键任务。特别是，利用颜色和深度信息的RGB-D分割方法因其比仅使用RGB信息的方法提供了更丰富的场景理解而越来越受到关注。然而，现有的大多数努力主要集中在语义分割上，因此存在一个关键的缺口。目前可用的实例级RGB-D分割数据集相对较少，这限制了现有方法只进行广泛的类别区分，而无法完全捕捉识别单个物体所需的细粒度细节。为弥合这一缺口，我们引入了三个实例级别的RGB-D分割基准数据集。这些数据集具有高度的通用性，支持从室内导航到机器人操作等一系列广泛的应用。此外，我们还在这些基准上对各种基线模型进行了广泛的评估。这项全面的分析不仅指出了这些模型的优势，也揭示了它们的不足之处，为未来工作指明了更稳健和普遍适用的解决方案的方向。最后，我们提出了一种简单而有效的方法，用于RGB-D数据的集成。广泛的评估证实了我们方法的有效性，提供了一个稳健的框架，以推动更深入的场景理解。
2501.01677	 | PG-SAG: Parallel Gaussian Splatting for Fine-Grained Large-Scale Urban Buildings Reconstruction via Semantic-Aware Grouping	 | Tengfei Wang,Xin Wang,Yongmao Hou,Yiwei Xu,Wendi Zhang,Zongqian Zhan	 | 该研究提出了一种名为PG-SAG的并行高斯采样方法，通过利用语义线索进行分区和高斯核优化，能够在不降低分辨率的情况下重建大规模城市的精细建筑物表面。实验结果表明，PG-SAG在建筑物表面重建方面显著优于其他先进方法。	 | 3D Gaussian Splatting (3DGS) has emerged as a transformative method in the field of real-time novel synthesis. Based on 3DGS, recent advancements cope with large-scale scenes via spatial-based partition strategy to reduce video memory and optimization time costs. In this work, we introduce a parallel Gaussian splatting method, termed PG-SAG, which fully exploits semantic cues for both partitioning and Gaussian kernel optimization, enabling fine-grained building surface reconstruction of large-scale urban areas without downsampling the original image resolution. First, the Cross-modal model - Language Segment Anything is leveraged to segment building masks. Then, the segmented building regions is grouped into sub-regions according to the visibility check across registered images. The Gaussian kernels for these sub-regions are optimized in parallel with masked pixels. In addition, the normal loss is re-formulated for the detected edges of masks to alleviate the ambiguities in normal vectors on edges. Finally, to improve the optimization of 3D Gaussians, we introduce a gradient-constrained balance-load loss that accounts for the complexity of the corresponding scenes, effectively minimizing the thread waiting time in the pixel-parallel rendering stage as well as the reconstruction lost. Extensive experiments are tested on various urban datasets, the results demonstrated the superior performance of our PG-SAG on building surface reconstruction, compared to several state-of-the-art 3DGS-based methods. Project Web:https://github.com/TFWang-9527/PG-SAG.	 | 3D高斯采样（3DGS）已经成为了实时新颖合成领域的一项变革性方法。基于3DGS，近年来的进步通过基于空间的分区策略来处理大规模场景，以减少视频内存和优化时间成本。在本工作中，我们引入了一种并行高斯采样方法，称为PG-SAG，该方法充分利用语义线索进行分区和高斯核优化，从而在不降低原始图像分辨率的情况下重建大规模城市区域的精细建筑物表面。首先，利用跨模态模型——语言分割任何Thing来分割建筑物掩码。然后，根据注册图像之间的可见性检查将分割的建筑物区域分组为子区域。这些子区域的高斯核是与掩码像素一起并行优化的。此外，重新制定了法线损失来缓解掩码边缘法向量的歧义性。最后，为了改进3D高斯的优化，我们引入了一种梯度约束平衡负载损失，该损失考虑了相应场景的复杂性，有效地减少了像素并行渲染阶段的线程等待时间以及重建损失。我们在各种城市数据集上进行了广泛的实验，结果表明，与几种最先进的3DGS方法相比，我们的PG-SAG在建筑物表面重建方面表现出更优异的性能。项目网站：https://github.com/TFWang-9527/PG-SAG。
2501.01648	 | Dual Mutual Learning Network with Global-local Awareness for RGB-D Salient Object Detection	 | Kang Yi,Haoran Tang,Yumeng Li,Jing Xu,Jun Zhang	 | 本文提出了GL-DMNet，一种结合全局-局部意识的双相互学习网络，用于RGB-D显著物体检测，通过位置和通道相互融合模块以及高效解码器优化跨模态特征融合，显著提升了检测性能。实验结果表明，GL-DMNet在六个基准数据集上优于现有方法，特别是在四个评估指标上平均提高了约3%。	 | RGB-D salient object detection (SOD), aiming to highlight prominent regions of a given scene by jointly modeling RGB and depth information, is one of the challenging pixel-level prediction tasks. Recently, the dual-attention mechanism has been devoted to this area due to its ability to strengthen the detection process. However, most existing methods directly fuse attentional cross-modality features under a manual-mandatory fusion paradigm without considering the inherent discrepancy between the RGB and depth, which may lead to a reduction in performance. Moreover, the long-range dependencies derived from global and local information make it difficult to leverage a unified efficient fusion strategy. Hence, in this paper, we propose the GL-DMNet, a novel dual mutual learning network with global-local awareness. Specifically, we present a position mutual fusion module and a channel mutual fusion module to exploit the interdependencies among different modalities in spatial and channel dimensions. Besides, we adopt an efficient decoder based on cascade transformer-infused reconstruction to integrate multi-level fusion features jointly. Extensive experiments on six benchmark datasets demonstrate that our proposed GL-DMNet performs better than 24 RGB-D SOD methods, achieving an average improvement of ~3% across four evaluation metrics compared to the second-best model (S3Net). Codes and results are available at https://github.com/kingkung2016/GL-DMNet.	 | RGB-D 醒目物体检测（SOD）旨在通过同时建模RGB和深度信息来突出给定场景中的显著区域，是具有挑战性的像素级预测任务之一。近年来，由于其增强检测过程的能力，双注意力机制被应用于这一领域。然而，现有大多数方法在手动强制融合框架下直接融合注意力跨模态特征，而没有考虑到RGB和深度之间的固有差异，这可能导致性能下降。此外，来自全局和局部信息的长程依赖性使得难以采用统一有效的融合策略。因此，在本文中，我们提出了GL-DMNet，这是一种具有全局-局部意识的新颖双相互学习网络。具体来说，我们提出了一种位置相互融合模块和通道相互融合模块，以在空间和通道维度中利用不同模态之间的依赖关系。此外，我们采用基于级联变压器重构的高效解码器来联合集成多级融合特征。在六个基准数据集上的大量实验证明，我们提出的GL-DMNet在24种RGB-D SOD方法中表现更优，与第二优模型（S3Net）相比，在四个评估指标上平均提高了约3%。代码和结果可在 https://github.com/kingkung2016/GL-DMNet 获取。
2501.01645	 | HLV-1K: A Large-scale Hour-Long Video Benchmark for Time-Specific Long Video Understanding	 | Heqing Zou,Tianze Luo,Guiyang Xie,Victor(Xiao Jie)Zhang,Fengmao Lv,Guangcong Wang,Junyang Chen,Zhuochen Wang,Hansheng Zhang,Huaijian Zhang	 | 本文构建了一个包含1009个一小时长视频的基准数据集HLV-1K，旨在评估长时间视频理解模型，并通过先进的方法展示了其在多种任务中的价值，促进更深层次的长时间视频理解研究。	 | Multimodal large language models have become a popular topic in deep visual understanding due to many promising real-world applications. However, hour-long video understanding, spanning over one hour and containing tens of thousands of visual frames, remains under-explored because of 1) challenging long-term video analyses, 2) inefficient large-model approaches, and 3) lack of large-scale benchmark datasets. Among them, in this paper, we focus on building a large-scale hour-long long video benchmark, HLV-1K, designed to evaluate long video understanding models. HLV-1K comprises 1009 hour-long videos with 14,847 high-quality question answering (QA) and multi-choice question asnwering (MCQA) pairs with time-aware query and diverse annotations, covering frame-level, within-event-level, cross-event-level, and long-term reasoning tasks. We evaluate our benchmark using existing state-of-the-art methods and demonstrate its value for testing deep long video understanding capabilities at different levels and for various tasks. This includes promoting future long video understanding tasks at a granular level, such as deep understanding of long live videos, meeting recordings, and movies.	 | 多模态大语言模型在深度视觉理解领域因许多令人期待的实际应用而变得流行。然而，一小时以上的视频理解，跨越一小时以上并包含数万个视觉帧，仍然未被充分探索，主要原因包括：1) 艰难的长时间视频分析，2) 不高效的大型模型方法，以及3) 缺乏大规模基准数据集。在这其中，本文重点在于构建一个大规模一小时长视频基准数据集，HLV-1K，旨在评估长时间视频理解模型。HLV-1K 包含1009个一小时长的视频，其中包含了14,847个高质量的问答（QA）和多项选择题（MCQA）对，具有时间感知的查询和多样化的注释，覆盖了帧级、事件内级、事件间级和长期推理任务。我们使用现有的最先进方法来评估这一基准数据集，并展示了其在不同层次和各种任务中测试深度长时间视频理解能力的价值。这包括促进未来的粒度级别的长时间视频理解任务，例如长直播视频、会议记录和电影的深层次理解。
2501.01642	 | iCBIR-Sli: Interpretable Content-Based Image Retrieval with 2D Slice Embeddings	 | Shuhei Tomoshige,Hayato Muraki,Kenichi Oishi,Hitoshi Iyatomi	 | 本文提出了一种名为iCBIR-Sli的可解释脑部MRI图像内容基于图像检索（CBIR）系统，通过有效聚合一系列2D切片信息，实现了高完整性、可用性、鲁棒性和互操作性的检索性能，且在多个公开数据集上的检索评估中达到了与现有深度学习模型相当的效果。	 | Current methods for searching brain MR images rely on text-based approaches, highlighting a significant need for content-based image retrieval (CBIR) systems. Directly applying 3D brain MR images to machine learning models offers the benefit of effectively learning the brain's structure; however, building the generalized model necessitates a large amount of training data. While models that consider depth direction and utilize continuous 2D slices have demonstrated success in segmentation and classification tasks involving 3D data, concerns remain. Specifically, using general 2D slices may lead to the oversight of pathological features and discontinuities in depth direction information. Furthermore, to the best of the authors' knowledge, there have been no attempts to develop a practical CBIR system that preserves the entire brain's structural information. In this study, we propose an interpretable CBIR method for brain MR images, named iCBIR-Sli (Interpretable CBIR with 2D Slice Embedding), which, for the first time globally, utilizes a series of 2D slices. iCBIR-Sli addresses the challenges associated with using 2D slices by effectively aggregating slice information, thereby achieving low-dimensional representations with high completeness, usability, robustness, and interoperability, which are qualities essential for effective CBIR. In retrieval evaluation experiments utilizing five publicly available brain MR datasets (ADNI2/3, OASIS3/4, AIBL) for Alzheimer's disease and cognitively normal, iCBIR-Sli demonstrated top-1 retrieval performance (macro F1 = 0.859), comparable to existing deep learning models explicitly designed for classification, without the need for an external classifier. Additionally, the method provided high interpretability by clearly identifying the brain regions indicative of the searched-for disease.	 | 当前用于搜索脑部MRI图像的方法主要依赖于文本方法，突显了内容基于图像检索（CBIR）系统的重要需求。直接将3D脑部MRI图像应用于机器学习模型可以有效地学习脑部结构，然而构建通用模型需要大量的训练数据。尽管有研究表明，考虑深度方向并使用连续2D切片的模型在涉及3D数据的分割和分类任务中已经取得了一定成功，但仍存在一些问题。具体而言，使用一般性的2D切片可能会忽略病理特征和深度方向信息的不连续性。此外，据作者所知，尚无研究尝试开发一种能够保留整个脑部结构信息的实用CBIR系统。在本研究中，我们提出了一种可解释的脑部MRI图像CBIR方法，名为iCBIR-Sli（具有2D切片嵌入的可解释CBIR），这在全球范围内首次利用一系列2D切片。iCBIR-Sli通过有效聚合切片信息，实现了低维表示，且具有高完整性、可用性、鲁棒性和互操作性，这些特性对于有效的CBIR至关重要。在使用ADNI2/3、OASIS3/4、AIBL等五个公开可用的脑部MRI数据集（阿尔茨海默病和认知正常）进行检索评估实验中，iCBIR-Sli在top-1检索性能上达到了0.859的宏F1值，与专门为分类设计的现有深度学习模型相当，无需外部分类器。此外，该方法通过清晰地识别出与所搜索疾病相关的脑区，提供了高可解释性。
2501.01640	 | Uncertainty and Energy based Loss Guided Semi-Supervised Semantic Segmentation	 | Rini Smita Thakur,Vinod K. Kurmi	 | 本文提出了一种新的交并集伪监督网络，通过引入随机不确定性建模和基于能量的损失来改进半监督语义分割，从而在性能指标上优于现有方法。	 | Semi-supervised (SS) semantic segmentation exploits both labeled and unlabeled images to overcome tedious and costly pixel-level annotation problems. Pseudolabel supervision is one of the core approaches of training networks with both pseudo labels and ground-truth labels. This work uses aleatoric or data uncertainty and energy based modeling in intersection-union pseudo supervised network.The aleatoric uncertainty is modeling the inherent noise variations of the data in a network with two predictive branches. The per-pixel variance parameter obtained from the network gives a quantitative idea about the data uncertainty. Moreover, energy-based loss realizes the potential of generative modeling on the downstream SS segmentation task. The aleatoric and energy loss are applied in conjunction with pseudo-intersection labels, pseudo-union labels, and ground-truth on the respective network branch. The comparative analysis with state-of-the-art methods has shown improvement in performance metrics.	 | 半监督（SS）语义分割通过利用标记和未标记的图像来克服繁琐且成本高昂的像素级标注问题。伪标签监督是训练网络的一种核心方法，使用伪标签和真实标签相结合的方式。本文在交并集伪监督网络中使用了随机不确定性或数据不确定性以及基于能量的建模。随机不确定性通过网络中的两个预测分支来建模数据固有的噪声变化。从网络中获得的每个像素的方差参数提供了关于数据不确定性的量化概念。此外，基于能量的损失实现了生成建模在下游半监督分割任务中的潜力。随机不确定性与能量损失被应用于各自的网络分支中，结合伪交并集标签和真实标签。与现有方法的比较分析显示了在性能指标方面的改进。
2501.01633	 | ACE: Anti-Editing Concept Erasure in Text-to-Image Models	 | Zihao Wang,Yuxiang Wei,Fan Li,Renjing Pei,Hang Xu,Wangmeng Zuo	 | 本文提出了一种反编辑概念擦除（ACE）方法，该方法在生成和编辑过程中有效擦除目标概念并防止无关编辑，从而提高文本到图像扩散模型的安全性和可控性。实验结果表明，ACE方法在删除IP角色和擦除明确概念及艺术风格方面优于现有方法。	 | Recent advance in text-to-image diffusion models have significantly facilitated the generation of high-quality images, but also raising concerns about the illegal creation of harmful content, such as copyrighted images. Existing concept erasure methods achieve superior results in preventing the production of erased concept from prompts, but typically perform poorly in preventing undesired editing. To address this issue, we propose an Anti-Editing Concept Erasure (ACE) method, which not only erases the target concept during generation but also filters out it during editing. Specifically, we propose to inject the erasure guidance into both conditional and the unconditional noise prediction, enabling the model to effectively prevent the creation of erasure concepts during both editing and generation. Furthermore, a stochastic correction guidance is introduced during training to address the erosion of unrelated concepts. We conducted erasure editing experiments with representative editing methods (i.e., LEDITS++ and MasaCtrl) to erase IP characters, and the results indicate that our ACE effectively filters out target concepts in both types of edits. Additional experiments on erasing explicit concepts and artistic styles further demonstrate that our ACE performs favorably against state-of-the-art methods. Our code will be publicly available at https://github.com/120L020904/ACE.	 | 近年来，文本到图像扩散模型的最新进展显著提高了高质量图像的生成能力，但也引发了关于非法生成有害内容（如版权图像）的担忧。现有的概念擦除方法在防止从提示中生成被擦除的概念方面取得了优异的结果，但在防止不必要的编辑方面通常表现不佳。为了解决这一问题，我们提出了一种反编辑概念擦除（ACE）方法，该方法不仅在生成过程中擦除目标概念，还在编辑过程中将其过滤掉。具体来说，我们提出将擦除指导注入条件噪声预测和无条件噪声预测中，使模型能够在编辑和生成过程中有效防止创建被擦除的概念。此外，在训练过程中引入随机校正指导，以解决无关概念的侵蚀问题。我们使用代表性编辑方法（即LEDITS++和MasaCtrl）进行了擦除编辑实验，以删除IP角色，并且结果表明我们的ACE可以有效地过滤掉两类编辑中的目标概念。此外，我们在擦除明确概念和艺术风格方面的额外实验进一步证明了ACE优于最先进的方法。我们的代码将在 https://github.com/120L020904/ACE 公开。
2501.01611	 | Google is all you need: Semi-Supervised Transfer Learning Strategy For Light Multimodal Multi-Task Classification Model	 | Haixu Liu,Penghao Jiang,Zerui Tao	 | 本研究提出了一种结合卷积神经网络和自然语言处理模型的多模态多标签分类系统，通过整合图像和文本数据来提高图像分类的准确性，初步结果表明其具有作为自动图像标注系统的潜力。	 | As the volume of digital image data increases, the effectiveness of image classification intensifies. This study introduces a robust multi-label classification system designed to assign multiple labels to a single image, addressing the complexity of images that may be associated with multiple categories (ranging from 1 to 19, excluding 12). We propose a multi-modal classifier that merges advanced image recognition algorithms with Natural Language Processing (NLP) models, incorporating a fusion module to integrate these distinct modalities. The purpose of integrating textual data is to enhance the accuracy of label prediction by providing contextual understanding that visual analysis alone cannot fully capture. Our proposed classification model combines Convolutional Neural Networks (CNN) for image processing with NLP techniques for analyzing textual description (i.e., captions). This approach includes rigorous training and validation phases, with each model component verified and analyzed through ablation experiments. Preliminary results demonstrate the classifier's accuracy and efficiency, highlighting its potential as an automatic image-labeling system.	 | 随着数字图像数据量的增加，图像分类的有效性也得到了增强。本研究介绍了一种稳健的多标签分类系统，该系统能够为单个图像分配多个标签，以应对可能与多个类别（不包括12）相关联的图像的复杂性。我们提出了一种多模态分类器，该分类器结合了先进的图像识别算法和自然语言处理（NLP）模型，并引入了融合模块以整合这些不同的模态。整合文本数据的目的在于通过提供视觉分析无法完全捕捉的上下文理解，来增强标签预测的准确性。我们提出的分类模型结合了卷积神经网络（CNN）用于图像处理，以及NLP技术用于分析文本描述（例如，标题）。该方法包括严格的训练和验证阶段，每个模型组件通过消融实验进行验证和分析。初步结果表明，该分类器的准确性和效率，突显了其作为自动图像标注系统的潜力。
2501.01601	 | Few-shot Implicit Function Generation via Equivariance	 | Suizhi Huang,Xingyi Yang,Hongtao Lu,Xinchao Wang	 | 该研究提出了少样本隐式函数生成问题，旨在仅从少数示例中生成多样且功能一致的隐式神经表示权重。为解决这一挑战，研究者提出了EquiGen框架，通过将网络权重投影到等变潜在空间中并使用对比学习和平滑扩增进行训练，从而在有限数据下实现有效的多样化生成。	 | Implicit Neural Representations (INRs) have emerged as a powerful framework for representing continuous signals. However, generating diverse INR weights remains challenging due to limited training data. We introduce Few-shot Implicit Function Generation, a new problem setup that aims to generate diverse yet functionally consistent INR weights from only a few examples. This is challenging because even for the same signal, the optimal INRs can vary significantly depending on their initializations. To tackle this, we propose EquiGen, a framework that can generate new INRs from limited data. The core idea is that functionally similar networks can be transformed into one another through weight permutations, forming an equivariance group. By projecting these weights into an equivariant latent space, we enable diverse generation within these groups, even with few examples. EquiGen implements this through an equivariant encoder trained via contrastive learning and smooth augmentation, an equivariance-guided diffusion process, and controlled perturbations in the equivariant subspace. Experiments on 2D image and 3D shape INR datasets demonstrate that our approach effectively generates diverse INR weights while preserving their functional properties in few-shot scenarios.	 | 隐式神经表示（INRs）已成为表示连续信号的一种强大框架。然而，生成多样化的INR权重仍然具有挑战性，因为训练数据有限。我们提出了少样本隐式函数生成（Few-shot Implicit Function Generation）这一新问题设置，旨在仅从少数几个示例中生成多样且功能上一致的INR权重。这具有挑战性，因为即使是同一信号，其最优INR也可能因初始化的不同而有很大差异。为了解决这一问题，我们提出了EquiGen框架，可以使用有限的数据生成新的INR。核心思想是，功能相似的网络可以通过权重排列相互转换，形成一个等变群。通过将这些权重投影到等变潜在空间中，即使在少数示例的情况下，我们也能在这些群内实现多样性生成。EquiGen 通过对比学习和平滑扩增来训练一个等变编码器，通过等变引导的扩散过程和等变子空间中的受控扰动来实现这一点。在2D图像和3D形状INR数据集上的实验表明，我们的方法能够在少样本情况下有效生成多样化的INR权重，同时保持其功能属性。
2501.01595	 | Adaptive Homophily Clustering: A Structure Homophily Graph Learning with Adaptive Filter for Hyperspectral Image	 | Yao Ding,Weijie Kang,Aitao Yang,Zhili Zhang,Junyang Zhao,Jie Feng,Danfeng Hong,Qinhe Zheng	 | 本文提出了一种适应性滤波器聚类方法（AHSGC），用于有效学习高光谱成像（HSI）的同质结构图，通过自适应滤波器图编码、基于KL散度的图嵌入聚类解码、同质性增强的结构学习和联合网络优化，提升了HSI聚类的精度、效率和鲁棒性。	 | Hyperspectral image (HSI) clustering has been a fundamental but challenging task with zero training labels. Currently, some deep graph clustering methods have been successfully explored for HSI due to their outstanding performance in effective spatial structural information encoding. Nevertheless, insufficient structural information utilization, poor feature presentation ability, and weak graph update capability limit their performance. Thus, in this paper, a homophily structure graph learning with an adaptive filter clustering method (AHSGC) for HSI is proposed. Specifically, homogeneous region generation is first developed for HSI processing and constructing the original graph. Afterward, an adaptive filter graph encoder is designed to adaptively capture the high and low frequency features on the graph for subsequence processing. Then, a graph embedding clustering self-training decoder is developed with KL Divergence, with which the pseudo-label is generated for network training. Meanwhile, homophily-enhanced structure learning is introduced to update the graph according to the clustering task, in which the orient correlation estimation is adopted to estimate the node connection, and graph edge sparsification is designed to adjust the edges in the graph dynamically. Finally, a joint network optimization is introduced to achieve network self-training and update the graph. The K-means is adopted to express the latent features. Extensive experiments and repeated comparative analysis have verified that our AHSGC contains high clustering accuracy, low computational complexity, and strong robustness. The code source will be available at https://github.com/DY-HYX.	 | 极谱成像（HSI）聚类一直是具有零训练标签的基本但具有挑战性的任务。目前，由于其在有效空间结构信息编码方面的出色性能，一些深度图聚类方法已经在HSI中成功地进行了探索。然而，这些方法在结构信息利用不足、特征表示能力差以及图更新能力弱等方面限制了它们的性能。因此，在本文中，提出了一种适应性滤波器聚类方法（AHSGC）来进行HSI的同质结构图学习。具体来说，首先开发了HSI处理和构建原始图的同质区域生成方法。随后，设计了一种自适应滤波器图编码器，以在图上自适应地捕捉高频和低频特征，以便后续处理。接着，开发了一个基于KL散度的图嵌入聚类自我训练解码器，通过该解码器生成伪标签用于网络训练。同时，引入了同质性增强的结构学习来根据聚类任务更新图，在此过程中采用了方向相关性估计来估计节点连接，并设计了图边稀疏化来动态调整图中的边。最后，引入了一种联合网络优化来实现网络自我训练并更新图。K-means被用来表示潜在特征。广泛实验和重复比较分析证明，我们的AHSGC具有高聚类精度、低计算复杂度和强鲁棒性。相关代码将在https://github.com/DY-HYX开源。
2501.01589	 | D$^3$-Human: Dynamic Disentangled Digital Human from Monocular Video	 | Honghu Chen,Bo Peng,Yunfan Tao,Juyong Zhang	 | D$^3$-Human方法能够从单目视频中重建动态分离的数字人体几何形状，有效解决了衣物遮挡人体带来的挑战，实现了高质量的穿着不同衣物的人体解耦重建，适用于衣物转移和动画制作等实际应用。	 | We introduce D$^3$-Human, a method for reconstructing Dynamic Disentangled Digital Human geometry from monocular videos. Past monocular video human reconstruction primarily focuses on reconstructing undecoupled clothed human bodies or only reconstructing clothing, making it difficult to apply directly in applications such as animation production. The challenge in reconstructing decoupled clothing and body lies in the occlusion caused by clothing over the body. To this end, the details of the visible area and the plausibility of the invisible area must be ensured during the reconstruction process. Our proposed method combines explicit and implicit representations to model the decoupled clothed human body, leveraging the robustness of explicit representations and the flexibility of implicit representations. Specifically, we reconstruct the visible region as SDF and propose a novel human manifold signed distance field (hmSDF) to segment the visible clothing and visible body, and then merge the visible and invisible body. Extensive experimental results demonstrate that, compared with existing reconstruction schemes, D$^3$-Human can achieve high-quality decoupled reconstruction of the human body wearing different clothing, and can be directly applied to clothing transfer and animation.	 | 我们介绍了一种名为D$^3$-Human的方法，该方法能够从单目视频中重建动态分离的数字人体几何形状。以往的单目视频人体重建主要集中在重建未解耦的穿着人体或仅重建衣物，这使得直接应用于如动画制作等实际应用变得困难。在解耦衣物和人体的重建中，面临的挑战主要是由于衣物遮挡人体而引起的遮挡问题。为了解决这一问题，在重建过程中必须确保可见部分的细节以及不可见部分的合理性。我们提出的方法结合了显式和隐式表示来建模解耦的穿着人体，利用显式表示的鲁棒性以及隐式表示的灵活性。具体而言，我们将可见区域重建为SDF，并提出了一种新型的人体流形符号距离场（hmSDF）来分割可见的衣物和可见的身体部分，然后合并可见和不可见的身体部分。大量的实验结果表明，与现有的重建方案相比，D$^3$-Human能够实现高质量的穿着不同衣物的人体解耦重建，并可以直接应用于衣物转移和动画制作。
2501.01557	 | Click-Calib: A Robust Extrinsic Calibration Method for Surround-View Systems	 | Lihao Wang	 | Click-Calib 是一种无图案的离线全景系统（SVS）外部校准方法，用户只需在自然场景中点击几个关键点即可完成校准，能实现短距离和长距离上的准确校准，并且代码已公开。	 | Surround-View System (SVS) is an essential component in Advanced Driver Assistance System (ADAS) and requires precise calibrations. However, conventional offline extrinsic calibration methods are cumbersome and time-consuming as they rely heavily on physical patterns. Additionally, these methods primarily focus on short-range areas surrounding the vehicle, resulting in lower calibration quality in more distant zones. To address these limitations, we propose Click-Calib, a pattern-free approach for offline SVS extrinsic calibration. Without requiring any special setup, the user only needs to click a few keypoints on the ground in natural scenes. Unlike other offline calibration approaches, Click-Calib optimizes camera poses over a wide range by minimizing reprojection distance errors of keypoints, thereby achieving accurate calibrations at both short and long distances. Furthermore, Click-Calib supports both single-frame and multiple-frame modes, with the latter offering even better results. Evaluations on our in-house dataset and the public WoodScape dataset demonstrate its superior accuracy and robustness compared to baseline methods. Code is avalaible at https://github.com/lwangvaleo/click_calib.	 | 以下是该摘要的中文翻译：  全景系统（SVS）是高级驾驶辅助系统（ADAS）的重要组成部分，需要精细的校准。然而，传统的离线外部校准方法繁琐且耗时，因为它们高度依赖于物理图案。此外，这些方法主要集中在车辆周围的短距离区域，导致在更远的区域校准质量较低。为了解决这些局限性，我们提出了一种无图案的离线SVS外部校准方法——Click-Calib。该方法无需任何特殊设置，用户只需在自然场景中点击几个关键点即可。与其它离线校准方法不同，Click-Calib 通过最小化关键点的重新投影距离误差来优化多种相机姿态，从而在短距离和长距离上都实现了准确的校准。此外，Click-Calib 支持单帧和多帧模式，后者能获得更好的结果。我们的内部数据集和公开的WoodScape数据集上的评估表明，Click-Calib 在准确性和鲁棒性方面优于基线方法。代码可在 https://github.com/lwangvaleo/click_calib 获取。
2501.01548	 | Task-Driven Fixation Network: An Efficient Architecture with Fixation Selection	 | Shuguang Wang,Yuanjing Wang	 | 本文提出了一种新型神经网络架构，通过自动选择感兴趣区域的焦点点，减少网络规模和计算开销，有效解决复杂任务。该模型包含低分辨率和高分辨率通道以及混合编码模块，其中焦点点生成器动态生成焦点点，使模型能够高效关注关键区域。	 | This paper presents a novel neural network architecture featuring automatic fixation point selection, designed to efficiently address complex tasks with reduced network size and computational overhead. The proposed model consists of: a low-resolution channel that captures low-resolution global features from input images; a high-resolution channel that sequentially extracts localized high-resolution features; and a hybrid encoding module that integrates the features from both channels. A defining characteristic of the hybrid encoding module is the inclusion of a fixation point generator, which dynamically produces fixation points, enabling the high-resolution channel to focus on regions of interest. The fixation points are generated in a task-driven manner, enabling the automatic selection of regions of interest. This approach avoids exhaustive high-resolution analysis of the entire image, maintaining task performance and computational efficiency.	 | 本文提出了一种新颖的神经网络架构，该架构具备自动选择焦点点的功能，旨在通过减少网络规模和计算开销来高效解决复杂任务。所提出的模型包含以下组成部分：低分辨率通道，用于从输入图像中捕获低分辨率全局特征；高分辨率通道，用于逐级提取局部高分辨率特征；以及一种混合编码模块，该模块整合了两个通道的特征。混合编码模块的一个关键特性是包含一个焦点点生成器，该生成器可以动态生成焦点点，使高分辨率通道能够专注于感兴趣区域。这些焦点点是根据任务驱动的方式生成的，从而实现感兴趣区域的自动选择。这种方法避免了对整个图像进行耗时的高分辨率分析，同时保持了任务性能和计算效率。
2501.01529	 | SAFER: Sharpness Aware layer-selective Finetuning for Enhanced Robustness in vision transformers	 | Bhavna Gopal,Huanrui Yang,Mark Horton,Yiran Chen	 | 本文提出了一种新型的分层选择性微调方法（SAFER）来减轻视觉变换器（ViTs）中的对抗性过拟合问题，通过仅对最容易发生过拟合的少量层进行优化，提高了ViTs在多种架构和数据集上的干净和对抗性准确性，典型改进约为5%，最高可达20%。	 | Vision transformers (ViTs) have become essential backbones in advanced computer vision applications and multi-modal foundation models. Despite their strengths, ViTs remain vulnerable to adversarial perturbations, comparable to or even exceeding the vulnerability of convolutional neural networks (CNNs). Furthermore, the large parameter count and complex architecture of ViTs make them particularly prone to adversarial overfitting, often compromising both clean and adversarial accuracy.   This paper mitigates adversarial overfitting in ViTs through a novel, layer-selective fine-tuning approach: SAFER. Instead of optimizing the entire model, we identify and selectively fine-tune a small subset of layers most susceptible to overfitting, applying sharpness-aware minimization to these layers while freezing the rest of the model. Our method consistently enhances both clean and adversarial accuracy over baseline approaches. Typical improvements are around 5%, with some cases achieving gains as high as 20% across various ViT architectures and datasets.	 | 视觉变换器（ViTs）已成为高级计算机视觉应用和多模态基础模型中的关键骨干网络。尽管具有诸多优点，但ViTs仍然容易受到对抗性扰动的影响，甚至比卷积神经网络（CNNs）更具脆弱性。此外，由于其庞大的参数数量和复杂的架构，ViTs特别容易遭受对抗性过拟合，这常常会损害其干净和对抗性准确性。本文通过一种新颖的分层选择性微调方法（SAFER）来减轻ViTs中的对抗性过拟合问题。我们不优化整个模型，而是识别出最容易发生过拟合的一小部分层，并对这些层应用锐化感知最小化，同时冻结其余部分的模型。我们的方法在多种ViT架构和数据集上都比基准方法提高了干净和对抗性准确性。典型改进约为5%，在某些情况下，可以达到高达20%的提升。
2501.01449	 | LS-GAN: Human Motion Synthesis with Latent-space GANs	 | Avinash Amballa,Gayathri Akkinapalli,Vinitra Muralikrishnan	 | 本文提出了一种新的基于生成对抗网络（GANs）的框架，用于条件动作合成，该框架能够在缩短训练和推理时间的同时，实现与当前最先进的扩散模型相当的结果，特别是在HumanML3D和HumanAct12基准上的表现显著。	 | Human motion synthesis conditioned on textual input has gained significant attention in recent years due to its potential applications in various domains such as gaming, film production, and virtual reality. Conditioned Motion synthesis takes a text input and outputs a 3D motion corresponding to the text. While previous works have explored motion synthesis using raw motion data and latent space representations with diffusion models, these approaches often suffer from high training and inference times. In this paper, we introduce a novel framework that utilizes Generative Adversarial Networks (GANs) in the latent space to enable faster training and inference while achieving results comparable to those of the state-of-the-art diffusion methods. We perform experiments on the HumanML3D, HumanAct12 benchmarks and demonstrate that a remarkably simple GAN in the latent space achieves a FID of 0.482 with more than 91% in FLOPs reduction compared to latent diffusion model. Our work opens up new possibilities for efficient and high-quality motion synthesis using latent space GANs.	 | 近年来，基于文本输入的人体动作合成因其在游戏、电影制作和虚拟现实等多个领域的潜在应用而引起了广泛关注。条件动作合成通过文本输入输出与文本对应的空间3D动作。虽然先前的研究使用原始动作数据和扩散模型的潜空间表示进行了动作合成的探索，但这些方法往往存在训练和推理时间较长的问题。在本文中，我们提出了一种新的框架，该框架利用潜空间中的生成对抗网络（GANs），以实现更快的训练和推理，并且在结果上与当前最先进的扩散方法相当。我们在HumanML3D和HumanAct12基准上进行了实验，并证明在潜空间中使用一个极其简单的GAN可以将FID（Frechet Inception Distance）降至0.482，同时在FLOPs（浮点运算次数）上减少超过91%的计算量。我们的工作为使用潜空间GAN进行高效和高质量的动作合成打开了新的可能性。
2501.01912	 | Exoplanet Detection via Differentiable Rendering	 | Brandon Y. Feng,Rodrigo Ferrer-Chávez,Aviad Levis,Jason J. Wang,Katherine L. Bouman,William T. Freeman	 | 直接成像外行星面临的主要挑战是宿主恒星和行星之间的高对比度以及波前畸变导致的星光闪耀，传统方法未充分利用波前传感数据。文章提出了一种可微渲染方法，利用波前传感数据优化星光减除，显著提高了对比度和行星检测极限，展示了可微渲染在增强外行星成像中的潜力。	 | Direct imaging of exoplanets is crucial for advancing our understanding of planetary systems beyond our solar system, but it faces significant challenges due to the high contrast between host stars and their planets. Wavefront aberrations introduce speckles in the telescope science images, which are patterns of diffracted starlight that can mimic the appearance of planets, complicating the detection of faint exoplanet signals. Traditional post-processing methods, operating primarily in the image intensity domain, do not integrate wavefront sensing data. These data, measured mainly for adaptive optics corrections, have been overlooked as a potential resource for post-processing, partly due to the challenge of the evolving nature of wavefront aberrations. In this paper, we present a differentiable rendering approach that leverages these wavefront sensing data to improve exoplanet detection. Our differentiable renderer models wave-based light propagation through a coronagraphic telescope system, allowing gradient-based optimization to significantly improve starlight subtraction and increase sensitivity to faint exoplanets. Simulation experiments based on the James Webb Space Telescope configuration demonstrate the effectiveness of our approach, achieving substantial improvements in contrast and planet detection limits. Our results showcase how the computational advancements enabled by differentiable rendering can revitalize previously underexploited wavefront data, opening new avenues for enhancing exoplanet imaging and characterization.	 | 直接成像外行星对于推进我们对外太阳系行星系统的理解至关重要，但这一过程面临着巨大的挑战，主要原因是宿主恒星和它们的行星之间的高对比度。波前畸变在望远镜科学图像中引入了闪耀斑点，这些是发散星光形成的模式，可能会模拟行星的外观，从而复杂化对微弱外行星信号的检测。传统的后处理方法主要在图像强度域中操作，没有整合波前传感数据。波前传感数据主要用于自适应光学校正，但由于波前畸变不断变化的特性，这些数据长期以来被忽视，作为后处理的潜在资源。在这篇文章中，我们提出了一种可微渲染方法，利用这些波前传感数据来改进外行星的探测。我们的可微渲染器通过冠状望远镜系统建模基于波的光线传播，允许基于梯度的优化显著提高星光减除和增加对微弱外行星的敏感度。基于詹姆斯·韦伯太空望远镜配置的模拟实验表明，我们的方法有效，实现了对比度和行星检测极限的显著提高。我们的结果展示了通过可微渲染带来的计算进步如何重新激发之前未充分利用的波前数据，为增强外行星成像和表征开辟了新途径。
2501.01895	 | EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation	 | Siyuan Huang,Liliang Chen,Pengfei Zhou,Shengcong Chen,Zhengkai Jiang,Yue Hu,Peng Gao,Hongsheng Li,Maoqing Yao,Guanghui Ren	 | EnerVerse 是一个全面的框架，用于机器人操作任务中的未来空间生成，结合了卷积和双向注意力机制，并引入了自由锚视图（FAV）空间以提高泛化能力；该框架通过生成模型和4D高斯点结合的数据引擎管道，有效提升了机器人的适应性和性能，尤其在长距离操作任务中表现优异。	 | We introduce EnerVerse, a comprehensive framework for embodied future space generation specifically designed for robotic manipulation tasks. EnerVerse seamlessly integrates convolutional and bidirectional attention mechanisms for inner-chunk space modeling, ensuring low-level consistency and continuity. Recognizing the inherent redundancy in video data, we propose a sparse memory context combined with a chunkwise unidirectional generative paradigm to enable the generation of infinitely long sequences. To further augment robotic capabilities, we introduce the Free Anchor View (FAV) space, which provides flexible perspectives to enhance observation and analysis. The FAV space mitigates motion modeling ambiguity, removes physical constraints in confined environments, and significantly improves the robot's generalization and adaptability across various tasks and settings. To address the prohibitive costs and labor intensity of acquiring multi-camera observations, we present a data engine pipeline that integrates a generative model with 4D Gaussian Splatting (4DGS). This pipeline leverages the generative model's robust generalization capabilities and the spatial constraints provided by 4DGS, enabling an iterative enhancement of data quality and diversity, thus creating a data flywheel effect that effectively narrows the sim-to-real gap. Finally, our experiments demonstrate that the embodied future space generation prior substantially enhances policy predictive capabilities, resulting in improved overall performance, particularly in long-range robotic manipulation tasks.	 | 我们介绍了一种名为EnerVerse的全面框架，该框架专门用于机器人操作任务中的未来空间生成。EnerVerse无缝地结合了卷积和双向注意力机制，以确保低级别的一致性和连续性。考虑到视频数据中的固有冗余性，我们提出了一种稀疏记忆上下文与分块单向生成范式相结合的方法，以生成无限长的序列。为了进一步增强机器人的能力，我们引入了自由锚视图（FAV）空间，该空间提供了灵活的视角，增强了观察和分析能力。FAV空间减少了运动建模的模糊性，去除了受限环境中物理约束的影响，并显著提高了机器人在各种任务和环境中的泛化能力和适应性。为了解决多摄像头观测获取的高昂成本和劳动强度，我们提供了一个数据引擎管道，该管道将生成模型与4D高斯点（4DGS）结合起来。该管道利用生成模型的强大泛化能力和4DGS提供的空间约束，实现了数据质量与多样性的逐步提升，从而形成一种数据飞轮效应，有效缩小了模拟与现实之间的差距。最后，我们的实验表明，基于未来空间生成的先验显著增强了政策的预测能力，从而在整体性能上取得了提升，特别是在长距离机器人操作任务中表现尤为明显。
2501.01788	 | Universal Online Temporal Calibration for Optimization-based Visual-Inertial Navigation Systems	 | Yunfei Fan,Tianyu Zhao,Linan Guo,Chen Chen,Xin Wang,Fengyi Zhou	 | 本文提出了一种通用的在线时间校准策略，通过将时间偏移 \( t_d \) 作为状态参数融入优化残差模型中，实现视觉和惯性的精确时间对准，从而提高基于优化的视觉-惯性导航系统的准确性和鲁棒性。实验结果表明，该方法在噪声传感器数据下能提供更准确的时间偏移估计和更快的收敛速度。	 | 6-Degree of Freedom (6DoF) motion estimation with a combination of visual and inertial sensors is a growing area with numerous real-world applications. However, precise calibration of the time offset between these two sensor types is a prerequisite for accurate and robust tracking. To address this, we propose a universal online temporal calibration strategy for optimization-based visual-inertial navigation systems. Technically, we incorporate the time offset td as a state parameter in the optimization residual model to align the IMU state to the corresponding image timestamp using td, angular velocity and translational velocity. This allows the temporal misalignment td to be optimized alongside other tracking states during the process. As our method only modifies the structure of the residual model, it can be applied to various optimization-based frameworks with different tracking frontends. We evaluate our calibration method with both EuRoC and simulation data and extensive experiments demonstrate that our approach provides more accurate time offset estimation and faster convergence, particularly in the presence of noisy sensor data.	 | 6自由度（6DoF）运动估计结合视觉和惯性传感器是快速增长的应用领域，具有众多实际应用场景。然而，精确校准这两种传感器类型之间的时间偏移是实现准确和稳健跟踪的前提。为了解决这一问题，我们提出了一种通用的在线时间校准策略，用于基于优化的视觉-惯性导航系统。具体而言，我们将时间偏移 \( t_d \) 作为状态参数融入优化残差模型中，利用 \( t_d \) 、角速度和线速度将惯性测量单元（IMU）状态与相应的图像时间戳对齐。这使得时间对准偏移 \( t_d \) 能够在优化过程中与其他跟踪状态一起优化。由于我们的方法仅修改了残差模型的结构，因此它可以应用于不同跟踪前端的各种基于优化的框架中。我们使用EuRoC数据集和模拟数据对我们的校准方法进行了评估，并且广泛实验表明，我们的方法提供了更准确的时间偏移估计和更快的收敛速度，特别是在存在噪声传感器数据的情况下效果尤为明显。
2501.01773	 | Compressed Domain Prior-Guided Video Super-Resolution for Cloud Gaming Content	 | Qizhe Wang,Qian Yin,Zhimeng Huang,Weijia Jiang,Yi Su,Siwei Ma,Jiaqi Zhang	 | 本文提出了一种名为CPGSR的新型轻量级超分辨率网络，通过从编码先验中提取特征并结合U-net主干，以及引入一种分区聚焦频谱损失，有效解决了压缩游戏视频内容的超分辨率问题，显著改善了重建质量。	 | Cloud gaming is an advanced form of Internet service that necessitates local terminals to decode within limited resources and time latency. Super-Resolution (SR) techniques are often employed on these terminals as an efficient way to reduce the required bit-rate bandwidth for cloud gaming. However, insufficient attention has been paid to SR of compressed game video content. Most SR networks amplify block artifacts and ringing effects in decoded frames while ignoring edge details of game content, leading to unsatisfactory reconstruction results. In this paper, we propose a novel lightweight network called Coding Prior-Guided Super-Resolution (CPGSR) to address the SR challenges in compressed game video content. First, we design a Compressed Domain Guided Block (CDGB) to extract features of different depths from coding priors, which are subsequently integrated with features from the U-net backbone. Then, a series of re-parameterization blocks are utilized for reconstruction. Ultimately, inspired by the quantization in video coding, we propose a partitioned focal frequency loss to effectively guide the model's focus on preserving high-frequency information. Extensive experiments demonstrate the advancement of our approach.	 | 以下是该摘要的中文翻译：  云计算游戏是一种高级的互联网服务形式，需要在有限的资源和时间延迟条件下进行本地终端解码。超分辨率（SR）技术经常被应用于这些终端，作为一种有效的方法来降低云游戏所需的比特率带宽。然而，对于压缩的游戏视频内容的SR问题，尚未得到足够的关注。大多数SR网络在放大解码帧中的块状伪影和振铃效应时，忽视了游戏内容的边缘细节，导致重建结果不尽人意。在本文中，我们提出了一种名为编码先验引导的超分辨率（CPGSR）的新型轻量级网络，以解决压缩游戏视频内容中的SR挑战。首先，我们设计了一个压缩域引导块（CDGB），以从编码先验中提取不同深度的特征，并将这些特征与U-net主干的特征进行整合。然后，我们使用一系列重构块。最后，受视频编码中的量化启发，我们提出了一个分区聚焦频谱损失，以有效引导模型关注保持高频信息。广泛的实验表明，我们的方法具有显著的改进。
2501.01752	 | Laparoscopic Scene Analysis for Intraoperative Visualisation of Gamma Probe Signals in Minimally Invasive Cancer Surgery	 | Baoru Huang	 | 癌症是全球性的重大健康挑战，英国每两分钟就有新的癌症诊断病例，而现有的术中可视化工具不足导致手术效果不佳；为此，本文开发了工具跟踪、姿态估计、分割工具以及腹腔镜图像深度估计算法和三维重建方法，以提高术中癌症检测的准确性和效率。	 | Cancer remains a significant health challenge worldwide, with a new diagnosis occurring every two minutes in the UK. Surgery is one of the main treatment options for cancer. However, surgeons rely on the sense of touch and naked eye with limited use of pre-operative image data to directly guide the excision of cancerous tissues and metastases due to the lack of reliable intraoperative visualisation tools. This leads to increased costs and harm to the patient where the cancer is removed with positive margins, or where other critical structures are unintentionally impacted. There is therefore a pressing need for more reliable and accurate intraoperative visualisation tools for minimally invasive surgery to improve surgical outcomes and enhance patient care.   A recent miniaturised cancer detection probe (i.e., SENSEI developed by Lightpoint Medical Ltd.) leverages the cancer-targeting ability of nuclear agents to more accurately identify cancer intra-operatively using the emitted gamma signal. However, the use of this probe presents a visualisation challenge as the probe is non-imaging and is air-gapped from the tissue, making it challenging for the surgeon to locate the probe-sensing area on the tissue surface. Geometrically, the sensing area is defined as the intersection point between the gamma probe axis and the tissue surface in 3D space but projected onto the 2D laparoscopic image. Hence, in this thesis, tool tracking, pose estimation, and segmentation tools were developed first, followed by laparoscope image depth estimation algorithms and 3D reconstruction methods.	 | 癌症仍然是全球性的重大健康挑战，在英国，每两分钟就有新的癌症诊断病例。手术是癌症治疗的主要选项之一。然而，由于缺乏可靠的术中可视化工具，外科医生主要依赖触觉和肉眼，并在有限使用术前影像数据的情况下直接指导癌变组织和转移瘤的切除。这导致了在切除癌变组织时有阳性边缘，或者无意中影响到其他关键结构，从而增加了患者的治疗成本和风险。因此，迫切需要更可靠和准确的术中可视化工具，以提高手术效果并增强患者护理。  最近，一种微型癌症检测探针（例如Lightpoint Medical Ltd.开发的SENSEI）利用核剂的癌变靶向能力，在通过发出的伽马信号更准确地术中识别癌变。然而，使用这种探针也带来了可视化挑战，因为探针是非成像的，并且与组织之间存在空气间隙，使得外科医生难以在组织表面找到探针检测区域。从几何学上讲，检测区域定义为伽马探针轴在三维空间与组织表面的交点，并投影到2D腹腔镜图像上。因此，在本论文中，首先开发了工具跟踪、姿态估计和分割工具，随后开发了腹腔镜图像深度估计算法和三维重建方法。
2501.01704	 | Optimal Fiducial Marker Placement for Satellite Proximity Operations Using Observability Gramians	 | Nicholas B. Andrews,Kristi A. Morgansen	 | 本文使用双四元数建模卫星对的运动，并通过经验可观测性Gramian方法分析了相对双四元数系统的可观测性，确定了在卫星表面放置最优标记器的位置以提供光学距离和姿态测量，从而最大化标记器之间的距离并对状态变化敏感。研究还介绍了四元数和双四元数的定义、性质及其在相对运动模型中的应用。	 | This paper investigates optimal fiducial marker placement on the surface of a satellite performing relative proximity operations with an observer satellite. The absolute and relative translation and attitude equations of motion for the satellite pair are modeled using dual quaternions. The observability of the relative dual quaternion system is analyzed using empirical observability Gramian methods. The optimal placement of a fiducial marker set, in which each marker gives simultaneous optical range and attitude measurements, is determined for the pair of satellites. A geostationary flyby between the observing body (chaser) and desired (target) satellites is numerically simulated and the optimal fiducial placement sets of five and ten on the surface of the desired satellite are solved. It is shown that the optimal solution maximizes the distance between fiducial markers and selects marker locations that are most sensitive to measuring changes in the state during the nonlinear trajectory, despite being visible for less time than other candidate marker locations. Definitions and properties of quaternions and dual quaternions, and parallels between the two, are presented alongside the relative motion model.	 | 本文研究了在执行相对接近操作的卫星表面放置最优标记器的位置。使用了双四元数来建模卫星对的绝对和相对平移及姿态运动方程。通过经验可观测性Gramian方法分析了相对双四元数系统的可观测性。确定了每个标记器同时提供光学距离和姿态测量的标记器集的最优放置位置，这对卫星对来说是关键。通过数值模拟，对观测体（追逐者）与目标（观测对象）卫星之间的地球静止飞掠进行了模拟，并计算了解目标卫星表面五个和十个最优标记器的位置。研究表明，最优解决方案最大化了标记器之间的距离，并选择了对测量状态变化最敏感的标记位置，尽管这些位置的可见时间比其他候选标记位置短。同时，本文还介绍了四元数和双四元数的定义、性质及其之间的相似性，并与相对运动模型进行了对比。
2501.01681	 | SNeRV: Spectra-preserving Neural Representation for Video	 | Jina Kim,Jihoo Lee,Je-Won Kang	 | 频谱保持NeRV（SNeRV）通过使用二维离散小波变换将视频分解为低频和高频特征来增强隐式视频表示，有效解决了现有NeRV方法在捕捉精细空间细节和运动模式方面的不足；该方法不仅提高了细节捕捉能力，还通过引入专门模块和时间域中的频谱分解进一步提升了重构质量。	 | Neural representation for video (NeRV), which employs a neural network to parameterize video signals, introduces a novel methodology in video representations. However, existing NeRV-based methods have difficulty in capturing fine spatial details and motion patterns due to spectral bias, in which a neural network learns high-frequency (HF) components at a slower rate than low-frequency (LF) components. In this paper, we propose spectra-preserving NeRV (SNeRV) as a novel approach to enhance implicit video representations by efficiently handling various frequency components. SNeRV uses 2D discrete wavelet transform (DWT) to decompose video into LF and HF features, preserving spatial structures and directly addressing the spectral bias issue. To balance the compactness, we encode only the LF components, while HF components that include fine textures are generated by a decoder. Specialized modules, including a multi-resolution fusion unit (MFU) and a high-frequency restorer (HFR), are integrated into a backbone to facilitate the representation. Furthermore, we extend SNeRV to effectively capture temporal correlations between adjacent video frames, by casting the extension as additional frequency decomposition to a temporal domain. This approach allows us to embed spatio-temporal LF features into the network, using temporally extended up-sampling blocks (TUBs). Experimental results demonstrate that SNeRV outperforms existing NeRV models in capturing fine details and achieves enhanced reconstruction, making it a promising approach in the field of implicit video representations. The codes are available at https://github.com/qwertja/SNeRV.	 | 以下是对该摘要的中文翻译：  视频的神经表示（NeRV），通过使用神经网络来参数化视频信号，引入了视频表示的新方法。然而，现有的基于NeRV的方法难以捕捉到精细的空间细节和运动模式，这是因为频谱偏见的问题，即神经网络在学习高频（HF）分量时比低频（LF）分量慢。在本文中，我们提出了一种新的方法——频谱保持NeRV（SNeRV），以通过有效地处理各种频谱分量来增强隐式视频表示。SNeRV 使用二维离散小波变换（DWT）将视频分解为低频和高频特征，保留空间结构并直接解决频谱偏见问题。为了平衡紧凑性，我们仅编码低频分量，而高频分量（包括精细纹理）则通过解码器生成。我们将专门的模块，包括多尺度融合单元（MFU）和高频修复器（HFR），集成到骨干网络中，以促进表示。此外，我们通过将扩展问题重新定义为时间域中的附加频谱分解，将 SNeRV 扩展为有效捕捉相邻视频帧之间的时序相关性。这种方法允许我们将时空低频特征嵌入到网络中，使用时间扩展上采样块（TUBs）。实验结果表明，SNeRV 在捕捉细节和实现增强重构方面优于现有的 NeRV 模型，在隐式视频表示领域具有巨大的前景。代码可在 https://github.com/qwertja/SNeRV 获取。
2501.01629	 | Crossing Language Borders: A Pipeline for Indonesian Manhwa Translation	 | Nithyasri Narasimhan,Sagarika Singh	 | 该项目开发了一种结合计算机视觉、OCR技术和机器翻译的自动化流程，用于将印尼语漫画（Manhwa）翻译成英语，旨在简化翻译过程并提高效率。该方法采用了YOLOv5xu进行气泡检测、Tesseract进行OCR处理以及MarianMT进行机器翻译，显示了良好的翻译效果。	 | In this project, we develop a practical and efficient solution for automating the Manhwa translation from Indonesian to English. Our approach combines computer vision, text recognition, and natural language processing techniques to streamline the traditionally manual process of Manhwa(Korean comics) translation. The pipeline includes fine-tuned YOLOv5xu for speech bubble detection, Tesseract for OCR and fine-tuned MarianMT for machine translation. By automating these steps, we aim to make Manhwa more accessible to a global audience while saving time and effort compared to manual translation methods. While most Manhwa translation efforts focus on Japanese-to-English, we focus on Indonesian-to-English translation to address the challenges of working with low-resource languages. Our model shows good results at each step and was able to translate from Indonesian to English efficiently.	 | 在这个项目中，我们开发了一种实用且高效的方法，用于自动化将印尼语漫画（Manhwa）翻译成英语的过程。我们的方法结合了计算机视觉、文本识别和自然语言处理技术，以简化传统上手动进行的Manhwa（韩漫）翻译过程。该流水线包括针对气泡检测微调的YOLOv5xu、用于OCR的Tesseract以及针对机器翻译微调的MarianMT。通过自动化这些步骤，我们旨在使Manhwa更便于全球观众浏览，并与手动翻译方法相比节省时间和精力。虽然大多数Manhwa翻译工作侧重于日语到英语的翻译，但我们专注于印尼语到英语的翻译，以应对使用低资源语言所带来的挑战。我们的模型在每一步都显示出了良好的结果，并且能够高效地将印尼语翻译成英语。
2501.01483	 | Embedding Similarity Guided License Plate Super Resolution	 | Abderrezzaq Sendjasni,Mohamed-Chaker Larabi	 | 该研究提出了一种结合基于像素的损失和嵌入相似性学习的框架，以提高低分辨率车牌图像的识别准确性。实验结果显示，该框架在多个评价指标上优于现有方法，特别是在高分辨率和超分辨率车牌的微细特征对齐方面表现出色。	 | Super-resolution (SR) techniques play a pivotal role in enhancing the quality of low-resolution images, particularly for applications such as security and surveillance, where accurate license plate recognition is crucial. This study proposes a novel framework that combines pixel-based loss with embedding similarity learning to address the unique challenges of license plate super-resolution (LPSR). The introduced pixel and embedding consistency loss (PECL) integrates a Siamese network and applies contrastive loss to force embedding similarities to improve perceptual and structural fidelity. By effectively balancing pixel-wise accuracy with embedding-level consistency, the framework achieves superior alignment of fine-grained features between high-resolution (HR) and super-resolved (SR) license plates. Extensive experiments on the CCPD dataset validate the efficacy of the proposed framework, demonstrating consistent improvements over state-of-the-art methods in terms of PSNR_RGB, PSNR_Y and optical character recognition (OCR) accuracy. These results highlight the potential of embedding similarity learning to advance both perceptual quality and task-specific performance in extreme super-resolution scenarios.	 | 超分辨率（SR）技术在提升低分辨率图像质量方面发挥着关键作用，特别是在安全和监控等应用中，准确的车牌识别至关重要。本研究提出了一种新颖的框架，将基于像素的损失与嵌入相似性学习相结合，以应对车牌超分辨率（LPSR）的独特挑战。引入的像素和嵌入一致性损失（PECL）集成了Siamese网络，并通过对比损失促使嵌入相似性提高感知和结构上的保真度。通过有效平衡像素级别的准确性与嵌入级别的一致性，该框架实现了高分辨率（HR）和超分辨率（SR）车牌之间微细特征的优越对齐。在CCPD数据集上的广泛实验验证了所提出框架的有效性，结果显示在RGB-PSNR、Y-PSNR和光学字符识别（OCR）准确性方面均优于现有最先进的方法。这些结果突显了嵌入相似性学习在极端超分辨率场景中提升感知质量及特定任务性能的潜力。
2501.01482	 | An unsupervised method for MRI recovery: Deep image prior with structured sparsity	 | Muhammad Ahmad Sultan,Chong Chen,Yingmin Liu,Katarzyna Gil,Karolina Zareba,Rizwan Ahmad	 | 该研究提出并验证了一种无需完全采样 k 空间数据的无监督 MRI 重建方法——分组结构稀疏的深度图像先验（DISCUS），并在模拟数据和实际患者数据中均展现了优于现有方法的重建质量。	 | Objective: To propose and validate an unsupervised MRI reconstruction method that does not require fully sampled k-space data. Materials and Methods: The proposed method, deep image prior with structured sparsity (DISCUS), extends the deep image prior (DIP) by introducing group sparsity to frame-specific code vectors, enabling the discovery of a low-dimensional manifold for capturing temporal variations. \discus was validated using four studies: (I) simulation of a dynamic Shepp-Logan phantom to demonstrate its manifold discovery capabilities, (II) comparison with compressed sensing and DIP-based methods using simulated single-shot late gadolinium enhancement (LGE) image series from six distinct digital cardiac phantoms in terms of normalized mean square error (NMSE) and structural similarity index measure (SSIM), (III) evaluation on retrospectively undersampled single-shot LGE data from eight patients, and (IV) evaluation on prospectively undersampled single-shot LGE data from eight patients, assessed via blind scoring from two expert readers. Results: DISCUS outperformed competing methods, demonstrating superior reconstruction quality in terms of NMSE and SSIM (Studies I--III) and expert reader scoring (Study IV). Discussion: An unsupervised image reconstruction method is presented and validated on simulated and measured data. These developments can benefit applications where acquiring fully sampled data is challenging.	 | Objective: 提出并验证一种无需完全采样 k 空间数据的无监督 MRI 重建方法。 材料与方法：所提出的方法，分组结构稀疏的深度图像先验（DISCUS），在帧特定编码向量中引入分组稀疏性，从而能够发现一个低维流形来捕捉时间变化。DISCUS 通过以下四项研究进行了验证：（I）动态 Shepp-Logan 幻影的模拟，以展示其流形发现能力；（II）使用来自六个不同数字心脏模型的仿真单次快照延迟钆增强（LGE）图像系列，与压缩感知和基于 DIP 的方法进行比较，评估指标为归一化均方误差（NMSE）和结构相似性指数（SSIM）；（III）对八名患者的回顾性欠采样单次快照 LGE 数据进行评估；（IV）对八名患者的前瞻性欠采样单次快照 LGE 数据进行评估，通过两组专家读者的盲评进行评估。结果：DISCUS 在 NMSE 和 SSIM（研究 I-III）以及专家读者评分（研究 IV）方面均优于竞争方法，显示出更好的重建质量。讨论：提出了一个无监督图像重建方法，并通过模拟和实际数据进行了验证。这些进展可以为获取完全采样数据具有挑战性的应用场景提供帮助。
2501.01481	 | Unleashing Correlation and Continuity for Hyperspectral Reconstruction from RGB Images	 | Fuxiang Feng,Runmin Cong,Shoushui Wei,Yipeng Zhang,Jun Li,Sam Kwong,Wei Zhang	 | 本文提出了一种相关性和连续性网络（CCNet），通过引入组间光谱相关性建模（GrSCM）模块和邻域间光谱连续性建模（NeSCM）模块，有效从RGB图像重构高光谱图像（HSI），并在NTIRE2022和NTIRE2020数据集上取得了最先进的性能。	 | Reconstructing Hyperspectral Images (HSI) from RGB images can yield high spatial resolution HSI at a lower cost, demonstrating significant application potential. This paper reveals that local correlation and global continuity of the spectral characteristics are crucial for HSI reconstruction tasks. Therefore, we fully explore these inter-spectral relationships and propose a Correlation and Continuity Network (CCNet) for HSI reconstruction from RGB images. For the correlation of local spectrum, we introduce the Group-wise Spectral Correlation Modeling (GrSCM) module, which efficiently establishes spectral band similarity within a localized range. For the continuity of global spectrum, we design the Neighborhood-wise Spectral Continuity Modeling (NeSCM) module, which employs memory units to recursively model the progressive variation characteristics at the global level. In order to explore the inherent complementarity of these two modules, we design the Patch-wise Adaptive Fusion (PAF) module to efficiently integrate global continuity features into the spectral features in a patch-wise adaptive manner. These innovations enhance the quality of reconstructed HSI. We perform comprehensive comparison and ablation experiments on the mainstream datasets NTIRE2022 and NTIRE2020 for the spectral reconstruction task. Compared to the current advanced spectral reconstruction algorithms, our designed algorithm achieves State-Of-The-Art (SOTA) performance.	 | 从RGB图像重构高光谱图像（HSI）可以实现较低成本的高空间分辨率HSI，显示出巨大的应用潜力。本文揭示了光谱特性的局部相关性和全局连续性对于HSI重构任务至关重要。因此，我们充分探索了这些跨光谱关系，并提出了一种相关性和连续性网络（CCNet）以从RGB图像重构HSI。对于局部光谱的相关性，我们引入了组间光谱相关性建模（GrSCM）模块，该模块高效地在局部范围内建立了光谱带相似性。对于全局光谱的连续性，我们设计了邻域间光谱连续性建模（NeSCM）模块，该模块使用记忆单元递归建模全局层次上的渐进变化特性。为了探索这两个模块的内在互补性，我们设计了块间自适应融合（PAF）模块，以块间自适应方式高效地将全局连续性特征整合到光谱特征中。这些创新提升了重构HSI的质量。我们对主流数据集NTIRE2022和NTIRE2020的光谱重构任务进行了全面的对比和消融实验。与当前先进的光谱重构算法相比，我们设计的算法达到了最先进（SOTA）的性能。
2501.01465	 | Tech Report: Divide and Conquer 3D Real-Time Reconstruction for Improved IGS	 | Yicheng Zhu	 | 该研究提出了一种模块化的流水线，通过分而治之的方式解决内窥镜视频中手术修改的跟踪挑战，该流水线整合了帧选择、深度估计和3D重建组件，并在Hamlyn数据集上展示了其有效性。	 | Tracking surgical modifications based on endoscopic videos is technically feasible and of great clinical advantages; however, it still remains challenging. This report presents a modular pipeline to divide and conquer the clinical challenges in the process. The pipeline integrates frame selection, depth estimation, and 3D reconstruction components, allowing for flexibility and adaptability in incorporating new methods. Recent advancements, including the integration of Depth-Anything V2 and EndoDAC for depth estimation, as well as improvements in the Iterative Closest Point (ICP) alignment process, are detailed. Experiments conducted on the Hamlyn dataset demonstrate the effectiveness of the integrated methods. System capability and limitations are both discussed.	 | 基于内窥镜视频跟踪手术修改在技术上是可行的，并且在临床上有巨大的优势；然而，这仍然具有挑战性。本报告介绍了一个模块化的流水线，以分而治之的方式解决该过程中的临床挑战。该流水线整合了帧选择、深度估计和3D重建组件，允许在集成新方法时具有灵活性和适应性。最近的进展包括将Depth-Anything V2和EndoDAC集成用于深度估计，以及改进的迭代最近点（ICP）对齐过程的优化，这些进展都在文中进行了详细描述。在Hamlyn数据集上的实验展示了集成方法的有效性。系统能力和局限性均进行了讨论。
2501.01464	 | Estimation of 3T MR images from 1.5T images regularized with Physics based Constraint	 | Prabhjot Kaur,Atul Singh Minhas,Chirag Kamal Ahuja,Anil Kumar Sao	 | 本文提出了一种无监督框架，旨在提高1.5T MRI图像的质量，通过交替最小化估计未知的高场图像和线性变换，并结合物理约束建立非线性关联，从而生成类似3T图像的高质量图像，实验结果表明该方法在图像质量和组织分割、体积定量方面优于现有方法。	 | Limited accessibility to high field MRI scanners (such as 7T, 11T) has motivated the development of post-processing methods to improve low field images. Several existing post-processing methods have shown the feasibility to improve 3T images to produce 7T-like images [3,18]. It has been observed that improving lower field (LF, <=1.5T) images comes with additional challenges due to poor image quality such as the function mapping 1.5T and higher field (HF, 3T) images is more complex than the function relating 3T and 7T images [10]. Except for [10], no method has been addressed to improve <=1.5T MRI images. Further, most of the existing methods [3,18] including [10] require example images, and also often rely on pixel to pixel correspondences between LF and HF images which are usually inaccurate for <=1.5T images. The focus of this paper is to address the unsupervised framework for quality improvement of 1.5T images and avoid the expensive requirements of example images and associated image registration. The LF and HF images are assumed to be related by a linear transformation (LT). The unknown HF image and unknown LT are estimated in alternate minimization framework. Further, a physics based constraint is proposed that provides an additional non-linear function relating LF and HF images in order to achieve the desired high contrast in estimated HF image. The experimental results demonstrate that the proposed approach provides processed 1.5T images, i.e., estimated 3T-like images with improved image quality, and is comparably better than the existing methods addressing similar problems. The improvement in image quality is also shown to provide better tissue segmentation and volume quantification as compared to scanner acquired 1.5T images.	 | 有限的高场MRI扫描仪（如7T、11T）的可访问性激发了开发后处理方法以提高低场图像质量。已有的一些后处理方法表明，可以将3T图像改进至类似7T图像的效果 [3,18]。观察到，改进低于1.5T场强（LF，≤1.5T）的图像存在额外的挑战，因为这类图像的质量较差，1.5T及以上场强（HF，3T）图像的函数映射比3T和7T图像之间的函数关系更为复杂 [10]。除了[10]之外，还没有方法专门用来提高1.5T MRI图像的质量。此外，大多数现有方法 [3,18] 包括[10] 都需要示例图像，并且通常依赖于低场与高场图像之间的像素到像素对应关系，这对于低于1.5T的图像来说通常不准确。本文的重点是建立一个无监督框架来提高1.5T图像的质量，并避免使用昂贵的示例图像及其相关图像配准要求。假设低场和高场图像由线性变换（LT）相关联。未知的高场图像和未知的LT通过交替最小化框架进行估计。此外，提出了一个基于物理约束的方法，该方法提供了一个非线性函数以进一步将低场和高场图像相关联，从而实现期望的高对比度。实验结果表明，所提出的方法生成了处理后的1.5T图像，即估计的3T类似图像，并且在图像质量方面优于处理类似问题的现有方法。图像质量的改进还显示出与扫描仪获取的1.5T图像相比，能提供更好的组织分割和体积定量。
2501.01460	 | GDSR: Global-Detail Integration through Dual-Branch Network with Wavelet Losses for Remote Sensing Image Super-Resolution	 | Qiwei Zhu,Kai Li,Guojing Zhang,Xiaoying Wang,Jianqiang Huang,Xilai Li	 | 该研究提出了一个名为GSDR的新框架，结合了Receptance Weighted Key Value (RWKV) 和全局-细节双分支结构，以有效处理遥感图像超分辨中的全局和局部依赖性，并通过引入全局-细节重构模块和小波损失进一步优化性能，实验结果表明GSDR在多个基准测试中显著优于现有方法，同时在参数量和计算效率上表现更佳。	 | In recent years, deep neural networks, including Convolutional Neural Networks, Transformers, and State Space Models, have achieved significant progress in Remote Sensing Image (RSI) Super-Resolution (SR). However, existing SR methods typically overlook the complementary relationship between global and local dependencies. These methods either focus on capturing local information or prioritize global information, which results in models that are unable to effectively capture both global and local features simultaneously. Moreover, their computational cost becomes prohibitive when applied to large-scale RSIs. To address these challenges, we introduce the novel application of Receptance Weighted Key Value (RWKV) to RSI-SR, which captures long-range dependencies with linear complexity. To simultaneously model global and local features, we propose the Global-Detail dual-branch structure, GDSR, which performs SR reconstruction by paralleling RWKV and convolutional operations to handle large-scale RSIs. Furthermore, we introduce the Global-Detail Reconstruction Module (GDRM) as an intermediary between the two branches to bridge their complementary roles. In addition, we propose Wavelet Loss, a loss function that effectively captures high-frequency detail information in images, thereby enhancing the visual quality of SR, particularly in terms of detail reconstruction. Extensive experiments on several benchmarks, including AID, AID_CDM, RSSRD-QH, and RSSRD-QH_CDM, demonstrate that GSDR outperforms the state-of-the-art Transformer-based method HAT by an average of 0.05 dB in PSNR, while using only 63% of its parameters and 51% of its FLOPs, achieving an inference speed 2.9 times faster. Furthermore, the Wavelet Loss shows excellent generalization across various architectures, providing a novel perspective for RSI-SR enhancement.	 | 近年来，包括卷积神经网络（CNN）、变压器（Transformer）和状态空间模型（State Space Models）在内的深度神经网络在遥感图像（Remote Sensing Image, RSI）超分辨（Super-Resolution, SR）方面取得了显著进展。然而，现有的SR方法通常忽视了全局依赖性和局部依赖性的互补关系。这些方法要么侧重于捕捉局部信息，要么优先考虑全局信息，导致模型无法同时有效捕捉全局和局部特征。此外，当应用于大规模RSI时，它们的计算成本变得非常高。为了解决这些挑战，我们提出将Receptance Weighted Key Value (RWKV) 应用于RSI-SR，以线性复杂度捕捉长距离依赖性。为了同时建模全局和局部特征，我们提出了全局-细节双分支结构（Global-Detail Dual-Branch Structure, GDSR），通过并行RWKV和卷积操作进行SR重构，以处理大规模RSI。此外，我们引入了全局-细节重构模块（Global-Detail Reconstruction Module, GDRM）作为两个分支之间的中介，以实现它们互补作用的桥梁。我们还提出了一种小波损失（Wavelet Loss），这种损失函数能够有效捕捉图像中的高频细节信息，从而在细节重建方面增强SR的视觉质量。在多个基准测试，包括AID、AID_CDM、RSSRD-QH和RSSRD-QH_CDM上进行的广泛实验表明，GSDR在峰值信噪比（PSNR）方面平均比基于Transformer的最新方法HAT高出0.05 dB，同时仅使用其63%的参数和51%的FLOPs，实现推理速度提高2.9倍。此外，小波损失在各种架构中具有出色的泛化能力，为RSI-SR增强提供了一个新的视角。
2501.01456	 | SS-CTML: Self-Supervised Cross-Task Mutual Learning for CT Image Reconstruction	 | Gaofeng Chen,Yaoduo Zhang,Li Huang,Pengfei Wang,Wenyu Zhang,Dong Zeng,Jianhua Ma,Ji He	 | 本文提出了一种自我监督跨任务互学习（SS-CTML）框架，用于CT图像重建，通过将全视角、稀视角和有限视角的重建任务进行互学习来优化神经网络，最终在临床数据集上取得了良好的定量和定性重建效果。	 | Supervised deep-learning (SDL) techniques with paired training datasets have been widely studied for X-ray computed tomography (CT) image reconstruction. However, due to the difficulties of obtaining paired training datasets in clinical routine, the SDL methods are still away from common uses in clinical practices. In recent years, self-supervised deep-learning (SSDL) techniques have shown great potential for the studies of CT image reconstruction. In this work, we propose a self-supervised cross-task mutual learning (SS-CTML) framework for CT image reconstruction. Specifically, a sparse-view scanned and a limited-view scanned sinogram data are first extracted from a full-view scanned sinogram data, which results in three individual reconstruction tasks, i.e., the full-view CT (FVCT) reconstruction, the sparse-view CT (SVCT) reconstruction, and limited-view CT (LVCT) reconstruction. Then, three neural networks are constructed for the three reconstruction tasks. Considering that the ultimate goals of the three tasks are all to reconstruct high-quality CT images, we therefore construct a set of cross-task mutual learning objectives for the three tasks, in which way, the three neural networks can be self-supervised optimized by learning from each other. Clinical datasets are adopted to evaluate the effectiveness of the proposed framework. Experimental results demonstrate that the SS-CTML framework can obtain promising CT image reconstruction performance in terms of both quantitative and qualitative measurements.	 | 监督深度学习（SDL）技术已经广泛用于X射线计算机断层扫描（CT）图像重建，尤其是在使用配对训练数据集的情况下。然而，由于在临床实践中获得配对训练数据集存在困难，SDL方法尚未在临床实践中得到广泛应用。近年来，自我监督深度学习（SSDL）技术在CT图像重建的研究中展现出了巨大的潜力。在本文中，我们提出了一种自我监督跨任务互学习（SS-CTML）框架，用于CT图像重建。具体来说，我们首先从全视角扫描的投影数据中提取稀视角扫描和有限视角扫描的投影数据，从而生成三个独立的重建任务：全视角CT（FVCT）重建、稀视角CT（SVCT）重建和有限视角CT（LVCT）重建。然后，我们为这三个重建任务构建了三个神经网络。考虑到这三个任务最终目标都是重建高质量的CT图像，我们为此构建了一组跨任务互学习目标，使得三个神经网络可以通过互相学习来进行自我监督优化。临床数据集被用于评估提出的框架的有效性。实验结果表明，SS-CTML框架在定量和定性测量方面都能获得令人满意的CT图像重建性能。
2501.01450	 | Real-Time Computational Visual Aberration Correcting Display Through High-Contrast Inverse Blurring	 | Akhilesh Balaji,Dhruv Ramu	 | 本文提出了一种实时视力矫正显示器框架，通过去卷积处理显示图像以矫正角膜折射视觉缺陷，并使用遮罩技术和球坐标调整点扩散函数，从而提供一致的视觉矫正效果。结果显示，该显示器在多种视角下都能显著提高视觉清晰度，SSIM值达到83.04%。	 | This paper presents a framework for developing a live vision-correcting display (VCD) to address refractive visual aberrations without the need for traditional vision correction devices like glasses or contact lenses, particularly in scenarios where wearing them may be inconvenient. We achieve this correction through deconvolution of the displayed image using a point spread function (PSF) associated with the viewer's eye. We address ringing artefacts using a masking technique applied to the prefiltered image. We also enhance the display's contrast and reduce color distortion by operating in the YUV/YCbCr color space, where deconvolution is performed solely on the luma (brightness) channel. Finally, we introduce a technique to calculate a real-time PSF that adapts based on the viewer's spherical coordinates relative to the screen. This ensures that the PSF remains accurate and undistorted even when the viewer observes the display from an angle relative to the screen normal, thereby providing consistent visual correction regardless of the viewing angle. The results of our display demonstrate significant improvements in visual clarity, achieving a structural similarity index (SSIM) of 83.04%, highlighting the effectiveness of our approach.	 | 本文提出了一种框架，用于开发一种实时视力矫正显示器（VCD），以解决角膜折射视觉缺陷，而无需传统的视力矫正设备如眼镜或隐形眼镜，特别是在佩戴这些设备可能不方便的情况下。我们通过使用与观者眼睛相关的点扩散函数（PSF）对显示图像进行去卷积来实现这种矫正。我们使用遮罩技术对预滤波图像进行处理来解决振铃伪象问题。我们还在YUV/YCbCr色彩空间中操作，仅在亮度（luma）通道上进行去卷积，从而增强显示对比度并减少色彩失真。最后，我们提出了一种计算实时PSF的技术，该技术根据观者相对于屏幕的球坐标进行调整。这确保了在观者从屏幕法线方向观察显示器时，PSF保持准确且未失真，从而无论观者视角如何，都能提供一致的视觉矫正效果。我们的显示器显示结果证明了视觉清晰度的显著改善，SSIM达到83.04%，突显了我们方法的有效性。
